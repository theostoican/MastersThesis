{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: jax[cpu] in ./.local/lib/python3.6/site-packages (0.2.17)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from jax[cpu]) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in ./.local/lib/python3.6/site-packages (from jax[cpu]) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /opt/conda/lib/python3.6/site-packages (from jax[cpu]) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: jaxlib==0.1.68; extra == \"cpu\" in ./.local/lib/python3.6/site-packages (from jax[cpu]) (0.1.68)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from absl-py->jax[cpu]) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from jaxlib==0.1.68; extra == \"cpu\"->jax[cpu]) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers<3.0,>=1.12 in ./.local/lib/python3.6/site-packages (from jaxlib==0.1.68; extra == \"cpu\"->jax[cpu]) (2.0)\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import nlopt\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "np.set_printoptions(precision=32)\n",
    "\n",
    "!pip install --upgrade \"jax[cpu]\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import nn as jax_nn\n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "jnp.set_printoptions(precision=32) \n",
    "from jax import jacfwd, jacrev\n",
    "from jax import grad as jax_grad\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H_student, D_out = 1, 10, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1_principal</th>\n",
       "      <th>2_principal</th>\n",
       "      <th>3_principal</th>\n",
       "      <th>4_principal</th>\n",
       "      <th>5_principal</th>\n",
       "      <th>6_principal</th>\n",
       "      <th>7_principal</th>\n",
       "      <th>8_principal</th>\n",
       "      <th>9_principal</th>\n",
       "      <th>10_principal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.140478</td>\n",
       "      <td>-5.226451</td>\n",
       "      <td>3.886993</td>\n",
       "      <td>-0.901512</td>\n",
       "      <td>4.929209</td>\n",
       "      <td>2.036187</td>\n",
       "      <td>4.706960</td>\n",
       "      <td>-4.764459</td>\n",
       "      <td>0.238225</td>\n",
       "      <td>-1.459020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19.292333</td>\n",
       "      <td>6.033014</td>\n",
       "      <td>1.308196</td>\n",
       "      <td>-2.383076</td>\n",
       "      <td>3.095021</td>\n",
       "      <td>-1.794193</td>\n",
       "      <td>-3.770784</td>\n",
       "      <td>0.148453</td>\n",
       "      <td>-4.154969</td>\n",
       "      <td>-4.295380</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-7.644504</td>\n",
       "      <td>-1.705801</td>\n",
       "      <td>2.289336</td>\n",
       "      <td>2.241256</td>\n",
       "      <td>5.094750</td>\n",
       "      <td>-4.152694</td>\n",
       "      <td>-1.011677</td>\n",
       "      <td>1.733929</td>\n",
       "      <td>0.422061</td>\n",
       "      <td>-0.072606</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.474206</td>\n",
       "      <td>5.836146</td>\n",
       "      <td>2.008588</td>\n",
       "      <td>4.271250</td>\n",
       "      <td>2.378019</td>\n",
       "      <td>2.179969</td>\n",
       "      <td>4.397159</td>\n",
       "      <td>-0.346711</td>\n",
       "      <td>1.018367</td>\n",
       "      <td>5.470587</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26.559575</td>\n",
       "      <td>6.024832</td>\n",
       "      <td>0.933257</td>\n",
       "      <td>-3.012613</td>\n",
       "      <td>9.488500</td>\n",
       "      <td>-2.333748</td>\n",
       "      <td>-6.146737</td>\n",
       "      <td>-1.796978</td>\n",
       "      <td>-4.180035</td>\n",
       "      <td>-5.717939</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  1_principal  2_principal  3_principal  4_principal  \\\n",
       "0           0    -5.140478    -5.226451     3.886993    -0.901512   \n",
       "1           1    19.292333     6.033014     1.308196    -2.383076   \n",
       "2           2    -7.644504    -1.705801     2.289336     2.241256   \n",
       "3           3    -0.474206     5.836146     2.008588     4.271250   \n",
       "4           4    26.559575     6.024832     0.933257    -3.012613   \n",
       "\n",
       "   5_principal  6_principal  7_principal  8_principal  9_principal  \\\n",
       "0     4.929209     2.036187     4.706960    -4.764459     0.238225   \n",
       "1     3.095021    -1.794193    -3.770784     0.148453    -4.154969   \n",
       "2     5.094750    -4.152694    -1.011677     1.733929     0.422061   \n",
       "3     2.378019     2.179969     4.397159    -0.346711     1.018367   \n",
       "4     9.488500    -2.333748    -6.146737    -1.796978    -4.180035   \n",
       "\n",
       "   10_principal  label  \n",
       "0     -1.459020    1.0  \n",
       "1     -4.295380   -1.0  \n",
       "2     -0.072606    1.0  \n",
       "3      5.470587   -1.0  \n",
       "4     -5.717939   -1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mnist/train_10pca.csv', float_precision='round_trip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A customizable student network, initialized using Glorot initialization.\n",
    "class StudentNetwork(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "\n",
    "    D_in: input dimension\n",
    "    H: dimension of hidden layer\n",
    "    D_out: output dimension of the first layer\n",
    "    \"\"\"\n",
    "    super(StudentNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H, bias=True).double()\n",
    "    self.linear2 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear3 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear4 = nn.Linear(H, D_out, bias=True).double()\n",
    "\n",
    "    nn.init.xavier_uniform_(self.linear1.weight)\n",
    "    nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    nn.init.xavier_uniform_(self.linear3.weight)\n",
    "    nn.init.xavier_uniform_(self.linear4.weight)\n",
    "    \n",
    "    nn.init.constant_(self.linear1.bias, 0)\n",
    "    nn.init.constant_(self.linear2.bias, 0)\n",
    "    nn.init.constant_(self.linear3.bias, 0)\n",
    "    nn.init.constant_(self.linear4.bias, 0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = torch.sigmoid(self.linear1(x))\n",
    "    h2 = torch.sigmoid(self.linear2(h1))\n",
    "    h3 = torch.sigmoid(self.linear3(h2))\n",
    "    y_pred = self.linear4(h3)\n",
    "    return h2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = []\n",
    "dataset_labels = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    pca_components = []\n",
    "    for idx_pca_component in range(1, 11):\n",
    "        pca_components.append(row[str(idx_pca_component) + '_principal'])\n",
    "    dataset_inputs.append(pca_components)\n",
    "    dataset_labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_inputs = torch.DoubleTensor(dataset_inputs[:10000]).to(device)\n",
    "torch_dataset_labels = torch.DoubleTensor([dataset_labels[:10000]]).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "jnp_dataset_inputs = jnp.array(dataset_inputs[:10000], dtype=jnp.float64)\n",
    "jnp_dataset_labels = jnp.array(dataset_labels[:10000], dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model = StudentNetwork(D_in, H_student, D_out)\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)\n",
    "\n",
    "checkpoint = torch.load(\"model_1e-6.pt\")\n",
    "student_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y = student_model(torch_dataset_inputs)\n",
    "loss = nn.MSELoss()(y, torch_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10269205534607684\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = torch.autograd.grad(loss, student_model.parameters(),\n",
    "                                      retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.950165295930524e-06\n"
     ]
    }
   ],
   "source": [
    "def eval_grad_norm(loss_grad):\n",
    "  cnt = 0\n",
    "  for g in loss_grad:\n",
    "      if cnt == 0:\n",
    "        g_vector = g.contiguous().view(-1)\n",
    "      else:\n",
    "        g_vector = torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "      cnt = 1\n",
    "  grad_norm = torch.norm(g_vector)\n",
    " \n",
    "  return grad_norm.cpu().detach().numpy()\n",
    "\n",
    "print(eval_grad_norm(loss_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(student_model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "weights = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape(H_student * D_in), \n",
    "                  trace[-1][5].reshape(H_student)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1026920579402963 9.950165264252857e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([-5.3945191734651505e-09,  7.4704447537238577e-10,\n",
       "              3.0471841279103927e-09,  3.6846601868945834e-09,\n",
       "              5.0230852672211108e-09,  1.6881852990028928e-08,\n",
       "              3.6429759954769452e-08,  6.4832805391893137e-08,\n",
       "              2.0617937002064219e-07,  3.2936145209763950e-07,\n",
       "              5.0340126290094939e-07,  8.1808164857814475e-07],            dtype=float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(w_layer1, b_layer1,\n",
    "            w_layer2, b_layer2,\n",
    "            w_layer3, b_layer3,\n",
    "            w_out, b_out):\n",
    "  h1 = jax_nn.sigmoid(jnp_dataset_inputs @ jnp.transpose(w_layer1) + b_layer1)\n",
    "  h2 = jax_nn.sigmoid(h1 @ jnp.transpose(w_layer2) + b_layer2)\n",
    "  h3 = jax_nn.sigmoid(h2 @ jnp.transpose(w_layer3) + b_layer3)\n",
    "\n",
    "  return jnp.transpose((h3 @ w_out + b_out).T)\n",
    "\n",
    "def jax_loss(w):\n",
    "  w_layer1 = w[0 : 100].reshape(H_student, D_in)\n",
    "  b_layer1 = w[100 : 110].reshape(H_student)\n",
    "  w_layer2 = w[110 : 210].reshape(H_student, H_student)\n",
    "  b_layer2 = w[210 : 220].reshape(H_student)\n",
    "  if len(w) == 341:\n",
    "    w_layer3 = w[220 : 320].reshape(H_student, H_student)\n",
    "    b_layer3 = w[320 : 330].reshape(H_student)\n",
    "    w_out = w[330 : 340].reshape(H_student, D_out)\n",
    "    b_out = w[340]\n",
    "  else:\n",
    "    w_layer3 = w[220 : 330].reshape(H_student + 1, H_student)\n",
    "    b_layer3 = w[330 : 341].reshape(H_student + 1)\n",
    "    w_out = w[341 : 352].reshape(H_student + 1, D_out)\n",
    "    b_out = w[352]\n",
    "\n",
    "  preds = jnp.transpose(predict(w_layer1, b_layer1,\n",
    "                                w_layer2, b_layer2,\n",
    "                                w_layer3, b_layer3,\n",
    "                                w_out, b_out))\n",
    "  return jnp.mean(jnp.square(preds - jnp_dataset_labels))\n",
    "\n",
    "def hessian(f):\n",
    "  return jacfwd(jacrev(f))\n",
    "\n",
    "print(jax_loss(weights), jnp.linalg.norm(jax_grad(jax_loss)(weights)))\n",
    "H = hessian(jax_loss)(weights)\n",
    "H = (H + H.T) / 2.0\n",
    "jnp.linalg.eigh(H)[0][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_obj(weights, grad):\n",
    "  loss_val = jax_loss(weights)\n",
    "  if grad.size > 0:\n",
    "    grad[:] = np.array(jax_grad(jax_loss)(weights), dtype=np.float64)\n",
    "  return np.float64(loss_val)\n",
    "\n",
    "def second_order_opt(weights, maxtime):\n",
    "  opt = nlopt.opt(nlopt.LD_SLSQP, len(weights))\n",
    "  opt.set_lower_bounds([w - 1000 for w in weights])\n",
    "  opt.set_upper_bounds([w + 1000 for w in weights])\n",
    "  opt.set_min_objective(loss_obj)\n",
    "  opt.set_maxtime(maxtime)\n",
    "  # opt.set_xtol_rel(1e-32)\n",
    "  opt.set_initial_step(1e-32)\n",
    "  final_weights = opt.optimize(weights)\n",
    "  return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_weights = second_order_opt(weights, 10)\n",
    "# print(jax_loss(final_weights), jnp.linalg.norm(jax_grad(jax_loss)(final_weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Y matrix for the first neuron on the third layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "first_derivative_sigmoid = lambda x : jax_nn.sigmoid(x) * \\\n",
    "                                          (1 - jax_nn.sigmoid(x))\n",
    "second_derivative_sigmoid = lambda x : jax_nn.sigmoid(x) * \\\n",
    "                                           ((1 - jax_nn.sigmoid(x)) ** 2)-\\\n",
    "                                           (jax_nn.sigmoid(x) ** 2) *\\\n",
    "                                           (1 - jax_nn.sigmoid(x))\n",
    "\n",
    "w_layer1 = weights[0 : 100].reshape(H_student, D_in)\n",
    "b_layer1 = weights[100 : 110].reshape(H_student)\n",
    "w_layer2 = weights[110 : 210].reshape(H_student, H_student)\n",
    "b_layer2 = weights[210 : 220].reshape(H_student)\n",
    "w_layer3 = weights[220 : 320].reshape(H_student, H_student)\n",
    "b_layer3 = weights[320 : 330].reshape(H_student)\n",
    "w_out = weights[330 : 340].reshape(H_student, D_out)\n",
    "b_out = weights[340]\n",
    "\n",
    "preds = predict(w_layer1, b_layer1,\n",
    "                w_layer2, b_layer2,\n",
    "                                w_layer3, b_layer3,\n",
    "                                w_out, b_out).reshape(10000)\n",
    "\n",
    "second_layer_output, last_layer_output = student_model(torch_dataset_inputs)\n",
    "second_layer_output = second_layer_output.cpu().detach().numpy()\n",
    "# print(y_model.cpu().detach().numpy().reshape(10000).shape, preds.shape)\n",
    "\n",
    "\n",
    "e = preds - jnp_dataset_labels\n",
    "\n",
    "print(e.shape)\n",
    "\n",
    "Y = jnp.zeros((11, 11))\n",
    "idx_neuron = 1\n",
    "\n",
    "for idx, x in enumerate(second_layer_output):\n",
    "    Y += w_out[idx_neuron][0] * second_derivative_sigmoid(jnp.dot(x, w_layer3[idx_neuron]) + b_layer3[idx_neuron]) * e[idx] * \\\n",
    "         jnp.array(np.append(x, 1)).reshape(len(x) + 1, 1) @ jnp.array(np.append(x, 1)).reshape(1, len(x) + 1)\n",
    "\n",
    "Y /= len(jnp_dataset_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.080671345101185e-05\n",
      "-1.3946583078264177e-05\n",
      "-8.259918828292368e-06\n",
      "-3.2219744215370507e-06\n",
      "-1.1884682267389679e-07\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'eigenvalue')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evals, _ = jnp.linalg.eigh(Y)\n",
    "for eval in evals:\n",
    "    if eval <= 0:\n",
    "        print(eval)\n",
    "print(len(evals))\n",
    "plt.scatter(np.arange(len(evals)), evals)\n",
    "plt.xlabel('eigenvalue index')\n",
    "plt.ylabel('eigenvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w_layer3 = np.append(np.append(weights[220 : 220 + idx_neuron * 10],\n",
    "                                   np.append(\n",
    "                                       weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10],\n",
    "                                       weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10])),\n",
    "                         weights[220 + (idx_neuron + 1) * 10 : 320]).reshape(H_student + 1, H_student)\n",
    "new_b_layer3 = np.append(np.append(weights[320 : 320 + idx_neuron],\n",
    "                                   np.append([weights[320 + idx_neuron]],\n",
    "                                            [weights[320 + idx_neuron]])),\n",
    "                         weights[320 + idx_neuron + 1 : 330]).reshape(H_student + 1)\n",
    "new_w_out = np.append(np.append(weights[330: 330 + idx_neuron],\n",
    "                                np.append([weights[330 + idx_neuron] * 0],\n",
    "                                          [weights[330 + idx_neuron] * 1])),\n",
    "                      weights[330 + idx_neuron + 1 : 340]).reshape(H_student + 1, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_weights = np.append(new_weights, weights[0 : 100])\n",
    "new_weights = np.append(new_weights, weights[100 : 110])\n",
    "\n",
    "new_weights = np.append(new_weights, weights[110 : 210])\n",
    "new_weights = np.append(new_weights, weights[210 : 220])\n",
    "\n",
    "new_weights = np.append(new_weights, new_w_layer3)\n",
    "new_weights = np.append(new_weights, new_b_layer3)\n",
    "\n",
    "new_weights = np.append(new_weights, new_w_out)\n",
    "new_weights = np.append(new_weights, [weights[340]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10269205794029619\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(jax_loss(new_weights))\n",
    "\n",
    "H = hessian(jax_loss)(new_weights)\n",
    "H = (H + H.T) / 2.0\n",
    "\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "cnt = 0\n",
    "\n",
    "for eval in evals:\n",
    "    if abs(eval) <= 1e-9:\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyNetwork(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out,\n",
    "               w_layer1, b_layer1,\n",
    "               w_layer2, b_layer2,\n",
    "               w_layer3, b_layer3,\n",
    "               w_out, b_out):\n",
    "    super(DummyNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H, bias=True).double()\n",
    "    self.linear2 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear3 = nn.Linear(H, H + 1, bias=True).double()\n",
    "    self.linear4 = nn.Linear(H + 1, D_out, bias=True).double()\n",
    "    \n",
    "    self.linear1.weight = torch.nn.Parameter(w_layer1)\n",
    "    self.linear2.weight = torch.nn.Parameter(w_layer2)\n",
    "    self.linear3.weight = torch.nn.Parameter(w_layer3)\n",
    "    self.linear4.weight = torch.nn.Parameter(w_out)\n",
    "    \n",
    "    self.linear1.bias = torch.nn.Parameter(b_layer1)\n",
    "    self.linear2.bias = torch.nn.Parameter(b_layer2)\n",
    "    self.linear3.bias = torch.nn.Parameter(b_layer3)\n",
    "    self.linear4.bias = torch.nn.Parameter(b_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = torch.sigmoid(self.linear1(x))\n",
    "    h2 = torch.sigmoid(self.linear2(h1))\n",
    "    h3 = torch.sigmoid(self.linear3(h2))\n",
    "    y_pred = self.linear4(h3)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_network = DummyNetwork(D_in, H_student, D_out, torch.DoubleTensor(w_layer1), torch.DoubleTensor(b_layer1),\n",
    "                            torch.DoubleTensor(w_layer2), torch.DoubleTensor(b_layer2),\n",
    "                            torch.DoubleTensor(new_w_layer3), torch.DoubleTensor(new_b_layer3),\n",
    "                            torch.DoubleTensor(new_w_out.T), torch.DoubleTensor([b_out]).reshape(1, 1))\n",
    "dummy_network = dummy_network.to(device)\n",
    "if device == 'cuda':\n",
    "    dummy_network = torch.nn.DataParallel(dummy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_labels, N = 2 * (10 ** 4), Ninner = 10 ** 3, Nstart = 10,\n",
    "          maxtime = 7, nlopt_threshold = 1e-7,\n",
    "          collect_history = True):\n",
    "  lr = 1e-4\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#   checkpoint = torch.load(\"local_min_model.pt\")\n",
    "#   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  loss_vals = []\n",
    "  trace = []\n",
    "  if collect_history:\n",
    "    trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "  for i in range(1, N + 1):\n",
    "#     if i % 2 == 0:\n",
    "#       optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "#     elif i % 2 == 1:\n",
    "#       optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "#     else:\n",
    "#       optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "    loss_tmp = []\n",
    "    for j in range(1, Ninner + 1):\n",
    "      y = model(x)\n",
    "      loss = loss_fn(y, y_labels)\n",
    "      loss_grad = torch.autograd.grad(loss, model.parameters(),\n",
    "                                      retain_graph=True)\n",
    "      grad_norm = eval_grad_norm(loss_grad)\n",
    "      if grad_norm <= 1e-5 and i > 1:\n",
    "#         print('found it')\n",
    "#         return loss_vals, trace\n",
    "        trace = []\n",
    "        trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "        new_weights_end = np.append(\n",
    "            np.append(\n",
    "                np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                          trace[-1][1].reshape(H_student)),\n",
    "                np.append(\n",
    "                    np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                              trace[-1][3].reshape(H_student)),\n",
    "                    np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                              trace[-1][5].reshape(H_student + 1)))),\n",
    "            np.append(trace[-1][6][0],\n",
    "                      trace[-1][7][0]))\n",
    "        \n",
    "        H = hessian(jax_loss)(new_weights_end)\n",
    "        H = (H + H.T) / 2.0\n",
    "        evals, _ = jnp.linalg.eigh(H)\n",
    "#         print(evals)\n",
    "        \n",
    "        proper_local_min = True\n",
    "        \n",
    "        for eval in evals:\n",
    "          if eval <= -1e-9:\n",
    "            proper_local_min = False\n",
    "            break\n",
    "                \n",
    "        if proper_local_min:\n",
    "          print('found it' + str(j))\n",
    "          EPOCH = 0\n",
    "          PATH = \"local_min_model.pt\"\n",
    "          LOSS = 0.4\n",
    "\n",
    "          torch.save({\n",
    "                    'epoch': EPOCH,\n",
    "                    'model_state_dict': student_model.state_dict(),\n",
    "                    'loss': LOSS,\n",
    "                    }, PATH)\n",
    "          return loss_vals, trace\n",
    "      loss_tmp.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward(retain_graph=True)\n",
    "      optimizer.step()\n",
    "      if i == 1 and (j % Nstart == 0) and j < Ninner:\n",
    "        loss_vals.append(np.mean(loss_tmp[j - Nstart  : j]))\n",
    "        if collect_history:\n",
    "          trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    loss_vals.append(np.mean(loss_tmp))\n",
    "    if collect_history:\n",
    "      trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    grad_norm = eval_grad_norm(loss_grad)\n",
    "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i,\n",
    "                                                          np.mean(loss_tmp),\n",
    "                                                          grad_norm))\n",
    "\n",
    "#   EPOCH = i\n",
    "#   PATH = \"model.pt\"\n",
    "#   LOSS = 0.4\n",
    "\n",
    "#   torch.save({\n",
    "#             'epoch': EPOCH,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': LOSS,\n",
    "#             }, PATH)\n",
    "  return loss_vals, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the 12 smallest eigenvaleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evals = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_mu = [[] for i in range(num_evals)]\n",
    "\n",
    "for mu in np.arange(-1, 2, 0.1):\n",
    "#     new_w_layer3 = np.append(weights[220 : 230], weights[220 : 320]).reshape(H_student + 1, H_student)\n",
    "#     new_b_layer3 = np.append([weights[320]], weights[320 : 330]).reshape(H_student + 1)\n",
    "#     new_w_out = np.append([mu * weights[330], (1 - mu) * weights[330]], weights[331 : 340]).reshape(H_student + 1, D_out)\n",
    "    \n",
    "    new_w_layer3 = np.append(np.append(weights[220 : 220 + idx_neuron * 10],\n",
    "                                       np.append(\n",
    "                                           weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10],\n",
    "                                           weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10])),\n",
    "                             weights[220 + (idx_neuron + 1) * 10 : 320]).reshape(H_student + 1, H_student)\n",
    "    new_b_layer3 = np.append(np.append(weights[320 : 320 + idx_neuron],\n",
    "                                       np.append([weights[320 + idx_neuron]],\n",
    "                                                [weights[320 + idx_neuron]])),\n",
    "                             weights[320 + idx_neuron + 1 : 330]).reshape(H_student + 1)\n",
    "    new_w_out = np.append(np.append(weights[330: 330 + idx_neuron],\n",
    "                                    np.append([mu * weights[330 + idx_neuron]],\n",
    "                                              [(1 - mu) * weights[330 + idx_neuron]])),\n",
    "                      weights[330 + idx_neuron + 1 : 340]).reshape(H_student + 1, D_out)    \n",
    "    \n",
    "    \n",
    "    new_weights = []\n",
    "    new_weights = np.append(new_weights, weights[0 : 100])\n",
    "    new_weights = np.append(new_weights, weights[100 : 110])\n",
    "\n",
    "    new_weights = np.append(new_weights, weights[110 : 210])\n",
    "    new_weights = np.append(new_weights, weights[210 : 220])\n",
    "\n",
    "    new_weights = np.append(new_weights, new_w_layer3)\n",
    "    new_weights = np.append(new_weights, new_b_layer3)\n",
    "\n",
    "    new_weights = np.append(new_weights, new_w_out)\n",
    "    new_weights = np.append(new_weights, [weights[340]])\n",
    "    \n",
    "    H = hessian(jax_loss)(new_weights)\n",
    "    H = (H + H.T) / 2.0\n",
    "    \n",
    "    evals, _ = jnp.linalg.eigh(H)\n",
    "    for i in range(num_evals):\n",
    "        evals_mu[i].append(evals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEICAYAAABiXeIWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydeXhVxfn4P3Pvzb3ZExISAlkJhDVAICEsIgKKIi4URRHFFdytUttvtdrWn9Za29rWDbe27loUaxV3AUFlTyAJeyAs2YAQErLn7vP7Y25CCAnZbnJv8HyeZ545M2fOnPdmTs57ZntfIaVEQ0NDQ0PD3eg8LYCGhoaGxrmJpmA0NDQ0NLoFTcFoaGhoaHQLmoLR0NDQ0OgWNAWjoaGhodEtGDwtgKfp27evTEhI8LQYGhoaGr2GrVu3npBSRrRV7ievYBISEsjMzPS0GBrdQWGhimNjPSuHRutobdQrEULkt6fcT17BaJzD3Hijiteu9agYGmdBa6NzGk3BaJy7/Pa3npZAoy20Njqn0RSMxrnLRRd5WgKNttDa6JxGUzAa5y4HD6o4MdGzcmi0zjnQRjabjaKiIsxms6dFcTu+vr7ExMTg4+PTqes1BaNx7nLbbSrWxve9l3OgjYqKiggKCiIhIQEhhKfFcRtSSsrKyigqKmLgwIGdqsMt+2CEELOEELlCiDwhxMMtnDcJIT5wnd8shEhocu43rvxcIcQlbdUphBjoqiPPVaexrXto/ER5/HEVNLyXc6CNzGYz4eHh55RyARBCEB4e3qWeWZcVjBBCDywFLgVGAAuEECOaFVsEnJRSDgb+AfzZde0I4DpgJDALeEkIoW+jzj8D/3DVddJVd6v30PgJc8EFKmh4L+dIG51ryqWBrv4udwyRpQN5UsqDLoGWAXOA3U3KzAH+n+v4I+BFoSSfAyyTUlqAQ0KIPFd9tFSnEGIPMAO43lXmLVe9L7d2D9mWP4LcXHjzTbjlFrDZYOZMWLwYFi6EujqYPRvuvhvmz4fKSpgzB+6/nzfzPsavpo4L3l3H7vOHUTQiGt/qeqa+v4Gd00ZwZGh//CtqmfLBJnbMGMnRpCgCy2qY/NFmcmaOoiQxkuDSKiZ+nEHWrDGUxvcl9FgF6Z9uZevsFMpiw+lz5CTjP9tGxhXjODmgD+GFZaR+mc2WOalURIUSkX+CsV/nsOmq8VRFBNPv4HHGrNzBhnkTqAkPpP/+Y4z6bhfr5k+kLjSAAblHSV67mx+un4w5yI+Y3cWM+HEv3y+cgiXARNzOQoat38eam8/H5mskIaeAIZv2s/rWC3AYDQzMOkzSlgOsXDwdqdcxKPMgg7Ye4ts7LwQgacsBErYXsHLxdACGbtxPzJ4jrL5NvUCGrcul/4ES1tw8FYARP+wlIv8E3984BYDktbvpc6SCH6+fDMCo1TsJKa1m3XWTABjz7Q4CKuvYcM0EAMZ+nYOpzsKmueMBSP0yG73NweY5qUhgyoebAFh37UQAJny6FaePnq2zUwCY+HEGlgATWbNGAzB5+WZqQ/zJuXgUAFOWbaIyIogdF44E4Pz3N3ByQB92ThsOwAXvrKc0PpzdU4cBMP2tHzk6qB97pwwB4MLXv6do+AByJyUBMPNfazk8Opb96YMAuPjV7ziQOpADaYkIh5OZ/1rD/vRBHBqbgN5q58I3vmffxCQOj4nDx2xl+ls/sve8IRQkx2KqtXTg2evneva2kDMzucmzl0nWrNFNnr1tbJ09psmzl0XGFWObPHs5bJkzrsmzt51NV6U1efZ2smFeepNnbzfrrp1IXai/eva+38P3CyZTH+RLzJ5ikn/MJXPWaKx+RkJLqhixYR/f3XQ+dl8fErYXMGRTHqtvnYrD6OO9z95V6Qy/4G7q9zpASmpDAwAIqKwDoDbEX6UrakGIxnRgRS1OnaAu2JU+WYtTr6Mu2A+AoJM1OAx66oJc6fIa7D566pumjQbqA30BCC6rxmbyOS1t9fXBHOBKn6jG6mfEHGACIOREFWZ/X4IGDQOnE/bvh759ITwcHA7Iy4OICAgLU+enTYP774erroITJ2gv7lAw0UBhk3QRMKG1MlJKuxCiEgh35W9qdm2067ilOsOBCimlvYXyrd3jjL+GEOIO4A6A0SZTe3/naVxT+y1+9WaEs45Iy1ao2w71Tlc6A+p8mqUNYFbpmebNKl3vQDjrucS80XVepS81bzgtfZl5PdTpwWxHOM1cbl53WvrK+h9OS//M/D3U6cCi0lfVrwWjDiw2hNPCvPrvQH8qfU39KhCn0vPrVoFTgNWKcFpZUP8t2AVYVPqGuq9Bf+r8wrqv1B/FakU4mqYtCIf9VNrWLN38vNWMcDiJbyyv0gmutLCZwe4ksa78tPTg+jJ0QoLdDHbJMPNxdX1eNQCDzSdV2l4PQjC8/pi63lEHNh0j646otF2lR9UVN56XNh1j6gob0/HW46TU5bvStcRaTzCu7nBjOtpWTlrdwcZ0f2sF4+sOuNI19LNWMaFuv0o7a4i01jCpLhccEuGsJdJSw3l1e8DmSluzmFK3C8wN57OgbmeTZ6ujz96WZs/epmbP3sZmz96GZs/e+mbP3o/Nnr0fTn/2zGuhXgdWGzqnhfmW1eCj0jitXP75GtAJmGAEp5XrzSsBgdNqcz17K7372asrY6+8BV9pAQm+TterSToB8HXaWk9L8DsjbW2Stp1K48Qobfg3Oa/SlsbzPtJ+Ki1VOqDxvAMf6SDA6Rrukg78ZfcvShBddTgmhJgHzJJSLnalbwQmSCnva1Jmp6tMkSt9AKUw/h+wSUr5riv/34Crhc+ss0n5wa78WOArKWVya/eQUp5V3aalpcnO7OR/+e6PsdudbRcUIAAhXMdCIED9U7nyG7uhAnSN+a5yQriuUzqg4ZyqQjQ5LxrrOnWvJvdsyEcipAQpEUiQTlcswelQaekEpxMhVRqnE+F0gNOBcDqQDhXjsKtgtzc5toHNDg6bOrbbEEhVp3QipDoW0qlkwXXsytfpdOiMBnRGHxVMRnQmH3RGIzqTEb2vEZ3RhNnHznFZQbH9OIesxZw01FJndBKkt5Mka+nn359d2UlYMTA4zcgg01HKyoo5WVNNrdNAjdOA2eEDdgMmuw5fqw6TTRLgMBDiMBEkffCz6/C1gY/Fic5sBbMZaXe07wExGMDPF53BB2EyIYw+CKMJ4eOD8DUhjEYVfHwQPkbQG0CvQwhdk1ggdAYVCz1CrwMB0gFIB9LhbIyl0wEOCU7XsdMJdjvSasFptYLNqmKzFafNClYbTqsFabXitFgR7RxnF0KAyRf8fLAZDViNgnofSZ3OTpXeRoXOgsVHYjGC2eDAZnBi0tvx09kI1NsIMRkI6xOBT3A8Hxzqg/9hO+N9tyITj5Hr0FMkAzBawd9mINYQRbw+iihdGKEEgtmGw2rBabHhNLtimxWHxYbTakNabThtdqTQI4VAIpBCBw2x0LnydUiha8zD4AM+RtVmBgPofU7Fer061hmQej1Cp0fq9EidDnR6EDrirhxGYnzDKjiBbPI/eurteuZQk2w1oTJafTXLFooDzS+QZxycniF0OiJiQ1u5ySn27NnD8OHDT8sTQmyVUqa1da07ejDFQFM7DzGuvJbKFAkhDEAIUNbGtS3llwGhQgiDqxfTtHxr9+gWFj87B4lE2O3gdCDtdvXidb1opd2uupp2dSztdqTNhrTZkXYb0mY7da5ZfvNA8zxrw7F6aUir9VReY9p6Wlq9ZGxd+9F6PTqTSb0w/XzR+fqh8/VF+PqqOKghbULnG4LOzxdh8kXn74fw80Pn54/O3x+dvx86v6Z5Kq3z80MYjWfcVkpJQXUBW45tIeNoBhklazlRr74b+vv3I10XxJyifNKryolKvAjn1F9z/ntVJEzzJyU2lL+uOcDjV47k5skJYDNDyS44sg2Kt1F9ZCtHKg5R7KOn0GCgMLgf2/2DKNRJjlgrsctTCsWoMxLn15+Bhihi9H2JFmFE6ULoKwMId/pjskpkbR3OulqctbU4a+twWsxIs6UxluZqnJUnkGYzToulMcb1/OBwIJ1O9ew0/RvQykuFhg8YAXo9Qq9XsU6HMJlUezW0ickVh/gifEMbz+l8fdEFBDQG6WeiwmDlBDWUUMUxWUmRo4wCx3EOWY5QZi087f59fAKJ1fkSa7UQW1XKYHMtMTY7AwwBRPRLQR+TCtGpMGAsBEVxvNrMlS+sRx8oeGBJEr/+aDv/ujSJxVWfYtn4IjnGerZEj2RLkGR51Qbs0o5BZ2BU31GMjxpPelQ64yLG4GvwPfNZcTqR9fU4G0JdPbK+znVch7OuHmd93akydfVIixmn2YI01+Osr8FprldtVmdW5SwWdY3FijSbkVbrafe0zXwR35NH2vz3Ob3RdAhdwweiCqLhK1II9QHamNckX4gzrmsot2PPHn52w/X835Il3LVITU0LIVh833188fXXREZEkJOR0aTO7p83ckcPxgDsAy5EveQzgOullLualLkXGCWlvEsIcR1wlZTyWiHESOB91LzLAGA1kIT6n2mxTiHEcuC/UsplQohXgO1Sypdau0db8ne2B5ObmoaztrbD13UYIZp86TYJBsOpr+CmX8ONaZ/GtM7oUgpGI8JkVC8Wo1F9UTdL63xdZU2+Zx4bemZVu5SSopoiMo5lKKVyLIPjdWrYq69fX8ZHjWdC+GjSj+4jJvMthLkChsyCC34N0an8sK+Um17fwpvj/ZiaFMEdWWbW5Jbyzm3pTB7c98wbWqqheCsUbIaCjVCUAdYa7MCxsDgKooZTFNqfQt8ACu21FNceobimmGpr9WnVBPgEEB0YzYDAAcQExhAVEEW4XzjhvuGNcagpFL1O366/AS5FIx2u3kpDz6SZImmM20GdrY6y+jLKzGWN8fG64xypUb+puKaY43XHkU3UmUEYiAqIIjowmhi/vsTancTWlBFbepDYo7sItLuGYSKGQdxEiJsEsenQZ+AZLzGL3cGC1zax52g1H909iWEnCrjx35vxGTOat25LB3MVbHkVNi6F+pPUDZ5J1ujL2WI7yZajW9hdvhundOKj82FMxBjSo9JJi0pjTMQYjPozP066A+l0qg8318fB/uPHGZaUpHqPUoJTuoa45Kl2dOVLVz6ufNlw7Aqyybkz8loq1+z9vTk7m4f++lfWvvdeY966zEwC/P25/dFHyfzf/xrzhcGA77Bhbf7ervRguqxgXDebDTwL6IHXpZR/FEI8AWRKKVcIIXyBd4CxQDlwXZMJ/EeB2wA7sERK+VVrdbryE4FlQBiQBSyUUlrOdo+z0VkFU7p0KTglwseg/sENrpd+87RBdbGVUmhQDA3nVP6p84YzlYm+7ZfRucDRmqNsObalUaEcrT0KQJhvWONX6/io8ST4hCIy/gkbXwRzJQy51KVYxjXWde9721h/4ATbVv8RnRBUf72Sq17aQGmNhRX3TiEu3P/swjjsULIDCjYphVOwCWpK1Dm/PhCdBtGpVEWN5EhwJMX22saXc9NQb68/o2qd0NHH1OcMxePv449Jb8KkN2HUG9WxwYRJd3qeXqfH6rBicVhOC1aHFbPdfNq5k5aTpymTcnN5qzL18+/HgMABRAdGN4YB/v2IMdcSUXYIQ3EWFGdC6V7XRT7qb96oUCaAf9hZ/6xSSn790XaWby1i6fXjuGx0f5g2jcKTdUyd/RjrHppBdKiaxFaK5jXVzvUnYfBMmPYw1ZFD2VayrfE52Vu+F4nEpDeREpGinpX+6SSHJ+Oj79zmwI7S0gu4p2iufPLy8khNT6eitPSU8nFKDuUfZs7VV5OzZcup8kKgDwpq8x4eVzC9mc4qGI3O0zDktbVkK5nHMtlaspUjtWqIIdQUSlq/tEalMih0kOrSV5fAppcg83WwVMHQ2UqxDBh7Wt1lNRYm/mk1N05M4PdRrh7m+PEcPlHLnKXr6Rds4uN7ziPQ1IHemJRw8tAphVO0FUr3NE7c0megGgKKUYqHqNFIg4kqa9VpPYXy+vIW0629+LuCXugJMYWcpsjCfMNOU2oNcZhfGD7CABX5qidXtFXFR3PUwggA/3ClWGPTlUKJHgc+fh2S6fV1h3ji893cP2MwD148VGVmZFBSaWbi6ioeuDCJJRcNOf0iS7VSNBtehPpyGDgVpvwCEqeDEFRaKtlasrWxt7vv5D4AfPW+jIkYQ2q/VFL7pTI6YnSLQ2ruoOkL+PHPdrH7SJVb6x8xIJjHrhjZrrLXXHMNK1asYN++fcTHxzfmHz58mMsvv5ydO3d2+P6enoPR0DgrTunkQMUBpVBKlEJpmEMJ8w0jtV8qN464kfFR40nqk4RONBnuKT8EG56HrPfAaYMRc9QLpv+YFu/18bZibA7Jdemx0O/U11lC3wCWXj+Om9/Ywi8+yObVhamNCyraRAgIS1QhxbVC3lKtXsBFma7htY2w8yN1TmdA9EsmZMBYQsIHkxg+GMIHQ2y8mjhu5W/U0Pto3kNpmrY77fjqfU/v5TTp+TScM+ha+deWEurKoCwPCrapuGS3+g11rvUwBl/19027TSmSmDQIje/SmP2P+0t58ovdXDyi3+lKZPx4+gFT8jfzYUYhP5+RhL5pu5iC4PxfQvodsPVNpWjemQv9U2DKLwgZfgUz4mYwI24GACfNJ8ksyWRbyTYySzJ5OedlJLJxDqdB4YyNHEuAT0Cnf4838tVXX1FbW8tll13Grl27TlMwnkLrwWg9GLdjc9jYXb6b7OPZbC3Zyrbj26i0VAIQ6R9JWr80UvulktYvjYEhA1vezHVsJ6z7B+z6GHQGGLMAznsAwge1el8pJRf9/XtC/Hz4+J7zIDtbnUhJaSzzxvpDPP7Zbn4+YzC/bPiKdhdVR9WLutildI5uB3PFqfNCD33ilbIJH6x+S9ggFQdEqBe7OyZeHXbVy6vIh7IDrpDnCgfA1RaA+tuGD1Y9r4bQb2SrirAzNPQeo4J9+e89k0/vPbra6AtdP+59fxtv3jqeaUMjW6/MboGcZbD+WSg/qGQ/bwmMng+GM+dgqqxVZB/Pbuwp7yrbhUM60Akdw8OGNyqblMgU+vq1MD/XDjw5RNaA2WwmPT2dFStW8MYbbxAQEMCvf/3rxvNaD0aj11JpqSSnNIes41lsK9nGrrJdWBxq4jcmMIbpsdMbvxxjAmPOvjs4f6NSLPu/AWMgTLoXJt4Lwf3blGNr/kkOlNbyl3lq4yRLlqi4iZ2rWyYnsOdoFS98l8fQqCAuHz2gsz/7TIL7Q/DlMPzyU3l15c1e8HlQfgAOrwNb3enXCz2YAsEUrH67KciVDgJjkIr1BrDUgLVG9aIsNUqZNE2fMdwmICRWKbLR16q4QcGFxKk6u4lqs43Fb2ciBPzzprQzhyZdbTRz1XeEBRj5IKPw7ArGYILUm2HsQtj9qXpWVtwHa56CyffBuJvV38xFsDGYqTFTmRqjNljW2erIKc1p7Ekv27uMt3e/DUBsUCxjI8c2hoEhA0/vTXsxTz75JDfddBMJCQmMGjWKFStWeFokQFMwGh1ESklRdRFZpVlkHc8iqySLA5VqI6FBGBgePpxrh16rvgojUojwb9Orqloxk7dSvSwKNqrx/um/hfTFalK9nfxnSyGBJgOXjXIpo2efPaOMEII//CyZA6W1/Gp5DgnhASRHh7T7Hh3GP0yF2PGn50sJ1UeV8ik/oBRRUyVhrVbH5iqoLD51zmF1KZwGBRQEwQNc6WbKKDRWKZI+A8Gne+YfzobDKVmyLJtDJ2p5Z1F6y4srXG1kNOi4elw0b6w/TGm1hYigNjZA6/SQfBWMnAsHVsO6Z+GbR+CHv0L6nZB+OwSc2SPx9/Fn0oBJTBqgduhbHVZ2l6nedtbxLNYVr2PFAfVyDjYGkxKZ0qhwRoaP7LZ5nK6Qm5vLypUrWb9+PQCjRo3iqaeeajy/YMEC1q5dy4kTJ4iJieHxxx9n0aJFrVXnVrQhMm2I7KzUWGvYWbaT7aXbG8NJi9oZH+QTxJjIMY3/gMl9k/EzdGDi11IN2e/D5lfVSzYkFib/HMbeCMY2Vno1o8psI/2Pq5g7NoY/XTWqzfKl1RaufHEdAvj0viltv9A0Osxfvt7LS2sP8MSckdw0KaHN8nnHq7no7z/wm0uHcecFrQ+FtkphBqz7O+R+CXoTjLoGJt4FUW0/Dw1IKcmvyifreBbZpUrpHKo8BKgPqCFhQxjddzSjI0YzJmIMsUGx7N271+NDZN2JtoqsC2gK5hRO6eRgxUG2n1CKJKc0hwMVBxr3RCSGJDI6Qv1zpUSkMCh0UOeGEMoPwubXIOtd9aUeMx4m3KUm8Ds59v/Opnx+98lOVtx3HqNjXLuTMzJUPH58i9fsLK5k3isbSB4Qwvu3T8Ro6B3DIb2BT7OLeWBZNgvS43hqbnLrw6LN2uiaVzZQVmNl9S8v6LyhxdJc9dGS8x81DBk/BSbcqVYedmI4sMJcQXZpduMH1o4TO6izq+HNUFMoTw99miFDh+Bn8MPP4NeufU69CU3BdIGfqoKRUnK09ig7T+xkZ9lOdp3Yxe6y3dTYagA1PNCgTMb0HUNyRDLBxuCu3BAOroXNr8C+b9QQx8i5MOFuiEnt8u+5/IUfcTjhy/unnHoxTZum4rP4Gvks5wg//08WC9Jj+dNVo7ssh4ZS3Fe/vIExMaG8u3jC2RV3szb6aGsRv1qewwd3TGRCYnjXBKk/CdvegS3/hMoCNd+UvhjG3dShodfmOJwODlQeaFQ4F/peSN/4U8NxJr0JP4Mfvgbfxri3zOW0hKZgusBPRcGU1Zexq2yXUigndrKrbBflZmW4z6AzMKTPEJLDkxkTOYbRfUcTHxzvHhPk1jrYvkx9UZbuBf++avlr2m3tmrhvDzuLK7n8hXVnDsU0rJhJTj7r9X/+ei8vrz3Ac9elMCcl+qxlNc5OjcXOZc//iM3uZMXPp9A3sI2hx2ZtVG91kP7HVcwc0Y+/z085y4UdwOlQw2abX4XDP4KPv1p1NuEuiGx7J3tb7NmzhyFDh1Bvr6fOXke9vZ56ez0OpzL3I4RoVDoNwaQ39RoT/9oqMo3TOFF/gj1le9hbvpfdZbvZVbarcWe8QJAYksiU6Ckk900mOTyZoWFD3W9m4/he2PY2ZL+nlupGjYafvQwjr3L7hPOyjAJMBh1zxjRTDm0olgZ+OXMIWw6V89v/7WRcXB9iwzo2/6Nxisc+3UVheR3L7pjUtnKBM9rIz6hnztgBLM8s4rErRhLi74bl0jo9DL9ChWM7VC86+33Y+gYMvABSb4Fhl6kVap1Er9MTaAwk0KhWsEkpsTltmO3mRoVTaankpMuyt07o8DX4ql6OXvVyepPSaS+agunFSCkpqSthd9lu9pTvYU+ZCsfrjzeWiQ2KZUzEGG4YfgMjw0cyInwE/j7d9AK11sKu/ynFUrhZmRMZdpn6Uoyb2C3G9eqtDj7NOsLsUf3PfBlt2KDiyZPPWodBr+PZ+SnMfu5HHliWxYd3TsKg771DGp5iRc4R/rutiPtnDCZ94NnNxjTSQhtdNz6OdzcV8GlOcbsWB3SIqFEwZylc9IRSMFvfhI9uVSsXxyxQw2cRXd8fJYTAqDdi1BsJNqmhZSklVodVKRyHUjoV5grKZXnjNb5631OKx9XT6c3Da5qC6SXYnDYOVx4m92Qu+8r3sbd8L3vL9zau6NIJHYkhiaT3T2d42HCGhw9nWNgwgoxt2xrqMkeyYOtbsOMjNWkfngQXPwmjr4PAdixT7gJf7jhKtcXOdeNjzzz5yCMqboe/99gwf56cm8wDy7J54bs8fjFzSJvXaJyisLyOR/+3g3Fxodx/YVL7L2yhjZKjQ0iODuY/Wwq5caKbhmqbExAOU3+lrEIcXKOe382vKNtnsRPVXpsRP+vwasazIYRQlhcMJkJRC1EalY6jHrPdjNluPq2nI1DXNCoevS8mg6l1Sw1eRu+Q8idGpaWS3PJcck/mkluey76T+8iryMPmck7ko/NhcOhgpsdNb1QmQ/oM6dgS4a5iroTtH6reyrHtahf6yLnqCzBuUo+YAgc1PJbYN6DlL+ZXX+1QXXNSovl+XykvfLefKUl9GZ/Qzq/wnzh2h5NffJCNlPDcdWM71vtrpY3mj4/jd5/sZEdx5alVgd2BTg+DL1KhphRy3lfP9Cd3w1cPqaXO426CAW6aD2pGU6WDa4SuYXit3u5SOg4zNbYaKiynrEIYdIZGhdMQG/VGrxti0xSMB7E6rByqPMT+iv3sP7mfvIo8cstzKakraSwT7hvO0LChLBy+kCFhQxjaZygJIQn46HrGUuxpOOxw6HulWHZ/qnaMR42C2c+of0S/bnwRtEDe8RoyDp/kN5cOa/kfa2jHhzqemJNM5uGTLFmWzZcPnE+Inwf+zr2MF9fkkZl/kueuS+n4/FUrbTQnZQB//GI3yzIKu1fBNCUwQpkjmnw/5G+AbW+pOcTMfyvbbGOuV5s7A89iacANNB1eCzGd2gRsc9qw2C2YHeZTisda03heJ3SY9KbG+ZyGY0/2djQF0wM4nA4KqwvJq8g7TZkUVBXgcDm0MugMJAQnkBaVxtA+QxnaZyhDwoZ02j6S25BSDYHtWA47/6vM1puCYcx8ZZZjwNge660054OMAgw6wVXjYlou8P33Kr7ggnbXGWgy8Nx1Kcx7ZSOP/m8HLywY63Vfhd5E5uFynl+9n7ljozu3Aq+VNgr29eGyUQNYkX2ER2cPJ6Aj1q+7ihCQcJ4Kl/4Zti+HrLfh64eUtYDEaWoV2rDLTjNL09346HzwMfoQyKl7OqUTi8OC2W4mKyeLG+fdyOIHFnPtLcoV1tHiozx636OUl5ajEzpuWXQLDzzwQKPrh+5GW6bsxmXKNqeNwupCDlYc5EDFAQ5WHuRg5UEOVR5qtM0lEMQExZAUmsTgPoNVHDqY+JB4z/RKWqP8oPrH2vGhsp+lN0LSxcqWVdIlHjE90hSr3cnEP60mPSGMV25sZR9NO/bBtMbSNXn89ZtcnrlmDPNSW1FgP3GqzDYuffZHdDr48v7zCfLtxPN7ljbKOFzONa9s5C/zRnNtWgtzbD3N8T2q977jI7Wvxscfhs5mz5B7GbJy+igAACAASURBVD4q5ZRPcw+yceNGHnzwQX5Y/wMWu4X8onyKjhSRlJxEeUU58y6cx/NvP8+goYMw6o0MDh3c5geUtky5h3FKJ/tP7udgZRNFUnGQ/Op87E57Y7kBAQMYGDqQ9Kh0BocOZkifIQwMGdh9q7i6Sk2pWgW2/QNlERgBCVPUkMGIK7u0Oc3drNpTQnmtVZnlb43XX+90/XddMIgf9pXy+093khbfh4S+55Zp964ipeTR/+3kWJWZ5XdN6pxygbO2UVp8HwZFBPBBRqF3KJjI4XDRYzDjd2qV5I4P1f9L9LXKDbdvqPofMQZ4rFcfGRnJrl27Gns7IxNHMjJR+ZIZGDKQUSNHISslkf6ROKWz23vnmoLpBFJKrv/ieqxOKzqhIzYolsSQRKbFTmNQ6CASQxK9W5E0pboE9n4Oez6DQz+AdEC/ZJj5BCRfDSHe+fX+ny0FRIf6cX7SWVapJSZ2un69TvCP+Slc+tyP3L8si4/umqyZkmnCx9uK+SznCL+6eAjj4rrw4XGWNhJCcN34OP745R72lVQzpF8PrIhsDzodxE9SYdafYfdOZWy0rgxWPaZ6/zqDCkKH8gDfBaJGwaVPt6voww8/jMViIT8//wx/MPn5+eRk5zB9ynSC/btglaMDaAqmE+h1ep6d/iz9AvoRHxyPSd/LDCVWFCqFsmeF8tKIVH5JzntATdb3G+FpCc9KYXkd6/JO8MCFzZxTNWfVKhVfdFGn7jMg1I+nrxrF3e9t4x+r9vHQrK7v+j4XOHyilt9/upP0gWHcPW1w1ypro42uGhfNX77ZywcZhfzuci98Lg1G5dkzbKCyGGAKUkrFYVOWr9GplWp6g3LH0FVlcxbO5nCspqaGq6++mmeffZbg4J5RLqApmE5zfsz5nhahY5zIUwplzwo1aQ+qpzLtYRh+per+95LJ7OWZhQBc09awyZNPqriTCgbg0lH9uW58LK98f4Dzk/oyeZCHF114GJvDyQPLstDrBM/OTzm7gm8PbbRReKCJi0dE8fG2In49aygmgxcbktTp4Yrn1LHTrlwtmCvAXA04lYLxC1FDaQ2KyE2YzWYeeuihRodjO3fuZPbs2QDYbDauvvpqbrjhBq666iq33bM9aArmXMXpVIpk/zeqt3J8t8qPToWLHldmM87iHdJbcTglH2YWMTUpgujQNvb9vPOOW+75+ytGsOVQOQ9+kMNXD5xPnwA3m9XpRTy7ah85RZW8dMM4BrT1928P7Wij+eNj+WLHUb7dVcIVY9zoIK470RlO+QJyOly+fSqgvlL5/hF68A0G3xClbLq4lLg1h2NSShYtWsTw4cN58MEH3fHLOoSmYM4lzJVw4DvY961y4FVbqr6S4lxjxcMv99o5lfbyw75SjlWZ+X9XtmO4JNY9E8P+RgPPLxjL3JfW8/DH23llYepPcunyxgNlvLT2APPTYpk9yj2GStvTRlMG9yU61I9lGQW9R8E0RadXe8T8QkE6XcqmEuorlMVncDmMC1ZKp4Ous8/mcGz9+vW88847jBo1ihSX6/CnnnqqsXfT3WgKpjcjpfJ9sf8b2L9SeYN02lUXfPBFMOQSFfufOzvS39tcQN9AIzOG9Wu78Ndfq3jWrC7fNzk6hF9fMow/frmHdzblu99GlpdzosbCLz7IZmB4AL+/wo1zIe1oI51OcN34WP62ch+HT9T27hV9Qqd6Lb4hysGetVa5vDZXQfURFfTGU8rGGKQWFZyFoUOHsnnz5tPS27ZtA2DKlCl4ciuKpmB6G5ZqOLwe8lYpxVJRoPIjRypvkEmXKAde3ehn3VMUV9Tz3d4S7p42qH0rup52rbxxg4IBWDRlIBsPlvHEZ7tJigxi0qAu+ivpJVjtTu56ZysV9Vb+dfNk9256bGcbzR8fy3Or9/P+lgIemX2OeI8UwuXmOlC5vbZbTymb+nKoOwEINYTWoGwMpl4zVwpdVDBCiDDgAyABOAxcK6U82UK5m4HfupJPSinfcuWnAm8CfsCXwANSStlavUKNSzwHzAbqgFuklNtcdTmAHa57FEgpr+zKb/MaHDYoylTOug6uVftTnHYw+EHiBXDeErUBMtQL9gl0M8u2FCCBBelx7bxgmVvvr9MJnr0uhblL13PPe1v59N4pLfuZP4eQUvK7T3aSmX+SFxaMJTk6pO2LOkI72ygy2JeLR/ZjeWYhD84cgq+PF0/2dxaDEQx9IaCvmkO11rgUTiVUVqkyOh+lcBpCJz3A9hRd/RR5GFgtpXxaCPGwK/1Q0wIuZfEYkAZIYKsQYoVLEb0M3A5sRimYWcBXZ6n3UiDJFSa4rp/gulW9lLJ7LNL1JFKqHcMNCiV/vXrQhE6ZZTnvAWWqIibd47vpexKbw8myjEKmD40kpk87X+pRUW6XI9jXh3/dPJ6fLV3P7W9n8t97JhPYk2ZMepg3Nxzmg8xC7ps+uHvmPzrQRjdMiOfLHcf4csfR1s0DnSvodK5FAMEQHK2WPFuqm8zfKBP/GHxPKRtjoJrv8SK6+p8xB5jmOn4LWEszBQNcAqyUUjk9EEKsBGYJIdYCwVLKTa78t4GfoRRMa/XOAd6WalBxkxAiVAjRX0p5tIu/w3NICSf2K0WSvx4Ofg+1Ln8u4YNhzHVKoSRM8aqd9D3Nyt0llFZbWDixnb0XgM8+U/EVV7hVloF9A1h6/ThufmMLv/ggm1cXpqLr6nJdL+TH/aX84fPdzBzRjwe7y31BB9po8qBwEvsG8N7mgnNfwTRFCDU0ZjCp3o2UYKs/pXBqT6gFPQjlXsAYpKwJGAM8rnC6qmD6NXm5HwNamnmNBgqbpItcedGu4+b5Z6u3tbqOAr5CiEzADjwtpfykNaGFEHcAdwDExXXgheUOnA5lViJ/g0upbHCNtQKB/dSwV+I05WnvJzDs1V7e3ZRPdKgfFwzpgCXbv/1NxW5WMABTkvry28uG8/hnu/nHqn388uKuO6nyJg6dqOW+97NIigziH/NTuk+BdqCNhBBcPyGOJ7/Yw56jVQzv33MbBr0K0aBI/CGonxpOs9WeUjg1xxoKqk2gRtc8jzGgy8uhO0qbdxNCrAJa6sc+2jThmjtx+3KFDtQbL6UsFkIkAt8JIXZIKQ+0UudrwGugjF26UdwzsVuVv5T89WpyvmATWCrVudA4SJoJ8ZMh/jwIS+xVE3g9xYHSGjYcKOP/LhnasY19H33UfUIBt0xOYO/Ral74Lo8h/YJ65xLaFqgy27j97Ux0Av51c1r3DgF2sI3mpcbw129yeXdTPn+cO6qbhOpl6HSnhslAfcRaa9XQurVG9W4aRkUMfi5l41I43TyH0+aTI6VsdRu0EKKkYYhKCNEfON5CsWJODXcBxKCGvIpdx03zi13HrdVbDMS2dI2UsiE+6Bp+Gwu0qGC6laqjULQFCreoyfmj2WA3q3PhSZA8VymTuElaD6WdvL+5AB+96LjBw77du+teCMETPxvJgdIa/u+jHAb2DXD/JHgP43BKlizL5vCJWt5ZNKHj/l06SgfbKNTfyOWjB/BJVjG/mT38nJ7/6jQ6/an5GzjVw7HWgqVG2UyrLVWbPaNGdetHbVdbZwVwM/C0K/60hTLfAE8JIRomEC4GfiOlLBdCVAkhJqIm+W8CXmij3hXAfUKIZajJ/UqXEuoD1EkpLUKIvsB5wF+6+Nvaxm6Bo9uhKMOlVDKgyjXqpzdC/xQYv1gtG46f3O2Ois5FzDYHH20t4pKRUUQEddDm28cfq7gbzWOYDHpeXpjKnBfXcfvbmay4b0rH5fQi/vpNLt/tPc4ffpbcM8uwO9FGCyfG8d9tRXySVczCifFtX/BTp2kPJwi12dNWrxYOeLk15aeBD4UQi4B84FoAIUQacJeUcrFLkfwByHBd80TDhD9wD6eWKX/lCq3Wi1ppNhvIQy1TvtWVPxx4VQjhBHSoOZjdXfxtrWO3wFtXwJFscPl5ISQWYsdDzL0Qm66+DAy990XjLXyWc4TKelvnXiTPP6/ibra/FBFk4rWb0rjmlY3c9e5W3r99gnfbzGqFT7KKeeX7A9wwIY4be+rF3Yk2SokNZeSAYN7dlM8NE+J+klYVWmLHjh1cfvnlPPzww9x9992AslE2depULBYLdrudefPm8fjjj6vhMXpgw6qU8icdUlNTZadYfquU3zwq5a5PpKw80rk6NNrkyhfXyQv/tlY6nc6OX1xRoUIP8XnOERn/0OfyVx9md05eD5JVcFImPfqlnP/qBmm1O3ruxp1so/c25cv4hz6XmYfLukGojrF7925Pi9DIhg0b5MSJExvTTqdTVldXSymltFqtMj09XW7cuLFDdbb0+4BM2Y73qzaA2Vnmdd6ZlUb72FlcSU5hBY9dMaJzX6khPTsfctno/uSWJPH86v0M6x/MoikDe/T+naWkyswdb2cSGWTipRtS8dH3oN+bTrbRnJQBPPXlHt7bVEBq/LljCqmrNDgca0AIQWCgcrFss9mw2Ww92uPTFIyG1/Le5nz8fPSd3/PwwQcqnj/ffUK1wZILk8g9VsUfv9hNUmQgU4ecxSGaF2C2Objj7UxqLHY+XjSZsJ62FN3JNgowGbhqXDTLXH5ivMXC9Z+3/Jm95XvdWuewsGE8lN58e2HLtORwzOFwkJqaSl5eHvfeey8TJkxooxb3obno0/BKqsw2Psk6wpVjBhDi18mllC+/rEIPotMJ/n5tCkP6BXHPe9v4bm9Jj96/I1TWqeXIOUWV/GN+CsOiPLCvpAttdMOEeKx2J8u3FrZd+CdAc4djDej1erKzsykqKmLLli3s3Lmzx2TSejAaXsknWcXU2xzc0JGd+8358kv3CdQBAkwG3rw1ncVvZ7DorUwemjWMO6cmetVkdN7xaha/lUlxRT1/mTeaS0a636xOu+hCGw2NCmJ8Qh/e31zA4imJXmFNob09DXdzNodjDYSGhjJ9+nS+/vprkpOTe0QurQej4XVIKXl3Uz6jY0IYHRPa+Yr8/VXwAFEhviy/czKzR/Xn6a/28osPsjHbHB6RpTlr9h5n7tIN1Fjs/Of2iR3fX+ROuthGCyfGc7isjvUHTrhRqN5Hc4djDb2U0tJSKioqAKivr2flypUMG9Zzrr81BaPhdWQcPsm+khoWTujiUtl331XBQ/gZ9by4YCy/ungIn2Qf4dpXN3Ks0uwxeaSUvPL9AW57K4O4cH8+vW8KaQkeniDvYhvNSo4iPMDIu5vy3ShU76LB4diSJUsATlMwR48eZfr06YwePZrx48czc+ZMLr/88h6TTUgPOqPxBtLS0mRmZqanxdBowv3/yWJN7nG2PHIRfsYu7CeZNk3Fa9e6Q6wu8e2uY/zig2wCTAZevTGVsXE9a7jUbHPw8H+380n2ES4b3Z9n5o3p2t/WXbihjZ7+ai///PEg6x+aQVRIz1sY37NnD8OHnyM+alqgpd8nhNgqpUxr61qtB6PhVZyosfDVzqNcPS6m6y/AlStV8AIuHhnFx/ech8lHx/zXNvHfrUVtX+QmjlWamf/qRj7JPsKvLh7CiwvGeodyAbe00fXpcTilZFlGgZuE0nAXmoLR8CqWZxZhc8iOmeVvDR8fFbyEoVFBrLh3Cqlxffjl8hye+nIPDmf3jiBkFZzkyhfXkXe8htduTOW+GUletdjAHW0UF+7P1KQIlm0pxO5wukkwDXegKRgNr8HplLy/JZ+JiWEMjgzqeoVvvqmCF9EnwMjbi9K5aVI8r/1wkNvezKCy3tYt9/p4WxHzX9uEyUfHf++ZzMWeWil2NtzURgsnxnOsysyqPS3Z29XwFJqC0fAafthfSmF5PTd0dXK/AS9UMAA+eh1PzEnmqbmjWJ93grkvrefbXceotdi7XLeUkj1Hq3js0508+GEOqXF9+PTeKZ7Z49Ie3NRG04dG0D/El/c2/3Qn+70RbR+Mhtfw7qYC+gaa3Lcnwwsm98/G9RPiGBQRwL3vZ3HHO1sx6nWkDwxj2tAIpg2NZFBEQLuGs2osdtbtP8Ha3OOszS3lWJVaqXbTpHh+d/mInjX90lHc1EYGvY4F6XH8feU+Dp+oJaFvDxhy1GgTTcFoeAXFFfV8t7eEu6cNwmjw4heim5mQGM6Gh2eQmV/O2txS1uYe58kv9vDkF3uIDfNj+tBIpg+NZGJieOPEvJSSvOM1rHEplIzD5dgckiCTgfOH9GXakEguGBpBv+CeX1HlSa4bH8vzq/fz/pYCHpl97q7q6k1oCkbDK1i2pQAJLEh3owvrf/5Txbff7r46uwGjQcfkQX2ZPKgvj8weTtHJukZlszyziLc35mMy6JiYGM6AUF9+2HeC4op6AIZFBXHblIFMHxpJanwf7+6ttIQb2ygy2JeLR/ZjeWYhD84cgq+Pl6yU+wmjKRgNj2O1O1mWUcj0oZHE9HHjzvsGQ4permCaE9PHn4UT41k4MR6zzUHG4XLW7FUKJ/NwOZMH9+Xe6YOZNjSCAaF+nha3a7i5jW6YEM+XO47xxfajXJ3aSSOpGm5DUzAaHueLHUcorbZw0yQ3O7latcq99XkAXx895ydFcH5SBL+/YoSnxXE/bm6jyYPCGRwZyBsbDnHVuGjvWpLdzbTkcKwBh8NBWloa0dHRfP755z0mUy/rT2uca0gp+fe6QwyODOQCLzdtr+H9CCG47byB7CyuYsuh8rYvOIcYNWoUy5Yt4+233z7j3HPPPecRawOagtHwKFsOlbOzuIrbzhvo/q/Nl15SQcN76YY2umpcNH38ffj3ukNurbc30NzhGEBRURFffPEFixcv7nF5tCEyDY/y73WH6OPvw1Xjot1f+Wefqfiee9xft4Z76IY28vXRc8OEeJauzSO/rJb48J5bsnzsqaew7HGvwzHT8GFEPfJIu8q25HBsyZIl/OUvf6G6utqtcrUHrQej4TEOn6hl5Z4SbpgQ3z0rfr76SgUN76Wb2uimSfEYdII31h92e93eSksOxz7//HMiIyNJTU31iExaD0bDY7y54TAGnXD/5L7GT57IYF+uGD2ADzML+cXMIZ33itpB2tvTcDetORxbv349K1as4Msvv8RsNlNVVcXChQt5t4fcWGg9GA2PUFlv48PMQq4YPYDI7toQ+NxzKmh4L93YRrdNGUid1cEHPwEry605HPvTn/5EUVERhw8fZtmyZcyYMaPHlAtoCkbDQ3yQUUCd1cFtUwZ2301Wr1ZBw3vpxjZKjg5hYmIYb23IP6etLJ/N4Zin6ZKCEUKECSFWCiH2u+IWvSgJIW52ldkvhLi5SX6qEGKHECJPCPG8cC0jaq1eIcQwIcRGIYRFCPGrZveYJYTIddX1cFd+l0b3Ync4eWuDspqcHB3SfTdasUIFDe+lm9to0ZREiivq+XrXsW67h6cZOnQomzdvxmAwNKa3bdt2Rrlp06b16B4Y6HoP5mFgtZQyCVjtSp+GECIMeAyYAKQDjzVRRC8DtwNJrjCrjXrLgfuBZ5rdQw8sBS4FRgALhBDn4K60c4Ovdx2juKKeRVMSPS2KxjnOhcMiSQj3/0kuWfYGuqpg5gBvuY7fAn7WQplLgJVSynIp5UlgJTBLCNEfCJZSbpLKb/PbTa5vsV4p5XEpZQbQ3IFGOpAnpTwopbQCy1x1aHgh/153iIRwfy4cFtm9N3rmGRU0vJdubiOdTnDreQPJKqhgW8HJbruPRst0VcH0k1IedR0fA/q1UCYaKGySLnLlRbuOm+e3t9723EPDy9hWcJKsggpuPW8gOl03m/HYuFEFDe+lB9poXmoMwb4GrRfjAdpcpiyEWAW05KDj0aYJKaUUQrjd/2t31CuEuAO4AyAuzo3WezXa5N/rDhHsa2BeTxgi/O9/u/8eGl2jB9oowGRgQXoc/1p3iOKKeqJ7u4HQXkSbPRgp5UVSyuQWwqdAiWuoC1fckr/SYiC2STrGlVfsOm6eTzvrbc89WvtNr0kp06SUaRERmv2rnqK4op6vdx5jQXocASZtC5ZGz3Hz5AQA3tpw2KNy/NTo6hDZCqBhVdjNwKctlPkGuFgI0cc1uX8x8I1rCKxKCDHRtXrspibXt6fepmQASUKIgUIII3Cdqw4NL6Lhn7vhn73befppFTS8lx5qowGhflyaHMV/thRQ4wbX1Brto6sK5mlgphBiP3CRK40QIk0I8S8AKWU58AeUEsgAnnDlAdwD/AvIAw4AX7VRb5QQogh4EPitEKJICBEspbQD96GU2R7gQynl6RbfNDxKjcXOfzYXcGlyVM/5MMnOVkHDe+nBNlo0ZSDVZjvLMwvbLqzhFro0TiGlLAMubCE/E1jcJP068Hor5ZI7UO8xTh9Wa3ruS+DLDoiv0YMszyyk2mJnUXdurGzOsmU9dy+NztGDbTQ2rg/j4kJ5Y/1hbpqUgL67F5loaDv5Nbofh1PyxvrDjIsLZWxci3txNTR6hEVTEikor2PVnhJPi+J2duzYQXx8PC+//PJp+Q3mY1JSUkhLS+tRmTQFo9HtrNpTQkF5Xc9vrPzDH1TQ8F56uI0uGdmP6FC/c3LJ8tkcjq1Zs4bs7GwyMzN7VCZNwWh0O/9ed4joUD8uGdnWdiY3k5urgob30sNtZNDruGVygsvRXWWP3benaMnhmCfR1opqdCs7iyvZcqicR2cPx6Dv4e+ZHrQaq9FJPNBG89NjeXbVPv697hD/mJ/i1rp//HAfJwpr3Fpn39hAzr92SLvKtuRwTAjBxRdfjBCCO++8kzvuuMOt8p0NrQej0a38e90hAox65qfHtl1YQ6MHCPb14drxsXyWc4SSKrOnxXEbLTkcA1i3bh3btm3jq6++YunSpfzwww89JpPWg9HoNkqqzHyWc4QbJ8UT7NszDp9O4/e/V/ETT/T8vTXah4fa6NbJA3lzw2He3niY/7tkmNvqbW9Pw9205nAMIDpaWc2KjIxk7ty5bNmyhalTp/aIXFoPRqPbeHPDYRxScuvkHlya3JTCQhU0vBcPtVFcuD8Xj+jHe5sLqD0HNl625nCstraW6urqxuNvv/2W5OQzdoZ0G1oPRqNbqKyz8c7GfC4b1Z+4cH/PCPHGG565r0b78WAb3XnBIL7ZVcL7mwu4fWrvdR3R4HBs/fr1gFpN9tRTTwFQUlLC3LlzAbDb7Vx//fXMmjWr1brcjaZgNLqFtzYepsZi555pgz0tioZGi4yL68PkQeG89uNBbpwUj6+P3tMidYoGh2NN0w0OxxITE8nJyfGUaNoQmYb7qbXYeX39IS4cFsmIAcGeE+Q3v1FBw3vxcBvdN30wpdUWlm8taruwRofRFIyG23l/cwEVdTbuneHh3ktZmQoa3ouH22jSoHDGxoXyytoD2BxOj8lxrqINkWm4FbPNwWs/HmTyoHDGedoszGuvefb+Gm3j4TYSQnDf9MEseiuTT7OP9Iyfop8QWg9Gw60s31pEabWF+6Zrcy8avYMZwyIZ3j+Yl9bm4XC63WfiTxpNwWi4DZvDyStrDzA2LpRJg8I9LQ786lcqaHgvXtBGQgjunT6Ig6W1fL3zmEdlOdfQFIyG2/g0+wjFFfXcN30wyoech6mvV0HDe/GSNro0uT+JEQG8uCYPKbVejLvQ5mA03ILDKXlpbR7DooKYMSzS0+Ioli71tAQabeElbaTXCe6+YBD/99F21uQeZ8awHjbMeo6i9WA03MLXO49xsLSWe72l96Kh0UF+Njaa6FA/XvxO68W4C03BaHQZKSVL1+SR2DeA2aP6e1qcUyxZooKG9+JFbeSj13HnBYlsK6hg08Hyti/wMlpzOFZRUcG8efMYNmwYw4cPZ+PGjT0mk6ZgNLrM2txSdh+t4q5pgzQ3tBq9mmvTYukbaGLpmjxPi9JhWnM49sADDzBr1iz27t1LTk4Ow4cP7zGZtDkYjS4hpeTFNXlEh/oxd2y0p8U5nWef9bQEGm3hZW3k66Pn9vMH8qev9pJdWEFKbKinReoQzR2OVVZW8sMPP/Dmm28CYDQaMRqNPSaPpmA0usSmg+VszT/JE3NG4tPTDsU0NLqBGybG89LaA7z4XR7/urljPuzXvPkax/MPulWeyPhEpt/SPidhzR2OHTp0iIiICG699VZycnJITU3lueeeIyAgwK0ytob2RtDoEkvX5NE30MS1aV7oUOzee1XQ8F68sI0CTQZuPS+BVXtK2HusytPitJuWHI7Z7Xa2bdvG3XffTVZWFgEBATz99NM9JpPWg9HoNNmFFazLO8FvLh3mnZZo/fw8LYFGW3hpG90yOYF//nCQpWsO8MKCse2+rr09DXfTmsOxmJgYYmJimDBhAgDz5s3rUQWj9WA0Os2L3+UR4ufDDRPjPS1KyzzzjAoa3ouXtlGov5GFk+L5YvsRDp2o9bQ4bdKaw7GoqChiY2PJzc0FYPXq1YwYMaLH5OqSghFChAkhVgoh9rviFq0bCiFudpXZL4S4uUl+qhBihxAiTwjxvHBtoGitXiHEMCHERiGERQjxq2b3OOyqK1sIkdmV36XRNnuPVbFqTwm3npdAoEnrCGuceyyekoiPXsfLa717RVmDw7ElruXeTRUMwAsvvMANN9zA6NGjyc7O5pFHHukx2brag3kYWC2lTAJWu9KnIYQIAx4DJgDpwGNNFNHLwO1Akis0uFprrd5y4H6gtU+e6VLKFCllx2bmNDrM0jUHCDDquWVygqdFaZ077lBBw3vx4jaKCDIxf3wsH28rprjC8+ZsWqPB4ZjBYGhMNzgcA0hJSSEzM5Pt27fzySef0KdPz1k576qCmQO85Tp+C/hZC2UuAVZKKcullCeBlcAsIUR/IFhKuUmqbbNvN7m+xXqllMellBmArYtya3SBQydq+WL7ERZOjCfUv+eWPHaY8HAVNLwXL2+jOy8YBMBr3x/wsCS9k66ObfSTUh51HR8DWjLgEw0UNkkXufKiXcfN89tbb3Mk8K0QQgKvSilbdTQhhLgDuAMgLi6uHVVrNOXltXkY9DoW2m6m4wAAIABJREFUnT/Q06KcnT/9ydMSaLSFl7dRw/6uZRmF3DcjiYggk6dF6lW02YMRQqwSQuxsIcxpWs7VC3G7AZ8O1DtFSjkOuBS4Vwgx9Sx1vialTJNSpkVERLhL1J8EBWV1fLytmOvGxxIZ5OtpcTQ0up27pw3C5nDyqtaL6TBtKhgp5UVSyuQWwqdAiWuoC1d8vIUqioGmmyRiXHnFruPm+bSz3uZyFrvi48D/UPM9Gm7m2dX70OsE90zrBQ7Fbr1VBQ3vpRe0UWJEIHPHxvD2pnyOVnrvXIw30tU5mBVAw6qwm4FPWyjzDXCxEKKPa3L/YuAb1xBYlRBiomv12E1Nrm9PvY0IIQKEEEENx6577DzbNRodZ19JNf/LKubmyQlEhfSC3ktsrAoa3ksvaaMlFyUhpeSF77x7RZm30dU5mKeBD4UQi4B84FoAIUQacJeUcrGUslwI8Qcgw3XNE1LKBlOl9wBvAn7AV65wtnqjgEwgGHAKIZYAI4C+wP9cq5wNwPtSyq+7+Ns0mvH3b/cRYDRwl2vi0+t54glPS6DRFr2kjWLD/LlufBz/2VLAnVMTiQ/vGVMrvZ0uKRgpZRlwYQv5mcDiJunXgddbKZfcgXqPcfqwWgNVwJiOyK7RMbYXVfD1rmM8cGESYQFevHJMQ6Ob+PmMwSzfWsizq/bzj/kpnhanV6Dt5NdoF898u49Qfx8We/vKsaYsXKiChvfSi9ooMtiXmycn8El2MbnHqj0tTq9AUzAabbLpYBk/7CvlnmmDCPL18bQ47WfoUBU0vJde1kZ3TR1EoNHA377N9bQoZ9CSw7Hc3FxSUlIaQ3BwMM/2oIsEzcaHxlmRUvLMN7n0CzZx06QET4vTMX73O09LoNEWvayN+gQYuX1qIn9fuc/r/MU0OBx78MEHufvuuwG1qz87OxsAh8NBdHQ0c+fO7TGZtB6MxllZm1tKZv5J7puR5J0WkzU0epjbpgwkLMDIM994Xy+mucOxpqxevZpBgwYRH99zxmm1HoxGqzidkme+zSU2zI/53ujvpS2uu07Fy5Z5Vg6N1umFbRRoMnDPtEE8+cUeNhw4QVPLXhWfHcB6xL3Wl40DAgi9on0rN5s7HGvKsmXLWLBggVtlawutB6PRKl/tPMauI1X84qIhGA298FFJSVFBw3vppW20cGI8UcG+XtWLacnhWANWq5UVK1ZwzTXX9KhMWg9Go0XsDid/W5lLUmQgc1Ki277AG3n4DOPeGt5GL20jXx8991+YxCP/2/H/27v38Cqqc/Hj3zf3GwkQbiEBBEEaLkoxilqLeEoBbRULFvXnBSvWW/ur1tqjR2s9ik+Lvx6v1dpa6iNoD1gtFlTQAhZrtYjUC1AQgtxDICGBkJB7sn5/zAQ3Ye/snezLzJ68n+eZZ8+ezF7zrrywV2bNzFrUN+Uc3x7qmUakBZpwrM2KFSsYP348/fuHMqxj5MThn6UqFpZ8UsKO8mP8ZMppJCaI0+Eo5TrfLSpgSG4GVXVNWEMmOifQhGNtFi1aFPPuMdAGRvnR0NzCk6uKGZufw9TRA5wOp+tmzrQW5V5xnKPkxAR+PPk0mloMVXXOzSASbMKxY8eOsXLlSmbMmBHz2LSLTJ1k8bq9lByp45czxmIPvxOfzj3X6QhUMHGeo0vOGMh7H5Vx8GgDOenJjvx/aZtwzPe974RjmZmZVFRUxDwu0AZGtVPb2Myv39nOhKG9+fqIPk6HE5677gq+j3JWnOcoMUHITk+mobmFw7WN9M7U+WJ8aReZOsELH+ziUE0DP506Mr7PXpSKkfTkRDJSkjh4tIHWVmevxbiNNjDquKq6Jn675gsuHNmXolN6Ox1O+C691FqUe3kkR/2zU2lqaaXyWKPTobiKdpGp4+a/t4Oj9c38ZEr8jA3VoW+cNCC3chuP5CgrNYms1CTKqhvolZmid17atIFRAByqaeAP/9jJt07PY0x+TvAPxIPbb3c6AhWMR3IkIvTPTuOL8hoqahrolx0HE/LFgHaRKQB+vbqY+qYWfjz5NKdDUSouZaYmkZ2WTHlNA80trU6H4wrawCi2l1Xz0od7uOrswQzvl+V0OJFz0UXWotzLYzkakJNGayscPNrgdCiuoF1kioff3EJGciJ3ftNjZy+XXOJ0BCoYj+UoLTmR3lkpVNY0kJuV0u1HINczmG7u3W3lrNlazv/9xnByszx2D/9tt1mLci8P5qh/j1QSEoTSqvqYHtffhGMAjz/+OKNHj2bMmDFcddVV1NfHLi5tYLqx5pZWHn5jM0NyM5h93ilOh6OUJyQlJtCvRxrV9U0crY/dEDJtE44tXLjw+LaSkhKeeuop1q9fz6ZNm2hpaWFxDKdG0AamG1u0bg/FZTX810WFpCZ58FR+8mRrUe7l0RzlZqWQmpRI6ZF6WmM4EKa/Cceam5upq6ujubmZ2tpaBg4cGLN49BpMN1VV18RjK7dxzrDeTB0d2yG8Y+aKK5yOQAXjsRytWLGCAwcOANDSaqhvauHdpESSE7v+XMyAAQO4KMQbIdpPOJafn89dd93F4MGDSU9PZ8qUKUyZMqXLsXSWnsF0U79eXcyRuibu//Yo7w4J8/3vW4tyLw/nKDFBSEwQmlpaicU5jL8Jxw4fPszSpUvZuXMn+/fv59ixY7z00ksxiMYS1hmMiPQGXgZOAXYBs4wxh/3sNxv4mf32YWPMAnv7mcALQDqwHLjdGGMClSsiVwN3AwJUA7caYz6zy5oGPAkkAvONMfPCqZuX7Tx0jAX/3MWsMwcxeqBHHqpUygXan2nUNbawvaya3KxUBvZMj9pxA004tmrVKoYOHUrfvn0BmDFjBh988AHXXHNN1GLxFe4ZzD3AamPMCGC1/f4EdmPxADABOBt4QETaprF+Fvg+MMJepgUpdydwgTFmLDAXeM4+RiLwDHARMAq4SkRGhVk3z/rl8i2kJCbwk6keuy25vUmTrEW5l8dzlJ6SSK/MFCpqGmloaonacQJNODZ48GDWrl1LbW0txhhWr15NYWFh1OJoL9wGZjqwwF5fAFzmZ5+pwEpjTKV9drMSmCYieUC2MWatsaaDW+jzeb/lGmM+8DlDWgsU2OtnA9uNMTuMMY3AYrsM1c4HXxzir5sPctuFw+nXw+PDWVx/vbUo9+oGOeqfnUaCELXbljuacGzChAlcfvnljB8/nrFjx9La2spNN90UlTj8Cfcif39jTKm9fgDwd7U4H9jr836fvS3fXm+/PdRy5wArOjjGhEBBi8hNwE1gtfDdRUurYe4bW8jvmc6c84c6HU70efyLyxO6QY6SExPom53Kgap6quub6JGWHNHyg0049uCDD/Lggw9G9JihCtrAiMgqwN+8uff5vrGvnUT8Wpa/ckXkQqwG5vwulvkcdvdaUVFRt5nA4ZX1e9lSepSn/89Xu8cTxk32MwjJkf0PrSKom+SoT2YqlccaKa2qJys1ybs31rQTtIExxgS8SV1EDopInjGm1O7yKvOzWwkwyed9AbDG3l7QbnuJvR6wXBE5HZgPXGSMaZsHtAQYFKAsBdQ0NPM/f91G0ZBefGtsntPhxMY3v2m9rlnjaBiqA90kRwkJQl52Grsra6k81ui9UTMCCPcazDJgtr0+G1jqZ5+3gSki0su+uD8FeNvuAjsqIueI1Zxf5/N5v+WKyGBgCXCtMWabzzE+AkaIyFARSQGutMtQtt/8bTuHahq8fVtyezfeaC3KvbpRjrLTk8m0Z75sae0eoy2Hew1mHvAnEZkD7AZmAYhIEXCLMeZGY0yliMzFagQAHjLGVNrrt/Hlbcor+PKait9ygZ8DucBv7C/JZmNMkTGmWUR+iNWYJQLPG2NOfJy1G9tbWcv8f+xkxlfzOWNQT6fDiZ0Y3YqpwtCNciQi5PVMY3tZDWXVDeTlRO+2ZbcIq4Gxu6hOmpLOGLMeuNHn/fPA8wH2G9OJcm/0Lbfdz5ZjPUuj2pn31uckivDTaR6ZqTJUtbXWa0aGs3GowLpZjjJSkuiVkcKhmkZ6Z6Z4c4gmH/okv8d9tKuSNzeUcvMFw7rFX0wnuPhia1Hu1Q1zNCAnDQEOxHi0ZSfoWGQeZt2WvJm8nDRunniq0+HE3q23Oh2BCqYb5ig5MYG+PVI5eLSemoZmslK9+zXs3ZopFq3bw4Z9VTx55TjSU7x9Ku6XxwZS9KRumqO+WakcPtbI/iN1DO+XRYJHb7zRLjKPKq9u4JG3Pue8U3O59IzYDc/tKlVV1qLcq5vmKCFBGNgznfqmFg7VRGZ65UATjj355JOMGTOG0aNH88QTT0TkWKHSBsajfrF8Cw1Nrcy9bEz3uS25venTrUW5VzfOUXZ6MjnpyZQdbaCxOfxxyvxNOLZp0yZ+//vfs27dOj777DPeeOMNtm/fHvaxQqUNjAd9sP0Qr31Swi2TTuXUvllOh+OcH/3IWpR7dfMctd14s/9IPSYCE5O1n3Bsy5YtTJgwgYyMDJKSkrjgggtYsmRJ2McJlV6D8ZiG5hZ+9pdNDMnN4LZJ3fDCvq8ZM5yOQAXjsRxt2zaX6potnfpMU0srlc2tHNyVQFLCyX/z98gq5LTT7g+prPYTjo0ZM4b77ruPiooK0tPTWb58OUVFRZ2KLxzawHjM797dwY5Dx1hww9ndY7yxjhw6ZL326eNsHCowzRHJiUJzCzQ2t5KYLF3u0m4/4diQIUMoLCzk7rvvZsqUKWRmZjJu3DgSE2P3vaANjIfsOnSMp/+2nW+fnscFp/V1OhznXX659erxca7imsdyFOqZRnvHGpr5oryGvlmp5HVhYrJAE44BzJkzhzlz5gBw7733UlBQ0FFREaUNjEcYY7h/6SZSEhO4/9s61xoAP/mJ0xGoYDRHAGSmJtE703rCv2dGSqcfK2g/4diyZV8OxVhWVka/fv3Ys2cPS5YsYe3atZEOPyBtYDzizY2lvFd8iP++ZBT9sz0+kVioLrnE6QhUMJqj4wZkp3G0rpmSI3Wc2jcz5K6ytgnH3n//fcC6m+wXv/jF8Z/PnDmTiooKkpOTeeaZZ+jZM3bjEWoD4wFH65t46PXNjM3P4dpzT3E6HPc4cMB6HeBvOiPlCpqj45ISE8jLSWPv4VoqaxvJzQxtSP9gE4699957EY81VNrAeMBjf91GeU0D82cXkZjQTZ958efKK61Xj/Tve5Lm6AQ9M5KprE3iQFU92WnJJCfG95Mk2sDEuY37qlj4z11ce84QTi/oRkPxh+Kee5yOQAWjOTqBiJDfM53ishoOVNUzqHd8jzKtDUwca2k13PeXjeRmpXLX1G42FH8opk1zOgIVjOboJGnJifTNSqGsuoFeGSlkpcXv13R8n391c3/8cDcb9lVx/7dHkZ3m7TnNu2TvXmtR7qU58qtfjzRSkhIoOVJHawSe8HdK/DaN3VzZ0Xp+9dZWvj6iD5ecnud0OO507bXWq/bvu5fmyK+EBGFgTjq7Ko5xqLqBfnF6Z6g2MHFq7ptbaGhp5aHp3Xgwy2B+9jOnI1DBaI4COj4YZnUDORnJcTn7pTYwcejv28p5/bP93DF5BEP7ZDodjntNnux0BCoYzVGH8nLSqa6vZv+Rek7JzYi7Pyb1Gkycqa5v4r+WbGRYn0xuuaCbD2YZzI4d1qLcS3PUoZSkBAbkpFFd38Th2ianw+k0bWDizC9XfE5pVR2/+u4ZOphlMDfcYC3KvTRHQeVmppCZkkRpVR1Nza0B9ws04dgNN9xAv379GDNmzAnb33rrLUaOHMnw4cOZN29eVGLXBiaO/KP4EP/74R5u/PowzhzSy+lw3O/BB61FuZfmKCgRoaBXOsZAyZG6gPPG+JtwDOD666/nrbfeOmFbS0sLP/jBD1ixYgWbN29m0aJFbN68OeKx6zWYOFFd38Tdf97AsL6Z3PnN05wOJz5ccIHTEahgNEchSU1OpH92GqVVdRyubaJ3Zorf/dpPOAYwceJEdu3adcK2devWMXz4cIYNGwbAlVdeydKlSxk1KrID5YbVwIhIb+Bl4BRgFzDLGHPYz36zgbbbRR42xiywt58JvACkA8uB240xJlC5InI1cDcgQDVwqzHmM7usXfa2FqDZGBO7WXVi4BfLra6xV245T7vGQrV1q/U6Uh9CdS2P5ej+4n1sqqmLaJljstKZO6KAPlkpHK1rorSqjh6pSSQnndwB1X7CsUBKSkoYNGjQ8fcFBQUnjGcWKeF2kd0DrDbGjABW2+9PYDcWDwATgLOBB0SkrX/nWeD7wAh7aXusN1C5O4ELjDFjgbnAc+0Od6ExZpzXGpd/FB9i0TrtGuu0m2+2FuVemqOQBesqaz/hmBuE20U2HZhkry8A1mCdYfiaCqw0xlQCiMhKYJqIrAGyjTFr7e0LgcuAFYHKNcZ84FPuWiB2M+c4RLvGwuAzZLlyKY/laO6I6H4lpSYnMiA7jf3tuso6mnDMn/z8fPb6jKCwb98+8vPzIx5vuGcw/Y0xpfb6AaC/n33yAd+xIPbZ2/Lt9fbbQy13DlZj1MYAfxWRf4nITR0FLSI3ich6EVlfXl7e0a6Oa+sa+x+9a6zzzjvPWpR7aY46LTfr5LvK2k84tmnTpg7LOOussyguLmbnzp00NjayePFiLr300ojHGrSBEZFVIrLJzzLddz9jna9FfNAcf+WKyIVYDYzv2dL5xpjxwEXAD0RkYgdlPmeMKTLGFPXt696phX27xsYP1q6xTtu0yVqUe2mOOq19V9nnn3/OypUrueOOOwBOamCuuuoqzj33XLZu3UpBQQF/+MMfSEpK4umnn2bq1KkUFhYya9YsRo8eHfFYg3aRGWMCPmorIgdFJM8YUyoieUCZn91K+LK7C6xurTX29oJ220vs9YDlisjpwHzgImNMhU+cJfZrmYi8hnW95+/B6udW2jUWAT/8ofWq41y5l+aoS3y7ygoGDetwwrFFixb5LePiiy/usBstEsLtIlsGzLbXZwNL/ezzNjBFRHrZF/enAG/bXWBHReQcscY/uM7n837LFZHBwBLgWmPMtrYDiEimiPRoW7ePEdd/FmnXWAT86lfWotxLc9Rl/rrK3Cbci/zzgD+JyBxgNzALQESKgFuMMTcaYypFZC7wkf2Zh9ou+AO38eVtyiv48pqK33KBnwO5wG/sMXnabkfuD7xmb0sC/tcYc+KTRXHkveJyFq3bw80TtWssLGed5XQEKhjNUZe1dZUVl9Ww70idK8cqk0BPhXYXRUVFZv369U6HcVx1fRPTnniP1OQElv/o63r2Eo5PP7Vex41zNg4VmAdytGXLFgoLCx07/qHqBqurrFdGwAcww+GvfiLyr1AeB9En+V2mrWvs1Vv1gcqw2Rc9tX/fxTRHYcvNSqEqyAOYTtEGxkW0ayzCnnjC6QhUMJqjsLm5q0wbGJeoqmvi7lc3cGrfTH6sd41FRhx3u3QbmqOISE1OZEBOGvuP1HG4tpHemalOhwToaMquYIzh3tc2UlbdwKOzxmnXWKR89JG1KPfSHEVMbmYKWalJ7D9ST0NTi9PhAHoG4wqv/Gsfb24o5T+njWTcoJ5Oh+MdP/2p9ar9++6lOYoYEWFQrwy2lVWzp7KWU/tlkeBwV5mewThsR3kN/73s35w7LJebJ+oMlRH19NPWotxLcxQxGzduZPipQ3n7lRepa2rh4NF6IPCEY4G2R5I2MA5qbG7l9sWfkpKUwGNXnEFigjsuzHnGmDHWotxLcxQxbROOvbL4j+RmplBe3UB1fZPfCcfA/0RkkaYNjIMeXbmVjSVVPDLzdPJy0p0Ox3s++MBalHtpjiKqbcKxvJx00pIS2Xe4jvO+dj69e/c+ad+JEyf63R5Jeg3GIf8oPsTv3t3B1RMGM3X0AKfD8aZ777VetX/fvTyWowdf/zeb9x+NaJmjBmbzwCWhDUTZNuHY3r17GJRXwPbyGvYdDjzNcrRpA+OAipoG7vzTpwzvl8XPvhXZKUqVj9/9zukIVDCao4hpP+HYkCFDyLNvXW6ubXQkJm1gYswYw91/3sCR2iZe+N7ZpKfoLclR45FpeD3NYzkK9Uwj0gJNOJabmUJ1fTNb9zXgxEmMXoOJsZfW7mbVljLuuegrjBqY7XQ43vbuu9ai3EtzFBGBJhxre8o/UYSmllZaW2PbymgDE0NbD1Tz8JtbmDSyL9/72ilOh+N9DzxgLcq9NEdh27p1a4cTjl13zdVce9kUdn5RTP4ga8Ix8D8RWaTpaMoxGk25vqmF6U+/T8WxRlbc/nX69nDHUA6etmOH9TpsmLNxqMA8kCOnR1MO1f4jdRyqaeCU3Eyy05ND/pyOphwHfrl8C1sPVvPC987SxiVW4vhLq9vQHMXMgJw0jjU0s+9wLSNSepCcGP0OLO0ii4HVWw6y4J+7mXP+UCaN7Od0ON3HqlXWotxLcxQzCSIM6p1Bq4G9lbUxuXVZz2CirOxoPT99dQOFedn85zRv3THjeg8/bL1OnuxsHCowzVFMpSUnktczjZLDVndZ3x5pUT2eNjBR1NzSyg8XfUJtYzO/vmocqUl6S3JMvfii0xGoYDRHMdc7I4Wa+mbKq61h/aM5RJU2MFH0q7e3sm5nJY9fcQbD+/VwOpzuZ9AgpyNQwWiOYk5EyO+ZTqsh6uMfagMTJW9tKuV3f9/BNecM5jtfLXA6nO6pbSC/adOcjUMFpjlyRFIMLvCDNjBRsaO8hrte2cAZg3py/7d1KBjHzJtnveqXl3tpjjxNG5gIq21s5taXPiY5UfjN1eP1uouTFi92OgIVjObI0/Q25QgyxnDfa5vYVlbNk1d+lfyeOgS/owYMsBblXpqjiNm4cSNDhgzh2WefPWG7v4nF9u7dy4UXXsioUaMYPXo0Tz75ZFRiCruBEZHeIrJSRIrt114B9ptt71MsIrN9tp8pIhtFZLuIPCVizfEZqFwRmS4iG0TkUxFZLyLnBztGrLy0djevfVLCnZNPY+JpfWN9eNXe669bi3IvzVHEtE04tnDhwhO2+5tYLCkpiUcffZTNmzezdu1annnmGTZv3hzxmCJxBnMPsNoYMwJYbb8/gYj0Bh4AJgBnAw/4NETPAt8HRthLW2dsoHJXA2cYY8YBNwDzQzhG1H2y5zAPvbGZC0f25QcXDo/VYVVHHn3UWpR7aY4iqm3CMV/+JhbLy8tj/PjxAPTo0YPCwkJKSkoiHk8krsFMBybZ6wuANcDd7faZCqw0xlQCiMhKYJqIrAGyjTFr7e0LgcuAFYHKNcbU+JSbCbQ9jur3GMCi8KvYsYqaBm7748f0z07j8SvGkaBTH7vDq686HYEKxms5WnEPHNgY2TIHjIWL5oW0a9uEY7t372bIkCEhfWbXrl188sknTJgwIZwo/YrEGUx/Y0ypvX4A6O9nn3xgr8/7ffa2fHu9/fYOyxWR74jI58CbWGcxHR3jJCJyk929tr68vDxI9TrW0mq44+VPqTjWyG+vOZOeGSlhlaciqE8fa1HupTmKmPYTjoWipqaGmTNn8sQTT5CdHfnpQ0I6gxGRVYC/K3H3+b4xxhgRifgAN+3LNca8BrwmIhOBuUCnxpkwxjwHPAfWaMrhxPbEqm28V3yIR2aOZUx+TjhFqUhbssR6nTHD2ThUYF7LUYhnGpEWaMKxjjQ1NTFz5kyuvvpqZkTp9x9SA2OMCfgFLiIHRSTPGFMqInlAmZ/dSviyuwugAKvLq8Re993e1hEYtFxjzN9FZJiI9OngGFHzzucH+fU725lVVMAVZw2O5qFUVzz1lPXqlS8vL9IcRUT7CceWLVvW4f7GGObMmUNhYSF33nln1OKKRBfZMqDtjq3ZwFI/+7wNTBGRXvaF9ynA23YX2FEROce+e+w6n8/7LVdEhvvcaTYeSAUqAh0jAvXza29lLXcs/pTRA7N5aPqY4B9Qsbd0qbUo99IchS3YhGP+JhZ7//33efHFF3nnnXcYN24c48aNY/ny5RGPLRIX+ecBfxKROcBuYBaAiBQBtxhjbjTGVIrIXOAj+zMPtV2MB24DXgDSsS7ur+ioXGAmcJ2INAF1wBXGGne6o2NEVH1TC7e89C8Anr36TNKS9WFKV8rRLkvX0xyFbeTIkXz44YcnvP/444+Pv1+0yP99TnExXL8xpgL4hp/t64Ebfd4/DzwfYL+TTgE6KPcR4JEAsfg9RqQZAyP79+DOb57G4NyMaB9OddXLL1uvV1zhbBwqMM2Rp+lQMV2QnpLIY1eMczoMFUzbE8365eVemiNP0wZGeVcU+pRVhHkkR8YY7EvDnhJuN5qORaa8KyPDWpR7eSBHaWlpVFRUxOSaRiwZY6ioqCAtreuzXuoZjPKul16yXq+5xtk4VGAeyFFBQQH79u0j3Ie23SgtLY2Cgq7PZ6UNjPKu+fOt1zj+8vI8D+QoOTmZoUOHOh2GK2kDo7xr5UqnI1DBaI48TRsY5V3JyU5HoILRHHmaXuRX3vXCC9ai3Etz5GnawCjv0i8v99MceZp47da6zhKRcqyhaLqiD3AoguE4zWv1Ae/VyWv1Ae/VyWv1gZPrNMQYE3Ta3m7fwIRDRNYbY4qcjiNSvFYf8F6dvFYf8F6dvFYf6HqdtItMKaVUVGgDo5RSKiq0gQnPc04HEGFeqw94r05eqw94r05eqw90sU56DUYppVRU6BmMUkqpqNAGRimlVFRoA9MJIvJdEfm3iLTaU0IH2m+aiGwVke0ick8sY+wMEektIitFpNh+7RVgvxYR+dRelsU6zmCC/b5FJFVEXrZ//qGInBL7KDsnhDpdLyLlPnm50V85biEiz4tImYhsCvBzEZGn7PpuEJHxsY6xM0KozyQRqfLJz89jHWNniMggEfmbiGy2v+Nu97NP53NkjNElxAUoBEYCa4CiAPskAl8Aw4AU4DNglNOxB4j1/wH32Ov3AI8E2K/G6Vg7qEPQ3zdwG/Bbe/1K4GWn445Ana4HnnY61k7UaSIwHtgU4OcXAyt1PGXMAAAD0UlEQVQAAc4BPnQ65jDrMwl4w+k4O1GfPGC8vd4D2Obn31ync6RnMJ1gjNlijNkaZLezge3GmB3GmEZgMTA9+tF1yXRggb2+ALjMwVi6KpTft289XwW+Ie6efjCe/g2FxBjzd6Cyg12mAwuNZS3QU0TyYhNd54VQn7hijCk1xnxsr1cDW4D8drt1OkfawERePrDX5/0+Tk6UW/Q3xpTa6weA/gH2SxOR9SKyVkTc1giF8vs+vo8xphmoAnJjEl3XhPpvaKbdVfGqiAyKTWhRE0//b0J1roh8JiIrRGS008GEyu5C/irwYbsfdTpHOlx/OyKyChjg50f3GWOWxjqecHVUH983xhgjIoHuWR9ijCkRkWHAOyKy0RjzRaRjVZ3yOrDIGNMgIjdjnaH9h8MxqS99jPX/pkZELgb+AoxwOKagRCQL+DNwhzHmaLjlaQPTjjFmcphFlAC+f00W2Nsc0VF9ROSgiOQZY0rtU92yAGWU2K87RGQN1l83bmlgQvl9t+2zT0SSgBygIjbhdUnQOhljfOOfj3U9LZ656v9NuHy/nI0xy0XkNyLSxxjj2kEwRSQZq3H5ozFmiZ9dOp0j7SKLvI+AESIyVERSsC4qu+7OK9syYLa9Phs46QxNRHqJSKq93gf4GrA5ZhEGF8rv27eelwPvGPuqpUsFrVO7vu9LsfrM49ky4Dr7TqVzgCqf7tu4IyID2q7zicjZWN+1rv2jxo71D8AWY8xjAXbrfI6cvnshnhbgO1j9jg3AQeBte/tAYLnPfhdj3YXxBVbXmuOxB6hPLrAaKAZWAb3t7UXAfHv9PGAj1p1MG4E5Tsftpx4n/b6Bh4BL7fU04BVgO7AOGOZ0zBGo0y+Bf9t5+RvwFadjDlKfRUAp0GT/H5oD3ALcYv9cgGfs+m4kwF2abllCqM8PffKzFjjP6ZiD1Od8wAAbgE/t5eJwc6RDxSillIoK7SJTSikVFdrAKKWUigptYJRSSkWFNjBKKaWiQhsYpZRSUaENjFJKqajQBkYppVRUaAOjlMuIyBoR+Yq9nhtozhGl3E4bGKXcZzjWU/wAp2M9Na1U3NEGRikXEZEhQIkxptXedDrW8B1KxR1tYJRylzM4sUE5E21gVJzSBkYpdxmHNTgnIjICaxZB7SJTcUkbGKXc5QwgQUQ+A36ONTXC7I4/opQ76WjKSrmIiBQD4401L7pScU3PYJRyCRHpgTV7tTYuyhP0DEYppVRU6BmMUkqpqNAGRimlVFRoA6OUUioqtIFRSikVFdrAKKWUigptYJRSSkWFNjBKKaWi4v8DWgjvTR+sqC0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_evals):\n",
    "    plt.plot(np.arange(-1, 2, 0.1), evals_mu[i], label=r'$\\lambda$' + str(i + 1))\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.axhline(y = 0, color = 'r', linestyle = 'dotted')\n",
    "plt.axvline(x = 0, color = 'r', linestyle = 'dotted')\n",
    "plt.axvline(x = 1, color = 'r', linestyle = 'dotted')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "losses = []\n",
    "for mu in np.arange(-3, 4, 0.1):\n",
    "    print('mu:', mu)\n",
    "    new_w_layer3 = np.append(weights[220 : 230], weights[220 : 320]).reshape(H_student + 1, H_student)\n",
    "    new_b_layer3 = np.append([weights[320]], weights[320 : 330]).reshape(H_student + 1)\n",
    "    new_w_out = np.append([mu * weights[330], (1 - mu) * weights[330]], weights[331 : 340]).reshape(H_student + 1, D_out)\n",
    "    \n",
    "    mu, sigma = 0, 0.0001 # mean and standard deviation\n",
    "    perturbation = np.random.normal(mu, sigma, len(new_weights))\n",
    "    \n",
    "    perturbed_w_layer1 = w_layer1 + perturbation[0 : w_layer1.size].reshape(D_in, H_student)\n",
    "    offset = len(w_layer1)\n",
    "    \n",
    "    perturbed_b_layer1 = b_layer1 + perturbation[offset : offset + len(b_layer1)].reshape(H_student)\n",
    "    offset += len(b_layer1)\n",
    "    \n",
    "    perturbed_w_layer2 = w_layer2 + perturbation[offset : offset + w_layer2.size].reshape(H_student, H_student)\n",
    "    offset += len(w_layer2)\n",
    "    \n",
    "    perturbed_b_layer2 = b_layer2 + perturbation[offset : offset + len(b_layer2)].reshape(H_student)\n",
    "    offset += len(b_layer2)\n",
    "    \n",
    "    pertured_w_layer3 = new_w_layer3 + perturbation[offset : offset + new_w_layer3.size].reshape(H_student + 1, H_student)\n",
    "    offset += len(new_w_layer3)\n",
    "    \n",
    "    perturbed_b_layer3 = new_b_layer3 + perturbation[offset : offset + len(new_b_layer3)].reshape(H_student + 1)\n",
    "    offset += len(new_b_layer3)\n",
    "    \n",
    "    \n",
    "    perturbed_w_out = new_w_out + perturbation[offset : offset + new_w_out.size].reshape(H_student + 1, D_out)\n",
    "\n",
    "\n",
    "    offset += len(new_w_out)\n",
    "    \n",
    "    perturbed_b_out = b_out + perturbation[offset : offset + 1]\n",
    "    \n",
    "    dummy_network = DummyNetwork(D_in, H_student, D_out,\n",
    "                                 torch.DoubleTensor(perturbed_w_layer1), torch.DoubleTensor(perturbed_b_layer1),\n",
    "                                 torch.DoubleTensor(perturbed_w_layer2), torch.DoubleTensor(perturbed_b_layer2),\n",
    "                                 torch.DoubleTensor(pertured_w_layer3), torch.DoubleTensor(perturbed_b_layer3),\n",
    "                                 torch.DoubleTensor(perturbed_w_out.T), torch.DoubleTensor([perturbed_b_out]))\n",
    "    dummy_network = dummy_network.to(device)\n",
    "    if device == 'cuda':\n",
    "        dummy_network = torch.nn.DataParallel(dummy_network)\n",
    "    \n",
    "    loss_vals, trace = train(dummy_network,\n",
    "                              torch_dataset_inputs,\n",
    "                              torch_dataset_labels)\n",
    "    plt.plot(loss_vals)\n",
    "    break\n",
    "    \n",
    "    trace = []\n",
    "    trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "    new_weights_end = np.append(\n",
    "        np.append(\n",
    "            np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                      trace[-1][1].reshape(H_student)),\n",
    "            np.append(\n",
    "                np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                      trace[-1][3].reshape(H_student)),\n",
    "                np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                      trace[-1][5].reshape(H_student + 1)))),\n",
    "        np.append(trace[-1][6][0],\n",
    "                  trace[-1][7][0]))\n",
    "\n",
    "\n",
    "    \n",
    "    mus.append(mu)\n",
    "    losses.append(jax_loss(new_weights_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "new_weights_end = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                  trace[-1][5].reshape(H_student + 1)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "print(jax_loss(new_weights_end), jnp.linalg.norm(jax_grad(jax_loss)(new_weights_end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])\n",
    "print(jnp.linalg.norm(new_weights_end - new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, loss: 0.10270604704257065, gradient norm: 0.011078588217699798\n",
      "Iteration: 2000, loss: 0.10269743060484009, gradient norm: 0.2875029134228121\n",
      "Iteration: 3000, loss: 0.10269770154291535, gradient norm: 0.7109272681656998\n",
      "Iteration: 4000, loss: 0.1026978519181091, gradient norm: 0.3418418293695122\n",
      "Iteration: 5000, loss: 0.10269775888767396, gradient norm: 0.36774263792057\n",
      "Iteration: 6000, loss: 0.10269744748612614, gradient norm: 0.6632348194846205\n",
      "Iteration: 7000, loss: 0.10269732537999365, gradient norm: 0.5554117927262803\n",
      "Iteration: 8000, loss: 0.10269685543982726, gradient norm: 0.510397573260132\n",
      "Iteration: 9000, loss: 0.10269578054651307, gradient norm: 1.1030923270214137\n",
      "Iteration: 10000, loss: 0.10269431888365248, gradient norm: 1.1670429564198743\n",
      "Iteration: 11000, loss: 0.10269216896995641, gradient norm: 0.14696966531964778\n",
      "Iteration: 12000, loss: 0.10268967014738001, gradient norm: 0.41511374564268033\n",
      "Iteration: 13000, loss: 0.10268688178360404, gradient norm: 1.4158136131910013\n",
      "Iteration: 14000, loss: 0.10268359523960578, gradient norm: 0.871337420098325\n",
      "Iteration: 15000, loss: 0.10267929761800253, gradient norm: 0.6515419804976587\n",
      "Iteration: 16000, loss: 0.10267490777721197, gradient norm: 0.47227162129148964\n",
      "Iteration: 17000, loss: 0.10266985691113954, gradient norm: 0.13841877629768365\n",
      "Iteration: 18000, loss: 0.10266439809847558, gradient norm: 1.3449515518488864\n",
      "Iteration: 19000, loss: 0.10265842671078497, gradient norm: 0.2569863073101318\n",
      "Iteration: 20000, loss: 0.10265235546286242, gradient norm: 0.2926937080088899\n",
      "Iteration: 21000, loss: 0.10264581228704593, gradient norm: 0.5079050981271134\n",
      "Iteration: 22000, loss: 0.10263913366029516, gradient norm: 0.6424113173415124\n",
      "Iteration: 23000, loss: 0.10263250706218094, gradient norm: 0.7328833508941015\n",
      "Iteration: 24000, loss: 0.10262532221060984, gradient norm: 0.4452260024020165\n",
      "Iteration: 25000, loss: 0.10261791009077266, gradient norm: 0.21610764520135683\n",
      "Iteration: 26000, loss: 0.10261041468741382, gradient norm: 0.9801486819698362\n",
      "Iteration: 27000, loss: 0.10260201100652802, gradient norm: 0.3789314415159555\n",
      "Iteration: 28000, loss: 0.10259337452469737, gradient norm: 0.503982208291663\n",
      "Iteration: 29000, loss: 0.10258386820864882, gradient norm: 0.7474813417656848\n",
      "Iteration: 30000, loss: 0.10257419363582115, gradient norm: 0.34855269233025427\n",
      "Iteration: 31000, loss: 0.10256371962450005, gradient norm: 0.5837323066618978\n",
      "Iteration: 32000, loss: 0.10255293085139135, gradient norm: 0.411713350973921\n",
      "Iteration: 33000, loss: 0.10254133736417254, gradient norm: 0.12223201149178518\n",
      "Iteration: 34000, loss: 0.10252971069990355, gradient norm: 0.4248931535447859\n",
      "Iteration: 35000, loss: 0.10251762986309076, gradient norm: 0.3901204724606226\n",
      "Iteration: 36000, loss: 0.10250596501867128, gradient norm: 1.0517270227564919\n",
      "Iteration: 37000, loss: 0.1024934984236642, gradient norm: 0.32977548092453485\n",
      "Iteration: 38000, loss: 0.10248146984141193, gradient norm: 0.6472029317888678\n",
      "Iteration: 39000, loss: 0.10246934743236999, gradient norm: 0.24821782378141405\n",
      "Iteration: 40000, loss: 0.10245707626981851, gradient norm: 0.15875363144260943\n",
      "Iteration: 41000, loss: 0.10244481786745833, gradient norm: 0.41724209869781176\n",
      "Iteration: 42000, loss: 0.10243304379170859, gradient norm: 0.18181892095649851\n",
      "Iteration: 43000, loss: 0.10242146666326923, gradient norm: 0.3930571971106035\n",
      "Iteration: 44000, loss: 0.10241047663870131, gradient norm: 0.43051655071297223\n",
      "Iteration: 45000, loss: 0.10240004100789056, gradient norm: 0.09058884183804389\n",
      "Iteration: 46000, loss: 0.10238992522797062, gradient norm: 0.3981511721578662\n",
      "Iteration: 47000, loss: 0.10238047165803518, gradient norm: 0.30844768173558673\n",
      "Iteration: 48000, loss: 0.10237117515761585, gradient norm: 0.6845812777925142\n",
      "Iteration: 49000, loss: 0.10236244569351367, gradient norm: 0.20882793189654716\n",
      "Iteration: 50000, loss: 0.10235285562532721, gradient norm: 0.46579257462536916\n",
      "Iteration: 51000, loss: 0.10234126761368599, gradient norm: 0.19808155786568513\n",
      "Iteration: 52000, loss: 0.10232951163367235, gradient norm: 0.3257103847660115\n",
      "Iteration: 53000, loss: 0.10231972468196597, gradient norm: 0.42352000403149315\n",
      "Iteration: 54000, loss: 0.10231021079273753, gradient norm: 0.7442844353569317\n",
      "Iteration: 55000, loss: 0.1023020074665949, gradient norm: 0.4690929578009973\n",
      "Iteration: 56000, loss: 0.10229341119689099, gradient norm: 0.561205152242326\n",
      "Iteration: 57000, loss: 0.10228563577845678, gradient norm: 0.2744238846584102\n",
      "Iteration: 58000, loss: 0.10227822627225139, gradient norm: 0.36141458769557716\n",
      "Iteration: 59000, loss: 0.10227108467691096, gradient norm: 0.9438675640262018\n",
      "Iteration: 60000, loss: 0.10226390458409007, gradient norm: 0.6074682828805645\n",
      "Iteration: 61000, loss: 0.10225728646259077, gradient norm: 0.9776455947000441\n",
      "Iteration: 62000, loss: 0.10225051237116983, gradient norm: 1.0368813850471763\n",
      "Iteration: 63000, loss: 0.10224430678866275, gradient norm: 0.2797121518184087\n",
      "Iteration: 64000, loss: 0.10223811108409933, gradient norm: 0.7578005393573621\n",
      "Iteration: 65000, loss: 0.10223206366029805, gradient norm: 0.7723572304123241\n",
      "Iteration: 66000, loss: 0.10222606449098644, gradient norm: 0.25408563859742517\n",
      "Iteration: 67000, loss: 0.10221997943181574, gradient norm: 1.1998285488285474\n",
      "Iteration: 68000, loss: 0.10221362463679662, gradient norm: 0.4188052653342016\n",
      "Iteration: 69000, loss: 0.10220702220522344, gradient norm: 0.5584236460337187\n",
      "Iteration: 70000, loss: 0.1022004360935799, gradient norm: 0.49910377935168254\n",
      "Iteration: 71000, loss: 0.10219384365715366, gradient norm: 0.31602299446092275\n",
      "Iteration: 72000, loss: 0.10218810290522796, gradient norm: 0.6398154078017799\n",
      "Iteration: 73000, loss: 0.10218298248906245, gradient norm: 0.2859228926162487\n",
      "Iteration: 74000, loss: 0.10217785171599081, gradient norm: 0.320624984852992\n",
      "Iteration: 75000, loss: 0.10217363593823378, gradient norm: 0.7253530854584602\n",
      "Iteration: 76000, loss: 0.10216930558491294, gradient norm: 0.26319033442737255\n",
      "Iteration: 77000, loss: 0.1021652150956325, gradient norm: 0.43053282695099987\n",
      "Iteration: 78000, loss: 0.10216159640745169, gradient norm: 0.5107187282871855\n",
      "Iteration: 79000, loss: 0.10215786105709589, gradient norm: 0.6384763311239396\n",
      "Iteration: 80000, loss: 0.10215448643826087, gradient norm: 0.34392993474986727\n",
      "Iteration: 81000, loss: 0.1021511616248903, gradient norm: 0.05599000595008253\n",
      "Iteration: 82000, loss: 0.10214825281914992, gradient norm: 0.47126709727711097\n",
      "Iteration: 83000, loss: 0.1021449779384028, gradient norm: 1.6493898926178927\n",
      "Iteration: 84000, loss: 0.1021421090498711, gradient norm: 0.5230959778548966\n",
      "Iteration: 85000, loss: 0.10213924879579273, gradient norm: 0.8739834952871922\n",
      "Iteration: 86000, loss: 0.10213654796987769, gradient norm: 0.6921990844910513\n",
      "Iteration: 87000, loss: 0.10213364303518484, gradient norm: 0.6816829032472262\n",
      "Iteration: 88000, loss: 0.10213108861368508, gradient norm: 0.8909086603701372\n",
      "Iteration: 89000, loss: 0.10212848458079524, gradient norm: 0.34203347484749025\n",
      "Iteration: 90000, loss: 0.10212606600045913, gradient norm: 0.34559910702774543\n",
      "Iteration: 91000, loss: 0.10212358164356869, gradient norm: 0.4727779525537908\n",
      "Iteration: 92000, loss: 0.10212118473333608, gradient norm: 0.07464613449669187\n",
      "Iteration: 93000, loss: 0.10211879724509726, gradient norm: 0.5412672750950294\n",
      "Iteration: 94000, loss: 0.10211651604868795, gradient norm: 0.2897740027084422\n",
      "Iteration: 95000, loss: 0.1021144571612324, gradient norm: 0.5143461948435331\n",
      "Iteration: 96000, loss: 0.10211198055535954, gradient norm: 0.2018261296023081\n",
      "Iteration: 97000, loss: 0.10210986626759741, gradient norm: 0.7216041916185869\n",
      "Iteration: 98000, loss: 0.1021078766390365, gradient norm: 0.6739995320952616\n",
      "Iteration: 99000, loss: 0.10210560110785535, gradient norm: 0.7615375580561377\n",
      "Iteration: 100000, loss: 0.10210362471559552, gradient norm: 0.37153641907170015\n",
      "Iteration: 101000, loss: 0.10210155481585437, gradient norm: 0.3482810576272231\n",
      "Iteration: 102000, loss: 0.10209969725564413, gradient norm: 0.4488788195145567\n",
      "Iteration: 103000, loss: 0.10209751877665904, gradient norm: 0.22096966760338294\n",
      "Iteration: 104000, loss: 0.10209583405785441, gradient norm: 0.29856225118036\n",
      "Iteration: 105000, loss: 0.10209385946016346, gradient norm: 0.35977367581883934\n",
      "Iteration: 106000, loss: 0.1020920470466886, gradient norm: 0.7206668679294013\n",
      "Iteration: 107000, loss: 0.10209035670229373, gradient norm: 0.8467291975453103\n",
      "Iteration: 108000, loss: 0.10208836754116582, gradient norm: 0.47594836705858407\n",
      "Iteration: 109000, loss: 0.10208664370330732, gradient norm: 0.025916431555236655\n",
      "Iteration: 110000, loss: 0.1020848941154907, gradient norm: 0.3079264063874447\n",
      "Iteration: 111000, loss: 0.10208327713794554, gradient norm: 0.506629883910621\n",
      "Iteration: 112000, loss: 0.10208162448903577, gradient norm: 0.36008557463824725\n",
      "Iteration: 113000, loss: 0.10207960466172397, gradient norm: 0.27290520969269694\n",
      "Iteration: 114000, loss: 0.10207800323967701, gradient norm: 0.21347431669045633\n",
      "Iteration: 115000, loss: 0.1020764032117782, gradient norm: 0.6525888112883291\n",
      "Iteration: 116000, loss: 0.10207463606247046, gradient norm: 0.11223916324329149\n",
      "Iteration: 117000, loss: 0.10207282997700377, gradient norm: 0.17749033707941017\n",
      "Iteration: 118000, loss: 0.1020683038031014, gradient norm: 0.480548951043922\n",
      "Iteration: 119000, loss: 0.10206451876548132, gradient norm: 0.654551753170205\n",
      "Iteration: 120000, loss: 0.10206236080343027, gradient norm: 0.9663922686692941\n",
      "Iteration: 121000, loss: 0.10206031165584047, gradient norm: 1.149062558098099\n",
      "Iteration: 122000, loss: 0.10205857060511388, gradient norm: 0.4664973462772034\n",
      "Iteration: 123000, loss: 0.10205682362440326, gradient norm: 1.2329057827432395\n",
      "Iteration: 124000, loss: 0.10205526858141227, gradient norm: 0.5882825926580791\n",
      "Iteration: 125000, loss: 0.10205312525503127, gradient norm: 0.5347397483339129\n",
      "Iteration: 126000, loss: 0.10205188241794233, gradient norm: 0.1804920476326297\n",
      "Iteration: 127000, loss: 0.10205010831195153, gradient norm: 0.05551392046007058\n",
      "Iteration: 128000, loss: 0.10204871088951106, gradient norm: 0.5241299675913816\n",
      "Iteration: 129000, loss: 0.10204677913237345, gradient norm: 0.8745881757427906\n",
      "Iteration: 130000, loss: 0.10204542350684877, gradient norm: 0.2181382334977087\n",
      "Iteration: 131000, loss: 0.10204360499801317, gradient norm: 0.7537507809962626\n",
      "Iteration: 132000, loss: 0.10204227111716699, gradient norm: 0.36420778588787234\n",
      "Iteration: 133000, loss: 0.10204042160777849, gradient norm: 0.9493621290106166\n",
      "Iteration: 134000, loss: 0.10203900805242531, gradient norm: 0.4729938512730869\n",
      "Iteration: 135000, loss: 0.10203743288287283, gradient norm: 0.45019006941524126\n",
      "Iteration: 136000, loss: 0.1020356941891014, gradient norm: 1.0555752375291512\n",
      "Iteration: 137000, loss: 0.10203409209533328, gradient norm: 0.19853979631698068\n",
      "Iteration: 138000, loss: 0.10203250448275324, gradient norm: 0.3351975355071201\n",
      "Iteration: 139000, loss: 0.10203075960753517, gradient norm: 0.42378582954115424\n",
      "Iteration: 140000, loss: 0.10202887944794312, gradient norm: 0.5677870663354203\n",
      "Iteration: 141000, loss: 0.10202744290736097, gradient norm: 0.7123088472749601\n",
      "Iteration: 142000, loss: 0.10202536234598196, gradient norm: 0.5114475122753598\n",
      "Iteration: 143000, loss: 0.10202373179074557, gradient norm: 0.44840922558267854\n",
      "Iteration: 144000, loss: 0.10202186255871913, gradient norm: 1.1566790072937834\n",
      "Iteration: 145000, loss: 0.10201973590913585, gradient norm: 0.555047258031012\n",
      "Iteration: 146000, loss: 0.10201776581571695, gradient norm: 0.4929265732366423\n",
      "Iteration: 147000, loss: 0.10201562834650818, gradient norm: 0.5759991838581163\n",
      "Iteration: 148000, loss: 0.10201334248396114, gradient norm: 0.16165569154018228\n",
      "Iteration: 149000, loss: 0.10201093229418247, gradient norm: 0.21234236374484566\n",
      "Iteration: 150000, loss: 0.10200843217588508, gradient norm: 0.12511268190496197\n",
      "Iteration: 151000, loss: 0.1020057233275871, gradient norm: 0.6120476849577857\n",
      "Iteration: 152000, loss: 0.10200285744423587, gradient norm: 0.36873694030922044\n",
      "Iteration: 153000, loss: 0.10199968667098136, gradient norm: 0.18778285632295105\n",
      "Iteration: 154000, loss: 0.1019964037566694, gradient norm: 0.29346105316089544\n",
      "Iteration: 155000, loss: 0.10199304740056314, gradient norm: 0.7126638077863613\n",
      "Iteration: 156000, loss: 0.10198963546825698, gradient norm: 0.2402685223598708\n",
      "Iteration: 157000, loss: 0.10198629385186862, gradient norm: 0.381208532346126\n",
      "Iteration: 158000, loss: 0.10198269713951325, gradient norm: 0.24795217874582262\n",
      "Iteration: 159000, loss: 0.10197939899482944, gradient norm: 0.9772668774926995\n",
      "Iteration: 160000, loss: 0.10197625225243977, gradient norm: 0.7185359381805969\n",
      "Iteration: 161000, loss: 0.10197318583716156, gradient norm: 0.3572722465169733\n",
      "Iteration: 162000, loss: 0.10197024088504093, gradient norm: 0.8494301173997448\n",
      "Iteration: 163000, loss: 0.10196735428614909, gradient norm: 0.5141392199444079\n",
      "Iteration: 164000, loss: 0.10196452998552366, gradient norm: 0.586804622837574\n",
      "Iteration: 165000, loss: 0.10196195460987925, gradient norm: 0.5693116715254991\n",
      "Iteration: 166000, loss: 0.10195940727705877, gradient norm: 0.11970671583634862\n",
      "Iteration: 167000, loss: 0.10195709045730954, gradient norm: 0.4708446763809395\n",
      "Iteration: 168000, loss: 0.10195445877519796, gradient norm: 0.6151984421979066\n",
      "Iteration: 169000, loss: 0.10195232967537303, gradient norm: 0.8384946876167276\n",
      "Iteration: 170000, loss: 0.10195017328670722, gradient norm: 0.6379343191248937\n",
      "Iteration: 171000, loss: 0.10194805025977195, gradient norm: 0.8839584397799123\n",
      "Iteration: 172000, loss: 0.10194597396325862, gradient norm: 0.1807583132129695\n",
      "Iteration: 173000, loss: 0.10194411265221205, gradient norm: 0.11276371741540824\n",
      "Iteration: 174000, loss: 0.10194203660219094, gradient norm: 0.30209310011353835\n",
      "Iteration: 175000, loss: 0.10194048641311529, gradient norm: 0.4208988644709388\n",
      "Iteration: 176000, loss: 0.10193851557665912, gradient norm: 0.5318279471763587\n",
      "Iteration: 177000, loss: 0.10193686893655834, gradient norm: 0.08529480691219778\n",
      "Iteration: 178000, loss: 0.10193521795807436, gradient norm: 1.1018778663227315\n",
      "Iteration: 179000, loss: 0.10193362668571791, gradient norm: 0.3191065917354291\n",
      "Iteration: 180000, loss: 0.10193202253337477, gradient norm: 0.8786418451330948\n",
      "Iteration: 181000, loss: 0.10193065871060594, gradient norm: 0.11695810023716335\n",
      "Iteration: 182000, loss: 0.10192911923819772, gradient norm: 0.6189801331995708\n",
      "Iteration: 183000, loss: 0.1019276777168113, gradient norm: 0.2998567896290818\n",
      "Iteration: 184000, loss: 0.10192628838606382, gradient norm: 0.44095385145317184\n",
      "Iteration: 185000, loss: 0.10192503472404477, gradient norm: 0.639808148024257\n",
      "Iteration: 186000, loss: 0.10192378258895962, gradient norm: 0.4641246164689502\n",
      "Iteration: 187000, loss: 0.10192242279041624, gradient norm: 0.7299644482599449\n",
      "Iteration: 188000, loss: 0.10192118624954176, gradient norm: 0.4551687196487254\n",
      "Iteration: 189000, loss: 0.1019199929337649, gradient norm: 0.44194737012910745\n",
      "Iteration: 190000, loss: 0.1019189869346759, gradient norm: 0.6813978819133816\n",
      "Iteration: 191000, loss: 0.10191775152246117, gradient norm: 0.500663921637436\n",
      "Iteration: 192000, loss: 0.101916573229308, gradient norm: 0.22751577536408255\n",
      "Iteration: 193000, loss: 0.10191556453564327, gradient norm: 0.4015501632627795\n",
      "Iteration: 194000, loss: 0.1019144184860956, gradient norm: 0.3573893365790173\n",
      "Iteration: 195000, loss: 0.10191341553222369, gradient norm: 0.46644194027196834\n",
      "Iteration: 196000, loss: 0.10191247115519585, gradient norm: 0.7105289573512641\n",
      "Iteration: 197000, loss: 0.10191142938909492, gradient norm: 0.6214945006254665\n",
      "Iteration: 198000, loss: 0.10191046658229962, gradient norm: 0.24137580131869138\n",
      "Iteration: 199000, loss: 0.101909486053612, gradient norm: 0.4645070842474534\n",
      "Iteration: 200000, loss: 0.1019084962066381, gradient norm: 0.5232231001010076\n",
      "Iteration: 201000, loss: 0.1019078727109833, gradient norm: 0.6770533315419212\n",
      "Iteration: 202000, loss: 0.10190663643724368, gradient norm: 0.4675196033017038\n",
      "Iteration: 203000, loss: 0.10190595043221737, gradient norm: 0.6555120344596874\n",
      "Iteration: 204000, loss: 0.1019049242619638, gradient norm: 0.2818241743004178\n",
      "Iteration: 205000, loss: 0.10190414606467088, gradient norm: 0.3967429661800652\n",
      "Iteration: 206000, loss: 0.10190336928615137, gradient norm: 0.372569533874115\n",
      "Iteration: 207000, loss: 0.10190255457132084, gradient norm: 0.2858518153362849\n",
      "Iteration: 208000, loss: 0.10190150449123613, gradient norm: 0.10699147523187921\n",
      "Iteration: 209000, loss: 0.10190095724882349, gradient norm: 0.45549175347354137\n",
      "Iteration: 210000, loss: 0.10189995929091401, gradient norm: 0.9017495785351133\n",
      "Iteration: 211000, loss: 0.10189949107091997, gradient norm: 0.5422883738947352\n",
      "Iteration: 212000, loss: 0.10189842136401314, gradient norm: 0.1909934807080938\n",
      "Iteration: 213000, loss: 0.10189775444719561, gradient norm: 0.6057056749971642\n",
      "Iteration: 214000, loss: 0.10189702282066887, gradient norm: 0.1411271487690615\n",
      "Iteration: 215000, loss: 0.10189631167272498, gradient norm: 0.17164764561639964\n",
      "Iteration: 216000, loss: 0.10189563069944299, gradient norm: 0.6790469352183813\n",
      "Iteration: 217000, loss: 0.10189483652403533, gradient norm: 0.4748222273259124\n",
      "Iteration: 218000, loss: 0.10189416412501637, gradient norm: 0.48620153653783926\n",
      "Iteration: 219000, loss: 0.10189354112474673, gradient norm: 0.27057598772754754\n",
      "Iteration: 220000, loss: 0.1018927592622372, gradient norm: 1.1083479339367401\n",
      "Iteration: 221000, loss: 0.10189196075943713, gradient norm: 0.6429985773574535\n",
      "Iteration: 222000, loss: 0.10189169417406534, gradient norm: 0.2524711286007303\n",
      "Iteration: 223000, loss: 0.10189053743678163, gradient norm: 0.5604121659981736\n",
      "Iteration: 224000, loss: 0.1018901459214375, gradient norm: 0.05107478046837195\n",
      "Iteration: 225000, loss: 0.10188944997284484, gradient norm: 1.241039003676337\n",
      "Iteration: 226000, loss: 0.10188878622059301, gradient norm: 0.5682466562676453\n",
      "Iteration: 227000, loss: 0.10188826820831423, gradient norm: 0.34694302966835966\n",
      "Iteration: 228000, loss: 0.1018874672084468, gradient norm: 0.5604001837815926\n",
      "Iteration: 229000, loss: 0.10188690883584357, gradient norm: 0.39576694901868903\n",
      "Iteration: 230000, loss: 0.10188620005284096, gradient norm: 0.18761372276691696\n",
      "Iteration: 231000, loss: 0.10188571440634556, gradient norm: 0.473406534537261\n",
      "Iteration: 232000, loss: 0.10188491889033775, gradient norm: 0.67537948267118\n",
      "Iteration: 233000, loss: 0.10188459488429173, gradient norm: 0.9334119049299645\n",
      "Iteration: 234000, loss: 0.10188398620052135, gradient norm: 0.8285886236860797\n",
      "Iteration: 235000, loss: 0.10188305351540075, gradient norm: 0.20629894483254388\n",
      "Iteration: 236000, loss: 0.10188268414168475, gradient norm: 0.2992000815314491\n",
      "Iteration: 237000, loss: 0.10188197517738119, gradient norm: 0.3062822688873691\n",
      "Iteration: 238000, loss: 0.10188138216671346, gradient norm: 0.442006282981045\n",
      "Iteration: 239000, loss: 0.10188087231804763, gradient norm: 0.28416377434670836\n",
      "Iteration: 240000, loss: 0.10188036412727226, gradient norm: 0.5333730368606031\n",
      "Iteration: 241000, loss: 0.10187973433496762, gradient norm: 0.923167553843832\n",
      "Iteration: 242000, loss: 0.10187912956170243, gradient norm: 0.23132532352637486\n",
      "Iteration: 243000, loss: 0.10187875990165766, gradient norm: 0.3050097679747689\n",
      "Iteration: 244000, loss: 0.10187795219586616, gradient norm: 0.41906676765946066\n",
      "Iteration: 245000, loss: 0.10187735797518196, gradient norm: 0.671673640511777\n",
      "Iteration: 246000, loss: 0.10187688147365813, gradient norm: 0.7871380561949028\n",
      "Iteration: 247000, loss: 0.10187644490881573, gradient norm: 0.2635511676737447\n",
      "Iteration: 248000, loss: 0.10187572751150975, gradient norm: 0.5421968964482157\n",
      "Iteration: 249000, loss: 0.10187532032976604, gradient norm: 0.29880167714938627\n",
      "Iteration: 250000, loss: 0.1018746909777921, gradient norm: 0.3786939643376632\n",
      "Iteration: 251000, loss: 0.10187415434692706, gradient norm: 0.6723030952950485\n",
      "Iteration: 252000, loss: 0.10187358475975784, gradient norm: 0.32335387941328747\n",
      "Iteration: 253000, loss: 0.10187298662132342, gradient norm: 0.2756824571356481\n",
      "Iteration: 254000, loss: 0.10187254963762366, gradient norm: 0.32357962777285276\n",
      "Iteration: 255000, loss: 0.10187190546842353, gradient norm: 0.7006477707891369\n",
      "Iteration: 256000, loss: 0.10187150481217198, gradient norm: 0.2676393799045012\n",
      "Iteration: 257000, loss: 0.10187081320484859, gradient norm: 0.08622536800961972\n",
      "Iteration: 258000, loss: 0.10187040578602702, gradient norm: 0.43748142201552254\n",
      "Iteration: 259000, loss: 0.10186972295773643, gradient norm: 0.5875969723186534\n",
      "Iteration: 260000, loss: 0.10186925680555964, gradient norm: 0.4548045909582595\n",
      "Iteration: 261000, loss: 0.10186876445988738, gradient norm: 0.33193457120628955\n",
      "Iteration: 262000, loss: 0.10186816523565753, gradient norm: 0.772977708215423\n",
      "Iteration: 263000, loss: 0.10186774889209274, gradient norm: 0.4993497365544076\n",
      "Iteration: 264000, loss: 0.10186715050141136, gradient norm: 0.6005272776318349\n",
      "Iteration: 265000, loss: 0.10186665432969967, gradient norm: 0.3490454274100114\n",
      "Iteration: 266000, loss: 0.10186592540932407, gradient norm: 0.13508653442566726\n",
      "Iteration: 267000, loss: 0.10186561877758515, gradient norm: 0.4188738626288039\n",
      "Iteration: 268000, loss: 0.10186487084719098, gradient norm: 0.35722711197662976\n",
      "Iteration: 269000, loss: 0.10186439832794675, gradient norm: 0.36021811700217\n",
      "Iteration: 270000, loss: 0.10186402462670847, gradient norm: 0.456539778451201\n",
      "Iteration: 271000, loss: 0.10186349205021569, gradient norm: 0.9717654042768213\n",
      "Iteration: 272000, loss: 0.10186271051789403, gradient norm: 0.5573834792036534\n",
      "Iteration: 273000, loss: 0.10186213153802476, gradient norm: 0.28896867580145497\n",
      "Iteration: 274000, loss: 0.1018618858104563, gradient norm: 0.04513128824758026\n",
      "Iteration: 275000, loss: 0.10186125652366096, gradient norm: 0.16947195175161098\n",
      "Iteration: 276000, loss: 0.10186054934448162, gradient norm: 0.7100735809346773\n",
      "Iteration: 277000, loss: 0.10186014885544716, gradient norm: 0.5389607659427958\n",
      "Iteration: 278000, loss: 0.10185951736962423, gradient norm: 0.7834138774702482\n",
      "Iteration: 279000, loss: 0.1018590848490366, gradient norm: 1.042401616473406\n",
      "Iteration: 280000, loss: 0.10185841327988845, gradient norm: 0.29418433282108436\n",
      "Iteration: 281000, loss: 0.10185804982842447, gradient norm: 0.2504639897176745\n",
      "Iteration: 282000, loss: 0.10185733067238588, gradient norm: 0.9900320157642754\n",
      "Iteration: 283000, loss: 0.10185677042382406, gradient norm: 0.16003557419439057\n",
      "Iteration: 284000, loss: 0.10185628823166541, gradient norm: 1.0245418177271575\n",
      "Iteration: 285000, loss: 0.1018557762800391, gradient norm: 0.17611286844262738\n",
      "Iteration: 286000, loss: 0.10185522656250251, gradient norm: 0.5333994301984158\n",
      "Iteration: 287000, loss: 0.10185468754346198, gradient norm: 0.062228861578850075\n",
      "Iteration: 288000, loss: 0.1018540698736089, gradient norm: 0.4623802285018712\n",
      "Iteration: 289000, loss: 0.10185373636605564, gradient norm: 0.22803981461086667\n",
      "Iteration: 290000, loss: 0.10185297562281875, gradient norm: 0.28113994965033734\n",
      "Iteration: 291000, loss: 0.10185242957014398, gradient norm: 0.124371265921787\n",
      "Iteration: 292000, loss: 0.10185206032220553, gradient norm: 0.7179263950544046\n",
      "Iteration: 293000, loss: 0.10185157379786144, gradient norm: 0.23257614367909307\n",
      "Iteration: 294000, loss: 0.10185084739694378, gradient norm: 0.39976504278122904\n",
      "Iteration: 295000, loss: 0.10185043129515639, gradient norm: 0.4467571855170119\n",
      "Iteration: 296000, loss: 0.10184994804593958, gradient norm: 0.5318924240608115\n",
      "Iteration: 297000, loss: 0.10184928638838672, gradient norm: 0.09358781617351612\n",
      "Iteration: 298000, loss: 0.10184897345541571, gradient norm: 0.719051289945231\n",
      "Iteration: 299000, loss: 0.10184832253943708, gradient norm: 0.5257594265435331\n",
      "Iteration: 300000, loss: 0.10184803521982567, gradient norm: 0.38433819724495694\n",
      "Iteration: 301000, loss: 0.10184730703031136, gradient norm: 0.366483267017075\n",
      "Iteration: 302000, loss: 0.10184691311219553, gradient norm: 0.5194835787431575\n",
      "Iteration: 303000, loss: 0.10184638326812086, gradient norm: 0.4520352330632342\n",
      "Iteration: 304000, loss: 0.10184576475751131, gradient norm: 0.16126451570105554\n",
      "Iteration: 305000, loss: 0.10184542813526963, gradient norm: 0.30454100723700045\n",
      "Iteration: 306000, loss: 0.10184496411877905, gradient norm: 0.8943951867536923\n",
      "Iteration: 307000, loss: 0.10184446216623104, gradient norm: 0.25708501717738824\n",
      "Iteration: 308000, loss: 0.10184403788168252, gradient norm: 0.484819974103703\n",
      "Iteration: 309000, loss: 0.10184347217106124, gradient norm: 0.30812246480026767\n",
      "Iteration: 310000, loss: 0.10184307680826014, gradient norm: 0.1464807977654035\n",
      "Iteration: 311000, loss: 0.10184252318310164, gradient norm: 1.1336447405064622\n",
      "Iteration: 312000, loss: 0.10184195197097835, gradient norm: 0.9334790399602333\n",
      "Iteration: 313000, loss: 0.10184184171810443, gradient norm: 0.41029188811556394\n",
      "Iteration: 314000, loss: 0.1018409143110507, gradient norm: 0.5755380718210324\n",
      "Iteration: 315000, loss: 0.10184085384336426, gradient norm: 0.3530437380631545\n",
      "Iteration: 316000, loss: 0.10184017456966679, gradient norm: 0.15756371369165995\n",
      "Iteration: 317000, loss: 0.10183998286130036, gradient norm: 0.9782875806679476\n",
      "Iteration: 318000, loss: 0.10183929315079113, gradient norm: 0.9032331057209186\n",
      "Iteration: 319000, loss: 0.10183890089870266, gradient norm: 0.2614827309295255\n",
      "Iteration: 320000, loss: 0.1018385929641078, gradient norm: 0.37216802972717666\n",
      "Iteration: 321000, loss: 0.10183815339162522, gradient norm: 0.8394669180871679\n",
      "Iteration: 322000, loss: 0.10183751115633825, gradient norm: 0.35385261805155177\n",
      "Iteration: 323000, loss: 0.101837137900254, gradient norm: 0.6759277232861872\n",
      "Iteration: 324000, loss: 0.10183690086415495, gradient norm: 0.19486295174039722\n",
      "Iteration: 325000, loss: 0.10183621029704738, gradient norm: 0.7045811633057969\n",
      "Iteration: 326000, loss: 0.10183607303038267, gradient norm: 0.6301033705579202\n",
      "Iteration: 327000, loss: 0.10183537504942444, gradient norm: 0.050387963282558894\n",
      "Iteration: 328000, loss: 0.10183541078284689, gradient norm: 1.0569402859799741\n",
      "Iteration: 329000, loss: 0.10183444656548565, gradient norm: 0.6510829784947061\n",
      "Iteration: 330000, loss: 0.10183423720595879, gradient norm: 0.5272926832542786\n",
      "Iteration: 331000, loss: 0.10183389402452425, gradient norm: 0.7340428349545384\n",
      "Iteration: 332000, loss: 0.10183343361500916, gradient norm: 0.15875633623525284\n",
      "Iteration: 333000, loss: 0.10183305006725356, gradient norm: 0.31108451724006936\n",
      "Iteration: 334000, loss: 0.10183255745094663, gradient norm: 0.3100329511976054\n",
      "Iteration: 335000, loss: 0.10183215475454688, gradient norm: 0.5682817266592798\n",
      "Iteration: 336000, loss: 0.10183200043170622, gradient norm: 0.6047398236916455\n",
      "Iteration: 337000, loss: 0.10183122684682877, gradient norm: 0.30417394892346244\n",
      "Iteration: 338000, loss: 0.10183116249219025, gradient norm: 0.5242834660504854\n",
      "Iteration: 339000, loss: 0.10183059675076513, gradient norm: 0.511828543204902\n",
      "Iteration: 340000, loss: 0.10183030332037665, gradient norm: 0.630304339144806\n",
      "Iteration: 341000, loss: 0.10182974163998375, gradient norm: 0.6859897509265342\n",
      "Iteration: 342000, loss: 0.10182945386012145, gradient norm: 0.5584286896176688\n",
      "Iteration: 343000, loss: 0.10182912669446832, gradient norm: 0.36487862466836263\n",
      "Iteration: 344000, loss: 0.1018286172563978, gradient norm: 0.6072753020865318\n",
      "Iteration: 345000, loss: 0.10182842346257788, gradient norm: 0.467881159874636\n",
      "Iteration: 346000, loss: 0.10182790474933628, gradient norm: 0.024218623178303634\n",
      "Iteration: 347000, loss: 0.10182753363440646, gradient norm: 0.3961841000157802\n",
      "Iteration: 348000, loss: 0.10182725871603641, gradient norm: 0.8252022882901462\n",
      "Iteration: 349000, loss: 0.10182670885327186, gradient norm: 0.42612011169390807\n",
      "Iteration: 350000, loss: 0.10182644592430716, gradient norm: 0.2255513844497759\n",
      "Iteration: 351000, loss: 0.10182601699093792, gradient norm: 0.3688778061183648\n",
      "Iteration: 352000, loss: 0.10182584627499668, gradient norm: 0.33068814372969746\n",
      "Iteration: 353000, loss: 0.10182524372754972, gradient norm: 0.4585384211485922\n",
      "Iteration: 354000, loss: 0.10182485541931614, gradient norm: 0.32801730786196753\n",
      "Iteration: 355000, loss: 0.10182463603462862, gradient norm: 0.24175939656274756\n",
      "Iteration: 356000, loss: 0.10182422526908118, gradient norm: 0.557100561580402\n",
      "Iteration: 357000, loss: 0.10182381047163719, gradient norm: 0.2782960270901367\n",
      "Iteration: 358000, loss: 0.10182348254689917, gradient norm: 0.3298207607477056\n",
      "Iteration: 359000, loss: 0.10182314480555282, gradient norm: 0.1444274894876595\n",
      "Iteration: 360000, loss: 0.10182268810672963, gradient norm: 1.028052186460101\n",
      "Iteration: 361000, loss: 0.10182238886095657, gradient norm: 0.7180867012886266\n",
      "Iteration: 362000, loss: 0.10182211654965678, gradient norm: 0.11658360066900653\n",
      "Iteration: 363000, loss: 0.1018216752554012, gradient norm: 0.4628201072030323\n",
      "Iteration: 364000, loss: 0.10182129825840107, gradient norm: 0.16812804957088964\n",
      "Iteration: 365000, loss: 0.1018209880272529, gradient norm: 0.7616143217234261\n",
      "Iteration: 366000, loss: 0.10182046221291308, gradient norm: 0.8067105046441856\n",
      "Iteration: 367000, loss: 0.10182036638732363, gradient norm: 0.27155337782894157\n",
      "Iteration: 368000, loss: 0.10181984361620468, gradient norm: 0.4921045018083626\n",
      "Iteration: 369000, loss: 0.10181962065483094, gradient norm: 0.5314184383119355\n",
      "Iteration: 370000, loss: 0.10181911723237004, gradient norm: 0.11273739388600634\n",
      "Iteration: 371000, loss: 0.10181896803021272, gradient norm: 0.7605513235848408\n",
      "Iteration: 372000, loss: 0.10181837178000458, gradient norm: 0.17500820770637396\n",
      "Iteration: 373000, loss: 0.1018181070855411, gradient norm: 0.2722147634232644\n",
      "Iteration: 374000, loss: 0.10181788995055352, gradient norm: 0.11538661947351413\n",
      "Iteration: 375000, loss: 0.10181736714750132, gradient norm: 0.32344009780824623\n",
      "Iteration: 376000, loss: 0.10181715250173543, gradient norm: 0.3782795049328651\n",
      "Iteration: 377000, loss: 0.10181665382370701, gradient norm: 0.25658859179852495\n",
      "Iteration: 378000, loss: 0.1018164059815267, gradient norm: 0.24436498108185056\n",
      "Iteration: 379000, loss: 0.10181600628450255, gradient norm: 0.26268526434942696\n",
      "Iteration: 380000, loss: 0.101815855867297, gradient norm: 0.7046256822110862\n",
      "Iteration: 381000, loss: 0.10181520158728787, gradient norm: 0.6179516569904486\n",
      "Iteration: 382000, loss: 0.10181496536149538, gradient norm: 0.23306645278978833\n",
      "Iteration: 383000, loss: 0.10181457902616427, gradient norm: 0.3937926205969965\n",
      "Iteration: 384000, loss: 0.10181437504949167, gradient norm: 0.6880748409542394\n",
      "Iteration: 385000, loss: 0.10181390858967072, gradient norm: 0.9870059042108165\n",
      "Iteration: 386000, loss: 0.1018134904694937, gradient norm: 0.36799146212842193\n",
      "Iteration: 387000, loss: 0.10181316395615687, gradient norm: 1.3223048458504651\n",
      "Iteration: 388000, loss: 0.10181282431099621, gradient norm: 0.44948121262070967\n",
      "Iteration: 389000, loss: 0.10181261676063381, gradient norm: 0.3123380731477865\n",
      "Iteration: 390000, loss: 0.10181200812491355, gradient norm: 0.2688775694447494\n",
      "Iteration: 391000, loss: 0.10181168241775472, gradient norm: 0.2694721975108551\n",
      "Iteration: 392000, loss: 0.10181127813342031, gradient norm: 0.12602465155245676\n",
      "Iteration: 393000, loss: 0.10181102163490519, gradient norm: 0.22934687797737668\n",
      "Iteration: 394000, loss: 0.10181067328495597, gradient norm: 0.42303116655368467\n",
      "Iteration: 395000, loss: 0.10181010170685117, gradient norm: 0.426890706263803\n",
      "Iteration: 396000, loss: 0.10180985208311331, gradient norm: 0.539103011660612\n",
      "Iteration: 397000, loss: 0.10180941825617267, gradient norm: 0.2829234771543664\n",
      "Iteration: 398000, loss: 0.10180891806387947, gradient norm: 0.03125389994352408\n",
      "Iteration: 399000, loss: 0.10180844907326295, gradient norm: 0.28887074017660713\n",
      "Iteration: 400000, loss: 0.1018079246498332, gradient norm: 0.06776042595191113\n",
      "Iteration: 401000, loss: 0.10180768238846298, gradient norm: 0.5784535068192355\n",
      "Iteration: 402000, loss: 0.10180696993919869, gradient norm: 0.1913238631617686\n",
      "Iteration: 403000, loss: 0.10180653501090815, gradient norm: 0.6306270405631046\n",
      "Iteration: 404000, loss: 0.10180588906236275, gradient norm: 0.2214956389574366\n",
      "Iteration: 405000, loss: 0.10180513944949997, gradient norm: 0.2489818262576035\n",
      "Iteration: 406000, loss: 0.1018043456066848, gradient norm: 0.6901043948347853\n",
      "Iteration: 407000, loss: 0.1018035608529162, gradient norm: 0.4577276598740253\n",
      "Iteration: 408000, loss: 0.1018029434226122, gradient norm: 0.2498927587929261\n",
      "Iteration: 409000, loss: 0.10180200331650247, gradient norm: 0.432399385113124\n",
      "Iteration: 410000, loss: 0.10180063568220434, gradient norm: 1.316434173508908\n",
      "Iteration: 411000, loss: 0.10179974603325448, gradient norm: 0.1525295024984347\n",
      "Iteration: 412000, loss: 0.10179862094040756, gradient norm: 0.3720968286959623\n",
      "Iteration: 413000, loss: 0.10179717527420354, gradient norm: 0.6105294979704919\n",
      "Iteration: 414000, loss: 0.10179603736584227, gradient norm: 0.2716882639340905\n",
      "Iteration: 415000, loss: 0.10179464792464694, gradient norm: 0.4469129488100159\n",
      "Iteration: 416000, loss: 0.10179341028846833, gradient norm: 0.049836564135796506\n",
      "Iteration: 417000, loss: 0.10179195265210311, gradient norm: 0.27544894001302517\n",
      "Iteration: 418000, loss: 0.10179083824468049, gradient norm: 0.30558702822111034\n",
      "Iteration: 419000, loss: 0.10178927006325392, gradient norm: 0.8851360095582439\n",
      "Iteration: 420000, loss: 0.10178806924801452, gradient norm: 0.07682346604544109\n",
      "Iteration: 421000, loss: 0.10178657244138968, gradient norm: 0.5151586567809439\n",
      "Iteration: 422000, loss: 0.10178508679145304, gradient norm: 0.3813353070842775\n",
      "Iteration: 423000, loss: 0.10178353604855264, gradient norm: 0.10064207071332323\n",
      "Iteration: 424000, loss: 0.10178144999513103, gradient norm: 1.0726452232832178\n",
      "Iteration: 425000, loss: 0.10177972451739409, gradient norm: 0.22073053135242945\n",
      "Iteration: 426000, loss: 0.10177780040846045, gradient norm: 0.4653169582353667\n",
      "Iteration: 427000, loss: 0.10177630604915763, gradient norm: 1.4136190394690298\n",
      "Iteration: 428000, loss: 0.10177487431927817, gradient norm: 1.0721472497365048\n",
      "Iteration: 429000, loss: 0.10177360633094802, gradient norm: 0.5527444582623035\n",
      "Iteration: 430000, loss: 0.1017721753950097, gradient norm: 0.28135789116314014\n",
      "Iteration: 431000, loss: 0.10177096446322846, gradient norm: 0.35040885881209205\n",
      "Iteration: 432000, loss: 0.10176975035471433, gradient norm: 0.9768159586154382\n",
      "Iteration: 433000, loss: 0.10176873690746714, gradient norm: 0.11738623127745734\n",
      "Iteration: 434000, loss: 0.1017675869067907, gradient norm: 0.5024670425200596\n",
      "Iteration: 435000, loss: 0.10176651453713918, gradient norm: 0.618570621123354\n",
      "Iteration: 436000, loss: 0.10176552504853044, gradient norm: 0.14568569485560554\n",
      "Iteration: 437000, loss: 0.1017643333971002, gradient norm: 0.05409920920251646\n",
      "Iteration: 438000, loss: 0.10176354576400481, gradient norm: 0.8429836387678246\n",
      "Iteration: 439000, loss: 0.10176267148506074, gradient norm: 0.10670865004368012\n",
      "Iteration: 440000, loss: 0.10176162979068112, gradient norm: 0.7886951272627094\n",
      "Iteration: 441000, loss: 0.10176084506327376, gradient norm: 0.03475946640177074\n",
      "Iteration: 442000, loss: 0.10176003661786201, gradient norm: 0.5028883970548771\n",
      "Iteration: 443000, loss: 0.10175882612322197, gradient norm: 0.2587475262266976\n",
      "Iteration: 444000, loss: 0.1017585242321999, gradient norm: 0.18536112175525324\n",
      "Iteration: 445000, loss: 0.10175731123553593, gradient norm: 0.514650068520561\n",
      "Iteration: 446000, loss: 0.1017565410987296, gradient norm: 0.208084753745011\n",
      "Iteration: 447000, loss: 0.10175574911978556, gradient norm: 0.4160029504787671\n",
      "Iteration: 448000, loss: 0.10175504771824606, gradient norm: 0.7129119697629575\n",
      "Iteration: 449000, loss: 0.10175426373667362, gradient norm: 0.40846510397312313\n",
      "Iteration: 450000, loss: 0.10175349868858452, gradient norm: 0.16708592217839738\n",
      "Iteration: 451000, loss: 0.10175283025496065, gradient norm: 0.40937228013015653\n",
      "Iteration: 452000, loss: 0.10175205383419499, gradient norm: 0.2471059480344414\n",
      "Iteration: 453000, loss: 0.10175131600913619, gradient norm: 0.081274671893817\n",
      "Iteration: 454000, loss: 0.10175063725701716, gradient norm: 0.2754692182412442\n",
      "Iteration: 455000, loss: 0.10174988598729202, gradient norm: 0.7755809398958017\n",
      "Iteration: 456000, loss: 0.10174936435119612, gradient norm: 0.20798928481060527\n",
      "Iteration: 457000, loss: 0.10174863646244289, gradient norm: 0.7739908997067921\n",
      "Iteration: 458000, loss: 0.10174785724298033, gradient norm: 0.6394351037831059\n",
      "Iteration: 459000, loss: 0.10174722079106312, gradient norm: 0.5163977137441111\n",
      "Iteration: 460000, loss: 0.10174671233548128, gradient norm: 0.09858321009999277\n",
      "Iteration: 461000, loss: 0.101746042147513, gradient norm: 0.6687296128177995\n",
      "Iteration: 462000, loss: 0.1017452904014226, gradient norm: 0.02429566090715552\n",
      "Iteration: 463000, loss: 0.10174476042766926, gradient norm: 0.27209334135564056\n",
      "Iteration: 464000, loss: 0.10174413926192566, gradient norm: 0.06875462442539065\n",
      "Iteration: 465000, loss: 0.10174355629129078, gradient norm: 0.9911607345196676\n",
      "Iteration: 466000, loss: 0.10174302133872365, gradient norm: 0.27500357675250997\n",
      "Iteration: 467000, loss: 0.10174232799672117, gradient norm: 1.0943978011763134\n",
      "Iteration: 468000, loss: 0.10174171752018826, gradient norm: 0.06074015853937752\n",
      "Iteration: 469000, loss: 0.10174113759929244, gradient norm: 0.42035805864938464\n",
      "Iteration: 470000, loss: 0.10174060370755739, gradient norm: 0.9080668711238111\n",
      "Iteration: 471000, loss: 0.10174017409851852, gradient norm: 0.5165054882101314\n",
      "Iteration: 472000, loss: 0.10173949623875762, gradient norm: 0.1524262227785156\n",
      "Iteration: 473000, loss: 0.10173883203559393, gradient norm: 0.3583470521312424\n",
      "Iteration: 474000, loss: 0.10173846959486346, gradient norm: 0.407659234613893\n",
      "Iteration: 475000, loss: 0.10173774362586731, gradient norm: 1.382830604272411\n",
      "Iteration: 476000, loss: 0.10173730740007754, gradient norm: 0.667798785606157\n",
      "Iteration: 477000, loss: 0.1017367633172292, gradient norm: 0.3216248647642663\n",
      "Iteration: 478000, loss: 0.10173624938335529, gradient norm: 0.7027959610404682\n",
      "Iteration: 479000, loss: 0.10173571749957856, gradient norm: 0.9059767384907854\n",
      "Iteration: 480000, loss: 0.10173506248183278, gradient norm: 0.3383162132762716\n",
      "Iteration: 481000, loss: 0.10173456476141911, gradient norm: 0.06282337756661824\n",
      "Iteration: 482000, loss: 0.10173407934754522, gradient norm: 0.2570819112516393\n",
      "Iteration: 483000, loss: 0.10173339644461528, gradient norm: 0.9581097108807339\n",
      "Iteration: 484000, loss: 0.1017330330410522, gradient norm: 0.42547614147012147\n",
      "Iteration: 485000, loss: 0.10173243833024191, gradient norm: 0.8409304807067931\n",
      "Iteration: 486000, loss: 0.10173201280747884, gradient norm: 0.15120022011751338\n",
      "Iteration: 487000, loss: 0.10173121771483824, gradient norm: 1.0440803741170082\n",
      "Iteration: 488000, loss: 0.1017309441683264, gradient norm: 0.8867716511792931\n",
      "Iteration: 489000, loss: 0.10173033284447264, gradient norm: 0.8846943885079003\n",
      "Iteration: 490000, loss: 0.10172981231911944, gradient norm: 0.8558538565745571\n",
      "Iteration: 491000, loss: 0.10172938808353553, gradient norm: 0.6258404987012286\n",
      "Iteration: 492000, loss: 0.10172869762537372, gradient norm: 0.38399466793910525\n",
      "Iteration: 493000, loss: 0.10172832173611099, gradient norm: 0.5405303828853523\n",
      "Iteration: 494000, loss: 0.10172770214225334, gradient norm: 0.11317370792537836\n",
      "Iteration: 495000, loss: 0.10172738121196372, gradient norm: 0.30572432833856394\n",
      "Iteration: 496000, loss: 0.10172667595371089, gradient norm: 0.34849482211858024\n",
      "Iteration: 497000, loss: 0.10172647821928947, gradient norm: 0.5706594368836422\n",
      "Iteration: 498000, loss: 0.10172584119254056, gradient norm: 0.42630486579396065\n",
      "Iteration: 499000, loss: 0.10172522933472689, gradient norm: 0.22721530210286411\n",
      "Iteration: 500000, loss: 0.10172511173371442, gradient norm: 0.19222109053920905\n",
      "Iteration: 501000, loss: 0.10172437152886707, gradient norm: 0.5573571730462821\n",
      "Iteration: 502000, loss: 0.10172410404848449, gradient norm: 0.5626594785131624\n",
      "Iteration: 503000, loss: 0.10172359729987195, gradient norm: 0.3841860157440273\n",
      "Iteration: 504000, loss: 0.10172319380906171, gradient norm: 0.21651284595204187\n",
      "Iteration: 505000, loss: 0.10172270075154378, gradient norm: 0.6495439680927187\n",
      "Iteration: 506000, loss: 0.1017222767023403, gradient norm: 0.14129855997592308\n",
      "Iteration: 507000, loss: 0.10172192736272023, gradient norm: 0.7259360035030717\n",
      "Iteration: 508000, loss: 0.10172146298409862, gradient norm: 0.08143632671315831\n",
      "Iteration: 509000, loss: 0.10172104163467713, gradient norm: 0.5778614004172\n",
      "Iteration: 510000, loss: 0.10172064176186889, gradient norm: 0.17205134965518723\n",
      "Iteration: 511000, loss: 0.10172016648578754, gradient norm: 0.7550852442118055\n",
      "Iteration: 512000, loss: 0.1017198691627453, gradient norm: 0.9494975921429714\n",
      "Iteration: 513000, loss: 0.10171935284777403, gradient norm: 0.1516494962696267\n",
      "Iteration: 514000, loss: 0.1017189940388837, gradient norm: 0.6887334707475945\n",
      "Iteration: 515000, loss: 0.10171881694845196, gradient norm: 0.35746100165348116\n",
      "Iteration: 516000, loss: 0.10171820809674952, gradient norm: 0.2421455950362508\n",
      "Iteration: 517000, loss: 0.10171786333836969, gradient norm: 0.27241193237674727\n",
      "Iteration: 518000, loss: 0.10171745202490691, gradient norm: 1.2478608868692596\n",
      "Iteration: 519000, loss: 0.10171722368010316, gradient norm: 0.6127222401444034\n",
      "Iteration: 520000, loss: 0.1017168144365009, gradient norm: 0.8221937219478326\n",
      "Iteration: 521000, loss: 0.10171644965014782, gradient norm: 0.7468594621628846\n",
      "Iteration: 522000, loss: 0.10171591783278179, gradient norm: 0.15857121540129576\n",
      "Iteration: 523000, loss: 0.10171557832405569, gradient norm: 0.7764372420368971\n",
      "Iteration: 524000, loss: 0.10171526042737776, gradient norm: 0.04171159377119174\n",
      "Iteration: 525000, loss: 0.10171489696971758, gradient norm: 0.5286312797294251\n",
      "Iteration: 526000, loss: 0.10171446683855677, gradient norm: 0.16141226860831698\n",
      "Iteration: 527000, loss: 0.10171425520008569, gradient norm: 0.16742091752844807\n",
      "Iteration: 528000, loss: 0.1017137646860694, gradient norm: 0.561975586992686\n",
      "Iteration: 529000, loss: 0.1017135719492327, gradient norm: 0.4376121610123672\n",
      "Iteration: 530000, loss: 0.10171308845889564, gradient norm: 0.5736078457706278\n",
      "Iteration: 531000, loss: 0.10171286331764663, gradient norm: 0.007373432170150484\n",
      "Iteration: 532000, loss: 0.10171227616411423, gradient norm: 0.21858356780683208\n",
      "Iteration: 533000, loss: 0.10171220595553997, gradient norm: 0.6324061411769937\n",
      "Iteration: 534000, loss: 0.10171167220933704, gradient norm: 0.3429205235524554\n",
      "Iteration: 535000, loss: 0.10171148041132906, gradient norm: 0.5981378515772888\n",
      "Iteration: 536000, loss: 0.10171092496439152, gradient norm: 0.7468750979975181\n",
      "Iteration: 537000, loss: 0.1017107433742577, gradient norm: 0.2042555859425135\n",
      "Iteration: 538000, loss: 0.10171029789854576, gradient norm: 0.283167783017915\n",
      "Iteration: 539000, loss: 0.10170998524874142, gradient norm: 0.3602353598604165\n",
      "Iteration: 540000, loss: 0.10170959281499069, gradient norm: 0.532457909889188\n",
      "Iteration: 541000, loss: 0.10170944626169415, gradient norm: 0.43391217554203226\n",
      "Iteration: 542000, loss: 0.10170885217371925, gradient norm: 0.23162382793236982\n",
      "Iteration: 543000, loss: 0.10170871050428519, gradient norm: 0.49250100481033854\n",
      "Iteration: 544000, loss: 0.10170830178260158, gradient norm: 0.1900889676132017\n",
      "Iteration: 545000, loss: 0.10170791939546711, gradient norm: 0.450758090778284\n",
      "Iteration: 546000, loss: 0.10170762555610059, gradient norm: 0.2937506498166368\n",
      "Iteration: 547000, loss: 0.10170729277632165, gradient norm: 0.7809898323616458\n",
      "Iteration: 548000, loss: 0.10170697220099277, gradient norm: 0.30726487311908185\n",
      "Iteration: 549000, loss: 0.1017069238296228, gradient norm: 0.2951232231030897\n",
      "Iteration: 550000, loss: 0.10170614962103793, gradient norm: 0.8377667190750688\n",
      "Iteration: 551000, loss: 0.10170596412129375, gradient norm: 0.4809016326457157\n",
      "Iteration: 552000, loss: 0.10170560966795197, gradient norm: 0.714864074284545\n",
      "Iteration: 553000, loss: 0.10170531921229937, gradient norm: 0.6390508753578898\n",
      "Iteration: 554000, loss: 0.1017049598045938, gradient norm: 0.9353140301590567\n",
      "Iteration: 555000, loss: 0.10170472830823203, gradient norm: 0.32618368948420057\n",
      "Iteration: 556000, loss: 0.1017045331262412, gradient norm: 0.8459959015748346\n",
      "Iteration: 557000, loss: 0.1017038616604369, gradient norm: 0.4175158292948454\n",
      "Iteration: 558000, loss: 0.10170367867174845, gradient norm: 0.7308944578982958\n",
      "Iteration: 559000, loss: 0.10170345309741106, gradient norm: 0.08745299550471672\n",
      "Iteration: 560000, loss: 0.10170323436008895, gradient norm: 0.7348264891699281\n",
      "Iteration: 561000, loss: 0.10170265956258431, gradient norm: 0.07187268209524253\n",
      "Iteration: 562000, loss: 0.10170247931428825, gradient norm: 0.18330290599487922\n",
      "Iteration: 563000, loss: 0.10170203196313264, gradient norm: 0.7312638331280491\n",
      "Iteration: 564000, loss: 0.10170182332068559, gradient norm: 0.7316990232985868\n",
      "Iteration: 565000, loss: 0.10170154860516249, gradient norm: 0.17829400291888733\n",
      "Iteration: 566000, loss: 0.10170119796220183, gradient norm: 0.4599930953851541\n",
      "Iteration: 567000, loss: 0.10170086882748723, gradient norm: 0.21551180584801066\n",
      "Iteration: 568000, loss: 0.10170042526588043, gradient norm: 0.725331667218312\n",
      "Iteration: 569000, loss: 0.1017003212231691, gradient norm: 0.5693894626764605\n",
      "Iteration: 570000, loss: 0.10169973882482185, gradient norm: 0.6485229446814993\n",
      "Iteration: 571000, loss: 0.1016997450805089, gradient norm: 0.6207339973296752\n",
      "Iteration: 572000, loss: 0.10169930505025299, gradient norm: 0.5930456372960731\n",
      "Iteration: 573000, loss: 0.10169915848717806, gradient norm: 0.2500035595778555\n",
      "Iteration: 574000, loss: 0.10169837456484059, gradient norm: 0.5022507955260805\n",
      "Iteration: 575000, loss: 0.10169836323042052, gradient norm: 0.32927524259085256\n",
      "Iteration: 576000, loss: 0.10169810573220314, gradient norm: 0.29164773200384203\n",
      "Iteration: 577000, loss: 0.10169765877269957, gradient norm: 0.7398613242433169\n",
      "Iteration: 578000, loss: 0.10169746908771679, gradient norm: 0.6153377605471099\n",
      "Iteration: 579000, loss: 0.10169709121580124, gradient norm: 0.46859548950141966\n",
      "Iteration: 580000, loss: 0.10169677702324642, gradient norm: 0.5808678685007551\n",
      "Iteration: 581000, loss: 0.10169642795755877, gradient norm: 0.014765632799173999\n",
      "Iteration: 582000, loss: 0.10169611210081618, gradient norm: 0.33277983708121495\n",
      "Iteration: 583000, loss: 0.10169601715264849, gradient norm: 0.0364213225084275\n",
      "Iteration: 584000, loss: 0.10169558735002482, gradient norm: 0.5237196265121091\n",
      "Iteration: 585000, loss: 0.10169528285866739, gradient norm: 0.464806432008816\n",
      "Iteration: 586000, loss: 0.10169491876873936, gradient norm: 0.6790415615148035\n",
      "Iteration: 587000, loss: 0.10169477673595878, gradient norm: 0.4301783555150212\n",
      "Iteration: 588000, loss: 0.10169413306870885, gradient norm: 0.05892426413060332\n",
      "Iteration: 589000, loss: 0.101693975092853, gradient norm: 0.3628268779364369\n",
      "Iteration: 590000, loss: 0.10169392216023128, gradient norm: 0.844397222704307\n",
      "Iteration: 591000, loss: 0.10169337529905474, gradient norm: 0.4669136982918938\n",
      "Iteration: 592000, loss: 0.10169299434114357, gradient norm: 0.2179367498723335\n",
      "Iteration: 593000, loss: 0.10169287011720621, gradient norm: 0.6593708725759456\n",
      "Iteration: 594000, loss: 0.10169255260196251, gradient norm: 0.3072278719198624\n",
      "Iteration: 595000, loss: 0.1016921932547086, gradient norm: 0.23679085091340063\n",
      "Iteration: 596000, loss: 0.10169184510496387, gradient norm: 0.4701025369105228\n",
      "Iteration: 597000, loss: 0.10169158162252016, gradient norm: 0.8841461758464618\n",
      "Iteration: 598000, loss: 0.10169132261126229, gradient norm: 0.21157614212548254\n",
      "Iteration: 599000, loss: 0.10169095626093527, gradient norm: 0.38267138314904997\n",
      "Iteration: 600000, loss: 0.1016907930104801, gradient norm: 0.6723817247034254\n",
      "Iteration: 601000, loss: 0.10169030664952337, gradient norm: 0.5151633725137464\n",
      "Iteration: 602000, loss: 0.10169014842674025, gradient norm: 0.26692076694934186\n",
      "Iteration: 603000, loss: 0.10168965728811358, gradient norm: 1.167584748275193\n",
      "Iteration: 604000, loss: 0.10168953204910289, gradient norm: 0.11435381772191941\n",
      "Iteration: 605000, loss: 0.10168911571443351, gradient norm: 0.28283733320046783\n",
      "Iteration: 606000, loss: 0.10168882635831551, gradient norm: 0.3017090870138227\n",
      "Iteration: 607000, loss: 0.10168858010072589, gradient norm: 0.8845158723207455\n",
      "Iteration: 608000, loss: 0.10168821499006779, gradient norm: 0.18443384064595766\n",
      "Iteration: 609000, loss: 0.10168795075639799, gradient norm: 0.4726132541647645\n",
      "Iteration: 610000, loss: 0.10168772062992082, gradient norm: 0.3661101068151773\n",
      "Iteration: 611000, loss: 0.10168740411210674, gradient norm: 0.2517953173588233\n",
      "Iteration: 612000, loss: 0.10168695743034804, gradient norm: 0.17181257676562312\n",
      "Iteration: 613000, loss: 0.10168667567855928, gradient norm: 0.2956344238612221\n",
      "Iteration: 614000, loss: 0.10168652832200852, gradient norm: 0.25475384145816843\n",
      "Iteration: 615000, loss: 0.10168619103813803, gradient norm: 0.16259592689731414\n",
      "Iteration: 616000, loss: 0.10168582805421295, gradient norm: 0.20169145110812056\n",
      "Iteration: 617000, loss: 0.10168548620838463, gradient norm: 0.35752066876196864\n",
      "Iteration: 618000, loss: 0.10168540062017874, gradient norm: 0.16756787674177306\n",
      "Iteration: 619000, loss: 0.10168483265108011, gradient norm: 0.6275805664682182\n",
      "Iteration: 620000, loss: 0.10168469156152656, gradient norm: 0.4039846685787173\n",
      "Iteration: 621000, loss: 0.10168431254219185, gradient norm: 0.8766481316177089\n",
      "Iteration: 622000, loss: 0.10168414678823962, gradient norm: 0.15293913174571622\n",
      "Iteration: 623000, loss: 0.1016836231241966, gradient norm: 0.10489460672030163\n",
      "Iteration: 624000, loss: 0.10168360828807899, gradient norm: 0.35649526070462595\n",
      "Iteration: 625000, loss: 0.10168318412727713, gradient norm: 1.2544903327822716\n",
      "Iteration: 626000, loss: 0.10168282530676546, gradient norm: 0.029497699270606952\n",
      "Iteration: 627000, loss: 0.10168249382355285, gradient norm: 0.4261915393529122\n",
      "Iteration: 628000, loss: 0.10168224106366636, gradient norm: 0.13767988022851513\n",
      "Iteration: 629000, loss: 0.10168200658905746, gradient norm: 0.6291771600274028\n",
      "Iteration: 630000, loss: 0.10168163212966731, gradient norm: 0.37247032269139696\n",
      "Iteration: 631000, loss: 0.10168138072544178, gradient norm: 0.13151054936702022\n",
      "Iteration: 632000, loss: 0.10168108046841079, gradient norm: 0.5787291936102583\n",
      "Iteration: 633000, loss: 0.10168085023132059, gradient norm: 0.6065978347651562\n",
      "Iteration: 634000, loss: 0.10168041998903089, gradient norm: 0.554337489604451\n",
      "Iteration: 635000, loss: 0.10168021736205181, gradient norm: 0.86937103208742\n",
      "Iteration: 636000, loss: 0.10167984099730965, gradient norm: 0.42532796202216233\n"
     ]
    }
   ],
   "source": [
    "_, _ = train(dummy_network, torch_dataset_inputs, torch_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = second_order_opt(final_weights, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax_loss(final_weights), jnp.linalg.norm(jax_grad(jax_loss)(final_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1026920579402963 0.10144003755288118\n",
      "9.95016526425352e-06 0.07283338667347572\n",
      "13.566518233982933\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "new_weights_end = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                  trace[-1][5].reshape(H_student + 1)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "\n",
    "print(jax_loss(weights), jax_loss(new_weights_end))\n",
    "print(jnp.linalg.norm(jax_grad(jax_loss)(weights)), jnp.linalg.norm(jax_grad(jax_loss)(new_weights_end)))\n",
    "print(np.linalg.norm(new_weights_end - new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 0\n",
    "PATH = \"local_min_model.pt\"\n",
    "LOSS = 0.4\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': dummy_network.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
