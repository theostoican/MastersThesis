{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: jax[cpu] in ./.local/lib/python3.6/site-packages (0.2.17)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum in ./.local/lib/python3.6/site-packages (from jax[cpu]) (3.3.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /opt/conda/lib/python3.6/site-packages (from jax[cpu]) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from jax[cpu]) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: jaxlib==0.1.68; extra == \"cpu\" in ./.local/lib/python3.6/site-packages (from jax[cpu]) (0.1.68)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from absl-py->jax[cpu]) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: flatbuffers<3.0,>=1.12 in ./.local/lib/python3.6/site-packages (from jaxlib==0.1.68; extra == \"cpu\"->jax[cpu]) (2.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.6/site-packages (from jaxlib==0.1.68; extra == \"cpu\"->jax[cpu]) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import nlopt\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "np.set_printoptions(precision=32)\n",
    "\n",
    "!pip install --upgrade \"jax[cpu]\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import nn as jax_nn\n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "jnp.set_printoptions(precision=32) \n",
    "from jax import jacfwd, jacrev\n",
    "from jax import grad as jax_grad\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H_student, D_out = 1, 10, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1_principal</th>\n",
       "      <th>2_principal</th>\n",
       "      <th>3_principal</th>\n",
       "      <th>4_principal</th>\n",
       "      <th>5_principal</th>\n",
       "      <th>6_principal</th>\n",
       "      <th>7_principal</th>\n",
       "      <th>8_principal</th>\n",
       "      <th>9_principal</th>\n",
       "      <th>10_principal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.140478</td>\n",
       "      <td>-5.226451</td>\n",
       "      <td>3.886993</td>\n",
       "      <td>-0.901512</td>\n",
       "      <td>4.929209</td>\n",
       "      <td>2.036187</td>\n",
       "      <td>4.706960</td>\n",
       "      <td>-4.764459</td>\n",
       "      <td>0.238225</td>\n",
       "      <td>-1.459020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19.292333</td>\n",
       "      <td>6.033014</td>\n",
       "      <td>1.308196</td>\n",
       "      <td>-2.383076</td>\n",
       "      <td>3.095021</td>\n",
       "      <td>-1.794193</td>\n",
       "      <td>-3.770784</td>\n",
       "      <td>0.148453</td>\n",
       "      <td>-4.154969</td>\n",
       "      <td>-4.295380</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-7.644504</td>\n",
       "      <td>-1.705801</td>\n",
       "      <td>2.289336</td>\n",
       "      <td>2.241256</td>\n",
       "      <td>5.094750</td>\n",
       "      <td>-4.152694</td>\n",
       "      <td>-1.011677</td>\n",
       "      <td>1.733929</td>\n",
       "      <td>0.422061</td>\n",
       "      <td>-0.072606</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.474206</td>\n",
       "      <td>5.836146</td>\n",
       "      <td>2.008588</td>\n",
       "      <td>4.271250</td>\n",
       "      <td>2.378019</td>\n",
       "      <td>2.179969</td>\n",
       "      <td>4.397159</td>\n",
       "      <td>-0.346711</td>\n",
       "      <td>1.018367</td>\n",
       "      <td>5.470587</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26.559575</td>\n",
       "      <td>6.024832</td>\n",
       "      <td>0.933257</td>\n",
       "      <td>-3.012613</td>\n",
       "      <td>9.488500</td>\n",
       "      <td>-2.333748</td>\n",
       "      <td>-6.146737</td>\n",
       "      <td>-1.796978</td>\n",
       "      <td>-4.180035</td>\n",
       "      <td>-5.717939</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  1_principal  2_principal  3_principal  4_principal  \\\n",
       "0           0    -5.140478    -5.226451     3.886993    -0.901512   \n",
       "1           1    19.292333     6.033014     1.308196    -2.383076   \n",
       "2           2    -7.644504    -1.705801     2.289336     2.241256   \n",
       "3           3    -0.474206     5.836146     2.008588     4.271250   \n",
       "4           4    26.559575     6.024832     0.933257    -3.012613   \n",
       "\n",
       "   5_principal  6_principal  7_principal  8_principal  9_principal  \\\n",
       "0     4.929209     2.036187     4.706960    -4.764459     0.238225   \n",
       "1     3.095021    -1.794193    -3.770784     0.148453    -4.154969   \n",
       "2     5.094750    -4.152694    -1.011677     1.733929     0.422061   \n",
       "3     2.378019     2.179969     4.397159    -0.346711     1.018367   \n",
       "4     9.488500    -2.333748    -6.146737    -1.796978    -4.180035   \n",
       "\n",
       "   10_principal  label  \n",
       "0     -1.459020    1.0  \n",
       "1     -4.295380   -1.0  \n",
       "2     -0.072606    1.0  \n",
       "3      5.470587   -1.0  \n",
       "4     -5.717939   -1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mnist/train_10pca.csv', float_precision='round_trip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A customizable student network, initialized using Glorot initialization.\n",
    "class StudentNetwork(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "\n",
    "    D_in: input dimension\n",
    "    H: dimension of hidden layer\n",
    "    D_out: output dimension of the first layer\n",
    "    \"\"\"\n",
    "    super(StudentNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H, bias=True).double()\n",
    "    self.linear2 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear3 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear4 = nn.Linear(H, D_out, bias=True).double()\n",
    "\n",
    "    nn.init.xavier_uniform_(self.linear1.weight)\n",
    "    nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    nn.init.xavier_uniform_(self.linear3.weight)\n",
    "    nn.init.xavier_uniform_(self.linear4.weight)\n",
    "    \n",
    "    nn.init.constant_(self.linear1.bias, 0)\n",
    "    nn.init.constant_(self.linear2.bias, 0)\n",
    "    nn.init.constant_(self.linear3.bias, 0)\n",
    "    nn.init.constant_(self.linear4.bias, 0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = torch.sigmoid(self.linear1(x))\n",
    "    h2 = torch.sigmoid(self.linear2(h1))\n",
    "    h3 = torch.sigmoid(self.linear3(h2))\n",
    "    y_pred = self.linear4(h3)\n",
    "    return h2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = []\n",
    "dataset_labels = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    pca_components = []\n",
    "    for idx_pca_component in range(1, 11):\n",
    "        pca_components.append(row[str(idx_pca_component) + '_principal'])\n",
    "    dataset_inputs.append(pca_components)\n",
    "    dataset_labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_inputs = torch.DoubleTensor(dataset_inputs[:10000]).to(device)\n",
    "torch_dataset_labels = torch.DoubleTensor([dataset_labels[:10000]]).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "jnp_dataset_inputs = jnp.array(dataset_inputs[:10000], dtype=jnp.float64)\n",
    "jnp_dataset_labels = jnp.array(dataset_labels[:10000], dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model = StudentNetwork(D_in, H_student, D_out)\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)\n",
    "\n",
    "checkpoint = torch.load(\"model_1e-6.pt\")\n",
    "student_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y = student_model(torch_dataset_inputs)\n",
    "loss = nn.MSELoss()(y, torch_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10269205534607684\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = torch.autograd.grad(loss, student_model.parameters(),\n",
    "                                      retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.950165295930524e-06\n"
     ]
    }
   ],
   "source": [
    "def eval_grad_norm(loss_grad):\n",
    "  cnt = 0\n",
    "  for g in loss_grad:\n",
    "      if cnt == 0:\n",
    "        g_vector = g.contiguous().view(-1)\n",
    "      else:\n",
    "        g_vector = torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "      cnt = 1\n",
    "  grad_norm = torch.norm(g_vector)\n",
    " \n",
    "  return grad_norm.cpu().detach().numpy()\n",
    "\n",
    "print(eval_grad_norm(loss_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(student_model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(student_model.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "weights = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape(H_student * D_in), \n",
    "                  trace[-1][5].reshape(H_student)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1026920579402963 9.950165264252857e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([-5.3945191734651505e-09,  7.4704447537238577e-10,\n",
       "              3.0471841279103927e-09,  3.6846601868945834e-09,\n",
       "              5.0230852672211108e-09,  1.6881852990028928e-08,\n",
       "              3.6429759954769452e-08,  6.4832805391893137e-08,\n",
       "              2.0617937002064219e-07,  3.2936145209763950e-07,\n",
       "              5.0340126290094939e-07,  8.1808164857814475e-07],            dtype=float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(w_layer1, b_layer1,\n",
    "            w_layer2, b_layer2,\n",
    "            w_layer3, b_layer3,\n",
    "            w_out, b_out):\n",
    "  h1 = jax_nn.sigmoid(jnp_dataset_inputs @ jnp.transpose(w_layer1) + b_layer1)\n",
    "  h2 = jax_nn.sigmoid(h1 @ jnp.transpose(w_layer2) + b_layer2)\n",
    "  h3 = jax_nn.sigmoid(h2 @ jnp.transpose(w_layer3) + b_layer3)\n",
    "\n",
    "  return jnp.transpose((h3 @ w_out + b_out).T)\n",
    "\n",
    "def jax_loss(w):\n",
    "  w_layer1 = w[0 : 100].reshape(H_student, D_in)\n",
    "  b_layer1 = w[100 : 110].reshape(H_student)\n",
    "  w_layer2 = w[110 : 210].reshape(H_student, H_student)\n",
    "  b_layer2 = w[210 : 220].reshape(H_student)\n",
    "  if len(w) == 341:\n",
    "    w_layer3 = w[220 : 320].reshape(H_student, H_student)\n",
    "    b_layer3 = w[320 : 330].reshape(H_student)\n",
    "    w_out = w[330 : 340].reshape(H_student, D_out)\n",
    "    b_out = w[340]\n",
    "  else:\n",
    "    w_layer3 = w[220 : 330].reshape(H_student + 1, H_student)\n",
    "    b_layer3 = w[330 : 341].reshape(H_student + 1)\n",
    "    w_out = w[341 : 352].reshape(H_student + 1, D_out)\n",
    "    b_out = w[352]\n",
    "\n",
    "  preds = jnp.transpose(predict(w_layer1, b_layer1,\n",
    "                                w_layer2, b_layer2,\n",
    "                                w_layer3, b_layer3,\n",
    "                                w_out, b_out))\n",
    "  return jnp.mean(jnp.square(preds - jnp_dataset_labels))\n",
    " \n",
    "def jax_loss_second_layer(w):\n",
    "    w_layer1 = w[0 : 100].reshape(H_student, D_in)\n",
    "    b_layer1 = w[100 : 110].reshape(H_student)\n",
    "    if len(w) == 341:\n",
    "        w_layer2 = w[110 : 210].reshape(H_student, H_student)\n",
    "        b_layer2 = w[210 : 220].reshape(H_student)\n",
    "        w_layer3 = w[220 : 320].reshape(H_student, H_student)\n",
    "        b_layer3 = w[320 : 330].reshape(H_student)\n",
    "        w_out = w[330 : 340].reshape(H_student, D_out)\n",
    "        b_out = w[340]\n",
    "    else:\n",
    "        w_layer2 = w[110 : 220].reshape(H_student + 1, H_student)\n",
    "        b_layer2 = w[220 : 231].reshape(H_student + 1)\n",
    "        w_layer3 = w[231 : 341].reshape(H_student, H_student + 1)\n",
    "        b_layer3 = w[341 : 351].reshape(H_student)\n",
    "        w_out = w[351 : 361].reshape(H_student, D_out)\n",
    "        b_out = w[361]\n",
    "\n",
    "    preds = jnp.transpose(predict(w_layer1, b_layer1,\n",
    "                                w_layer2, b_layer2,\n",
    "                                w_layer3, b_layer3,\n",
    "                                w_out, b_out))\n",
    "    return jnp.mean(jnp.square(preds - jnp_dataset_labels))\n",
    "\n",
    "def hessian(f):\n",
    "  return jacfwd(jacrev(f))\n",
    "\n",
    "print(jax_loss(weights), jnp.linalg.norm(jax_grad(jax_loss)(weights)))\n",
    "H = hessian(jax_loss)(weights)\n",
    "H = (H + H.T) / 2.0\n",
    "jnp.linalg.eigh(H)[0][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_obj(weights, grad):\n",
    "  loss_val = jax_loss(weights)\n",
    "  if grad.size > 0:\n",
    "    grad[:] = np.array(jax_grad(jax_loss)(weights), dtype=np.float64)\n",
    "  return np.float64(loss_val)\n",
    "\n",
    "def second_order_opt(weights, maxtime):\n",
    "  opt = nlopt.opt(nlopt.LD_SLSQP, len(weights))\n",
    "  opt.set_lower_bounds([w - 1000 for w in weights])\n",
    "  opt.set_upper_bounds([w + 1000 for w in weights])\n",
    "  opt.set_min_objective(loss_obj)\n",
    "  opt.set_maxtime(maxtime)\n",
    "  # opt.set_xtol_rel(1e-32)\n",
    "  opt.set_initial_step(1e-32)\n",
    "  final_weights = opt.optimize(weights)\n",
    "  return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_weights = second_order_opt(weights, 10)\n",
    "# print(jax_loss(final_weights), jnp.linalg.norm(jax_grad(jax_loss)(final_weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Y matrix for the first neuron on the second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivative_sigmoid = lambda x : jax_nn.sigmoid(x) * \\\n",
    "                                          (1 - jax_nn.sigmoid(x))\n",
    "second_derivative_sigmoid = lambda x : jax_nn.sigmoid(x) * \\\n",
    "                                           ((1 - jax_nn.sigmoid(x)) ** 2)-\\\n",
    "                                           (jax_nn.sigmoid(x) ** 2) *\\\n",
    "                                           (1 - jax_nn.sigmoid(x))\n",
    "\n",
    "w_layer1 = weights[0 : 100].reshape(H_student, D_in)\n",
    "b_layer1 = weights[100 : 110].reshape(H_student)\n",
    "w_layer2 = weights[110 : 210].reshape(H_student, H_student)\n",
    "b_layer2 = weights[210 : 220].reshape(H_student)\n",
    "w_layer3 = weights[220 : 320].reshape(H_student, H_student)\n",
    "b_layer3 = weights[320 : 330].reshape(H_student)\n",
    "w_out = weights[330 : 340].reshape(H_student, D_out)\n",
    "b_out = weights[340]\n",
    "\n",
    "preds = predict(w_layer1, b_layer1,\n",
    "                w_layer2, b_layer2,\n",
    "                                w_layer3, b_layer3,\n",
    "                                w_out, b_out).reshape(10000)\n",
    "\n",
    "second_layer_output, last_layer_output = student_model(torch_dataset_inputs)\n",
    "second_layer_output = second_layer_output.cpu().detach().numpy()\n",
    "# print(y_model.cpu().detach().numpy().reshape(10000).shape, preds.shape)\n",
    "\n",
    "\n",
    "e = preds - jnp_dataset_labels\n",
    "\n",
    "\n",
    "Y = jnp.zeros((11, 11))\n",
    "idx_neuron = 1\n",
    "\n",
    "w_layer3_grads = jax_grad(jax_loss_second_layer)(weights)[220 : 320].reshape(H_student, H_student)[idx_neuron, :]\n",
    "\n",
    "for idx, x in enumerate(second_layer_output):\n",
    "    Y += jnp.dot(w_layer3[idx_neuron, :], w_layer3_grads) * \\\n",
    "         second_derivative_sigmoid(jnp.dot(x, w_layer3[idx_neuron]) + b_layer3[idx_neuron]) * e[idx] * \\\n",
    "         jnp.array(np.append(x, 1)).reshape(len(x) + 1, 1) @ jnp.array(np.append(x, 1)).reshape(1, len(x) + 1)\n",
    "\n",
    "Y /= len(jnp_dataset_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'eigenvalue')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evals, _ = jnp.linalg.eigh(Y)\n",
    "print(len(evals))\n",
    "plt.scatter(np.arange(len(evals)), evals)\n",
    "plt.xlabel('eigenvalue index')\n",
    "plt.ylabel('eigenvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w_layer2 = np.append(np.append(weights[110 : 110 + idx_neuron * 10],\n",
    "                                   np.append(\n",
    "                                       weights[110 + idx_neuron * 10 : 110 + (idx_neuron + 1) * 10],\n",
    "                                       weights[110 + idx_neuron * 10 : 110 + (idx_neuron + 1) * 10])),\n",
    "                         weights[110 + (idx_neuron + 1) * 10 : 210]).reshape(H_student + 1, H_student)\n",
    "new_b_layer2 = np.append(np.append(weights[210 : 210 + idx_neuron],\n",
    "                                   np.append([weights[210 + idx_neuron]],\n",
    "                                            [weights[210 + idx_neuron]])),\n",
    "                         weights[210 + idx_neuron + 1 : 220]).reshape(H_student + 1)\n",
    "\n",
    "new_w_layer3 = np.array([*weights[220 : 220 + idx_neuron], *[0 * weights[220 + idx_neuron]], *[1 * weights[220 + idx_neuron]], *weights[220 + idx_neuron + 1 : 230],\n",
    "               *weights[230 : 230 + idx_neuron], *[0 * weights[230 + idx_neuron]], *[1 * weights[230 + idx_neuron]], *weights[230 + idx_neuron + 1 : 240],\n",
    "*weights[240 : 240 + idx_neuron], *[0 * weights[240 + idx_neuron]], *[1 * weights[240 + idx_neuron]], *weights[240 + idx_neuron + 1 : 250],\n",
    "*weights[250 : 250 + idx_neuron], *[0 * weights[250 + idx_neuron]], *[1 * weights[250 + idx_neuron]], *weights[250 + idx_neuron + 1 : 260],\n",
    "*weights[260 : 260 + idx_neuron], *[0 * weights[260 + idx_neuron]], *[1 * weights[260 + idx_neuron]], *weights[260 + idx_neuron + 1: 270],\n",
    "*weights[270 : 270 + idx_neuron], *[0 * weights[270 + idx_neuron]], *[1 * weights[270 + idx_neuron]], *weights[270 + idx_neuron + 1: 280],\n",
    "*weights[280 : 280 + idx_neuron], *[0 * weights[280 + idx_neuron]], *[1 * weights[280 + idx_neuron]], *weights[280 + idx_neuron + 1: 290],\n",
    "*weights[290 : 290 + idx_neuron], *[0 * weights[290 + idx_neuron]], *[1 * weights[290 + idx_neuron]], *weights[290 + idx_neuron + 1: 300],\n",
    "*weights[300 : 300 + idx_neuron], *[0 * weights[300 + idx_neuron]], *[1 * weights[300 + idx_neuron]], *weights[300 + idx_neuron + 1: 310],\n",
    "*weights[310 : 310 + idx_neuron], *[0 * weights[310 + idx_neuron]], *[1 * weights[310 + idx_neuron]], *weights[310 + idx_neuron + 1: 320]])\n",
    "\n",
    "new_w_layer3 = new_w_layer3.reshape(H_student, H_student + 1)\n",
    "\n",
    "new_b_layer3 = np.append(np.append(weights[320 : 320 + idx_neuron],\n",
    "                                   [weights[320 + idx_neuron]]),\n",
    "                         weights[320 + idx_neuron + 1 : 330]).reshape(H_student)\n",
    "new_w_out = np.append(np.append(weights[330: 330 + idx_neuron],\n",
    "                                [weights[330 + idx_neuron]]),\n",
    "                      weights[330 + idx_neuron + 1 : 340]).reshape(H_student, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = []\n",
    "new_weights = np.append(new_weights, weights[0 : 100])\n",
    "new_weights = np.append(new_weights, weights[100 : 110])\n",
    "\n",
    "# new_weights = np.append(new_weights, weights[110 : 210])\n",
    "# new_weights = np.append(new_weights, weights[210 : 220])\n",
    "\n",
    "new_weights = np.append(new_weights, new_w_layer2)\n",
    "new_weights = np.append(new_weights, new_b_layer2)\n",
    "\n",
    "new_weights = np.append(new_weights, new_w_layer3)\n",
    "new_weights = np.append(new_weights, new_b_layer3)\n",
    "\n",
    "new_weights = np.append(new_weights, new_w_out)\n",
    "new_weights = np.append(new_weights, [weights[340]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1026920579402964\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(jax_loss_second_layer(new_weights))\n",
    "\n",
    "H = hessian(jax_loss_second_layer)(new_weights)\n",
    "H = (H + H.T) / 2.0\n",
    "\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "cnt = 0\n",
    "\n",
    "for eval in evals:\n",
    "    if abs(eval) <= 1e-9:\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyNetwork(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out,\n",
    "               w_layer1, b_layer1,\n",
    "               w_layer2, b_layer2,\n",
    "               w_layer3, b_layer3,\n",
    "               w_out, b_out):\n",
    "    super(DummyNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H, bias=True).double()\n",
    "    self.linear2 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear3 = nn.Linear(H, H + 1, bias=True).double()\n",
    "    self.linear4 = nn.Linear(H + 1, D_out, bias=True).double()\n",
    "    \n",
    "    self.linear1.weight = torch.nn.Parameter(w_layer1)\n",
    "    self.linear2.weight = torch.nn.Parameter(w_layer2)\n",
    "    self.linear3.weight = torch.nn.Parameter(w_layer3)\n",
    "    self.linear4.weight = torch.nn.Parameter(w_out)\n",
    "    \n",
    "    self.linear1.bias = torch.nn.Parameter(b_layer1)\n",
    "    self.linear2.bias = torch.nn.Parameter(b_layer2)\n",
    "    self.linear3.bias = torch.nn.Parameter(b_layer3)\n",
    "    self.linear4.bias = torch.nn.Parameter(b_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = torch.sigmoid(self.linear1(x))\n",
    "    h2 = torch.sigmoid(self.linear2(h1))\n",
    "    h3 = torch.sigmoid(self.linear3(h2))\n",
    "    y_pred = self.linear4(h3)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_network = DummyNetwork(D_in, H_student, D_out, torch.DoubleTensor(w_layer1), torch.DoubleTensor(b_layer1),\n",
    "                            torch.DoubleTensor(w_layer2), torch.DoubleTensor(b_layer2),\n",
    "                            torch.DoubleTensor(new_w_layer3), torch.DoubleTensor(new_b_layer3),\n",
    "                            torch.DoubleTensor(new_w_out.T), torch.DoubleTensor([b_out]).reshape(1, 1))\n",
    "dummy_network = dummy_network.to(device)\n",
    "if device == 'cuda':\n",
    "    dummy_network = torch.nn.DataParallel(dummy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_labels, N = 2 * (10 ** 4), Ninner = 10 ** 3, Nstart = 10,\n",
    "          maxtime = 7, nlopt_threshold = 1e-7,\n",
    "          collect_history = True):\n",
    "  lr = 1e-4\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#   checkpoint = torch.load(\"local_min_model.pt\")\n",
    "#   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  loss_vals = []\n",
    "  trace = []\n",
    "  if collect_history:\n",
    "    trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "  for i in range(1, N + 1):\n",
    "#     if i % 2 == 0:\n",
    "#       optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "#     elif i % 2 == 1:\n",
    "#       optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "#     else:\n",
    "#       optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "    loss_tmp = []\n",
    "    for j in range(1, Ninner + 1):\n",
    "      y = model(x)\n",
    "      loss = loss_fn(y, y_labels)\n",
    "      loss_grad = torch.autograd.grad(loss, model.parameters(),\n",
    "                                      retain_graph=True)\n",
    "      grad_norm = eval_grad_norm(loss_grad)\n",
    "      if grad_norm <= 1e-5 and i > 1:\n",
    "#         print('found it')\n",
    "#         return loss_vals, trace\n",
    "        trace = []\n",
    "        trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(model.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "        new_weights_end = np.append(\n",
    "            np.append(\n",
    "                np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                          trace[-1][1].reshape(H_student)),\n",
    "                np.append(\n",
    "                    np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                              trace[-1][3].reshape(H_student)),\n",
    "                    np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                              trace[-1][5].reshape(H_student + 1)))),\n",
    "            np.append(trace[-1][6][0],\n",
    "                      trace[-1][7][0]))\n",
    "        \n",
    "        H = hessian(jax_loss)(new_weights_end)\n",
    "        H = (H + H.T) / 2.0\n",
    "        evals, _ = jnp.linalg.eigh(H)\n",
    "#         print(evals)\n",
    "        \n",
    "        proper_local_min = True\n",
    "        \n",
    "        for eval in evals:\n",
    "          if eval <= -1e-9:\n",
    "            proper_local_min = False\n",
    "            break\n",
    "                \n",
    "        if proper_local_min:\n",
    "          print('found it' + str(j))\n",
    "          EPOCH = 0\n",
    "          PATH = \"local_min_model.pt\"\n",
    "          LOSS = 0.4\n",
    "\n",
    "          torch.save({\n",
    "                    'epoch': EPOCH,\n",
    "                    'model_state_dict': student_model.state_dict(),\n",
    "                    'loss': LOSS,\n",
    "                    }, PATH)\n",
    "          return loss_vals, trace\n",
    "      loss_tmp.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward(retain_graph=True)\n",
    "      optimizer.step()\n",
    "      if i == 1 and (j % Nstart == 0) and j < Ninner:\n",
    "        loss_vals.append(np.mean(loss_tmp[j - Nstart  : j]))\n",
    "        if collect_history:\n",
    "          trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    loss_vals.append(np.mean(loss_tmp))\n",
    "    if collect_history:\n",
    "      trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    grad_norm = eval_grad_norm(loss_grad)\n",
    "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i,\n",
    "                                                          np.mean(loss_tmp),\n",
    "                                                          grad_norm))\n",
    "\n",
    "#   EPOCH = i\n",
    "#   PATH = \"model.pt\"\n",
    "#   LOSS = 0.4\n",
    "\n",
    "#   torch.save({\n",
    "#             'epoch': EPOCH,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': LOSS,\n",
    "#             }, PATH)\n",
    "  return loss_vals, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the 12 smallest eigenvaleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_evals = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_mu = [[] for i in range(num_evals)]\n",
    "\n",
    "for mu in np.arange(-1, 2, 0.1):\n",
    "    new_w_layer2 = np.append(np.append(weights[110 : 110 + idx_neuron * 10],\n",
    "                                       np.append(\n",
    "                                           weights[110 + idx_neuron * 10 : 110 + (idx_neuron + 1) * 10],\n",
    "                                           weights[110 + idx_neuron * 10 : 110 + (idx_neuron + 1) * 10])),\n",
    "                             weights[110 + (idx_neuron + 1) * 10 : 210]).reshape(H_student + 1, H_student)\n",
    "    new_b_layer2 = np.append(np.append(weights[210 : 210 + idx_neuron],\n",
    "                                       np.append([weights[210 + idx_neuron]],\n",
    "                                                [weights[210 + idx_neuron]])),\n",
    "                             weights[210 + idx_neuron + 1 : 220]).reshape(H_student + 1)\n",
    "\n",
    "    new_w_layer3 = np.array([*weights[220 : 220 + idx_neuron], *[mu * weights[220 + idx_neuron]], *[(1 - mu) * weights[220 + idx_neuron]], *weights[220 + idx_neuron + 1 : 230],\n",
    "                   *weights[230 : 230 + idx_neuron], *[mu * weights[230 + idx_neuron]], *[(1 - mu) * weights[230 + idx_neuron]], *weights[230 + idx_neuron + 1 : 240],\n",
    "    *weights[240 : 240 + idx_neuron], *[mu * weights[240 + idx_neuron]], *[(1 - mu) * weights[240 + idx_neuron]], *weights[240 + idx_neuron + 1 : 250],\n",
    "    *weights[250 : 250 + idx_neuron], *[mu * weights[250 + idx_neuron]], *[(1 - mu) * weights[250 + idx_neuron]], *weights[250 + idx_neuron + 1 : 260],\n",
    "    *weights[260 : 260 + idx_neuron], *[mu * weights[260 + idx_neuron]], *[(1 - mu) * weights[260 + idx_neuron]], *weights[260 + idx_neuron + 1: 270],\n",
    "    *weights[270 : 270 + idx_neuron], *[mu * weights[270 + idx_neuron]], *[(1 - mu) * weights[270 + idx_neuron]], *weights[270 + idx_neuron + 1: 280],\n",
    "    *weights[280 : 280 + idx_neuron], *[mu * weights[280 + idx_neuron]], *[(1 - mu) * weights[280 + idx_neuron]], *weights[280 + idx_neuron + 1: 290],\n",
    "    *weights[290 : 290 + idx_neuron], *[mu * weights[290 + idx_neuron]], *[(1 - mu) * weights[290 + idx_neuron]], *weights[290 + idx_neuron + 1: 300],\n",
    "    *weights[300 : 300 + idx_neuron], *[mu * weights[300 + idx_neuron]], *[(1 - mu) * weights[300 + idx_neuron]], *weights[300 + idx_neuron + 1: 310],\n",
    "    *weights[310 : 310 + idx_neuron], *[mu * weights[310 + idx_neuron]], *[(1 - mu) * weights[310 + idx_neuron]], *weights[310 + idx_neuron + 1: 320]])\n",
    "\n",
    "    new_w_layer3 = new_w_layer3.reshape(H_student, H_student + 1)\n",
    "\n",
    "    new_b_layer3 = np.append(np.append(weights[320 : 320 + idx_neuron],\n",
    "                                       [weights[320 + idx_neuron]]),\n",
    "                             weights[320 + idx_neuron + 1 : 330]).reshape(H_student)\n",
    "    new_w_out = np.append(np.append(weights[330: 330 + idx_neuron],\n",
    "                                    [weights[330 + idx_neuron]]),\n",
    "                          weights[330 + idx_neuron + 1 : 340]).reshape(H_student, D_out)\n",
    "    \n",
    "#     new_w_layer3 = np.append(np.append(weights[220 : 220 + idx_neuron * 10],\n",
    "#                                        np.append(\n",
    "#                                            weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10],\n",
    "#                                            weights[220 + idx_neuron * 10 : 220 + (idx_neuron + 1) * 10])),\n",
    "#                              weights[220 + (idx_neuron + 1) * 10 : 320]).reshape(H_student + 1, H_student)\n",
    "#     new_b_layer3 = np.append(np.append(weights[320 : 320 + idx_neuron],\n",
    "#                                        np.append([weights[320 + idx_neuron]],\n",
    "#                                                 [weights[320 + idx_neuron]])),\n",
    "#                              weights[320 + idx_neuron + 1 : 330]).reshape(H_student + 1)\n",
    "#     new_w_out = np.append(np.append(weights[330: 330 + idx_neuron],\n",
    "#                                     np.append([mu * weights[330 + idx_neuron]],\n",
    "#                                               [(1 - mu) * weights[330 + idx_neuron]])),\n",
    "#                       weights[330 + idx_neuron + 1 : 340]).reshape(H_student + 1, D_out)    \n",
    "    \n",
    "    \n",
    "    new_weights = []\n",
    "    new_weights = np.append(new_weights, weights[0 : 100])\n",
    "    new_weights = np.append(new_weights, weights[100 : 110])\n",
    "\n",
    "#     new_weights = np.append(new_weights, weights[110 : 210])\n",
    "#     new_weights = np.append(new_weights, weights[210 : 220])\n",
    "\n",
    "    new_weights = np.append(new_weights, new_w_layer2)\n",
    "    new_weights = np.append(new_weights, new_b_layer2)\n",
    "\n",
    "    new_weights = np.append(new_weights, new_w_layer3)\n",
    "    new_weights = np.append(new_weights, new_b_layer3)\n",
    "\n",
    "    new_weights = np.append(new_weights, new_w_out)\n",
    "    new_weights = np.append(new_weights, [weights[340]])\n",
    "    \n",
    "    H = hessian(jax_loss_second_layer)(new_weights)\n",
    "    H = (H + H.T) / 2.0\n",
    "    \n",
    "    evals, _ = jnp.linalg.eigh(H)\n",
    "    for i in range(num_evals):\n",
    "        evals_mu[i].append(evals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_evals):\n",
    "    plt.plot(np.arange(-1, 2, 0.1), evals_mu[i], label=r'$\\lambda$' + str(i + 1))\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.axhline(y = 0, color = 'r', linestyle = 'dotted')\n",
    "plt.axvline(x = 0, color = 'r', linestyle = 'dotted')\n",
    "plt.axvline(x = 1, color = 'r', linestyle = 'dotted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = []\n",
    "losses = []\n",
    "for mu in np.arange(-3, 4, 0.1):\n",
    "    print('mu:', mu)\n",
    "    new_w_layer3 = np.append(weights[220 : 230], weights[220 : 320]).reshape(H_student + 1, H_student)\n",
    "    new_b_layer3 = np.append([weights[320]], weights[320 : 330]).reshape(H_student + 1)\n",
    "    new_w_out = np.append([mu * weights[330], (1 - mu) * weights[330]], weights[331 : 340]).reshape(H_student + 1, D_out)\n",
    "    \n",
    "    mu, sigma = 0, 0.0001 # mean and standard deviation\n",
    "    perturbation = np.random.normal(mu, sigma, len(new_weights))\n",
    "    \n",
    "    perturbed_w_layer1 = w_layer1 + perturbation[0 : w_layer1.size].reshape(D_in, H_student)\n",
    "    offset = len(w_layer1)\n",
    "    \n",
    "    perturbed_b_layer1 = b_layer1 + perturbation[offset : offset + len(b_layer1)].reshape(H_student)\n",
    "    offset += len(b_layer1)\n",
    "    \n",
    "    perturbed_w_layer2 = w_layer2 + perturbation[offset : offset + w_layer2.size].reshape(H_student, H_student)\n",
    "    offset += len(w_layer2)\n",
    "    \n",
    "    perturbed_b_layer2 = b_layer2 + perturbation[offset : offset + len(b_layer2)].reshape(H_student)\n",
    "    offset += len(b_layer2)\n",
    "    \n",
    "    pertured_w_layer3 = new_w_layer3 + perturbation[offset : offset + new_w_layer3.size].reshape(H_student + 1, H_student)\n",
    "    offset += len(new_w_layer3)\n",
    "    \n",
    "    perturbed_b_layer3 = new_b_layer3 + perturbation[offset : offset + len(new_b_layer3)].reshape(H_student + 1)\n",
    "    offset += len(new_b_layer3)\n",
    "    \n",
    "    \n",
    "    perturbed_w_out = new_w_out + perturbation[offset : offset + new_w_out.size].reshape(H_student + 1, D_out)\n",
    "\n",
    "\n",
    "    offset += len(new_w_out)\n",
    "    \n",
    "    perturbed_b_out = b_out + perturbation[offset : offset + 1]\n",
    "    \n",
    "    dummy_network = DummyNetwork(D_in, H_student, D_out,\n",
    "                                 torch.DoubleTensor(perturbed_w_layer1), torch.DoubleTensor(perturbed_b_layer1),\n",
    "                                 torch.DoubleTensor(perturbed_w_layer2), torch.DoubleTensor(perturbed_b_layer2),\n",
    "                                 torch.DoubleTensor(pertured_w_layer3), torch.DoubleTensor(perturbed_b_layer3),\n",
    "                                 torch.DoubleTensor(perturbed_w_out.T), torch.DoubleTensor([perturbed_b_out]))\n",
    "    dummy_network = dummy_network.to(device)\n",
    "    if device == 'cuda':\n",
    "        dummy_network = torch.nn.DataParallel(dummy_network)\n",
    "    \n",
    "    loss_vals, trace = train(dummy_network,\n",
    "                              torch_dataset_inputs,\n",
    "                              torch_dataset_labels)\n",
    "    plt.plot(loss_vals)\n",
    "    break\n",
    "    \n",
    "    trace = []\n",
    "    trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "    new_weights_end = np.append(\n",
    "        np.append(\n",
    "            np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                      trace[-1][1].reshape(H_student)),\n",
    "            np.append(\n",
    "                np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                      trace[-1][3].reshape(H_student)),\n",
    "                np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                      trace[-1][5].reshape(H_student + 1)))),\n",
    "        np.append(trace[-1][6][0],\n",
    "                  trace[-1][7][0]))\n",
    "\n",
    "\n",
    "    \n",
    "    mus.append(mu)\n",
    "    losses.append(jax_loss(new_weights_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "new_weights_end = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                  trace[-1][5].reshape(H_student + 1)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "print(jax_loss(new_weights_end), jnp.linalg.norm(jax_grad(jax_loss)(new_weights_end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])\n",
    "print(jnp.linalg.norm(new_weights_end - new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, loss: 0.10270604704257065, gradient norm: 0.011078588217699798\n",
      "Iteration: 2000, loss: 0.10269743060484009, gradient norm: 0.2875029134228121\n",
      "Iteration: 3000, loss: 0.10269770154291535, gradient norm: 0.7109272681656998\n",
      "Iteration: 4000, loss: 0.1026978519181091, gradient norm: 0.3418418293695122\n",
      "Iteration: 5000, loss: 0.10269775888767396, gradient norm: 0.36774263792057\n",
      "Iteration: 6000, loss: 0.10269744748612614, gradient norm: 0.6632348194846205\n",
      "Iteration: 7000, loss: 0.10269732537999365, gradient norm: 0.5554117927262803\n",
      "Iteration: 8000, loss: 0.10269685543982726, gradient norm: 0.510397573260132\n",
      "Iteration: 9000, loss: 0.10269578054651307, gradient norm: 1.1030923270214137\n",
      "Iteration: 10000, loss: 0.10269431888365248, gradient norm: 1.1670429564198743\n",
      "Iteration: 11000, loss: 0.10269216896995641, gradient norm: 0.14696966531964778\n",
      "Iteration: 12000, loss: 0.10268967014738001, gradient norm: 0.41511374564268033\n",
      "Iteration: 13000, loss: 0.10268688178360404, gradient norm: 1.4158136131910013\n",
      "Iteration: 14000, loss: 0.10268359523960578, gradient norm: 0.871337420098325\n",
      "Iteration: 15000, loss: 0.10267929761800253, gradient norm: 0.6515419804976587\n",
      "Iteration: 16000, loss: 0.10267490777721197, gradient norm: 0.47227162129148964\n",
      "Iteration: 17000, loss: 0.10266985691113954, gradient norm: 0.13841877629768365\n",
      "Iteration: 18000, loss: 0.10266439809847558, gradient norm: 1.3449515518488864\n",
      "Iteration: 19000, loss: 0.10265842671078497, gradient norm: 0.2569863073101318\n",
      "Iteration: 20000, loss: 0.10265235546286242, gradient norm: 0.2926937080088899\n",
      "Iteration: 21000, loss: 0.10264581228704593, gradient norm: 0.5079050981271134\n",
      "Iteration: 22000, loss: 0.10263913366029516, gradient norm: 0.6424113173415124\n",
      "Iteration: 23000, loss: 0.10263250706218094, gradient norm: 0.7328833508941015\n",
      "Iteration: 24000, loss: 0.10262532221060984, gradient norm: 0.4452260024020165\n",
      "Iteration: 25000, loss: 0.10261791009077266, gradient norm: 0.21610764520135683\n",
      "Iteration: 26000, loss: 0.10261041468741382, gradient norm: 0.9801486819698362\n",
      "Iteration: 27000, loss: 0.10260201100652802, gradient norm: 0.3789314415159555\n",
      "Iteration: 28000, loss: 0.10259337452469737, gradient norm: 0.503982208291663\n",
      "Iteration: 29000, loss: 0.10258386820864882, gradient norm: 0.7474813417656848\n",
      "Iteration: 30000, loss: 0.10257419363582115, gradient norm: 0.34855269233025427\n",
      "Iteration: 31000, loss: 0.10256371962450005, gradient norm: 0.5837323066618978\n",
      "Iteration: 32000, loss: 0.10255293085139135, gradient norm: 0.411713350973921\n",
      "Iteration: 33000, loss: 0.10254133736417254, gradient norm: 0.12223201149178518\n",
      "Iteration: 34000, loss: 0.10252971069990355, gradient norm: 0.4248931535447859\n",
      "Iteration: 35000, loss: 0.10251762986309076, gradient norm: 0.3901204724606226\n",
      "Iteration: 36000, loss: 0.10250596501867128, gradient norm: 1.0517270227564919\n",
      "Iteration: 37000, loss: 0.1024934984236642, gradient norm: 0.32977548092453485\n",
      "Iteration: 38000, loss: 0.10248146984141193, gradient norm: 0.6472029317888678\n",
      "Iteration: 39000, loss: 0.10246934743236999, gradient norm: 0.24821782378141405\n",
      "Iteration: 40000, loss: 0.10245707626981851, gradient norm: 0.15875363144260943\n",
      "Iteration: 41000, loss: 0.10244481786745833, gradient norm: 0.41724209869781176\n",
      "Iteration: 42000, loss: 0.10243304379170859, gradient norm: 0.18181892095649851\n",
      "Iteration: 43000, loss: 0.10242146666326923, gradient norm: 0.3930571971106035\n",
      "Iteration: 44000, loss: 0.10241047663870131, gradient norm: 0.43051655071297223\n",
      "Iteration: 45000, loss: 0.10240004100789056, gradient norm: 0.09058884183804389\n",
      "Iteration: 46000, loss: 0.10238992522797062, gradient norm: 0.3981511721578662\n",
      "Iteration: 47000, loss: 0.10238047165803518, gradient norm: 0.30844768173558673\n",
      "Iteration: 48000, loss: 0.10237117515761585, gradient norm: 0.6845812777925142\n",
      "Iteration: 49000, loss: 0.10236244569351367, gradient norm: 0.20882793189654716\n",
      "Iteration: 50000, loss: 0.10235285562532721, gradient norm: 0.46579257462536916\n",
      "Iteration: 51000, loss: 0.10234126761368599, gradient norm: 0.19808155786568513\n",
      "Iteration: 52000, loss: 0.10232951163367235, gradient norm: 0.3257103847660115\n",
      "Iteration: 53000, loss: 0.10231972468196597, gradient norm: 0.42352000403149315\n",
      "Iteration: 54000, loss: 0.10231021079273753, gradient norm: 0.7442844353569317\n",
      "Iteration: 55000, loss: 0.1023020074665949, gradient norm: 0.4690929578009973\n",
      "Iteration: 56000, loss: 0.10229341119689099, gradient norm: 0.561205152242326\n",
      "Iteration: 57000, loss: 0.10228563577845678, gradient norm: 0.2744238846584102\n",
      "Iteration: 58000, loss: 0.10227822627225139, gradient norm: 0.36141458769557716\n",
      "Iteration: 59000, loss: 0.10227108467691096, gradient norm: 0.9438675640262018\n",
      "Iteration: 60000, loss: 0.10226390458409007, gradient norm: 0.6074682828805645\n",
      "Iteration: 61000, loss: 0.10225728646259077, gradient norm: 0.9776455947000441\n",
      "Iteration: 62000, loss: 0.10225051237116983, gradient norm: 1.0368813850471763\n",
      "Iteration: 63000, loss: 0.10224430678866275, gradient norm: 0.2797121518184087\n",
      "Iteration: 64000, loss: 0.10223811108409933, gradient norm: 0.7578005393573621\n",
      "Iteration: 65000, loss: 0.10223206366029805, gradient norm: 0.7723572304123241\n",
      "Iteration: 66000, loss: 0.10222606449098644, gradient norm: 0.25408563859742517\n",
      "Iteration: 67000, loss: 0.10221997943181574, gradient norm: 1.1998285488285474\n",
      "Iteration: 68000, loss: 0.10221362463679662, gradient norm: 0.4188052653342016\n",
      "Iteration: 69000, loss: 0.10220702220522344, gradient norm: 0.5584236460337187\n",
      "Iteration: 70000, loss: 0.1022004360935799, gradient norm: 0.49910377935168254\n",
      "Iteration: 71000, loss: 0.10219384365715366, gradient norm: 0.31602299446092275\n",
      "Iteration: 72000, loss: 0.10218810290522796, gradient norm: 0.6398154078017799\n",
      "Iteration: 73000, loss: 0.10218298248906245, gradient norm: 0.2859228926162487\n",
      "Iteration: 74000, loss: 0.10217785171599081, gradient norm: 0.320624984852992\n",
      "Iteration: 75000, loss: 0.10217363593823378, gradient norm: 0.7253530854584602\n",
      "Iteration: 76000, loss: 0.10216930558491294, gradient norm: 0.26319033442737255\n",
      "Iteration: 77000, loss: 0.1021652150956325, gradient norm: 0.43053282695099987\n",
      "Iteration: 78000, loss: 0.10216159640745169, gradient norm: 0.5107187282871855\n",
      "Iteration: 79000, loss: 0.10215786105709589, gradient norm: 0.6384763311239396\n",
      "Iteration: 80000, loss: 0.10215448643826087, gradient norm: 0.34392993474986727\n",
      "Iteration: 81000, loss: 0.1021511616248903, gradient norm: 0.05599000595008253\n",
      "Iteration: 82000, loss: 0.10214825281914992, gradient norm: 0.47126709727711097\n",
      "Iteration: 83000, loss: 0.1021449779384028, gradient norm: 1.6493898926178927\n",
      "Iteration: 84000, loss: 0.1021421090498711, gradient norm: 0.5230959778548966\n",
      "Iteration: 85000, loss: 0.10213924879579273, gradient norm: 0.8739834952871922\n",
      "Iteration: 86000, loss: 0.10213654796987769, gradient norm: 0.6921990844910513\n",
      "Iteration: 87000, loss: 0.10213364303518484, gradient norm: 0.6816829032472262\n",
      "Iteration: 88000, loss: 0.10213108861368508, gradient norm: 0.8909086603701372\n",
      "Iteration: 89000, loss: 0.10212848458079524, gradient norm: 0.34203347484749025\n",
      "Iteration: 90000, loss: 0.10212606600045913, gradient norm: 0.34559910702774543\n",
      "Iteration: 91000, loss: 0.10212358164356869, gradient norm: 0.4727779525537908\n",
      "Iteration: 92000, loss: 0.10212118473333608, gradient norm: 0.07464613449669187\n",
      "Iteration: 93000, loss: 0.10211879724509726, gradient norm: 0.5412672750950294\n",
      "Iteration: 94000, loss: 0.10211651604868795, gradient norm: 0.2897740027084422\n",
      "Iteration: 95000, loss: 0.1021144571612324, gradient norm: 0.5143461948435331\n",
      "Iteration: 96000, loss: 0.10211198055535954, gradient norm: 0.2018261296023081\n",
      "Iteration: 97000, loss: 0.10210986626759741, gradient norm: 0.7216041916185869\n",
      "Iteration: 98000, loss: 0.1021078766390365, gradient norm: 0.6739995320952616\n",
      "Iteration: 99000, loss: 0.10210560110785535, gradient norm: 0.7615375580561377\n",
      "Iteration: 100000, loss: 0.10210362471559552, gradient norm: 0.37153641907170015\n",
      "Iteration: 101000, loss: 0.10210155481585437, gradient norm: 0.3482810576272231\n",
      "Iteration: 102000, loss: 0.10209969725564413, gradient norm: 0.4488788195145567\n",
      "Iteration: 103000, loss: 0.10209751877665904, gradient norm: 0.22096966760338294\n",
      "Iteration: 104000, loss: 0.10209583405785441, gradient norm: 0.29856225118036\n",
      "Iteration: 105000, loss: 0.10209385946016346, gradient norm: 0.35977367581883934\n",
      "Iteration: 106000, loss: 0.1020920470466886, gradient norm: 0.7206668679294013\n",
      "Iteration: 107000, loss: 0.10209035670229373, gradient norm: 0.8467291975453103\n",
      "Iteration: 108000, loss: 0.10208836754116582, gradient norm: 0.47594836705858407\n",
      "Iteration: 109000, loss: 0.10208664370330732, gradient norm: 0.025916431555236655\n",
      "Iteration: 110000, loss: 0.1020848941154907, gradient norm: 0.3079264063874447\n",
      "Iteration: 111000, loss: 0.10208327713794554, gradient norm: 0.506629883910621\n",
      "Iteration: 112000, loss: 0.10208162448903577, gradient norm: 0.36008557463824725\n",
      "Iteration: 113000, loss: 0.10207960466172397, gradient norm: 0.27290520969269694\n",
      "Iteration: 114000, loss: 0.10207800323967701, gradient norm: 0.21347431669045633\n",
      "Iteration: 115000, loss: 0.1020764032117782, gradient norm: 0.6525888112883291\n",
      "Iteration: 116000, loss: 0.10207463606247046, gradient norm: 0.11223916324329149\n",
      "Iteration: 117000, loss: 0.10207282997700377, gradient norm: 0.17749033707941017\n",
      "Iteration: 118000, loss: 0.1020683038031014, gradient norm: 0.480548951043922\n",
      "Iteration: 119000, loss: 0.10206451876548132, gradient norm: 0.654551753170205\n",
      "Iteration: 120000, loss: 0.10206236080343027, gradient norm: 0.9663922686692941\n",
      "Iteration: 121000, loss: 0.10206031165584047, gradient norm: 1.149062558098099\n",
      "Iteration: 122000, loss: 0.10205857060511388, gradient norm: 0.4664973462772034\n",
      "Iteration: 123000, loss: 0.10205682362440326, gradient norm: 1.2329057827432395\n",
      "Iteration: 124000, loss: 0.10205526858141227, gradient norm: 0.5882825926580791\n",
      "Iteration: 125000, loss: 0.10205312525503127, gradient norm: 0.5347397483339129\n",
      "Iteration: 126000, loss: 0.10205188241794233, gradient norm: 0.1804920476326297\n",
      "Iteration: 127000, loss: 0.10205010831195153, gradient norm: 0.05551392046007058\n",
      "Iteration: 128000, loss: 0.10204871088951106, gradient norm: 0.5241299675913816\n",
      "Iteration: 129000, loss: 0.10204677913237345, gradient norm: 0.8745881757427906\n",
      "Iteration: 130000, loss: 0.10204542350684877, gradient norm: 0.2181382334977087\n",
      "Iteration: 131000, loss: 0.10204360499801317, gradient norm: 0.7537507809962626\n",
      "Iteration: 132000, loss: 0.10204227111716699, gradient norm: 0.36420778588787234\n",
      "Iteration: 133000, loss: 0.10204042160777849, gradient norm: 0.9493621290106166\n",
      "Iteration: 134000, loss: 0.10203900805242531, gradient norm: 0.4729938512730869\n",
      "Iteration: 135000, loss: 0.10203743288287283, gradient norm: 0.45019006941524126\n",
      "Iteration: 136000, loss: 0.1020356941891014, gradient norm: 1.0555752375291512\n",
      "Iteration: 137000, loss: 0.10203409209533328, gradient norm: 0.19853979631698068\n",
      "Iteration: 138000, loss: 0.10203250448275324, gradient norm: 0.3351975355071201\n",
      "Iteration: 139000, loss: 0.10203075960753517, gradient norm: 0.42378582954115424\n",
      "Iteration: 140000, loss: 0.10202887944794312, gradient norm: 0.5677870663354203\n",
      "Iteration: 141000, loss: 0.10202744290736097, gradient norm: 0.7123088472749601\n",
      "Iteration: 142000, loss: 0.10202536234598196, gradient norm: 0.5114475122753598\n",
      "Iteration: 143000, loss: 0.10202373179074557, gradient norm: 0.44840922558267854\n",
      "Iteration: 144000, loss: 0.10202186255871913, gradient norm: 1.1566790072937834\n",
      "Iteration: 145000, loss: 0.10201973590913585, gradient norm: 0.555047258031012\n",
      "Iteration: 146000, loss: 0.10201776581571695, gradient norm: 0.4929265732366423\n",
      "Iteration: 147000, loss: 0.10201562834650818, gradient norm: 0.5759991838581163\n",
      "Iteration: 148000, loss: 0.10201334248396114, gradient norm: 0.16165569154018228\n",
      "Iteration: 149000, loss: 0.10201093229418247, gradient norm: 0.21234236374484566\n",
      "Iteration: 150000, loss: 0.10200843217588508, gradient norm: 0.12511268190496197\n",
      "Iteration: 151000, loss: 0.1020057233275871, gradient norm: 0.6120476849577857\n",
      "Iteration: 152000, loss: 0.10200285744423587, gradient norm: 0.36873694030922044\n",
      "Iteration: 153000, loss: 0.10199968667098136, gradient norm: 0.18778285632295105\n",
      "Iteration: 154000, loss: 0.1019964037566694, gradient norm: 0.29346105316089544\n",
      "Iteration: 155000, loss: 0.10199304740056314, gradient norm: 0.7126638077863613\n",
      "Iteration: 156000, loss: 0.10198963546825698, gradient norm: 0.2402685223598708\n",
      "Iteration: 157000, loss: 0.10198629385186862, gradient norm: 0.381208532346126\n",
      "Iteration: 158000, loss: 0.10198269713951325, gradient norm: 0.24795217874582262\n",
      "Iteration: 159000, loss: 0.10197939899482944, gradient norm: 0.9772668774926995\n",
      "Iteration: 160000, loss: 0.10197625225243977, gradient norm: 0.7185359381805969\n",
      "Iteration: 161000, loss: 0.10197318583716156, gradient norm: 0.3572722465169733\n",
      "Iteration: 162000, loss: 0.10197024088504093, gradient norm: 0.8494301173997448\n",
      "Iteration: 163000, loss: 0.10196735428614909, gradient norm: 0.5141392199444079\n",
      "Iteration: 164000, loss: 0.10196452998552366, gradient norm: 0.586804622837574\n",
      "Iteration: 165000, loss: 0.10196195460987925, gradient norm: 0.5693116715254991\n",
      "Iteration: 166000, loss: 0.10195940727705877, gradient norm: 0.11970671583634862\n",
      "Iteration: 167000, loss: 0.10195709045730954, gradient norm: 0.4708446763809395\n",
      "Iteration: 168000, loss: 0.10195445877519796, gradient norm: 0.6151984421979066\n",
      "Iteration: 169000, loss: 0.10195232967537303, gradient norm: 0.8384946876167276\n",
      "Iteration: 170000, loss: 0.10195017328670722, gradient norm: 0.6379343191248937\n",
      "Iteration: 171000, loss: 0.10194805025977195, gradient norm: 0.8839584397799123\n",
      "Iteration: 172000, loss: 0.10194597396325862, gradient norm: 0.1807583132129695\n",
      "Iteration: 173000, loss: 0.10194411265221205, gradient norm: 0.11276371741540824\n",
      "Iteration: 174000, loss: 0.10194203660219094, gradient norm: 0.30209310011353835\n",
      "Iteration: 175000, loss: 0.10194048641311529, gradient norm: 0.4208988644709388\n",
      "Iteration: 176000, loss: 0.10193851557665912, gradient norm: 0.5318279471763587\n",
      "Iteration: 177000, loss: 0.10193686893655834, gradient norm: 0.08529480691219778\n",
      "Iteration: 178000, loss: 0.10193521795807436, gradient norm: 1.1018778663227315\n",
      "Iteration: 179000, loss: 0.10193362668571791, gradient norm: 0.3191065917354291\n",
      "Iteration: 180000, loss: 0.10193202253337477, gradient norm: 0.8786418451330948\n",
      "Iteration: 181000, loss: 0.10193065871060594, gradient norm: 0.11695810023716335\n",
      "Iteration: 182000, loss: 0.10192911923819772, gradient norm: 0.6189801331995708\n",
      "Iteration: 183000, loss: 0.1019276777168113, gradient norm: 0.2998567896290818\n",
      "Iteration: 184000, loss: 0.10192628838606382, gradient norm: 0.44095385145317184\n",
      "Iteration: 185000, loss: 0.10192503472404477, gradient norm: 0.639808148024257\n",
      "Iteration: 186000, loss: 0.10192378258895962, gradient norm: 0.4641246164689502\n",
      "Iteration: 187000, loss: 0.10192242279041624, gradient norm: 0.7299644482599449\n",
      "Iteration: 188000, loss: 0.10192118624954176, gradient norm: 0.4551687196487254\n",
      "Iteration: 189000, loss: 0.1019199929337649, gradient norm: 0.44194737012910745\n",
      "Iteration: 190000, loss: 0.1019189869346759, gradient norm: 0.6813978819133816\n",
      "Iteration: 191000, loss: 0.10191775152246117, gradient norm: 0.500663921637436\n",
      "Iteration: 192000, loss: 0.101916573229308, gradient norm: 0.22751577536408255\n",
      "Iteration: 193000, loss: 0.10191556453564327, gradient norm: 0.4015501632627795\n",
      "Iteration: 194000, loss: 0.1019144184860956, gradient norm: 0.3573893365790173\n",
      "Iteration: 195000, loss: 0.10191341553222369, gradient norm: 0.46644194027196834\n",
      "Iteration: 196000, loss: 0.10191247115519585, gradient norm: 0.7105289573512641\n",
      "Iteration: 197000, loss: 0.10191142938909492, gradient norm: 0.6214945006254665\n",
      "Iteration: 198000, loss: 0.10191046658229962, gradient norm: 0.24137580131869138\n",
      "Iteration: 199000, loss: 0.101909486053612, gradient norm: 0.4645070842474534\n",
      "Iteration: 200000, loss: 0.1019084962066381, gradient norm: 0.5232231001010076\n",
      "Iteration: 201000, loss: 0.1019078727109833, gradient norm: 0.6770533315419212\n",
      "Iteration: 202000, loss: 0.10190663643724368, gradient norm: 0.4675196033017038\n",
      "Iteration: 203000, loss: 0.10190595043221737, gradient norm: 0.6555120344596874\n",
      "Iteration: 204000, loss: 0.1019049242619638, gradient norm: 0.2818241743004178\n",
      "Iteration: 205000, loss: 0.10190414606467088, gradient norm: 0.3967429661800652\n",
      "Iteration: 206000, loss: 0.10190336928615137, gradient norm: 0.372569533874115\n",
      "Iteration: 207000, loss: 0.10190255457132084, gradient norm: 0.2858518153362849\n",
      "Iteration: 208000, loss: 0.10190150449123613, gradient norm: 0.10699147523187921\n",
      "Iteration: 209000, loss: 0.10190095724882349, gradient norm: 0.45549175347354137\n",
      "Iteration: 210000, loss: 0.10189995929091401, gradient norm: 0.9017495785351133\n",
      "Iteration: 211000, loss: 0.10189949107091997, gradient norm: 0.5422883738947352\n",
      "Iteration: 212000, loss: 0.10189842136401314, gradient norm: 0.1909934807080938\n",
      "Iteration: 213000, loss: 0.10189775444719561, gradient norm: 0.6057056749971642\n",
      "Iteration: 214000, loss: 0.10189702282066887, gradient norm: 0.1411271487690615\n",
      "Iteration: 215000, loss: 0.10189631167272498, gradient norm: 0.17164764561639964\n",
      "Iteration: 216000, loss: 0.10189563069944299, gradient norm: 0.6790469352183813\n",
      "Iteration: 217000, loss: 0.10189483652403533, gradient norm: 0.4748222273259124\n",
      "Iteration: 218000, loss: 0.10189416412501637, gradient norm: 0.48620153653783926\n",
      "Iteration: 219000, loss: 0.10189354112474673, gradient norm: 0.27057598772754754\n",
      "Iteration: 220000, loss: 0.1018927592622372, gradient norm: 1.1083479339367401\n",
      "Iteration: 221000, loss: 0.10189196075943713, gradient norm: 0.6429985773574535\n",
      "Iteration: 222000, loss: 0.10189169417406534, gradient norm: 0.2524711286007303\n",
      "Iteration: 223000, loss: 0.10189053743678163, gradient norm: 0.5604121659981736\n",
      "Iteration: 224000, loss: 0.1018901459214375, gradient norm: 0.05107478046837195\n",
      "Iteration: 225000, loss: 0.10188944997284484, gradient norm: 1.241039003676337\n",
      "Iteration: 226000, loss: 0.10188878622059301, gradient norm: 0.5682466562676453\n",
      "Iteration: 227000, loss: 0.10188826820831423, gradient norm: 0.34694302966835966\n",
      "Iteration: 228000, loss: 0.1018874672084468, gradient norm: 0.5604001837815926\n",
      "Iteration: 229000, loss: 0.10188690883584357, gradient norm: 0.39576694901868903\n",
      "Iteration: 230000, loss: 0.10188620005284096, gradient norm: 0.18761372276691696\n",
      "Iteration: 231000, loss: 0.10188571440634556, gradient norm: 0.473406534537261\n",
      "Iteration: 232000, loss: 0.10188491889033775, gradient norm: 0.67537948267118\n",
      "Iteration: 233000, loss: 0.10188459488429173, gradient norm: 0.9334119049299645\n",
      "Iteration: 234000, loss: 0.10188398620052135, gradient norm: 0.8285886236860797\n",
      "Iteration: 235000, loss: 0.10188305351540075, gradient norm: 0.20629894483254388\n",
      "Iteration: 236000, loss: 0.10188268414168475, gradient norm: 0.2992000815314491\n",
      "Iteration: 237000, loss: 0.10188197517738119, gradient norm: 0.3062822688873691\n",
      "Iteration: 238000, loss: 0.10188138216671346, gradient norm: 0.442006282981045\n",
      "Iteration: 239000, loss: 0.10188087231804763, gradient norm: 0.28416377434670836\n",
      "Iteration: 240000, loss: 0.10188036412727226, gradient norm: 0.5333730368606031\n",
      "Iteration: 241000, loss: 0.10187973433496762, gradient norm: 0.923167553843832\n",
      "Iteration: 242000, loss: 0.10187912956170243, gradient norm: 0.23132532352637486\n",
      "Iteration: 243000, loss: 0.10187875990165766, gradient norm: 0.3050097679747689\n",
      "Iteration: 244000, loss: 0.10187795219586616, gradient norm: 0.41906676765946066\n",
      "Iteration: 245000, loss: 0.10187735797518196, gradient norm: 0.671673640511777\n",
      "Iteration: 246000, loss: 0.10187688147365813, gradient norm: 0.7871380561949028\n",
      "Iteration: 247000, loss: 0.10187644490881573, gradient norm: 0.2635511676737447\n",
      "Iteration: 248000, loss: 0.10187572751150975, gradient norm: 0.5421968964482157\n",
      "Iteration: 249000, loss: 0.10187532032976604, gradient norm: 0.29880167714938627\n",
      "Iteration: 250000, loss: 0.1018746909777921, gradient norm: 0.3786939643376632\n",
      "Iteration: 251000, loss: 0.10187415434692706, gradient norm: 0.6723030952950485\n",
      "Iteration: 252000, loss: 0.10187358475975784, gradient norm: 0.32335387941328747\n",
      "Iteration: 253000, loss: 0.10187298662132342, gradient norm: 0.2756824571356481\n",
      "Iteration: 254000, loss: 0.10187254963762366, gradient norm: 0.32357962777285276\n",
      "Iteration: 255000, loss: 0.10187190546842353, gradient norm: 0.7006477707891369\n",
      "Iteration: 256000, loss: 0.10187150481217198, gradient norm: 0.2676393799045012\n",
      "Iteration: 257000, loss: 0.10187081320484859, gradient norm: 0.08622536800961972\n",
      "Iteration: 258000, loss: 0.10187040578602702, gradient norm: 0.43748142201552254\n",
      "Iteration: 259000, loss: 0.10186972295773643, gradient norm: 0.5875969723186534\n",
      "Iteration: 260000, loss: 0.10186925680555964, gradient norm: 0.4548045909582595\n",
      "Iteration: 261000, loss: 0.10186876445988738, gradient norm: 0.33193457120628955\n",
      "Iteration: 262000, loss: 0.10186816523565753, gradient norm: 0.772977708215423\n",
      "Iteration: 263000, loss: 0.10186774889209274, gradient norm: 0.4993497365544076\n",
      "Iteration: 264000, loss: 0.10186715050141136, gradient norm: 0.6005272776318349\n",
      "Iteration: 265000, loss: 0.10186665432969967, gradient norm: 0.3490454274100114\n",
      "Iteration: 266000, loss: 0.10186592540932407, gradient norm: 0.13508653442566726\n",
      "Iteration: 267000, loss: 0.10186561877758515, gradient norm: 0.4188738626288039\n",
      "Iteration: 268000, loss: 0.10186487084719098, gradient norm: 0.35722711197662976\n",
      "Iteration: 269000, loss: 0.10186439832794675, gradient norm: 0.36021811700217\n",
      "Iteration: 270000, loss: 0.10186402462670847, gradient norm: 0.456539778451201\n",
      "Iteration: 271000, loss: 0.10186349205021569, gradient norm: 0.9717654042768213\n",
      "Iteration: 272000, loss: 0.10186271051789403, gradient norm: 0.5573834792036534\n",
      "Iteration: 273000, loss: 0.10186213153802476, gradient norm: 0.28896867580145497\n",
      "Iteration: 274000, loss: 0.1018618858104563, gradient norm: 0.04513128824758026\n",
      "Iteration: 275000, loss: 0.10186125652366096, gradient norm: 0.16947195175161098\n",
      "Iteration: 276000, loss: 0.10186054934448162, gradient norm: 0.7100735809346773\n",
      "Iteration: 277000, loss: 0.10186014885544716, gradient norm: 0.5389607659427958\n",
      "Iteration: 278000, loss: 0.10185951736962423, gradient norm: 0.7834138774702482\n",
      "Iteration: 279000, loss: 0.1018590848490366, gradient norm: 1.042401616473406\n",
      "Iteration: 280000, loss: 0.10185841327988845, gradient norm: 0.29418433282108436\n",
      "Iteration: 281000, loss: 0.10185804982842447, gradient norm: 0.2504639897176745\n",
      "Iteration: 282000, loss: 0.10185733067238588, gradient norm: 0.9900320157642754\n",
      "Iteration: 283000, loss: 0.10185677042382406, gradient norm: 0.16003557419439057\n",
      "Iteration: 284000, loss: 0.10185628823166541, gradient norm: 1.0245418177271575\n",
      "Iteration: 285000, loss: 0.1018557762800391, gradient norm: 0.17611286844262738\n",
      "Iteration: 286000, loss: 0.10185522656250251, gradient norm: 0.5333994301984158\n",
      "Iteration: 287000, loss: 0.10185468754346198, gradient norm: 0.062228861578850075\n",
      "Iteration: 288000, loss: 0.1018540698736089, gradient norm: 0.4623802285018712\n",
      "Iteration: 289000, loss: 0.10185373636605564, gradient norm: 0.22803981461086667\n",
      "Iteration: 290000, loss: 0.10185297562281875, gradient norm: 0.28113994965033734\n",
      "Iteration: 291000, loss: 0.10185242957014398, gradient norm: 0.124371265921787\n",
      "Iteration: 292000, loss: 0.10185206032220553, gradient norm: 0.7179263950544046\n",
      "Iteration: 293000, loss: 0.10185157379786144, gradient norm: 0.23257614367909307\n",
      "Iteration: 294000, loss: 0.10185084739694378, gradient norm: 0.39976504278122904\n",
      "Iteration: 295000, loss: 0.10185043129515639, gradient norm: 0.4467571855170119\n",
      "Iteration: 296000, loss: 0.10184994804593958, gradient norm: 0.5318924240608115\n",
      "Iteration: 297000, loss: 0.10184928638838672, gradient norm: 0.09358781617351612\n",
      "Iteration: 298000, loss: 0.10184897345541571, gradient norm: 0.719051289945231\n",
      "Iteration: 299000, loss: 0.10184832253943708, gradient norm: 0.5257594265435331\n",
      "Iteration: 300000, loss: 0.10184803521982567, gradient norm: 0.38433819724495694\n",
      "Iteration: 301000, loss: 0.10184730703031136, gradient norm: 0.366483267017075\n",
      "Iteration: 302000, loss: 0.10184691311219553, gradient norm: 0.5194835787431575\n",
      "Iteration: 303000, loss: 0.10184638326812086, gradient norm: 0.4520352330632342\n",
      "Iteration: 304000, loss: 0.10184576475751131, gradient norm: 0.16126451570105554\n",
      "Iteration: 305000, loss: 0.10184542813526963, gradient norm: 0.30454100723700045\n",
      "Iteration: 306000, loss: 0.10184496411877905, gradient norm: 0.8943951867536923\n",
      "Iteration: 307000, loss: 0.10184446216623104, gradient norm: 0.25708501717738824\n",
      "Iteration: 308000, loss: 0.10184403788168252, gradient norm: 0.484819974103703\n",
      "Iteration: 309000, loss: 0.10184347217106124, gradient norm: 0.30812246480026767\n",
      "Iteration: 310000, loss: 0.10184307680826014, gradient norm: 0.1464807977654035\n",
      "Iteration: 311000, loss: 0.10184252318310164, gradient norm: 1.1336447405064622\n",
      "Iteration: 312000, loss: 0.10184195197097835, gradient norm: 0.9334790399602333\n",
      "Iteration: 313000, loss: 0.10184184171810443, gradient norm: 0.41029188811556394\n",
      "Iteration: 314000, loss: 0.1018409143110507, gradient norm: 0.5755380718210324\n",
      "Iteration: 315000, loss: 0.10184085384336426, gradient norm: 0.3530437380631545\n",
      "Iteration: 316000, loss: 0.10184017456966679, gradient norm: 0.15756371369165995\n",
      "Iteration: 317000, loss: 0.10183998286130036, gradient norm: 0.9782875806679476\n",
      "Iteration: 318000, loss: 0.10183929315079113, gradient norm: 0.9032331057209186\n",
      "Iteration: 319000, loss: 0.10183890089870266, gradient norm: 0.2614827309295255\n",
      "Iteration: 320000, loss: 0.1018385929641078, gradient norm: 0.37216802972717666\n",
      "Iteration: 321000, loss: 0.10183815339162522, gradient norm: 0.8394669180871679\n",
      "Iteration: 322000, loss: 0.10183751115633825, gradient norm: 0.35385261805155177\n",
      "Iteration: 323000, loss: 0.101837137900254, gradient norm: 0.6759277232861872\n",
      "Iteration: 324000, loss: 0.10183690086415495, gradient norm: 0.19486295174039722\n",
      "Iteration: 325000, loss: 0.10183621029704738, gradient norm: 0.7045811633057969\n",
      "Iteration: 326000, loss: 0.10183607303038267, gradient norm: 0.6301033705579202\n",
      "Iteration: 327000, loss: 0.10183537504942444, gradient norm: 0.050387963282558894\n",
      "Iteration: 328000, loss: 0.10183541078284689, gradient norm: 1.0569402859799741\n",
      "Iteration: 329000, loss: 0.10183444656548565, gradient norm: 0.6510829784947061\n",
      "Iteration: 330000, loss: 0.10183423720595879, gradient norm: 0.5272926832542786\n",
      "Iteration: 331000, loss: 0.10183389402452425, gradient norm: 0.7340428349545384\n",
      "Iteration: 332000, loss: 0.10183343361500916, gradient norm: 0.15875633623525284\n",
      "Iteration: 333000, loss: 0.10183305006725356, gradient norm: 0.31108451724006936\n",
      "Iteration: 334000, loss: 0.10183255745094663, gradient norm: 0.3100329511976054\n",
      "Iteration: 335000, loss: 0.10183215475454688, gradient norm: 0.5682817266592798\n",
      "Iteration: 336000, loss: 0.10183200043170622, gradient norm: 0.6047398236916455\n",
      "Iteration: 337000, loss: 0.10183122684682877, gradient norm: 0.30417394892346244\n",
      "Iteration: 338000, loss: 0.10183116249219025, gradient norm: 0.5242834660504854\n",
      "Iteration: 339000, loss: 0.10183059675076513, gradient norm: 0.511828543204902\n",
      "Iteration: 340000, loss: 0.10183030332037665, gradient norm: 0.630304339144806\n",
      "Iteration: 341000, loss: 0.10182974163998375, gradient norm: 0.6859897509265342\n",
      "Iteration: 342000, loss: 0.10182945386012145, gradient norm: 0.5584286896176688\n",
      "Iteration: 343000, loss: 0.10182912669446832, gradient norm: 0.36487862466836263\n",
      "Iteration: 344000, loss: 0.1018286172563978, gradient norm: 0.6072753020865318\n",
      "Iteration: 345000, loss: 0.10182842346257788, gradient norm: 0.467881159874636\n",
      "Iteration: 346000, loss: 0.10182790474933628, gradient norm: 0.024218623178303634\n",
      "Iteration: 347000, loss: 0.10182753363440646, gradient norm: 0.3961841000157802\n",
      "Iteration: 348000, loss: 0.10182725871603641, gradient norm: 0.8252022882901462\n",
      "Iteration: 349000, loss: 0.10182670885327186, gradient norm: 0.42612011169390807\n",
      "Iteration: 350000, loss: 0.10182644592430716, gradient norm: 0.2255513844497759\n",
      "Iteration: 351000, loss: 0.10182601699093792, gradient norm: 0.3688778061183648\n",
      "Iteration: 352000, loss: 0.10182584627499668, gradient norm: 0.33068814372969746\n",
      "Iteration: 353000, loss: 0.10182524372754972, gradient norm: 0.4585384211485922\n",
      "Iteration: 354000, loss: 0.10182485541931614, gradient norm: 0.32801730786196753\n",
      "Iteration: 355000, loss: 0.10182463603462862, gradient norm: 0.24175939656274756\n",
      "Iteration: 356000, loss: 0.10182422526908118, gradient norm: 0.557100561580402\n",
      "Iteration: 357000, loss: 0.10182381047163719, gradient norm: 0.2782960270901367\n",
      "Iteration: 358000, loss: 0.10182348254689917, gradient norm: 0.3298207607477056\n",
      "Iteration: 359000, loss: 0.10182314480555282, gradient norm: 0.1444274894876595\n",
      "Iteration: 360000, loss: 0.10182268810672963, gradient norm: 1.028052186460101\n",
      "Iteration: 361000, loss: 0.10182238886095657, gradient norm: 0.7180867012886266\n",
      "Iteration: 362000, loss: 0.10182211654965678, gradient norm: 0.11658360066900653\n",
      "Iteration: 363000, loss: 0.1018216752554012, gradient norm: 0.4628201072030323\n",
      "Iteration: 364000, loss: 0.10182129825840107, gradient norm: 0.16812804957088964\n",
      "Iteration: 365000, loss: 0.1018209880272529, gradient norm: 0.7616143217234261\n",
      "Iteration: 366000, loss: 0.10182046221291308, gradient norm: 0.8067105046441856\n",
      "Iteration: 367000, loss: 0.10182036638732363, gradient norm: 0.27155337782894157\n",
      "Iteration: 368000, loss: 0.10181984361620468, gradient norm: 0.4921045018083626\n",
      "Iteration: 369000, loss: 0.10181962065483094, gradient norm: 0.5314184383119355\n",
      "Iteration: 370000, loss: 0.10181911723237004, gradient norm: 0.11273739388600634\n",
      "Iteration: 371000, loss: 0.10181896803021272, gradient norm: 0.7605513235848408\n",
      "Iteration: 372000, loss: 0.10181837178000458, gradient norm: 0.17500820770637396\n",
      "Iteration: 373000, loss: 0.1018181070855411, gradient norm: 0.2722147634232644\n",
      "Iteration: 374000, loss: 0.10181788995055352, gradient norm: 0.11538661947351413\n",
      "Iteration: 375000, loss: 0.10181736714750132, gradient norm: 0.32344009780824623\n",
      "Iteration: 376000, loss: 0.10181715250173543, gradient norm: 0.3782795049328651\n",
      "Iteration: 377000, loss: 0.10181665382370701, gradient norm: 0.25658859179852495\n",
      "Iteration: 378000, loss: 0.1018164059815267, gradient norm: 0.24436498108185056\n",
      "Iteration: 379000, loss: 0.10181600628450255, gradient norm: 0.26268526434942696\n",
      "Iteration: 380000, loss: 0.101815855867297, gradient norm: 0.7046256822110862\n",
      "Iteration: 381000, loss: 0.10181520158728787, gradient norm: 0.6179516569904486\n",
      "Iteration: 382000, loss: 0.10181496536149538, gradient norm: 0.23306645278978833\n",
      "Iteration: 383000, loss: 0.10181457902616427, gradient norm: 0.3937926205969965\n",
      "Iteration: 384000, loss: 0.10181437504949167, gradient norm: 0.6880748409542394\n",
      "Iteration: 385000, loss: 0.10181390858967072, gradient norm: 0.9870059042108165\n",
      "Iteration: 386000, loss: 0.1018134904694937, gradient norm: 0.36799146212842193\n",
      "Iteration: 387000, loss: 0.10181316395615687, gradient norm: 1.3223048458504651\n",
      "Iteration: 388000, loss: 0.10181282431099621, gradient norm: 0.44948121262070967\n",
      "Iteration: 389000, loss: 0.10181261676063381, gradient norm: 0.3123380731477865\n",
      "Iteration: 390000, loss: 0.10181200812491355, gradient norm: 0.2688775694447494\n",
      "Iteration: 391000, loss: 0.10181168241775472, gradient norm: 0.2694721975108551\n",
      "Iteration: 392000, loss: 0.10181127813342031, gradient norm: 0.12602465155245676\n",
      "Iteration: 393000, loss: 0.10181102163490519, gradient norm: 0.22934687797737668\n",
      "Iteration: 394000, loss: 0.10181067328495597, gradient norm: 0.42303116655368467\n",
      "Iteration: 395000, loss: 0.10181010170685117, gradient norm: 0.426890706263803\n",
      "Iteration: 396000, loss: 0.10180985208311331, gradient norm: 0.539103011660612\n",
      "Iteration: 397000, loss: 0.10180941825617267, gradient norm: 0.2829234771543664\n",
      "Iteration: 398000, loss: 0.10180891806387947, gradient norm: 0.03125389994352408\n",
      "Iteration: 399000, loss: 0.10180844907326295, gradient norm: 0.28887074017660713\n",
      "Iteration: 400000, loss: 0.1018079246498332, gradient norm: 0.06776042595191113\n",
      "Iteration: 401000, loss: 0.10180768238846298, gradient norm: 0.5784535068192355\n",
      "Iteration: 402000, loss: 0.10180696993919869, gradient norm: 0.1913238631617686\n",
      "Iteration: 403000, loss: 0.10180653501090815, gradient norm: 0.6306270405631046\n",
      "Iteration: 404000, loss: 0.10180588906236275, gradient norm: 0.2214956389574366\n",
      "Iteration: 405000, loss: 0.10180513944949997, gradient norm: 0.2489818262576035\n",
      "Iteration: 406000, loss: 0.1018043456066848, gradient norm: 0.6901043948347853\n",
      "Iteration: 407000, loss: 0.1018035608529162, gradient norm: 0.4577276598740253\n",
      "Iteration: 408000, loss: 0.1018029434226122, gradient norm: 0.2498927587929261\n",
      "Iteration: 409000, loss: 0.10180200331650247, gradient norm: 0.432399385113124\n",
      "Iteration: 410000, loss: 0.10180063568220434, gradient norm: 1.316434173508908\n",
      "Iteration: 411000, loss: 0.10179974603325448, gradient norm: 0.1525295024984347\n",
      "Iteration: 412000, loss: 0.10179862094040756, gradient norm: 0.3720968286959623\n",
      "Iteration: 413000, loss: 0.10179717527420354, gradient norm: 0.6105294979704919\n",
      "Iteration: 414000, loss: 0.10179603736584227, gradient norm: 0.2716882639340905\n",
      "Iteration: 415000, loss: 0.10179464792464694, gradient norm: 0.4469129488100159\n",
      "Iteration: 416000, loss: 0.10179341028846833, gradient norm: 0.049836564135796506\n",
      "Iteration: 417000, loss: 0.10179195265210311, gradient norm: 0.27544894001302517\n",
      "Iteration: 418000, loss: 0.10179083824468049, gradient norm: 0.30558702822111034\n",
      "Iteration: 419000, loss: 0.10178927006325392, gradient norm: 0.8851360095582439\n",
      "Iteration: 420000, loss: 0.10178806924801452, gradient norm: 0.07682346604544109\n",
      "Iteration: 421000, loss: 0.10178657244138968, gradient norm: 0.5151586567809439\n",
      "Iteration: 422000, loss: 0.10178508679145304, gradient norm: 0.3813353070842775\n",
      "Iteration: 423000, loss: 0.10178353604855264, gradient norm: 0.10064207071332323\n",
      "Iteration: 424000, loss: 0.10178144999513103, gradient norm: 1.0726452232832178\n",
      "Iteration: 425000, loss: 0.10177972451739409, gradient norm: 0.22073053135242945\n",
      "Iteration: 426000, loss: 0.10177780040846045, gradient norm: 0.4653169582353667\n",
      "Iteration: 427000, loss: 0.10177630604915763, gradient norm: 1.4136190394690298\n",
      "Iteration: 428000, loss: 0.10177487431927817, gradient norm: 1.0721472497365048\n",
      "Iteration: 429000, loss: 0.10177360633094802, gradient norm: 0.5527444582623035\n",
      "Iteration: 430000, loss: 0.1017721753950097, gradient norm: 0.28135789116314014\n",
      "Iteration: 431000, loss: 0.10177096446322846, gradient norm: 0.35040885881209205\n",
      "Iteration: 432000, loss: 0.10176975035471433, gradient norm: 0.9768159586154382\n",
      "Iteration: 433000, loss: 0.10176873690746714, gradient norm: 0.11738623127745734\n",
      "Iteration: 434000, loss: 0.1017675869067907, gradient norm: 0.5024670425200596\n",
      "Iteration: 435000, loss: 0.10176651453713918, gradient norm: 0.618570621123354\n",
      "Iteration: 436000, loss: 0.10176552504853044, gradient norm: 0.14568569485560554\n",
      "Iteration: 437000, loss: 0.1017643333971002, gradient norm: 0.05409920920251646\n",
      "Iteration: 438000, loss: 0.10176354576400481, gradient norm: 0.8429836387678246\n",
      "Iteration: 439000, loss: 0.10176267148506074, gradient norm: 0.10670865004368012\n",
      "Iteration: 440000, loss: 0.10176162979068112, gradient norm: 0.7886951272627094\n",
      "Iteration: 441000, loss: 0.10176084506327376, gradient norm: 0.03475946640177074\n",
      "Iteration: 442000, loss: 0.10176003661786201, gradient norm: 0.5028883970548771\n",
      "Iteration: 443000, loss: 0.10175882612322197, gradient norm: 0.2587475262266976\n",
      "Iteration: 444000, loss: 0.1017585242321999, gradient norm: 0.18536112175525324\n",
      "Iteration: 445000, loss: 0.10175731123553593, gradient norm: 0.514650068520561\n",
      "Iteration: 446000, loss: 0.1017565410987296, gradient norm: 0.208084753745011\n",
      "Iteration: 447000, loss: 0.10175574911978556, gradient norm: 0.4160029504787671\n",
      "Iteration: 448000, loss: 0.10175504771824606, gradient norm: 0.7129119697629575\n",
      "Iteration: 449000, loss: 0.10175426373667362, gradient norm: 0.40846510397312313\n",
      "Iteration: 450000, loss: 0.10175349868858452, gradient norm: 0.16708592217839738\n",
      "Iteration: 451000, loss: 0.10175283025496065, gradient norm: 0.40937228013015653\n",
      "Iteration: 452000, loss: 0.10175205383419499, gradient norm: 0.2471059480344414\n",
      "Iteration: 453000, loss: 0.10175131600913619, gradient norm: 0.081274671893817\n",
      "Iteration: 454000, loss: 0.10175063725701716, gradient norm: 0.2754692182412442\n",
      "Iteration: 455000, loss: 0.10174988598729202, gradient norm: 0.7755809398958017\n",
      "Iteration: 456000, loss: 0.10174936435119612, gradient norm: 0.20798928481060527\n",
      "Iteration: 457000, loss: 0.10174863646244289, gradient norm: 0.7739908997067921\n",
      "Iteration: 458000, loss: 0.10174785724298033, gradient norm: 0.6394351037831059\n",
      "Iteration: 459000, loss: 0.10174722079106312, gradient norm: 0.5163977137441111\n",
      "Iteration: 460000, loss: 0.10174671233548128, gradient norm: 0.09858321009999277\n",
      "Iteration: 461000, loss: 0.101746042147513, gradient norm: 0.6687296128177995\n",
      "Iteration: 462000, loss: 0.1017452904014226, gradient norm: 0.02429566090715552\n",
      "Iteration: 463000, loss: 0.10174476042766926, gradient norm: 0.27209334135564056\n",
      "Iteration: 464000, loss: 0.10174413926192566, gradient norm: 0.06875462442539065\n",
      "Iteration: 465000, loss: 0.10174355629129078, gradient norm: 0.9911607345196676\n",
      "Iteration: 466000, loss: 0.10174302133872365, gradient norm: 0.27500357675250997\n",
      "Iteration: 467000, loss: 0.10174232799672117, gradient norm: 1.0943978011763134\n",
      "Iteration: 468000, loss: 0.10174171752018826, gradient norm: 0.06074015853937752\n",
      "Iteration: 469000, loss: 0.10174113759929244, gradient norm: 0.42035805864938464\n",
      "Iteration: 470000, loss: 0.10174060370755739, gradient norm: 0.9080668711238111\n",
      "Iteration: 471000, loss: 0.10174017409851852, gradient norm: 0.5165054882101314\n",
      "Iteration: 472000, loss: 0.10173949623875762, gradient norm: 0.1524262227785156\n",
      "Iteration: 473000, loss: 0.10173883203559393, gradient norm: 0.3583470521312424\n",
      "Iteration: 474000, loss: 0.10173846959486346, gradient norm: 0.407659234613893\n",
      "Iteration: 475000, loss: 0.10173774362586731, gradient norm: 1.382830604272411\n",
      "Iteration: 476000, loss: 0.10173730740007754, gradient norm: 0.667798785606157\n",
      "Iteration: 477000, loss: 0.1017367633172292, gradient norm: 0.3216248647642663\n",
      "Iteration: 478000, loss: 0.10173624938335529, gradient norm: 0.7027959610404682\n",
      "Iteration: 479000, loss: 0.10173571749957856, gradient norm: 0.9059767384907854\n",
      "Iteration: 480000, loss: 0.10173506248183278, gradient norm: 0.3383162132762716\n",
      "Iteration: 481000, loss: 0.10173456476141911, gradient norm: 0.06282337756661824\n",
      "Iteration: 482000, loss: 0.10173407934754522, gradient norm: 0.2570819112516393\n",
      "Iteration: 483000, loss: 0.10173339644461528, gradient norm: 0.9581097108807339\n",
      "Iteration: 484000, loss: 0.1017330330410522, gradient norm: 0.42547614147012147\n",
      "Iteration: 485000, loss: 0.10173243833024191, gradient norm: 0.8409304807067931\n",
      "Iteration: 486000, loss: 0.10173201280747884, gradient norm: 0.15120022011751338\n",
      "Iteration: 487000, loss: 0.10173121771483824, gradient norm: 1.0440803741170082\n",
      "Iteration: 488000, loss: 0.1017309441683264, gradient norm: 0.8867716511792931\n",
      "Iteration: 489000, loss: 0.10173033284447264, gradient norm: 0.8846943885079003\n",
      "Iteration: 490000, loss: 0.10172981231911944, gradient norm: 0.8558538565745571\n",
      "Iteration: 491000, loss: 0.10172938808353553, gradient norm: 0.6258404987012286\n",
      "Iteration: 492000, loss: 0.10172869762537372, gradient norm: 0.38399466793910525\n",
      "Iteration: 493000, loss: 0.10172832173611099, gradient norm: 0.5405303828853523\n",
      "Iteration: 494000, loss: 0.10172770214225334, gradient norm: 0.11317370792537836\n",
      "Iteration: 495000, loss: 0.10172738121196372, gradient norm: 0.30572432833856394\n",
      "Iteration: 496000, loss: 0.10172667595371089, gradient norm: 0.34849482211858024\n",
      "Iteration: 497000, loss: 0.10172647821928947, gradient norm: 0.5706594368836422\n",
      "Iteration: 498000, loss: 0.10172584119254056, gradient norm: 0.42630486579396065\n",
      "Iteration: 499000, loss: 0.10172522933472689, gradient norm: 0.22721530210286411\n",
      "Iteration: 500000, loss: 0.10172511173371442, gradient norm: 0.19222109053920905\n",
      "Iteration: 501000, loss: 0.10172437152886707, gradient norm: 0.5573571730462821\n",
      "Iteration: 502000, loss: 0.10172410404848449, gradient norm: 0.5626594785131624\n",
      "Iteration: 503000, loss: 0.10172359729987195, gradient norm: 0.3841860157440273\n",
      "Iteration: 504000, loss: 0.10172319380906171, gradient norm: 0.21651284595204187\n",
      "Iteration: 505000, loss: 0.10172270075154378, gradient norm: 0.6495439680927187\n",
      "Iteration: 506000, loss: 0.1017222767023403, gradient norm: 0.14129855997592308\n",
      "Iteration: 507000, loss: 0.10172192736272023, gradient norm: 0.7259360035030717\n",
      "Iteration: 508000, loss: 0.10172146298409862, gradient norm: 0.08143632671315831\n",
      "Iteration: 509000, loss: 0.10172104163467713, gradient norm: 0.5778614004172\n",
      "Iteration: 510000, loss: 0.10172064176186889, gradient norm: 0.17205134965518723\n",
      "Iteration: 511000, loss: 0.10172016648578754, gradient norm: 0.7550852442118055\n",
      "Iteration: 512000, loss: 0.1017198691627453, gradient norm: 0.9494975921429714\n",
      "Iteration: 513000, loss: 0.10171935284777403, gradient norm: 0.1516494962696267\n",
      "Iteration: 514000, loss: 0.1017189940388837, gradient norm: 0.6887334707475945\n",
      "Iteration: 515000, loss: 0.10171881694845196, gradient norm: 0.35746100165348116\n",
      "Iteration: 516000, loss: 0.10171820809674952, gradient norm: 0.2421455950362508\n",
      "Iteration: 517000, loss: 0.10171786333836969, gradient norm: 0.27241193237674727\n",
      "Iteration: 518000, loss: 0.10171745202490691, gradient norm: 1.2478608868692596\n",
      "Iteration: 519000, loss: 0.10171722368010316, gradient norm: 0.6127222401444034\n",
      "Iteration: 520000, loss: 0.1017168144365009, gradient norm: 0.8221937219478326\n",
      "Iteration: 521000, loss: 0.10171644965014782, gradient norm: 0.7468594621628846\n",
      "Iteration: 522000, loss: 0.10171591783278179, gradient norm: 0.15857121540129576\n",
      "Iteration: 523000, loss: 0.10171557832405569, gradient norm: 0.7764372420368971\n",
      "Iteration: 524000, loss: 0.10171526042737776, gradient norm: 0.04171159377119174\n",
      "Iteration: 525000, loss: 0.10171489696971758, gradient norm: 0.5286312797294251\n",
      "Iteration: 526000, loss: 0.10171446683855677, gradient norm: 0.16141226860831698\n",
      "Iteration: 527000, loss: 0.10171425520008569, gradient norm: 0.16742091752844807\n",
      "Iteration: 528000, loss: 0.1017137646860694, gradient norm: 0.561975586992686\n",
      "Iteration: 529000, loss: 0.1017135719492327, gradient norm: 0.4376121610123672\n",
      "Iteration: 530000, loss: 0.10171308845889564, gradient norm: 0.5736078457706278\n",
      "Iteration: 531000, loss: 0.10171286331764663, gradient norm: 0.007373432170150484\n",
      "Iteration: 532000, loss: 0.10171227616411423, gradient norm: 0.21858356780683208\n",
      "Iteration: 533000, loss: 0.10171220595553997, gradient norm: 0.6324061411769937\n",
      "Iteration: 534000, loss: 0.10171167220933704, gradient norm: 0.3429205235524554\n",
      "Iteration: 535000, loss: 0.10171148041132906, gradient norm: 0.5981378515772888\n",
      "Iteration: 536000, loss: 0.10171092496439152, gradient norm: 0.7468750979975181\n",
      "Iteration: 537000, loss: 0.1017107433742577, gradient norm: 0.2042555859425135\n",
      "Iteration: 538000, loss: 0.10171029789854576, gradient norm: 0.283167783017915\n",
      "Iteration: 539000, loss: 0.10170998524874142, gradient norm: 0.3602353598604165\n",
      "Iteration: 540000, loss: 0.10170959281499069, gradient norm: 0.532457909889188\n",
      "Iteration: 541000, loss: 0.10170944626169415, gradient norm: 0.43391217554203226\n",
      "Iteration: 542000, loss: 0.10170885217371925, gradient norm: 0.23162382793236982\n",
      "Iteration: 543000, loss: 0.10170871050428519, gradient norm: 0.49250100481033854\n",
      "Iteration: 544000, loss: 0.10170830178260158, gradient norm: 0.1900889676132017\n",
      "Iteration: 545000, loss: 0.10170791939546711, gradient norm: 0.450758090778284\n",
      "Iteration: 546000, loss: 0.10170762555610059, gradient norm: 0.2937506498166368\n",
      "Iteration: 547000, loss: 0.10170729277632165, gradient norm: 0.7809898323616458\n",
      "Iteration: 548000, loss: 0.10170697220099277, gradient norm: 0.30726487311908185\n",
      "Iteration: 549000, loss: 0.1017069238296228, gradient norm: 0.2951232231030897\n",
      "Iteration: 550000, loss: 0.10170614962103793, gradient norm: 0.8377667190750688\n",
      "Iteration: 551000, loss: 0.10170596412129375, gradient norm: 0.4809016326457157\n",
      "Iteration: 552000, loss: 0.10170560966795197, gradient norm: 0.714864074284545\n",
      "Iteration: 553000, loss: 0.10170531921229937, gradient norm: 0.6390508753578898\n",
      "Iteration: 554000, loss: 0.1017049598045938, gradient norm: 0.9353140301590567\n",
      "Iteration: 555000, loss: 0.10170472830823203, gradient norm: 0.32618368948420057\n",
      "Iteration: 556000, loss: 0.1017045331262412, gradient norm: 0.8459959015748346\n",
      "Iteration: 557000, loss: 0.1017038616604369, gradient norm: 0.4175158292948454\n",
      "Iteration: 558000, loss: 0.10170367867174845, gradient norm: 0.7308944578982958\n",
      "Iteration: 559000, loss: 0.10170345309741106, gradient norm: 0.08745299550471672\n",
      "Iteration: 560000, loss: 0.10170323436008895, gradient norm: 0.7348264891699281\n",
      "Iteration: 561000, loss: 0.10170265956258431, gradient norm: 0.07187268209524253\n",
      "Iteration: 562000, loss: 0.10170247931428825, gradient norm: 0.18330290599487922\n",
      "Iteration: 563000, loss: 0.10170203196313264, gradient norm: 0.7312638331280491\n",
      "Iteration: 564000, loss: 0.10170182332068559, gradient norm: 0.7316990232985868\n",
      "Iteration: 565000, loss: 0.10170154860516249, gradient norm: 0.17829400291888733\n",
      "Iteration: 566000, loss: 0.10170119796220183, gradient norm: 0.4599930953851541\n",
      "Iteration: 567000, loss: 0.10170086882748723, gradient norm: 0.21551180584801066\n",
      "Iteration: 568000, loss: 0.10170042526588043, gradient norm: 0.725331667218312\n",
      "Iteration: 569000, loss: 0.1017003212231691, gradient norm: 0.5693894626764605\n",
      "Iteration: 570000, loss: 0.10169973882482185, gradient norm: 0.6485229446814993\n",
      "Iteration: 571000, loss: 0.1016997450805089, gradient norm: 0.6207339973296752\n",
      "Iteration: 572000, loss: 0.10169930505025299, gradient norm: 0.5930456372960731\n",
      "Iteration: 573000, loss: 0.10169915848717806, gradient norm: 0.2500035595778555\n",
      "Iteration: 574000, loss: 0.10169837456484059, gradient norm: 0.5022507955260805\n",
      "Iteration: 575000, loss: 0.10169836323042052, gradient norm: 0.32927524259085256\n",
      "Iteration: 576000, loss: 0.10169810573220314, gradient norm: 0.29164773200384203\n",
      "Iteration: 577000, loss: 0.10169765877269957, gradient norm: 0.7398613242433169\n",
      "Iteration: 578000, loss: 0.10169746908771679, gradient norm: 0.6153377605471099\n",
      "Iteration: 579000, loss: 0.10169709121580124, gradient norm: 0.46859548950141966\n",
      "Iteration: 580000, loss: 0.10169677702324642, gradient norm: 0.5808678685007551\n",
      "Iteration: 581000, loss: 0.10169642795755877, gradient norm: 0.014765632799173999\n",
      "Iteration: 582000, loss: 0.10169611210081618, gradient norm: 0.33277983708121495\n",
      "Iteration: 583000, loss: 0.10169601715264849, gradient norm: 0.0364213225084275\n",
      "Iteration: 584000, loss: 0.10169558735002482, gradient norm: 0.5237196265121091\n",
      "Iteration: 585000, loss: 0.10169528285866739, gradient norm: 0.464806432008816\n",
      "Iteration: 586000, loss: 0.10169491876873936, gradient norm: 0.6790415615148035\n",
      "Iteration: 587000, loss: 0.10169477673595878, gradient norm: 0.4301783555150212\n",
      "Iteration: 588000, loss: 0.10169413306870885, gradient norm: 0.05892426413060332\n",
      "Iteration: 589000, loss: 0.101693975092853, gradient norm: 0.3628268779364369\n",
      "Iteration: 590000, loss: 0.10169392216023128, gradient norm: 0.844397222704307\n",
      "Iteration: 591000, loss: 0.10169337529905474, gradient norm: 0.4669136982918938\n",
      "Iteration: 592000, loss: 0.10169299434114357, gradient norm: 0.2179367498723335\n",
      "Iteration: 593000, loss: 0.10169287011720621, gradient norm: 0.6593708725759456\n",
      "Iteration: 594000, loss: 0.10169255260196251, gradient norm: 0.3072278719198624\n",
      "Iteration: 595000, loss: 0.1016921932547086, gradient norm: 0.23679085091340063\n",
      "Iteration: 596000, loss: 0.10169184510496387, gradient norm: 0.4701025369105228\n",
      "Iteration: 597000, loss: 0.10169158162252016, gradient norm: 0.8841461758464618\n",
      "Iteration: 598000, loss: 0.10169132261126229, gradient norm: 0.21157614212548254\n",
      "Iteration: 599000, loss: 0.10169095626093527, gradient norm: 0.38267138314904997\n",
      "Iteration: 600000, loss: 0.1016907930104801, gradient norm: 0.6723817247034254\n",
      "Iteration: 601000, loss: 0.10169030664952337, gradient norm: 0.5151633725137464\n",
      "Iteration: 602000, loss: 0.10169014842674025, gradient norm: 0.26692076694934186\n",
      "Iteration: 603000, loss: 0.10168965728811358, gradient norm: 1.167584748275193\n",
      "Iteration: 604000, loss: 0.10168953204910289, gradient norm: 0.11435381772191941\n",
      "Iteration: 605000, loss: 0.10168911571443351, gradient norm: 0.28283733320046783\n",
      "Iteration: 606000, loss: 0.10168882635831551, gradient norm: 0.3017090870138227\n",
      "Iteration: 607000, loss: 0.10168858010072589, gradient norm: 0.8845158723207455\n",
      "Iteration: 608000, loss: 0.10168821499006779, gradient norm: 0.18443384064595766\n",
      "Iteration: 609000, loss: 0.10168795075639799, gradient norm: 0.4726132541647645\n",
      "Iteration: 610000, loss: 0.10168772062992082, gradient norm: 0.3661101068151773\n",
      "Iteration: 611000, loss: 0.10168740411210674, gradient norm: 0.2517953173588233\n",
      "Iteration: 612000, loss: 0.10168695743034804, gradient norm: 0.17181257676562312\n",
      "Iteration: 613000, loss: 0.10168667567855928, gradient norm: 0.2956344238612221\n",
      "Iteration: 614000, loss: 0.10168652832200852, gradient norm: 0.25475384145816843\n",
      "Iteration: 615000, loss: 0.10168619103813803, gradient norm: 0.16259592689731414\n",
      "Iteration: 616000, loss: 0.10168582805421295, gradient norm: 0.20169145110812056\n",
      "Iteration: 617000, loss: 0.10168548620838463, gradient norm: 0.35752066876196864\n",
      "Iteration: 618000, loss: 0.10168540062017874, gradient norm: 0.16756787674177306\n",
      "Iteration: 619000, loss: 0.10168483265108011, gradient norm: 0.6275805664682182\n",
      "Iteration: 620000, loss: 0.10168469156152656, gradient norm: 0.4039846685787173\n",
      "Iteration: 621000, loss: 0.10168431254219185, gradient norm: 0.8766481316177089\n",
      "Iteration: 622000, loss: 0.10168414678823962, gradient norm: 0.15293913174571622\n",
      "Iteration: 623000, loss: 0.1016836231241966, gradient norm: 0.10489460672030163\n",
      "Iteration: 624000, loss: 0.10168360828807899, gradient norm: 0.35649526070462595\n",
      "Iteration: 625000, loss: 0.10168318412727713, gradient norm: 1.2544903327822716\n",
      "Iteration: 626000, loss: 0.10168282530676546, gradient norm: 0.029497699270606952\n",
      "Iteration: 627000, loss: 0.10168249382355285, gradient norm: 0.4261915393529122\n",
      "Iteration: 628000, loss: 0.10168224106366636, gradient norm: 0.13767988022851513\n",
      "Iteration: 629000, loss: 0.10168200658905746, gradient norm: 0.6291771600274028\n",
      "Iteration: 630000, loss: 0.10168163212966731, gradient norm: 0.37247032269139696\n",
      "Iteration: 631000, loss: 0.10168138072544178, gradient norm: 0.13151054936702022\n",
      "Iteration: 632000, loss: 0.10168108046841079, gradient norm: 0.5787291936102583\n",
      "Iteration: 633000, loss: 0.10168085023132059, gradient norm: 0.6065978347651562\n",
      "Iteration: 634000, loss: 0.10168041998903089, gradient norm: 0.554337489604451\n",
      "Iteration: 635000, loss: 0.10168021736205181, gradient norm: 0.86937103208742\n",
      "Iteration: 636000, loss: 0.10167984099730965, gradient norm: 0.42532796202216233\n"
     ]
    }
   ],
   "source": [
    "_, _ = train(dummy_network, torch_dataset_inputs, torch_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = second_order_opt(final_weights, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax_loss(final_weights), jnp.linalg.norm(jax_grad(jax_loss)(final_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1026920579402963 0.10144003755288118\n",
      "9.95016526425352e-06 0.07283338667347572\n",
      "13.566518233982933\n"
     ]
    }
   ],
   "source": [
    "trace = []\n",
    "trace.append((deepcopy(dummy_network.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear1.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear2.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear3.bias.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.weight.cpu().data.detach().numpy()),\n",
    "              deepcopy(dummy_network.module.linear4.bias.cpu().data.detach().numpy())))\n",
    "\n",
    "new_weights_end = np.append(\n",
    "    np.append(\n",
    "        np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "                  trace[-1][1].reshape(H_student)),\n",
    "        np.append(\n",
    "            np.append(trace[-1][2].reshape(H_student * D_in), \n",
    "                  trace[-1][3].reshape(H_student)),\n",
    "            np.append(trace[-1][4].reshape((H_student + 1) * D_in), \n",
    "                  trace[-1][5].reshape(H_student + 1)))),\n",
    "    np.append(trace[-1][6][0],\n",
    "              trace[-1][7][0]))\n",
    "\n",
    "print(jax_loss(weights), jax_loss(new_weights_end))\n",
    "print(jnp.linalg.norm(jax_grad(jax_loss)(weights)), jnp.linalg.norm(jax_grad(jax_loss)(new_weights_end)))\n",
    "print(np.linalg.norm(new_weights_end - new_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = hessian(jax_loss)(new_weights_end)\n",
    "H = (H + H.T) / 2.0\n",
    "evals, _ = jnp.linalg.eigh(H)\n",
    "print(evals[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 0\n",
    "PATH = \"local_min_model.pt\"\n",
    "LOSS = 0.4\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': dummy_network.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
