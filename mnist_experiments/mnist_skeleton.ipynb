{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import nlopt\n",
    "from numpy import *\n",
    "import numpy as np \n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "np.set_printoptions(precision=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1_principal</th>\n",
       "      <th>2_principal</th>\n",
       "      <th>3_principal</th>\n",
       "      <th>4_principal</th>\n",
       "      <th>5_principal</th>\n",
       "      <th>6_principal</th>\n",
       "      <th>7_principal</th>\n",
       "      <th>8_principal</th>\n",
       "      <th>9_principal</th>\n",
       "      <th>10_principal</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-5.140478</td>\n",
       "      <td>-5.226451</td>\n",
       "      <td>3.886993</td>\n",
       "      <td>-0.901512</td>\n",
       "      <td>4.929209</td>\n",
       "      <td>2.036187</td>\n",
       "      <td>4.706960</td>\n",
       "      <td>-4.764459</td>\n",
       "      <td>0.238225</td>\n",
       "      <td>-1.459020</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19.292333</td>\n",
       "      <td>6.033014</td>\n",
       "      <td>1.308196</td>\n",
       "      <td>-2.383076</td>\n",
       "      <td>3.095021</td>\n",
       "      <td>-1.794193</td>\n",
       "      <td>-3.770784</td>\n",
       "      <td>0.148453</td>\n",
       "      <td>-4.154969</td>\n",
       "      <td>-4.295380</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-7.644504</td>\n",
       "      <td>-1.705801</td>\n",
       "      <td>2.289336</td>\n",
       "      <td>2.241256</td>\n",
       "      <td>5.094750</td>\n",
       "      <td>-4.152694</td>\n",
       "      <td>-1.011677</td>\n",
       "      <td>1.733929</td>\n",
       "      <td>0.422061</td>\n",
       "      <td>-0.072606</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.474206</td>\n",
       "      <td>5.836146</td>\n",
       "      <td>2.008588</td>\n",
       "      <td>4.271250</td>\n",
       "      <td>2.378019</td>\n",
       "      <td>2.179969</td>\n",
       "      <td>4.397159</td>\n",
       "      <td>-0.346711</td>\n",
       "      <td>1.018367</td>\n",
       "      <td>5.470587</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26.559575</td>\n",
       "      <td>6.024832</td>\n",
       "      <td>0.933257</td>\n",
       "      <td>-3.012613</td>\n",
       "      <td>9.488500</td>\n",
       "      <td>-2.333748</td>\n",
       "      <td>-6.146737</td>\n",
       "      <td>-1.796978</td>\n",
       "      <td>-4.180035</td>\n",
       "      <td>-5.717939</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  1_principal  2_principal  3_principal  4_principal  \\\n",
       "0           0    -5.140478    -5.226451     3.886993    -0.901512   \n",
       "1           1    19.292333     6.033014     1.308196    -2.383076   \n",
       "2           2    -7.644504    -1.705801     2.289336     2.241256   \n",
       "3           3    -0.474206     5.836146     2.008588     4.271250   \n",
       "4           4    26.559575     6.024832     0.933257    -3.012613   \n",
       "\n",
       "   5_principal  6_principal  7_principal  8_principal  9_principal  \\\n",
       "0     4.929209     2.036187     4.706960    -4.764459     0.238225   \n",
       "1     3.095021    -1.794193    -3.770784     0.148453    -4.154969   \n",
       "2     5.094750    -4.152694    -1.011677     1.733929     0.422061   \n",
       "3     2.378019     2.179969     4.397159    -0.346711     1.018367   \n",
       "4     9.488500    -2.333748    -6.146737    -1.796978    -4.180035   \n",
       "\n",
       "   10_principal  label  \n",
       "0     -1.459020    1.0  \n",
       "1     -4.295380   -1.0  \n",
       "2     -0.072606    1.0  \n",
       "3      5.470587   -1.0  \n",
       "4     -5.717939   -1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('mnist/train_10pca.csv', float_precision='round_trip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = []\n",
    "dataset_labels = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    pca_components = []\n",
    "    for idx_pca_component in range(1, 11):\n",
    "        pca_components.append(row[str(idx_pca_component) + '_principal'])\n",
    "    dataset_inputs.append(pca_components)\n",
    "    dataset_labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset_inputs = torch.DoubleTensor(dataset_inputs[:10000]).to(device)\n",
    "torch_dataset_labels = torch.DoubleTensor([dataset_labels[:10000]]).T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jnp_dataset_inputs = jnp.array(dataset_inputs, dtype=jnp.float64)\n",
    "# jnp_dataset_labels = jnp.array(dataset_labels, dtype=jnp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H_student, D_out = 1, 10, 10, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A customizable student network, initialized using Glorot initialization.\n",
    "class StudentNetwork(nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "\n",
    "    D_in: input dimension\n",
    "    H: dimension of hidden layer\n",
    "    D_out: output dimension of the first layer\n",
    "    \"\"\"\n",
    "    super(StudentNetwork, self).__init__()\n",
    "    self.linear1 = nn.Linear(D_in, H, bias=True).double()\n",
    "    self.linear2 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear3 = nn.Linear(H, H, bias=True).double()\n",
    "    self.linear4 = nn.Linear(H, D_out, bias=True).double()\n",
    "\n",
    "    nn.init.xavier_uniform_(self.linear1.weight)\n",
    "    nn.init.xavier_uniform_(self.linear2.weight)\n",
    "    nn.init.xavier_uniform_(self.linear3.weight)\n",
    "    nn.init.xavier_uniform_(self.linear4.weight)\n",
    "    \n",
    "    nn.init.constant_(self.linear1.bias, 0)\n",
    "    nn.init.constant_(self.linear2.bias, 0)\n",
    "    nn.init.constant_(self.linear3.bias, 0)\n",
    "    nn.init.constant_(self.linear4.bias, 0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    h1 = torch.sigmoid(self.linear1(x))\n",
    "    h2 = torch.sigmoid(self.linear2(h1))\n",
    "    h3 = torch.sigmoid(self.linear3(h2))\n",
    "    y_pred = self.linear4(h3)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient norm evaluation\n",
    "def eval_grad_norm(loss_grad):\n",
    "  cnt = 0\n",
    "  for g in loss_grad:\n",
    "      if cnt == 0:\n",
    "        g_vector = g.contiguous().view(-1)\n",
    "      else:\n",
    "        g_vector = torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "      cnt = 1\n",
    "  grad_norm = torch.norm(g_vector)\n",
    " \n",
    "  return grad_norm.cpu().detach().numpy()\n",
    "\n",
    "## Main training entry point.\n",
    "def train(model, x, y_labels, N = 2 * (10 ** 4), Ninner = (10 ** 3), Nstart = 10,\n",
    "          maxtime = 7, nlopt_threshold = 1e-7,\n",
    "          collect_history = True):\n",
    "  lr = 1e-4\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#   checkpoint = torch.load(\"model_1e-6.pt\")\n",
    "#   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "  # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "\n",
    "  loss_fn = nn.MSELoss()\n",
    "  loss_vals = []\n",
    "  trace = []\n",
    "  if collect_history:\n",
    "    trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "  for i in range(1, N + 1):\n",
    "    loss_tmp = []\n",
    "    for j in range(1, Ninner + 1):\n",
    "      y = model(x)\n",
    "      loss = loss_fn(y, y_labels)\n",
    "      loss_grad = torch.autograd.grad(loss, model.parameters(),\n",
    "                                      retain_graph=True)\n",
    "      grad_norm = eval_grad_norm(loss_grad)\n",
    "      if grad_norm <= 1e-5:\n",
    "        print('found it')\n",
    "        EPOCH = 0\n",
    "        PATH = \"model.pt\"\n",
    "        LOSS = 0.4\n",
    "\n",
    "        torch.save({\n",
    "                'epoch': EPOCH,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'loss': LOSS,\n",
    "                }, PATH)\n",
    "        return loss_vals, trace\n",
    "      loss_tmp.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward(retain_graph=True)\n",
    "      optimizer.step()\n",
    "      if i == 1 and (j % Nstart == 0) and j < Ninner:\n",
    "        loss_vals.append(np.mean(loss_tmp[j - Nstart  : j]))\n",
    "        if collect_history:\n",
    "          trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                      deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    loss_vals.append(np.mean(loss_tmp))\n",
    "    if collect_history:\n",
    "      trace.append((deepcopy(model.module.linear1.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear2.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear3.weight.cpu().data.detach().numpy()),\n",
    "                  deepcopy(model.module.linear4.weight.cpu().data.detach().numpy())))\n",
    "    grad_norm = eval_grad_norm(loss_grad)\n",
    "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i,\n",
    "                                                          np.mean(loss_tmp),\n",
    "                                                          grad_norm))\n",
    "\n",
    "  EPOCH = i\n",
    "  PATH = \"model.pt\"\n",
    "  LOSS = 0.4\n",
    "\n",
    "  torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n",
    "  return loss_vals, trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = StudentNetwork(D_in, H_student, D_out)\n",
    "student_model = student_model.to(device)\n",
    "if device == 'cuda':\n",
    "    student_model = torch.nn.DataParallel(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, loss: 1.165907049586785, gradient norm: 0.3162582681198705\n",
      "Iteration: 2000, loss: 0.9325171109677285, gradient norm: 0.19451078992794202\n",
      "Iteration: 3000, loss: 0.7611628090584927, gradient norm: 0.26223010789899215\n",
      "Iteration: 4000, loss: 0.5351762659163797, gradient norm: 0.13088197562733125\n",
      "Iteration: 5000, loss: 0.43865494755095163, gradient norm: 0.050580198918123674\n",
      "Iteration: 6000, loss: 0.4019358038093246, gradient norm: 0.04017600303636649\n",
      "Iteration: 7000, loss: 0.36994085043586145, gradient norm: 0.03760954879431134\n",
      "Iteration: 8000, loss: 0.3406315797358295, gradient norm: 0.026875255065164267\n",
      "Iteration: 9000, loss: 0.31974056786983646, gradient norm: 0.018674704868762992\n",
      "Iteration: 10000, loss: 0.3062557089198105, gradient norm: 0.015756643134196405\n",
      "Iteration: 11000, loss: 0.2950911547280249, gradient norm: 0.014684294737807914\n",
      "Iteration: 12000, loss: 0.2832178650399634, gradient norm: 0.012455427314955015\n",
      "Iteration: 13000, loss: 0.27060061006175307, gradient norm: 0.011199252831942919\n",
      "Iteration: 14000, loss: 0.25816775931668334, gradient norm: 0.010257931442044937\n",
      "Iteration: 15000, loss: 0.24711354298260058, gradient norm: 0.009916781089721953\n",
      "Iteration: 16000, loss: 0.2372640626564219, gradient norm: 0.007669974924114625\n",
      "Iteration: 17000, loss: 0.23041663800805987, gradient norm: 0.005882453034933919\n",
      "Iteration: 18000, loss: 0.2257099022080328, gradient norm: 0.004947162888516903\n",
      "Iteration: 19000, loss: 0.22187385970244708, gradient norm: 0.004442946802241624\n",
      "Iteration: 20000, loss: 0.21825059728657675, gradient norm: 0.0077706745695032805\n",
      "Iteration: 21000, loss: 0.2127735861097782, gradient norm: 0.004586875456662468\n",
      "Iteration: 22000, loss: 0.20916873617375, gradient norm: 0.003669408961312317\n",
      "Iteration: 23000, loss: 0.20656771237299615, gradient norm: 0.003410246765842373\n",
      "Iteration: 24000, loss: 0.20414261694069666, gradient norm: 0.003498884772328519\n",
      "Iteration: 25000, loss: 0.2015690988797597, gradient norm: 0.0036612312243770747\n",
      "Iteration: 26000, loss: 0.19868048872661562, gradient norm: 0.003608248150734946\n",
      "Iteration: 27000, loss: 0.1957635007266602, gradient norm: 0.003396342188895659\n",
      "Iteration: 28000, loss: 0.19311658907881188, gradient norm: 0.0029168093117779\n",
      "Iteration: 29000, loss: 0.19092902608550838, gradient norm: 0.0025210161954425507\n",
      "Iteration: 30000, loss: 0.1891647623429677, gradient norm: 0.0020888010953935467\n",
      "Iteration: 31000, loss: 0.18759397181339546, gradient norm: 0.0019962975993911467\n",
      "Iteration: 32000, loss: 0.1860035795985972, gradient norm: 0.0016799697717512318\n",
      "Iteration: 33000, loss: 0.18461114426274627, gradient norm: 0.0024601898677247915\n",
      "Iteration: 34000, loss: 0.1832569854917424, gradient norm: 0.0014668356701175082\n",
      "Iteration: 35000, loss: 0.18193462172873873, gradient norm: 0.0014753194636505306\n",
      "Iteration: 36000, loss: 0.1806021752456054, gradient norm: 0.0013992297780935515\n",
      "Iteration: 37000, loss: 0.17918023019798457, gradient norm: 0.0014101430164621584\n",
      "Iteration: 38000, loss: 0.17771062466048249, gradient norm: 0.0018839179773982254\n",
      "Iteration: 39000, loss: 0.17635841002297717, gradient norm: 0.0012476493421361109\n",
      "Iteration: 40000, loss: 0.1751621013048674, gradient norm: 0.0011871471694472856\n",
      "Iteration: 41000, loss: 0.17405676298477457, gradient norm: 0.0011933668857295773\n",
      "Iteration: 42000, loss: 0.17297306761171147, gradient norm: 0.0029038455478897374\n",
      "Iteration: 43000, loss: 0.1718888249849733, gradient norm: 0.0011421322814083778\n",
      "Iteration: 44000, loss: 0.1708795148744685, gradient norm: 0.0011225410531041907\n",
      "Iteration: 45000, loss: 0.16992253865970577, gradient norm: 0.0012117282887526486\n",
      "Iteration: 46000, loss: 0.1690207874035334, gradient norm: 0.000986315547680635\n",
      "Iteration: 47000, loss: 0.16817149535085482, gradient norm: 0.000996263768055432\n",
      "Iteration: 48000, loss: 0.16734301838382915, gradient norm: 0.0010067902506561508\n",
      "Iteration: 49000, loss: 0.1663248943874488, gradient norm: 0.0025050226424402564\n",
      "Iteration: 50000, loss: 0.16522141919816266, gradient norm: 0.002072103846252346\n",
      "Iteration: 51000, loss: 0.16423794101528813, gradient norm: 0.0013480376665577366\n",
      "Iteration: 52000, loss: 0.16332410044532694, gradient norm: 0.0010391913821392108\n",
      "Iteration: 53000, loss: 0.16245911487168763, gradient norm: 0.002677664072088899\n",
      "Iteration: 54000, loss: 0.16158807838032851, gradient norm: 0.0012642118768884014\n",
      "Iteration: 55000, loss: 0.16063787636386354, gradient norm: 0.001444816333726041\n",
      "Iteration: 56000, loss: 0.15968860724964642, gradient norm: 0.0012629740709292426\n",
      "Iteration: 57000, loss: 0.1588310657956001, gradient norm: 0.001172059092288133\n",
      "Iteration: 58000, loss: 0.15804505255881365, gradient norm: 0.0010841085784334924\n",
      "Iteration: 59000, loss: 0.1572986289718441, gradient norm: 0.0010888363891600522\n",
      "Iteration: 60000, loss: 0.1565773630491173, gradient norm: 0.002216671182673528\n",
      "Iteration: 61000, loss: 0.155883332741279, gradient norm: 0.000995286719834395\n",
      "Iteration: 62000, loss: 0.15522343504862193, gradient norm: 0.001022100582463459\n",
      "Iteration: 63000, loss: 0.15460055439159828, gradient norm: 0.0008819153991599776\n",
      "Iteration: 64000, loss: 0.1540088419153862, gradient norm: 0.000881071087406403\n",
      "Iteration: 65000, loss: 0.15343893839064654, gradient norm: 0.0011654418248352814\n",
      "Iteration: 66000, loss: 0.1528800021226849, gradient norm: 0.0016587835638398785\n",
      "Iteration: 67000, loss: 0.1523224151028553, gradient norm: 0.0008327966126054794\n",
      "Iteration: 68000, loss: 0.15175510437075337, gradient norm: 0.0027024973311892695\n",
      "Iteration: 69000, loss: 0.1511899642688321, gradient norm: 0.0028138461477681454\n",
      "Iteration: 70000, loss: 0.15063505829090523, gradient norm: 0.0008058615625116357\n",
      "Iteration: 71000, loss: 0.15009273627462108, gradient norm: 0.0008743733268609311\n",
      "Iteration: 72000, loss: 0.14956557050679697, gradient norm: 0.0008681678412758428\n",
      "Iteration: 73000, loss: 0.14905691884853445, gradient norm: 0.0007783434834995487\n",
      "Iteration: 74000, loss: 0.148570081625539, gradient norm: 0.0007523409270427247\n",
      "Iteration: 75000, loss: 0.1481054458783372, gradient norm: 0.0008581649976076236\n",
      "Iteration: 76000, loss: 0.14766118179425383, gradient norm: 0.001021975918108026\n",
      "Iteration: 77000, loss: 0.14723328849526737, gradient norm: 0.0013159837578712705\n",
      "Iteration: 78000, loss: 0.14682009379694635, gradient norm: 0.0007878966053402914\n",
      "Iteration: 79000, loss: 0.1464212701915793, gradient norm: 0.002462182597676145\n",
      "Iteration: 80000, loss: 0.14603350226113987, gradient norm: 0.0009265249442341163\n",
      "Iteration: 81000, loss: 0.14565106123305366, gradient norm: 0.0006441788211052439\n",
      "Iteration: 82000, loss: 0.14527187476778713, gradient norm: 0.004319470792496582\n",
      "Iteration: 83000, loss: 0.14488544925827843, gradient norm: 0.0016351002885771726\n",
      "Iteration: 84000, loss: 0.14448469937948574, gradient norm: 0.004223182260225569\n",
      "Iteration: 85000, loss: 0.14407007299838043, gradient norm: 0.005181648356562404\n",
      "Iteration: 86000, loss: 0.14362045443711666, gradient norm: 0.0019149961156046161\n",
      "Iteration: 87000, loss: 0.14313889228484197, gradient norm: 0.0007321992685410631\n",
      "Iteration: 88000, loss: 0.14268831889569816, gradient norm: 0.0012360612777714114\n",
      "Iteration: 89000, loss: 0.14225738799181778, gradient norm: 0.0012217706363607838\n",
      "Iteration: 90000, loss: 0.14184119923115798, gradient norm: 0.0008417289068327736\n",
      "Iteration: 91000, loss: 0.1414401604338618, gradient norm: 0.005350209140032854\n",
      "Iteration: 92000, loss: 0.14105411843505186, gradient norm: 0.001630033283623074\n",
      "Iteration: 93000, loss: 0.14068233474765823, gradient norm: 0.001105221020597698\n",
      "Iteration: 94000, loss: 0.14032580738330216, gradient norm: 0.0008331278422715919\n",
      "Iteration: 95000, loss: 0.13998060972109763, gradient norm: 0.005345969215298783\n",
      "Iteration: 96000, loss: 0.1396382514959601, gradient norm: 0.0010129385231753233\n",
      "Iteration: 97000, loss: 0.13929077811021853, gradient norm: 0.002482984249789075\n",
      "Iteration: 98000, loss: 0.1389479776838768, gradient norm: 0.000552788088542115\n",
      "Iteration: 99000, loss: 0.1386114432822336, gradient norm: 0.001490720819101793\n",
      "Iteration: 100000, loss: 0.13828013040965487, gradient norm: 0.004715513304628832\n",
      "Iteration: 101000, loss: 0.1379545039050173, gradient norm: 0.0029969503538537956\n",
      "Iteration: 102000, loss: 0.13763147423005864, gradient norm: 0.0014384476811533755\n",
      "Iteration: 103000, loss: 0.1373081472671735, gradient norm: 0.0005540208672977068\n",
      "Iteration: 104000, loss: 0.13699078996496167, gradient norm: 0.0005289819602455985\n",
      "Iteration: 105000, loss: 0.1366821042476324, gradient norm: 0.006058249518206637\n",
      "Iteration: 106000, loss: 0.1363820537747529, gradient norm: 0.0006626097121440889\n",
      "Iteration: 107000, loss: 0.13609156587741347, gradient norm: 0.0038310521952588097\n",
      "Iteration: 108000, loss: 0.13581030077683917, gradient norm: 0.0005685434689470817\n",
      "Iteration: 109000, loss: 0.13553604620837517, gradient norm: 0.00044417832043182104\n",
      "Iteration: 110000, loss: 0.1352653204727485, gradient norm: 0.0022556345422398887\n",
      "Iteration: 111000, loss: 0.13496265017095233, gradient norm: 0.0006761992770839921\n",
      "Iteration: 112000, loss: 0.13465999916698876, gradient norm: 0.0004545379406281163\n",
      "Iteration: 113000, loss: 0.13437976765424048, gradient norm: 0.0008443410194555645\n",
      "Iteration: 114000, loss: 0.13411114086904985, gradient norm: 0.0008798076344299845\n",
      "Iteration: 115000, loss: 0.1338507649779922, gradient norm: 0.0009519664227437316\n",
      "Iteration: 116000, loss: 0.1335956346593922, gradient norm: 0.002421323895371124\n",
      "Iteration: 117000, loss: 0.1333448026125455, gradient norm: 0.0007740222022895567\n",
      "Iteration: 118000, loss: 0.13309798565690306, gradient norm: 0.0005473745395918132\n",
      "Iteration: 119000, loss: 0.132854162758693, gradient norm: 0.0005062894179837059\n",
      "Iteration: 120000, loss: 0.13261310927311226, gradient norm: 0.0016284723971189176\n",
      "Iteration: 121000, loss: 0.13232656508285326, gradient norm: 0.00044651031020187434\n",
      "Iteration: 122000, loss: 0.1319853756860992, gradient norm: 0.0004743044978532076\n",
      "Iteration: 123000, loss: 0.13169635442405345, gradient norm: 0.011305467914805864\n",
      "Iteration: 124000, loss: 0.13143723923174477, gradient norm: 0.004933032158306903\n",
      "Iteration: 125000, loss: 0.13119757715361635, gradient norm: 0.011193291091524537\n",
      "Iteration: 126000, loss: 0.13097316974093381, gradient norm: 0.0005687235200295629\n",
      "Iteration: 127000, loss: 0.13076534539210524, gradient norm: 0.002693273021859825\n",
      "Iteration: 128000, loss: 0.13057286686806555, gradient norm: 0.015326518203152084\n",
      "Iteration: 129000, loss: 0.13039245041558392, gradient norm: 0.013643038695095094\n",
      "Iteration: 130000, loss: 0.1302218124789247, gradient norm: 0.0034867949154404844\n",
      "Iteration: 131000, loss: 0.13006004935311158, gradient norm: 0.0016608926287994758\n",
      "Iteration: 132000, loss: 0.1299056771335882, gradient norm: 0.009927382128117303\n",
      "Iteration: 133000, loss: 0.12975713886116563, gradient norm: 0.0010723451633003604\n",
      "Iteration: 134000, loss: 0.12961430280705274, gradient norm: 0.0003752922787268776\n",
      "Iteration: 135000, loss: 0.12947624921741824, gradient norm: 0.0019673703267040123\n",
      "Iteration: 136000, loss: 0.1293405182128484, gradient norm: 0.012630055273305334\n",
      "Iteration: 137000, loss: 0.12920659620999084, gradient norm: 0.0009839534225710995\n",
      "Iteration: 138000, loss: 0.1290767231722628, gradient norm: 0.002406025339916035\n",
      "Iteration: 139000, loss: 0.12895009084608944, gradient norm: 0.0033529561353490738\n",
      "Iteration: 140000, loss: 0.1288245885996921, gradient norm: 0.0003836419020385729\n",
      "Iteration: 141000, loss: 0.12869806253110067, gradient norm: 0.00024783112460227707\n",
      "Iteration: 142000, loss: 0.12856760214883273, gradient norm: 0.008396310925494699\n",
      "Iteration: 143000, loss: 0.12843111738469348, gradient norm: 0.0017903320386839705\n",
      "Iteration: 144000, loss: 0.12829289285155238, gradient norm: 0.009734238807453811\n",
      "Iteration: 145000, loss: 0.1281588333424344, gradient norm: 0.003578804538545965\n",
      "Iteration: 146000, loss: 0.1280284392243609, gradient norm: 0.0016607036405795724\n",
      "Iteration: 147000, loss: 0.12790023427077768, gradient norm: 0.0012620697829137076\n",
      "Iteration: 148000, loss: 0.12777060443072902, gradient norm: 0.0003007906681331462\n",
      "Iteration: 149000, loss: 0.12762864909700752, gradient norm: 0.0002801583856705747\n",
      "Iteration: 150000, loss: 0.12747190958227242, gradient norm: 0.008846935568621919\n",
      "Iteration: 151000, loss: 0.1273063746732759, gradient norm: 0.004186571559501201\n",
      "Iteration: 152000, loss: 0.12715226389154144, gradient norm: 0.0009105328670923216\n",
      "Iteration: 153000, loss: 0.12700303420438153, gradient norm: 0.0004899999620981527\n",
      "Iteration: 154000, loss: 0.12685624676736054, gradient norm: 0.00376358072196894\n",
      "Iteration: 155000, loss: 0.1267086320240983, gradient norm: 0.028488596751714817\n",
      "Iteration: 156000, loss: 0.12655949814602546, gradient norm: 0.04048069738216911\n",
      "Iteration: 157000, loss: 0.12641198102438062, gradient norm: 0.0018936650060817262\n",
      "Iteration: 158000, loss: 0.1262624668620736, gradient norm: 0.0016549876739041388\n",
      "Iteration: 159000, loss: 0.12610579594087332, gradient norm: 0.003913134666399784\n",
      "Iteration: 160000, loss: 0.12595305532199028, gradient norm: 0.005060109261392531\n",
      "Iteration: 161000, loss: 0.1258060934238379, gradient norm: 0.020188881437578122\n",
      "Iteration: 162000, loss: 0.1256642652330859, gradient norm: 0.03244571454874522\n",
      "Iteration: 163000, loss: 0.125531932402601, gradient norm: 0.030048739326722244\n",
      "Iteration: 164000, loss: 0.12540604826452656, gradient norm: 0.0025425195124079473\n",
      "Iteration: 165000, loss: 0.12528626552723524, gradient norm: 0.011226205757610892\n",
      "Iteration: 166000, loss: 0.12517207067165945, gradient norm: 0.01638572000823761\n",
      "Iteration: 167000, loss: 0.12506299686544586, gradient norm: 0.0087594941817431\n",
      "Iteration: 168000, loss: 0.12495778309876808, gradient norm: 0.00020818596547576506\n",
      "Iteration: 169000, loss: 0.12485594933724228, gradient norm: 0.02612209891692162\n",
      "Iteration: 170000, loss: 0.12475636654585322, gradient norm: 0.0002334792348724746\n",
      "Iteration: 171000, loss: 0.12465907335560444, gradient norm: 0.010999787677054758\n",
      "Iteration: 172000, loss: 0.1245634255688151, gradient norm: 0.00902116043937453\n",
      "Iteration: 173000, loss: 0.12446915600044546, gradient norm: 0.002852017091046742\n",
      "Iteration: 174000, loss: 0.12437717105698146, gradient norm: 0.004460192263931684\n",
      "Iteration: 175000, loss: 0.12428752806480999, gradient norm: 0.0004536726504432939\n",
      "Iteration: 176000, loss: 0.12420050901733026, gradient norm: 0.004800569295948149\n",
      "Iteration: 177000, loss: 0.1241147652804996, gradient norm: 0.01605992157490564\n",
      "Iteration: 178000, loss: 0.1240291075570483, gradient norm: 0.003811859905997927\n",
      "Iteration: 179000, loss: 0.12394209374533217, gradient norm: 0.003298476886069575\n",
      "Iteration: 180000, loss: 0.12385488695068952, gradient norm: 0.008918689518733584\n",
      "Iteration: 181000, loss: 0.1237680631652322, gradient norm: 0.016710093119528955\n",
      "Iteration: 182000, loss: 0.12368098528873687, gradient norm: 0.008393481209716227\n",
      "Iteration: 183000, loss: 0.1235908230721233, gradient norm: 0.005149521045012156\n",
      "Iteration: 184000, loss: 0.12350104000232341, gradient norm: 0.0008505920416397447\n",
      "Iteration: 185000, loss: 0.12341260881848601, gradient norm: 0.001148671906314306\n",
      "Iteration: 186000, loss: 0.1233232958587786, gradient norm: 0.02221527575461057\n",
      "Iteration: 187000, loss: 0.12323118014808311, gradient norm: 0.03389543557330823\n",
      "Iteration: 188000, loss: 0.12313562842342687, gradient norm: 0.003419059921202431\n",
      "Iteration: 189000, loss: 0.1230405836848959, gradient norm: 0.002633501026981185\n",
      "Iteration: 190000, loss: 0.12294864934751092, gradient norm: 0.0025912359804557555\n",
      "Iteration: 191000, loss: 0.12285984199260577, gradient norm: 0.026928854429534337\n",
      "Iteration: 192000, loss: 0.12277415463599536, gradient norm: 0.007750082652198664\n",
      "Iteration: 193000, loss: 0.12269133994415712, gradient norm: 0.0026381882965904957\n",
      "Iteration: 194000, loss: 0.12261178834278369, gradient norm: 0.01675185294775767\n",
      "Iteration: 195000, loss: 0.12253517213537482, gradient norm: 0.00180803283273807\n",
      "Iteration: 196000, loss: 0.1224618014180252, gradient norm: 0.008002141592198904\n",
      "Iteration: 197000, loss: 0.12239130862976987, gradient norm: 0.01726413135571599\n",
      "Iteration: 198000, loss: 0.12232342841631964, gradient norm: 0.008261878854956672\n",
      "Iteration: 199000, loss: 0.12225836496911156, gradient norm: 0.0023499991936638586\n",
      "Iteration: 200000, loss: 0.12219606498497651, gradient norm: 0.0012336524235630239\n",
      "Iteration: 201000, loss: 0.12213615998813508, gradient norm: 0.004834183861758916\n",
      "Iteration: 202000, loss: 0.12207861667092013, gradient norm: 0.007899822451119467\n",
      "Iteration: 203000, loss: 0.1220233093949095, gradient norm: 0.015867664527532025\n",
      "Iteration: 204000, loss: 0.12197057047645266, gradient norm: 0.0025466095322623735\n",
      "Iteration: 205000, loss: 0.12191988978491171, gradient norm: 0.0425892742562731\n",
      "Iteration: 206000, loss: 0.12187104907965028, gradient norm: 0.03152967881727506\n",
      "Iteration: 207000, loss: 0.12182389659530296, gradient norm: 0.003826787493639298\n",
      "Iteration: 208000, loss: 0.12177854847139157, gradient norm: 0.0016299788717851378\n",
      "Iteration: 209000, loss: 0.12173521122358952, gradient norm: 0.052687518076682295\n",
      "Iteration: 210000, loss: 0.12169326442984031, gradient norm: 0.002538663153331895\n",
      "Iteration: 211000, loss: 0.12165246753638258, gradient norm: 0.0036274604953198926\n",
      "Iteration: 212000, loss: 0.12161300335097533, gradient norm: 0.004219428571350566\n",
      "Iteration: 213000, loss: 0.12157508517714072, gradient norm: 0.009837005676522188\n",
      "Iteration: 214000, loss: 0.12153853934294989, gradient norm: 0.002169712835851869\n",
      "Iteration: 215000, loss: 0.12150292678621247, gradient norm: 0.0003825488647741351\n",
      "Iteration: 216000, loss: 0.121468461198907, gradient norm: 0.008461067687246578\n",
      "Iteration: 217000, loss: 0.12143483633875143, gradient norm: 0.00536447794863452\n",
      "Iteration: 218000, loss: 0.12140215820376576, gradient norm: 0.042299480535455944\n",
      "Iteration: 219000, loss: 0.12136992790504533, gradient norm: 0.005778844600742447\n",
      "Iteration: 220000, loss: 0.12133762703644922, gradient norm: 0.0024473804764182584\n",
      "Iteration: 221000, loss: 0.12130302538152447, gradient norm: 0.03691478760025321\n",
      "Iteration: 222000, loss: 0.12126575743190562, gradient norm: 0.00578049518316307\n",
      "Iteration: 223000, loss: 0.1212256977203224, gradient norm: 0.04861949318237003\n",
      "Iteration: 224000, loss: 0.12117359781059261, gradient norm: 0.004029052942516826\n",
      "Iteration: 225000, loss: 0.12109064004429436, gradient norm: 0.005232542367100373\n",
      "Iteration: 226000, loss: 0.12098650177874676, gradient norm: 0.0005124492758870923\n",
      "Iteration: 227000, loss: 0.12091544032123532, gradient norm: 0.003173881849260252\n",
      "Iteration: 228000, loss: 0.12085745023521173, gradient norm: 0.059380195629102406\n",
      "Iteration: 229000, loss: 0.12080546697841373, gradient norm: 0.0008114259559728815\n",
      "Iteration: 230000, loss: 0.12075682309287522, gradient norm: 0.0429325127561534\n",
      "Iteration: 231000, loss: 0.12071004958468419, gradient norm: 0.003573245049467398\n",
      "Iteration: 232000, loss: 0.12066404652234823, gradient norm: 0.019911662917674953\n",
      "Iteration: 233000, loss: 0.12061698483150175, gradient norm: 0.005382448752194839\n",
      "Iteration: 234000, loss: 0.12056749876195266, gradient norm: 0.0014388467789603582\n",
      "Iteration: 235000, loss: 0.12051593501196665, gradient norm: 0.0001586831720480452\n",
      "Iteration: 236000, loss: 0.12046437193539651, gradient norm: 0.010194103734916641\n",
      "Iteration: 237000, loss: 0.12041324371244609, gradient norm: 0.030256614846443323\n",
      "Iteration: 238000, loss: 0.12036411004494577, gradient norm: 0.0134976248981123\n",
      "Iteration: 239000, loss: 0.12031722117528014, gradient norm: 0.006584458949341928\n",
      "Iteration: 240000, loss: 0.12027248485803702, gradient norm: 0.07873640491375734\n",
      "Iteration: 241000, loss: 0.12022993163375281, gradient norm: 0.002405528640004463\n",
      "Iteration: 242000, loss: 0.12018910780219161, gradient norm: 0.004185133453042874\n",
      "Iteration: 243000, loss: 0.12014984955737765, gradient norm: 0.017488544920521806\n",
      "Iteration: 244000, loss: 0.12011195949920381, gradient norm: 0.08098415050267858\n",
      "Iteration: 245000, loss: 0.12007520637115424, gradient norm: 0.024730513315034848\n",
      "Iteration: 246000, loss: 0.12003950438763045, gradient norm: 0.022345169463794893\n",
      "Iteration: 247000, loss: 0.1200048961989246, gradient norm: 0.006237072520235149\n",
      "Iteration: 248000, loss: 0.11997119721620825, gradient norm: 0.014624743377510187\n",
      "Iteration: 249000, loss: 0.11993818643274363, gradient norm: 0.0039971868253614986\n",
      "Iteration: 250000, loss: 0.11990590288162742, gradient norm: 0.0032646292676577634\n",
      "Iteration: 251000, loss: 0.11987436743890166, gradient norm: 0.013927842690216036\n",
      "Iteration: 252000, loss: 0.11984324495188654, gradient norm: 0.021419334252707874\n",
      "Iteration: 253000, loss: 0.11981259210474557, gradient norm: 0.009798970264261925\n",
      "Iteration: 254000, loss: 0.1197824517585908, gradient norm: 0.013543247803283018\n",
      "Iteration: 255000, loss: 0.11975259731626427, gradient norm: 0.028018961877598073\n",
      "Iteration: 256000, loss: 0.11972284448051597, gradient norm: 0.01896528939161421\n",
      "Iteration: 257000, loss: 0.11969325224568288, gradient norm: 0.0033564652877255364\n",
      "Iteration: 258000, loss: 0.11966406088090652, gradient norm: 0.015057150815750164\n",
      "Iteration: 259000, loss: 0.11963476125941602, gradient norm: 0.0024680943013912007\n",
      "Iteration: 260000, loss: 0.11960560072445361, gradient norm: 0.10062947574411853\n",
      "Iteration: 261000, loss: 0.1195722686887237, gradient norm: 0.002033409319766312\n",
      "Iteration: 262000, loss: 0.11952846197618629, gradient norm: 0.002912786347727256\n",
      "Iteration: 263000, loss: 0.11949469505516284, gradient norm: 0.02089343646937555\n",
      "Iteration: 264000, loss: 0.11946065643327586, gradient norm: 0.036421444883402834\n",
      "Iteration: 265000, loss: 0.11942496190451124, gradient norm: 0.005554963826311895\n",
      "Iteration: 266000, loss: 0.1193884767193461, gradient norm: 0.0281325893826241\n",
      "Iteration: 267000, loss: 0.11935265053104403, gradient norm: 0.015100107614375224\n",
      "Iteration: 268000, loss: 0.1193177055317545, gradient norm: 0.05416643542010839\n",
      "Iteration: 269000, loss: 0.11928206193948417, gradient norm: 0.00770236281631156\n",
      "Iteration: 270000, loss: 0.11920032602738678, gradient norm: 0.009981188463350938\n",
      "Iteration: 271000, loss: 0.11909841754900614, gradient norm: 0.0009844790326429427\n",
      "Iteration: 272000, loss: 0.1190446511901683, gradient norm: 0.0029024088015523635\n",
      "Iteration: 273000, loss: 0.11899414423128363, gradient norm: 0.01071315605866702\n",
      "Iteration: 274000, loss: 0.11894560102843561, gradient norm: 0.004237664080844542\n",
      "Iteration: 275000, loss: 0.11890105413581574, gradient norm: 0.0021473376731343343\n",
      "Iteration: 276000, loss: 0.11885952796621535, gradient norm: 0.001773804775730703\n",
      "Iteration: 277000, loss: 0.11882060453149397, gradient norm: 0.01844471667809242\n",
      "Iteration: 278000, loss: 0.11878364021469451, gradient norm: 0.02703146248612607\n",
      "Iteration: 279000, loss: 0.11874838807951311, gradient norm: 0.04486500232800164\n",
      "Iteration: 280000, loss: 0.11871464831916857, gradient norm: 0.007693991642946323\n",
      "Iteration: 281000, loss: 0.11868205968325889, gradient norm: 0.0028756905426258225\n",
      "Iteration: 282000, loss: 0.11865060551947687, gradient norm: 0.004854289639722779\n",
      "Iteration: 283000, loss: 0.11862021552246299, gradient norm: 0.0034071154787170582\n",
      "Iteration: 284000, loss: 0.11859083678266955, gradient norm: 0.005705936672102284\n",
      "Iteration: 285000, loss: 0.11856241812226649, gradient norm: 0.0031514161926685996\n",
      "Iteration: 286000, loss: 0.11853516923954975, gradient norm: 0.0781853880670366\n",
      "Iteration: 287000, loss: 0.11850869967673917, gradient norm: 0.005870941346736456\n",
      "Iteration: 288000, loss: 0.1184829299756868, gradient norm: 0.03430926490968177\n",
      "Iteration: 289000, loss: 0.11845779933087873, gradient norm: 0.04502382617005238\n",
      "Iteration: 290000, loss: 0.11843330867055363, gradient norm: 0.1059870746063542\n",
      "Iteration: 291000, loss: 0.11840943303072811, gradient norm: 0.00792539722682574\n",
      "Iteration: 292000, loss: 0.11838615130705672, gradient norm: 0.006207397981227062\n",
      "Iteration: 293000, loss: 0.1183633530774463, gradient norm: 0.025775937580819856\n",
      "Iteration: 294000, loss: 0.11834107948588544, gradient norm: 0.02000358551731603\n",
      "Iteration: 295000, loss: 0.11831906644873359, gradient norm: 0.016157730804273827\n",
      "Iteration: 296000, loss: 0.118297311023416, gradient norm: 0.016246338632308528\n",
      "Iteration: 297000, loss: 0.11827554807574708, gradient norm: 0.03272955598006881\n",
      "Iteration: 298000, loss: 0.118251789842751, gradient norm: 0.02700532948588674\n",
      "Iteration: 299000, loss: 0.11821584957386012, gradient norm: 0.021162183850339827\n",
      "Iteration: 300000, loss: 0.1180390039156195, gradient norm: 0.003145423527683734\n",
      "Iteration: 301000, loss: 0.1178319294915033, gradient norm: 0.0954223314288213\n",
      "Iteration: 302000, loss: 0.11776830223942229, gradient norm: 0.007178475968286905\n",
      "Iteration: 303000, loss: 0.11772177907187645, gradient norm: 0.031562308142234693\n",
      "Iteration: 304000, loss: 0.1176823080406059, gradient norm: 0.008949302433712562\n",
      "Iteration: 305000, loss: 0.11764718592959728, gradient norm: 0.004392789733068083\n",
      "Iteration: 306000, loss: 0.11761474692559122, gradient norm: 0.01545672614373778\n",
      "Iteration: 307000, loss: 0.11758426719639162, gradient norm: 0.07032449543212203\n",
      "Iteration: 308000, loss: 0.1175551031012181, gradient norm: 0.009946962185158975\n",
      "Iteration: 309000, loss: 0.1175268217339474, gradient norm: 0.005155527899525199\n",
      "Iteration: 310000, loss: 0.11749907206397557, gradient norm: 0.024294500668024653\n",
      "Iteration: 311000, loss: 0.11747148769908665, gradient norm: 0.05688098298145331\n",
      "Iteration: 312000, loss: 0.1174438879446085, gradient norm: 0.039916629440881174\n",
      "Iteration: 313000, loss: 0.11741588734110547, gradient norm: 0.05125147617856984\n",
      "Iteration: 314000, loss: 0.11738747534330637, gradient norm: 0.004800683266678675\n",
      "Iteration: 315000, loss: 0.11735871312818781, gradient norm: 0.0038083004800166125\n",
      "Iteration: 316000, loss: 0.11732946166027111, gradient norm: 0.037956033139756234\n",
      "Iteration: 317000, loss: 0.11729951065033073, gradient norm: 0.004445707346812713\n",
      "Iteration: 318000, loss: 0.11726928302104632, gradient norm: 0.009437077779532146\n",
      "Iteration: 319000, loss: 0.11723923235866394, gradient norm: 0.03033007615846761\n",
      "Iteration: 320000, loss: 0.1172094354620987, gradient norm: 0.006379546333331466\n",
      "Iteration: 321000, loss: 0.11718005358986651, gradient norm: 0.03160259078288378\n",
      "Iteration: 322000, loss: 0.11715116621162909, gradient norm: 0.004892386399672426\n",
      "Iteration: 323000, loss: 0.11712289269609082, gradient norm: 0.0010933103800353091\n",
      "Iteration: 324000, loss: 0.11709524687316451, gradient norm: 0.024304355470768\n",
      "Iteration: 325000, loss: 0.11706799865472285, gradient norm: 0.006600867601242464\n",
      "Iteration: 326000, loss: 0.11704133265270286, gradient norm: 0.03935815467229825\n",
      "Iteration: 327000, loss: 0.11701516332861335, gradient norm: 0.00474498824061404\n",
      "Iteration: 328000, loss: 0.11698942356811375, gradient norm: 0.008085900787542673\n",
      "Iteration: 329000, loss: 0.1169642535757227, gradient norm: 0.01056020809026694\n",
      "Iteration: 330000, loss: 0.11693954526814625, gradient norm: 0.01390728294955597\n",
      "Iteration: 331000, loss: 0.11691527764896631, gradient norm: 0.04191196310766501\n",
      "Iteration: 332000, loss: 0.11689142570590377, gradient norm: 0.0071489889333951825\n",
      "Iteration: 333000, loss: 0.11686803057663053, gradient norm: 0.007108860443228469\n",
      "Iteration: 334000, loss: 0.11684500903033589, gradient norm: 0.011163333448943987\n",
      "Iteration: 335000, loss: 0.11682227055704596, gradient norm: 0.08680044829490194\n",
      "Iteration: 336000, loss: 0.11679987652636598, gradient norm: 0.01689384371968508\n",
      "Iteration: 337000, loss: 0.11677772958254999, gradient norm: 0.09503994041832214\n",
      "Iteration: 338000, loss: 0.11675593786098697, gradient norm: 0.025269717264565107\n",
      "Iteration: 339000, loss: 0.1167343133281714, gradient norm: 0.12051300917832292\n",
      "Iteration: 340000, loss: 0.1167129689830544, gradient norm: 0.05900144008090392\n",
      "Iteration: 341000, loss: 0.11669184641230035, gradient norm: 0.030262268332651644\n",
      "Iteration: 342000, loss: 0.11667092437792258, gradient norm: 0.0017870969166640677\n",
      "Iteration: 343000, loss: 0.11665034066742205, gradient norm: 0.04059261998164605\n",
      "Iteration: 344000, loss: 0.11662998011258634, gradient norm: 0.01629056057336634\n",
      "Iteration: 345000, loss: 0.11660983877118218, gradient norm: 0.047875424880604645\n",
      "Iteration: 346000, loss: 0.1165897916273604, gradient norm: 0.002555153720547182\n",
      "Iteration: 347000, loss: 0.11656966175171905, gradient norm: 0.004787313326910543\n",
      "Iteration: 348000, loss: 0.11654848933351684, gradient norm: 0.02123305334143957\n",
      "Iteration: 349000, loss: 0.11652623996850989, gradient norm: 0.03326665625706124\n",
      "Iteration: 350000, loss: 0.11650464105175726, gradient norm: 0.0012184882053774102\n",
      "Iteration: 351000, loss: 0.11648400969454963, gradient norm: 0.005967676866173891\n",
      "Iteration: 352000, loss: 0.11646423754050753, gradient norm: 0.06516898561763527\n",
      "Iteration: 353000, loss: 0.11644511381746465, gradient norm: 0.11624874065590342\n",
      "Iteration: 354000, loss: 0.11642661159225466, gradient norm: 0.02185309214370372\n",
      "Iteration: 355000, loss: 0.1164086348331819, gradient norm: 0.10372317386670463\n",
      "Iteration: 356000, loss: 0.11639107637421692, gradient norm: 0.09901925407493313\n",
      "Iteration: 357000, loss: 0.11637395279124316, gradient norm: 0.04633717652650102\n",
      "Iteration: 358000, loss: 0.11635717884977828, gradient norm: 0.008610947003085909\n",
      "Iteration: 359000, loss: 0.11634078225126294, gradient norm: 0.02965072392315479\n",
      "Iteration: 360000, loss: 0.11632471179463852, gradient norm: 0.018308884816114843\n",
      "Iteration: 361000, loss: 0.11630889638103764, gradient norm: 0.08457558708138946\n",
      "Iteration: 362000, loss: 0.11629338399310464, gradient norm: 0.005251784084687036\n",
      "Iteration: 363000, loss: 0.11627815610376446, gradient norm: 0.00697150036976117\n",
      "Iteration: 364000, loss: 0.11626312851638909, gradient norm: 0.017115459656110214\n",
      "Iteration: 365000, loss: 0.11624830155449643, gradient norm: 0.11053723369479458\n",
      "Iteration: 366000, loss: 0.11623371324368073, gradient norm: 0.0153425749111987\n",
      "Iteration: 367000, loss: 0.1162193116358762, gradient norm: 0.03489100465020313\n",
      "Iteration: 368000, loss: 0.11620512607552264, gradient norm: 0.10581316884454758\n",
      "Iteration: 369000, loss: 0.11619120866678463, gradient norm: 0.010612980530305259\n",
      "Iteration: 370000, loss: 0.1161773469172243, gradient norm: 0.09703987081762785\n",
      "Iteration: 371000, loss: 0.1161637838130002, gradient norm: 0.10198020719302298\n",
      "Iteration: 372000, loss: 0.11615037263238423, gradient norm: 0.06679516695250169\n",
      "Iteration: 373000, loss: 0.11613705775228926, gradient norm: 0.0025333350278807483\n",
      "Iteration: 374000, loss: 0.11612402458338343, gradient norm: 0.06915516829733714\n",
      "Iteration: 375000, loss: 0.11611103402273817, gradient norm: 0.009826355922375057\n",
      "Iteration: 376000, loss: 0.1160982508529715, gradient norm: 0.02478312060393347\n",
      "Iteration: 377000, loss: 0.11608551979480751, gradient norm: 0.036204906105821985\n",
      "Iteration: 378000, loss: 0.11607292747130052, gradient norm: 0.07314593813783399\n",
      "Iteration: 379000, loss: 0.11606053820615769, gradient norm: 0.02824663872091725\n",
      "Iteration: 380000, loss: 0.11604821276426311, gradient norm: 0.009115332454617436\n",
      "Iteration: 381000, loss: 0.1160360678002672, gradient norm: 0.01620679203016314\n",
      "Iteration: 382000, loss: 0.11602410172491216, gradient norm: 0.009617932564748055\n",
      "Iteration: 383000, loss: 0.11601223974093527, gradient norm: 0.016091708724763454\n",
      "Iteration: 384000, loss: 0.11600043111445747, gradient norm: 0.04964511361293098\n",
      "Iteration: 385000, loss: 0.11598875198896948, gradient norm: 0.02492517990441666\n",
      "Iteration: 386000, loss: 0.11597718120997104, gradient norm: 0.02912975066874569\n",
      "Iteration: 387000, loss: 0.11596565176702174, gradient norm: 0.004016439495001896\n",
      "Iteration: 388000, loss: 0.1159542685676812, gradient norm: 0.0173624594834639\n",
      "Iteration: 389000, loss: 0.11594294066084362, gradient norm: 0.02020649336142516\n",
      "Iteration: 390000, loss: 0.11593170758302179, gradient norm: 0.013201239625095779\n",
      "Iteration: 391000, loss: 0.11592053368110884, gradient norm: 0.030608665527592137\n",
      "Iteration: 392000, loss: 0.11590939542542861, gradient norm: 0.02809980185933418\n",
      "Iteration: 393000, loss: 0.11589827350817683, gradient norm: 0.003772047810401472\n",
      "Iteration: 394000, loss: 0.11588712118897883, gradient norm: 0.07620390418968893\n",
      "Iteration: 395000, loss: 0.11587574613367355, gradient norm: 0.09719259544102694\n",
      "Iteration: 396000, loss: 0.11586420797311867, gradient norm: 0.04404630588834295\n",
      "Iteration: 397000, loss: 0.11585250091936983, gradient norm: 0.14715932603542953\n",
      "Iteration: 398000, loss: 0.1158408556013337, gradient norm: 0.18857638300137525\n",
      "Iteration: 399000, loss: 0.11582916287846379, gradient norm: 0.0919408968265002\n",
      "Iteration: 400000, loss: 0.1158176049729922, gradient norm: 0.0626846844913886\n",
      "Iteration: 401000, loss: 0.11580592612394591, gradient norm: 0.009613683572041511\n",
      "Iteration: 402000, loss: 0.11579428933587287, gradient norm: 0.0007761167631142859\n",
      "Iteration: 403000, loss: 0.11578269563626037, gradient norm: 0.00953013382431608\n",
      "Iteration: 404000, loss: 0.11577111297029649, gradient norm: 0.002848233665477147\n",
      "Iteration: 405000, loss: 0.11575979159691235, gradient norm: 0.05386323122473058\n",
      "Iteration: 406000, loss: 0.11574869496891055, gradient norm: 0.0034647643957468337\n",
      "Iteration: 407000, loss: 0.11573790363793429, gradient norm: 0.1470638821179254\n",
      "Iteration: 408000, loss: 0.11572730341007713, gradient norm: 0.06404360281441986\n",
      "Iteration: 409000, loss: 0.11571681245973978, gradient norm: 0.007474412047282282\n",
      "Iteration: 410000, loss: 0.11570651841688054, gradient norm: 0.041168675565617194\n",
      "Iteration: 411000, loss: 0.11569637996153577, gradient norm: 0.032488019985540996\n",
      "Iteration: 412000, loss: 0.11568638367099862, gradient norm: 0.11416319006897178\n",
      "Iteration: 413000, loss: 0.11567644352603787, gradient norm: 0.0034405585980023267\n",
      "Iteration: 414000, loss: 0.11566663569614716, gradient norm: 0.002163662868213555\n",
      "Iteration: 415000, loss: 0.11565700186422577, gradient norm: 0.024180075147648996\n",
      "Iteration: 416000, loss: 0.11564743360261007, gradient norm: 0.0033126211802573663\n",
      "Iteration: 417000, loss: 0.11563796020913082, gradient norm: 0.011220125235139322\n",
      "Iteration: 418000, loss: 0.11562861024274652, gradient norm: 0.08896442114256099\n",
      "Iteration: 419000, loss: 0.11561927968334783, gradient norm: 0.07127801570322262\n",
      "Iteration: 420000, loss: 0.1156100719768671, gradient norm: 0.05268436964998621\n",
      "Iteration: 421000, loss: 0.1156009414698685, gradient norm: 0.021367229043331985\n",
      "Iteration: 422000, loss: 0.11559183596334219, gradient norm: 0.0033486597485232533\n",
      "Iteration: 423000, loss: 0.11558294164515558, gradient norm: 0.01108737040262685\n",
      "Iteration: 424000, loss: 0.11557404344382141, gradient norm: 0.03199337996898183\n",
      "Iteration: 425000, loss: 0.11556519399180042, gradient norm: 0.002038560780629238\n",
      "Iteration: 426000, loss: 0.11555650722628914, gradient norm: 0.04945308483931052\n",
      "Iteration: 427000, loss: 0.11554766367961886, gradient norm: 0.013691124205755188\n",
      "Iteration: 428000, loss: 0.11553891832818682, gradient norm: 0.003200947969579574\n",
      "Iteration: 429000, loss: 0.11549552017068777, gradient norm: 0.20121328017907003\n",
      "Iteration: 430000, loss: 0.11546524509608948, gradient norm: 0.042677400513792124\n",
      "Iteration: 431000, loss: 0.11545096700919101, gradient norm: 0.03578488935582412\n",
      "Iteration: 432000, loss: 0.11543898174791734, gradient norm: 0.02558941123478567\n",
      "Iteration: 433000, loss: 0.11542806639751158, gradient norm: 0.09146901902479358\n",
      "Iteration: 434000, loss: 0.11541763033477705, gradient norm: 0.09491074740011955\n",
      "Iteration: 435000, loss: 0.11540763333453884, gradient norm: 0.03783635485195845\n",
      "Iteration: 436000, loss: 0.11539778977959231, gradient norm: 0.05778408923787799\n",
      "Iteration: 437000, loss: 0.11538819904582874, gradient norm: 0.046001570567880117\n",
      "Iteration: 438000, loss: 0.11537871471147687, gradient norm: 0.2302689406945985\n",
      "Iteration: 439000, loss: 0.11536942631274859, gradient norm: 0.003665804758328315\n",
      "Iteration: 440000, loss: 0.11536024958114101, gradient norm: 0.06222886025607187\n",
      "Iteration: 441000, loss: 0.11535113125996833, gradient norm: 0.012605568139086231\n",
      "Iteration: 442000, loss: 0.11534208759401561, gradient norm: 0.020097987799043607\n",
      "Iteration: 443000, loss: 0.11533322948312358, gradient norm: 0.006181056293986714\n",
      "Iteration: 444000, loss: 0.11532447640079692, gradient norm: 0.0958662842765189\n",
      "Iteration: 445000, loss: 0.11531591183355654, gradient norm: 0.034687692398707434\n",
      "Iteration: 446000, loss: 0.11530735351901013, gradient norm: 0.04090131465181808\n",
      "Iteration: 447000, loss: 0.11529894755977617, gradient norm: 0.04206102262377188\n",
      "Iteration: 448000, loss: 0.1152905245810527, gradient norm: 0.12100252336017596\n",
      "Iteration: 449000, loss: 0.11528222180450143, gradient norm: 0.019039176414335415\n",
      "Iteration: 450000, loss: 0.11527399414167294, gradient norm: 0.03496013927679677\n",
      "Iteration: 451000, loss: 0.11526578863132544, gradient norm: 0.027743252820146093\n",
      "Iteration: 452000, loss: 0.11525767056499413, gradient norm: 0.016940057939164442\n",
      "Iteration: 453000, loss: 0.11524963824606832, gradient norm: 0.004946918260376965\n",
      "Iteration: 454000, loss: 0.11524167786604184, gradient norm: 0.038255905908725775\n",
      "Iteration: 455000, loss: 0.1152338030782415, gradient norm: 0.09046459622598897\n",
      "Iteration: 456000, loss: 0.1152259033650896, gradient norm: 0.10022760685795616\n",
      "Iteration: 457000, loss: 0.11521811166233019, gradient norm: 0.0009063040316290308\n",
      "Iteration: 458000, loss: 0.11521041649450203, gradient norm: 0.010779547683138813\n",
      "Iteration: 459000, loss: 0.11520275151772723, gradient norm: 0.0012513395166011726\n",
      "Iteration: 460000, loss: 0.11519517375645698, gradient norm: 0.03151977185407283\n",
      "Iteration: 461000, loss: 0.11518755961065624, gradient norm: 0.008940897435196177\n",
      "Iteration: 462000, loss: 0.11518008591431775, gradient norm: 0.06992462010523791\n",
      "Iteration: 463000, loss: 0.11517246387094822, gradient norm: 0.005382008707139002\n",
      "Iteration: 464000, loss: 0.11516497843848744, gradient norm: 0.010906061928216013\n",
      "Iteration: 465000, loss: 0.11515751015182232, gradient norm: 0.027161095269885974\n",
      "Iteration: 466000, loss: 0.11515002142310365, gradient norm: 0.011829931507718204\n",
      "Iteration: 467000, loss: 0.11514257583779197, gradient norm: 0.01795320413425465\n",
      "Iteration: 468000, loss: 0.11513502070467481, gradient norm: 0.014702130160503785\n",
      "Iteration: 469000, loss: 0.11512751683722605, gradient norm: 0.0035771495710267805\n",
      "Iteration: 470000, loss: 0.11511988746694178, gradient norm: 0.0072524353455093084\n",
      "Iteration: 471000, loss: 0.1151118613795112, gradient norm: 0.015421693453948815\n",
      "Iteration: 472000, loss: 0.11510305810540901, gradient norm: 0.005318229993554005\n",
      "Iteration: 473000, loss: 0.11509274147045372, gradient norm: 0.00542751571049883\n",
      "Iteration: 474000, loss: 0.11508123717941827, gradient norm: 0.005483470086441228\n",
      "Iteration: 475000, loss: 0.11506994912135382, gradient norm: 0.10665566082755785\n",
      "Iteration: 476000, loss: 0.1150591749460417, gradient norm: 0.0035894714600077714\n",
      "Iteration: 477000, loss: 0.1150491703129052, gradient norm: 0.10354157996693832\n",
      "Iteration: 478000, loss: 0.11503957038639491, gradient norm: 0.16084079826568706\n",
      "Iteration: 479000, loss: 0.11503042063260947, gradient norm: 0.13316920549491695\n",
      "Iteration: 480000, loss: 0.11502153888393075, gradient norm: 0.14789471642528468\n",
      "Iteration: 481000, loss: 0.11501289795389809, gradient norm: 0.015501407175475742\n",
      "Iteration: 482000, loss: 0.11500458395037055, gradient norm: 0.020929142464058567\n",
      "Iteration: 483000, loss: 0.11499636512629448, gradient norm: 0.01666667599551137\n",
      "Iteration: 484000, loss: 0.11498827236024593, gradient norm: 0.06117323607585924\n",
      "Iteration: 485000, loss: 0.11498024286365131, gradient norm: 0.186824959098436\n",
      "Iteration: 486000, loss: 0.11497237847029403, gradient norm: 0.00038366022970987636\n",
      "Iteration: 487000, loss: 0.11496458643740394, gradient norm: 0.008047812862716766\n",
      "Iteration: 488000, loss: 0.11495684777617463, gradient norm: 0.140784133771685\n",
      "Iteration: 489000, loss: 0.11494909418751961, gradient norm: 0.02557414582369227\n",
      "Iteration: 490000, loss: 0.11494146702532024, gradient norm: 0.015158991007765982\n",
      "Iteration: 491000, loss: 0.11493381375521614, gradient norm: 0.0017829897060787719\n",
      "Iteration: 492000, loss: 0.11492626019887081, gradient norm: 0.014549799572860223\n",
      "Iteration: 493000, loss: 0.11491874733585102, gradient norm: 0.04569439430511834\n",
      "Iteration: 494000, loss: 0.11491127949223982, gradient norm: 0.06915024310333188\n",
      "Iteration: 495000, loss: 0.11490375458304214, gradient norm: 0.019457472208331203\n",
      "Iteration: 496000, loss: 0.11489626217525077, gradient norm: 0.011441858373835299\n",
      "Iteration: 497000, loss: 0.11488882425336173, gradient norm: 0.09407050703359283\n",
      "Iteration: 498000, loss: 0.11488127213508298, gradient norm: 0.022100678173296313\n",
      "Iteration: 499000, loss: 0.11487375105579352, gradient norm: 0.026583241109805714\n",
      "Iteration: 500000, loss: 0.11486607638219798, gradient norm: 0.043072192237051846\n",
      "Iteration: 501000, loss: 0.11485830444523862, gradient norm: 0.06010517936345598\n",
      "Iteration: 502000, loss: 0.11485039439566089, gradient norm: 0.022308911324126574\n",
      "Iteration: 503000, loss: 0.11484235718504524, gradient norm: 0.016172845630032663\n",
      "Iteration: 504000, loss: 0.11483390439366412, gradient norm: 0.010169772580908374\n",
      "Iteration: 505000, loss: 0.11482491780332126, gradient norm: 0.07701395913644411\n",
      "Iteration: 506000, loss: 0.11481537526690491, gradient norm: 0.011203122988353461\n",
      "Iteration: 507000, loss: 0.11480493743057865, gradient norm: 0.0328238400583218\n",
      "Iteration: 508000, loss: 0.11479378097072981, gradient norm: 0.00789928388856676\n",
      "Iteration: 509000, loss: 0.11478198970483018, gradient norm: 0.066635744785207\n",
      "Iteration: 510000, loss: 0.1147698074981493, gradient norm: 0.10793866043064197\n",
      "Iteration: 511000, loss: 0.11475795172442697, gradient norm: 0.01812324319600117\n",
      "Iteration: 512000, loss: 0.11474600766369521, gradient norm: 0.007610112337207996\n",
      "Iteration: 513000, loss: 0.11473421158198809, gradient norm: 0.04282023782191384\n",
      "Iteration: 514000, loss: 0.1147226265719347, gradient norm: 0.01703427735025611\n",
      "Iteration: 515000, loss: 0.11471125246083394, gradient norm: 0.00891651465767988\n",
      "Iteration: 516000, loss: 0.11469999714050025, gradient norm: 0.10994105345304815\n",
      "Iteration: 517000, loss: 0.11468887559325978, gradient norm: 0.08920957034000672\n",
      "Iteration: 518000, loss: 0.11467794296099709, gradient norm: 0.010077596141190547\n",
      "Iteration: 519000, loss: 0.11466706132778436, gradient norm: 0.0754152519108657\n",
      "Iteration: 520000, loss: 0.11465614047359367, gradient norm: 0.1307557958046796\n",
      "Iteration: 521000, loss: 0.11464534375923412, gradient norm: 0.15362341914660266\n",
      "Iteration: 522000, loss: 0.11463442110577, gradient norm: 0.09490843752554637\n",
      "Iteration: 523000, loss: 0.11462028677113918, gradient norm: 0.003591211152804776\n",
      "Iteration: 524000, loss: 0.11455920587097564, gradient norm: 0.019136942510579687\n",
      "Iteration: 525000, loss: 0.11452729805823345, gradient norm: 0.022807169108884747\n",
      "Iteration: 526000, loss: 0.11450727724604708, gradient norm: 0.012873711263055301\n",
      "Iteration: 527000, loss: 0.11449087944878936, gradient norm: 0.02796769745198235\n",
      "Iteration: 528000, loss: 0.1144763041361009, gradient norm: 0.017770590029704315\n",
      "Iteration: 529000, loss: 0.11446284549714755, gradient norm: 0.012765108160691764\n",
      "Iteration: 530000, loss: 0.11445017769454943, gradient norm: 0.014851372699967325\n",
      "Iteration: 531000, loss: 0.11443814680120383, gradient norm: 0.03314565205878146\n",
      "Iteration: 532000, loss: 0.11442656182535076, gradient norm: 0.014530199331751339\n",
      "Iteration: 533000, loss: 0.11441541560255512, gradient norm: 0.11401236539755254\n",
      "Iteration: 534000, loss: 0.11440460287893042, gradient norm: 0.04051926129251083\n",
      "Iteration: 535000, loss: 0.11439411099973941, gradient norm: 0.051707011481849405\n",
      "Iteration: 536000, loss: 0.11438378065825008, gradient norm: 0.004783757037742032\n",
      "Iteration: 537000, loss: 0.11437370149756682, gradient norm: 0.010896129992022186\n",
      "Iteration: 538000, loss: 0.11436387110470796, gradient norm: 0.05786302185096308\n",
      "Iteration: 539000, loss: 0.11435411194015103, gradient norm: 0.04617463240261444\n",
      "Iteration: 540000, loss: 0.11434445591831154, gradient norm: 0.04513011806348224\n",
      "Iteration: 541000, loss: 0.11433500673333069, gradient norm: 0.030382360109777913\n",
      "Iteration: 542000, loss: 0.11432555546909753, gradient norm: 0.018420589437855277\n",
      "Iteration: 543000, loss: 0.11431623381387833, gradient norm: 0.005145648437123016\n",
      "Iteration: 544000, loss: 0.1143069362761303, gradient norm: 0.06188866322755454\n",
      "Iteration: 545000, loss: 0.11429765082576822, gradient norm: 0.05053083190479775\n",
      "Iteration: 546000, loss: 0.1142882055564025, gradient norm: 0.0005083534008196375\n",
      "Iteration: 547000, loss: 0.11427873756384942, gradient norm: 0.05714702747711953\n",
      "Iteration: 548000, loss: 0.11426886286725024, gradient norm: 0.016237873025721886\n",
      "Iteration: 549000, loss: 0.11425855131655285, gradient norm: 0.07196737951781823\n",
      "Iteration: 550000, loss: 0.11424681774129355, gradient norm: 0.0047380991924873325\n",
      "Iteration: 551000, loss: 0.11420093706740835, gradient norm: 0.04806233430314881\n",
      "Iteration: 552000, loss: 0.11414705252559608, gradient norm: 0.00953500857556245\n",
      "Iteration: 553000, loss: 0.11410977701368148, gradient norm: 0.009461414943656562\n",
      "Iteration: 554000, loss: 0.11407668162656184, gradient norm: 0.008758850849747686\n",
      "Iteration: 555000, loss: 0.11405086308004857, gradient norm: 0.006512857018018429\n",
      "Iteration: 556000, loss: 0.1140298895588734, gradient norm: 0.08144171740820888\n",
      "Iteration: 557000, loss: 0.11401141945508504, gradient norm: 0.043065602665219006\n",
      "Iteration: 558000, loss: 0.11399450661298573, gradient norm: 0.08072090332604238\n",
      "Iteration: 559000, loss: 0.11397867134384143, gradient norm: 0.09094560395024909\n",
      "Iteration: 560000, loss: 0.11396367052342904, gradient norm: 0.015367762169083228\n",
      "Iteration: 561000, loss: 0.11394942978186708, gradient norm: 0.03472781372895179\n",
      "Iteration: 562000, loss: 0.11393563488108854, gradient norm: 0.006625861258375191\n",
      "Iteration: 563000, loss: 0.11392234431658996, gradient norm: 0.05488874412661617\n",
      "Iteration: 564000, loss: 0.11390946715596623, gradient norm: 0.0068484895431933255\n",
      "Iteration: 565000, loss: 0.11389678102692945, gradient norm: 0.024662223308475527\n",
      "Iteration: 566000, loss: 0.11388452632746185, gradient norm: 0.017473739895781583\n",
      "Iteration: 567000, loss: 0.11387244226404164, gradient norm: 0.061768393969718255\n",
      "Iteration: 568000, loss: 0.11386064297916967, gradient norm: 0.05653583427094057\n",
      "Iteration: 569000, loss: 0.11384907793544545, gradient norm: 0.014578409670176144\n",
      "Iteration: 570000, loss: 0.11383762559641047, gradient norm: 0.07555210176913695\n",
      "Iteration: 571000, loss: 0.11382641511660466, gradient norm: 0.029424844470912676\n",
      "Iteration: 572000, loss: 0.1138153160735037, gradient norm: 0.026587025525120815\n",
      "Iteration: 573000, loss: 0.11380437127173093, gradient norm: 0.05009418978312205\n",
      "Iteration: 574000, loss: 0.11379355521882414, gradient norm: 0.1960646344942553\n",
      "Iteration: 575000, loss: 0.11378287029320938, gradient norm: 0.1574666858152579\n",
      "Iteration: 576000, loss: 0.11377226213971417, gradient norm: 0.038810268552097814\n",
      "Iteration: 577000, loss: 0.1137617745283098, gradient norm: 0.024699486752886954\n",
      "Iteration: 578000, loss: 0.11375131708477185, gradient norm: 0.09614121872413305\n",
      "Iteration: 579000, loss: 0.11374091944562902, gradient norm: 0.09758585215829842\n",
      "Iteration: 580000, loss: 0.1137305893236344, gradient norm: 0.01666485189397703\n",
      "Iteration: 581000, loss: 0.11372018491089537, gradient norm: 0.07485171908622497\n",
      "Iteration: 582000, loss: 0.11370982995645343, gradient norm: 0.023409243081825834\n",
      "Iteration: 583000, loss: 0.1136994729011394, gradient norm: 0.007535338546521834\n",
      "Iteration: 584000, loss: 0.11368902604151282, gradient norm: 0.05719310563571893\n",
      "Iteration: 585000, loss: 0.11367847211540114, gradient norm: 0.008397949230873773\n",
      "Iteration: 586000, loss: 0.11366778915822044, gradient norm: 0.001119508573394697\n",
      "Iteration: 587000, loss: 0.11365710336174567, gradient norm: 0.026054623874523418\n",
      "Iteration: 588000, loss: 0.11364629900332548, gradient norm: 0.011008126963578392\n",
      "Iteration: 589000, loss: 0.11363553305356866, gradient norm: 0.00948341393162213\n",
      "Iteration: 590000, loss: 0.11362476163509148, gradient norm: 0.06261099101842035\n",
      "Iteration: 591000, loss: 0.11361415288675451, gradient norm: 0.04846429301379046\n",
      "Iteration: 592000, loss: 0.11360357769083645, gradient norm: 0.05028186180015751\n",
      "Iteration: 593000, loss: 0.11359306996147436, gradient norm: 0.005583541253499614\n",
      "Iteration: 594000, loss: 0.1135827437423688, gradient norm: 0.03787645874409602\n",
      "Iteration: 595000, loss: 0.11357249751105677, gradient norm: 0.13790296144338995\n",
      "Iteration: 596000, loss: 0.11356250248028564, gradient norm: 0.028569115808685857\n",
      "Iteration: 597000, loss: 0.11355250372567047, gradient norm: 0.004974521666486875\n",
      "Iteration: 598000, loss: 0.1135426417875088, gradient norm: 0.015156602909360141\n",
      "Iteration: 599000, loss: 0.11353303361673392, gradient norm: 0.032453792976610336\n",
      "Iteration: 600000, loss: 0.11352337790520517, gradient norm: 0.014663686716769505\n",
      "Iteration: 601000, loss: 0.11351397049607413, gradient norm: 0.20838877825070654\n",
      "Iteration: 602000, loss: 0.11350472417727948, gradient norm: 0.06866532091794499\n",
      "Iteration: 603000, loss: 0.11349549407271692, gradient norm: 0.010092329772723986\n",
      "Iteration: 604000, loss: 0.11348641737634865, gradient norm: 0.025304970714579867\n",
      "Iteration: 605000, loss: 0.11347747960185055, gradient norm: 0.02209484734995583\n",
      "Iteration: 606000, loss: 0.11346854088735538, gradient norm: 0.024396974895121173\n",
      "Iteration: 607000, loss: 0.11345976084316685, gradient norm: 0.1260099332390205\n",
      "Iteration: 608000, loss: 0.11345107339432119, gradient norm: 0.0034888429802181404\n",
      "Iteration: 609000, loss: 0.11344244813680325, gradient norm: 0.009200966533289799\n",
      "Iteration: 610000, loss: 0.11343394875887347, gradient norm: 0.015748370958601245\n",
      "Iteration: 611000, loss: 0.11342546661580459, gradient norm: 0.02438192669812335\n",
      "Iteration: 612000, loss: 0.11341711194537717, gradient norm: 0.011079628339622777\n",
      "Iteration: 613000, loss: 0.11340882166183305, gradient norm: 0.005123617467609037\n",
      "Iteration: 614000, loss: 0.11340058400885288, gradient norm: 0.009137427075382336\n",
      "Iteration: 615000, loss: 0.11339250393399417, gradient norm: 0.07534250296100274\n",
      "Iteration: 616000, loss: 0.11338439148934498, gradient norm: 0.027532732471382177\n",
      "Iteration: 617000, loss: 0.11337640346042507, gradient norm: 0.024852763573063427\n",
      "Iteration: 618000, loss: 0.11336853559661936, gradient norm: 0.05768148304688953\n",
      "Iteration: 619000, loss: 0.11336065068843228, gradient norm: 0.031543893849935514\n",
      "Iteration: 620000, loss: 0.1133529065810893, gradient norm: 0.014038754965219913\n",
      "Iteration: 621000, loss: 0.11334516991630765, gradient norm: 0.03280112466278653\n",
      "Iteration: 622000, loss: 0.11333762243869182, gradient norm: 0.28260644115521505\n",
      "Iteration: 623000, loss: 0.11333002769024136, gradient norm: 0.10509462233472969\n",
      "Iteration: 624000, loss: 0.11332265156885957, gradient norm: 0.048215102845347156\n",
      "Iteration: 625000, loss: 0.11331518770937528, gradient norm: 0.016663999732770645\n",
      "Iteration: 626000, loss: 0.11330780402921008, gradient norm: 0.020698475855077753\n",
      "Iteration: 627000, loss: 0.11330054054996974, gradient norm: 0.005969130089067509\n",
      "Iteration: 628000, loss: 0.11329330488409518, gradient norm: 0.007111617246762278\n",
      "Iteration: 629000, loss: 0.11328613663758585, gradient norm: 0.03484429359442167\n",
      "Iteration: 630000, loss: 0.1132790604836423, gradient norm: 0.01866388648150398\n",
      "Iteration: 631000, loss: 0.11327201620012614, gradient norm: 0.00934948176225851\n",
      "Iteration: 632000, loss: 0.11326502816854582, gradient norm: 0.05719210790162816\n",
      "Iteration: 633000, loss: 0.11325813510933833, gradient norm: 0.005588827425514105\n",
      "Iteration: 634000, loss: 0.11325118795853027, gradient norm: 0.007385181737645925\n",
      "Iteration: 635000, loss: 0.11324443500758266, gradient norm: 0.01494554714658526\n",
      "Iteration: 636000, loss: 0.11323763754519002, gradient norm: 0.19383153430138192\n",
      "Iteration: 637000, loss: 0.11323090631249684, gradient norm: 0.04624661272515983\n",
      "Iteration: 638000, loss: 0.11322418861542445, gradient norm: 0.012627387023060264\n",
      "Iteration: 639000, loss: 0.11321753125083381, gradient norm: 0.13433933364360565\n",
      "Iteration: 640000, loss: 0.11321099340094758, gradient norm: 0.010806385310159018\n",
      "Iteration: 641000, loss: 0.11320449031978673, gradient norm: 0.001767816199280258\n",
      "Iteration: 642000, loss: 0.11319803893972219, gradient norm: 0.21684314643602023\n",
      "Iteration: 643000, loss: 0.11319158562916823, gradient norm: 0.006215873258867263\n",
      "Iteration: 644000, loss: 0.11318526294378452, gradient norm: 0.07705144053305628\n",
      "Iteration: 645000, loss: 0.11317886539898853, gradient norm: 0.056710012674774476\n",
      "Iteration: 646000, loss: 0.11317261901027456, gradient norm: 0.020068973238690217\n",
      "Iteration: 647000, loss: 0.11316634308645898, gradient norm: 0.037040952614680975\n",
      "Iteration: 648000, loss: 0.1131600517670584, gradient norm: 0.006649589910461259\n",
      "Iteration: 649000, loss: 0.11315394924582714, gradient norm: 0.006855717482323865\n",
      "Iteration: 650000, loss: 0.1131477797754373, gradient norm: 0.0669453288473729\n",
      "Iteration: 651000, loss: 0.11314169411404168, gradient norm: 0.011268316372582264\n",
      "Iteration: 652000, loss: 0.11313562700312219, gradient norm: 0.029153198104424956\n",
      "Iteration: 653000, loss: 0.1131296824348035, gradient norm: 0.004518582796233979\n",
      "Iteration: 654000, loss: 0.11312360194145135, gradient norm: 0.003713160576541601\n",
      "Iteration: 655000, loss: 0.11311769253669417, gradient norm: 0.03640143830523829\n",
      "Iteration: 656000, loss: 0.11311181119178836, gradient norm: 0.019025667257256705\n",
      "Iteration: 657000, loss: 0.11310603346805777, gradient norm: 0.08277217551109177\n",
      "Iteration: 658000, loss: 0.11310011692334813, gradient norm: 0.04064539604551409\n",
      "Iteration: 659000, loss: 0.11309432138219119, gradient norm: 0.025229196812746883\n",
      "Iteration: 660000, loss: 0.11308856586005421, gradient norm: 0.03269293054463103\n",
      "Iteration: 661000, loss: 0.11308276228119205, gradient norm: 0.002140991320719644\n",
      "Iteration: 662000, loss: 0.1130771432600791, gradient norm: 0.04380696836660099\n",
      "Iteration: 663000, loss: 0.1130714575014373, gradient norm: 0.11986950124324994\n",
      "Iteration: 664000, loss: 0.1130658162507641, gradient norm: 0.16782920032202797\n",
      "Iteration: 665000, loss: 0.11306022898675999, gradient norm: 0.02421870613244893\n",
      "Iteration: 666000, loss: 0.11305477075675309, gradient norm: 0.06284925246107527\n",
      "Iteration: 667000, loss: 0.11304914896403874, gradient norm: 0.002733590521739044\n",
      "Iteration: 668000, loss: 0.11304366230636864, gradient norm: 0.009561924722264503\n",
      "Iteration: 669000, loss: 0.11303827934770842, gradient norm: 0.003957419075959551\n",
      "Iteration: 670000, loss: 0.11303276660241841, gradient norm: 0.017235121990438383\n",
      "Iteration: 671000, loss: 0.11302732104504952, gradient norm: 0.04317680574799219\n",
      "Iteration: 672000, loss: 0.11302196820131752, gradient norm: 0.0044586788478171956\n",
      "Iteration: 673000, loss: 0.11301656974454707, gradient norm: 0.020091382181433963\n",
      "Iteration: 674000, loss: 0.11301123599551827, gradient norm: 0.07148587754194193\n",
      "Iteration: 675000, loss: 0.1130059443284024, gradient norm: 0.08024839261531513\n",
      "Iteration: 676000, loss: 0.1130006019534857, gradient norm: 0.13348214987904905\n",
      "Iteration: 677000, loss: 0.11299531997445406, gradient norm: 0.08673083319038655\n",
      "Iteration: 678000, loss: 0.11299006211928671, gradient norm: 0.12409353568005187\n",
      "Iteration: 679000, loss: 0.11298485100720301, gradient norm: 0.0045758999131899495\n",
      "Iteration: 680000, loss: 0.11297965777217789, gradient norm: 0.01317705913787531\n",
      "Iteration: 681000, loss: 0.11297444827928294, gradient norm: 0.006075511491086656\n",
      "Iteration: 682000, loss: 0.11296930729678904, gradient norm: 0.10632142299861848\n",
      "Iteration: 683000, loss: 0.11296424485915918, gradient norm: 0.28625954454493113\n",
      "Iteration: 684000, loss: 0.11295903788641304, gradient norm: 0.027606519673219417\n",
      "Iteration: 685000, loss: 0.11295401587419199, gradient norm: 0.24608886181264533\n",
      "Iteration: 686000, loss: 0.11294885820411465, gradient norm: 0.07870476195986803\n",
      "Iteration: 687000, loss: 0.11294392543610268, gradient norm: 0.08573741703800383\n",
      "Iteration: 688000, loss: 0.11293887395649627, gradient norm: 0.053274140022926506\n",
      "Iteration: 689000, loss: 0.11293384479962093, gradient norm: 0.02087778903609967\n",
      "Iteration: 690000, loss: 0.11292892501013053, gradient norm: 0.004693970086899652\n",
      "Iteration: 691000, loss: 0.11292397738344291, gradient norm: 0.0028212571993464866\n",
      "Iteration: 692000, loss: 0.11291902867947229, gradient norm: 0.018718262552393672\n",
      "Iteration: 693000, loss: 0.11291413269636383, gradient norm: 0.14922548340983582\n",
      "Iteration: 694000, loss: 0.11290922818587038, gradient norm: 0.07489456630899684\n",
      "Iteration: 695000, loss: 0.11290430447734348, gradient norm: 0.025631537213649858\n",
      "Iteration: 696000, loss: 0.11289948899883441, gradient norm: 0.1403989714273839\n",
      "Iteration: 697000, loss: 0.11289460945074167, gradient norm: 0.0036814302109766423\n",
      "Iteration: 698000, loss: 0.11288985591428627, gradient norm: 0.04323424635552897\n",
      "Iteration: 699000, loss: 0.11288491226757728, gradient norm: 0.009043881321524293\n",
      "Iteration: 700000, loss: 0.11288022350359565, gradient norm: 0.05110406951467507\n",
      "Iteration: 701000, loss: 0.1128753493239325, gradient norm: 0.0078095970536605505\n",
      "Iteration: 702000, loss: 0.11287065227543862, gradient norm: 0.023117294101574124\n",
      "Iteration: 703000, loss: 0.11286585831822703, gradient norm: 0.05333170729959501\n",
      "Iteration: 704000, loss: 0.1128611251298793, gradient norm: 0.025389325940463996\n",
      "Iteration: 705000, loss: 0.11285646274210018, gradient norm: 0.10533470953231094\n",
      "Iteration: 706000, loss: 0.11285167267675, gradient norm: 0.008701771693173299\n",
      "Iteration: 707000, loss: 0.11284703513049352, gradient norm: 0.0035747344163528268\n",
      "Iteration: 708000, loss: 0.11284238641558772, gradient norm: 0.003839406684011713\n",
      "Iteration: 709000, loss: 0.11283770772865884, gradient norm: 0.01487815908978917\n",
      "Iteration: 710000, loss: 0.11283304282923795, gradient norm: 0.12567517311926804\n",
      "Iteration: 711000, loss: 0.11282835297590096, gradient norm: 0.029324322264588996\n",
      "Iteration: 712000, loss: 0.11282380221423449, gradient norm: 0.2440416888028952\n",
      "Iteration: 713000, loss: 0.11281910308145114, gradient norm: 0.044133542460472075\n",
      "Iteration: 714000, loss: 0.11281448423058933, gradient norm: 0.10695482711012042\n",
      "Iteration: 715000, loss: 0.11280987276733688, gradient norm: 0.009906554293941541\n",
      "Iteration: 716000, loss: 0.11280529392654351, gradient norm: 0.00861090202657886\n",
      "Iteration: 717000, loss: 0.11280067380859995, gradient norm: 0.11419028590831436\n",
      "Iteration: 718000, loss: 0.11279609137114087, gradient norm: 0.008900041731858103\n",
      "Iteration: 719000, loss: 0.11279154538420931, gradient norm: 0.08352814767344273\n",
      "Iteration: 720000, loss: 0.11278701676072453, gradient norm: 0.055158919341072365\n",
      "Iteration: 721000, loss: 0.11278244418466107, gradient norm: 0.011570689334735545\n",
      "Iteration: 722000, loss: 0.11277786766350024, gradient norm: 0.01856825907713588\n",
      "Iteration: 723000, loss: 0.1127734063248248, gradient norm: 0.06513943675624528\n",
      "Iteration: 724000, loss: 0.11276882689057348, gradient norm: 0.010327995229092819\n",
      "Iteration: 725000, loss: 0.11276430555366508, gradient norm: 0.011274129462762218\n",
      "Iteration: 726000, loss: 0.11275972746915215, gradient norm: 0.0636551334119704\n",
      "Iteration: 727000, loss: 0.11275532216700461, gradient norm: 0.14273118192557183\n",
      "Iteration: 728000, loss: 0.11275082403873918, gradient norm: 0.14794732339371813\n",
      "Iteration: 729000, loss: 0.11274619551127169, gradient norm: 0.02196443028594673\n",
      "Iteration: 730000, loss: 0.11274175112285778, gradient norm: 0.018261593111897286\n",
      "Iteration: 731000, loss: 0.11273721902639887, gradient norm: 0.00899833340545982\n",
      "Iteration: 732000, loss: 0.11273276290041134, gradient norm: 0.1956712065766426\n",
      "Iteration: 733000, loss: 0.11272818435018274, gradient norm: 0.002390451214486642\n",
      "Iteration: 734000, loss: 0.11272378160096076, gradient norm: 0.026774763480301115\n",
      "Iteration: 735000, loss: 0.11271913499623201, gradient norm: 0.0018187628799291205\n",
      "Iteration: 736000, loss: 0.11271472570718286, gradient norm: 0.10644812544969355\n",
      "Iteration: 737000, loss: 0.11271014832542067, gradient norm: 0.011157329144113331\n",
      "Iteration: 738000, loss: 0.11270564809570857, gradient norm: 0.005478165394461196\n",
      "Iteration: 739000, loss: 0.11270109975585407, gradient norm: 0.11176449815720874\n",
      "Iteration: 740000, loss: 0.11269647098694124, gradient norm: 0.0048288965738144665\n",
      "Iteration: 741000, loss: 0.11269193510782753, gradient norm: 0.00973021652399024\n",
      "Iteration: 742000, loss: 0.11268721769691198, gradient norm: 0.20203596733827822\n",
      "Iteration: 743000, loss: 0.11268240502536492, gradient norm: 0.10053477464514174\n",
      "Iteration: 744000, loss: 0.11267727910386123, gradient norm: 0.006705441702597045\n",
      "Iteration: 745000, loss: 0.1126705605476084, gradient norm: 0.025846985293126224\n",
      "Iteration: 746000, loss: 0.1126591811069041, gradient norm: 0.044848482875824894\n",
      "Iteration: 747000, loss: 0.11264559386814017, gradient norm: 0.010119524298485501\n",
      "Iteration: 748000, loss: 0.11263353347010656, gradient norm: 0.044435674150398584\n",
      "Iteration: 749000, loss: 0.11262316618929796, gradient norm: 0.018264459588160538\n",
      "Iteration: 750000, loss: 0.1126140625246507, gradient norm: 0.007345872637101198\n",
      "Iteration: 751000, loss: 0.1126059423959814, gradient norm: 0.04090144580792531\n",
      "Iteration: 752000, loss: 0.1125986372696313, gradient norm: 0.016328657783790418\n",
      "Iteration: 753000, loss: 0.11259180836478222, gradient norm: 0.056537584527874656\n",
      "Iteration: 754000, loss: 0.11258528417682898, gradient norm: 0.1992872232193565\n",
      "Iteration: 755000, loss: 0.1125791274463617, gradient norm: 0.015786672022226254\n",
      "Iteration: 756000, loss: 0.11257306332232458, gradient norm: 0.03152896995957757\n",
      "Iteration: 757000, loss: 0.1125671658972505, gradient norm: 0.01647436111028616\n",
      "Iteration: 758000, loss: 0.11256129169141948, gradient norm: 0.052163568331059354\n",
      "Iteration: 759000, loss: 0.11255541344106834, gradient norm: 0.012035733755518137\n",
      "Iteration: 760000, loss: 0.11254954080382538, gradient norm: 0.006411729802715127\n",
      "Iteration: 761000, loss: 0.11254356144532573, gradient norm: 0.0223918476089178\n",
      "Iteration: 762000, loss: 0.11253749438984727, gradient norm: 0.003252017804430303\n",
      "Iteration: 763000, loss: 0.11253142882729458, gradient norm: 0.10013593143500081\n",
      "Iteration: 764000, loss: 0.11252496980709248, gradient norm: 0.019352951040112868\n",
      "Iteration: 765000, loss: 0.1125186543724228, gradient norm: 0.027400220245775514\n",
      "Iteration: 766000, loss: 0.11251225390538852, gradient norm: 0.02607023705708977\n",
      "Iteration: 767000, loss: 0.11250578857588914, gradient norm: 0.03613699202352587\n",
      "Iteration: 768000, loss: 0.11249932745942855, gradient norm: 0.005737620173342349\n",
      "Iteration: 769000, loss: 0.11249287087260314, gradient norm: 0.0033091141137873534\n",
      "Iteration: 770000, loss: 0.11248659550278259, gradient norm: 0.08209843524334003\n",
      "Iteration: 771000, loss: 0.11248042694783368, gradient norm: 0.0737457504672664\n",
      "Iteration: 772000, loss: 0.11247426723576555, gradient norm: 0.18410374034707389\n",
      "Iteration: 773000, loss: 0.11246826118543253, gradient norm: 0.002862325735452667\n",
      "Iteration: 774000, loss: 0.11246235383791318, gradient norm: 0.02653072371491078\n",
      "Iteration: 775000, loss: 0.1124564911040369, gradient norm: 0.038481623235734556\n",
      "Iteration: 776000, loss: 0.11245066273688833, gradient norm: 0.0372813670123794\n",
      "Iteration: 777000, loss: 0.1124451012121149, gradient norm: 0.00799889278303158\n",
      "Iteration: 778000, loss: 0.11243944176066255, gradient norm: 0.011100444831180313\n",
      "Iteration: 779000, loss: 0.11243407799445397, gradient norm: 0.018954358191308418\n",
      "Iteration: 780000, loss: 0.11242843194235926, gradient norm: 0.006693200944817417\n",
      "Iteration: 781000, loss: 0.11242312726751516, gradient norm: 0.09868107914814206\n",
      "Iteration: 782000, loss: 0.11241778251244602, gradient norm: 0.025308227761615128\n",
      "Iteration: 783000, loss: 0.11241245648034438, gradient norm: 0.01095188056106513\n",
      "Iteration: 784000, loss: 0.1124072435667141, gradient norm: 0.0387022925228011\n",
      "Iteration: 785000, loss: 0.11240203232914128, gradient norm: 0.0033264228640565996\n",
      "Iteration: 786000, loss: 0.11239691993591265, gradient norm: 0.01707529260554974\n",
      "Iteration: 787000, loss: 0.1123917945002663, gradient norm: 0.012467779647241531\n",
      "Iteration: 788000, loss: 0.112386720634424, gradient norm: 0.011593215079106739\n",
      "Iteration: 789000, loss: 0.11238165640793883, gradient norm: 0.028631212892620633\n",
      "Iteration: 790000, loss: 0.11237675405273245, gradient norm: 0.04192169822273345\n",
      "Iteration: 791000, loss: 0.11237172927852249, gradient norm: 0.017170418069075606\n",
      "Iteration: 792000, loss: 0.11236678647842462, gradient norm: 0.0033016547563799164\n",
      "Iteration: 793000, loss: 0.1123620145284466, gradient norm: 0.017690790323665776\n",
      "Iteration: 794000, loss: 0.11235704623365891, gradient norm: 0.14768282400180682\n",
      "Iteration: 795000, loss: 0.11235231991343557, gradient norm: 0.10291686683028879\n",
      "Iteration: 796000, loss: 0.11234744275774937, gradient norm: 0.02248057945200389\n",
      "Iteration: 797000, loss: 0.11234268012979072, gradient norm: 0.0031790647264519057\n",
      "Iteration: 798000, loss: 0.11233801764740488, gradient norm: 0.06785641328078273\n",
      "Iteration: 799000, loss: 0.1123332363151751, gradient norm: 0.01909284979613865\n",
      "Iteration: 800000, loss: 0.1123285942386608, gradient norm: 0.06356214233798238\n",
      "Iteration: 801000, loss: 0.11232394280747772, gradient norm: 0.022281376677794083\n",
      "Iteration: 802000, loss: 0.11231930155454944, gradient norm: 0.03183566109164443\n",
      "Iteration: 803000, loss: 0.11231469872286365, gradient norm: 0.005503555671483042\n",
      "Iteration: 804000, loss: 0.11231003924462418, gradient norm: 0.006140518655748705\n",
      "Iteration: 805000, loss: 0.11230563069597949, gradient norm: 0.004437129582464548\n",
      "Iteration: 806000, loss: 0.11230101808115157, gradient norm: 0.1814673992557862\n",
      "Iteration: 807000, loss: 0.11229649919952821, gradient norm: 0.028358903410558692\n",
      "Iteration: 808000, loss: 0.11229207257287999, gradient norm: 0.04670684306638127\n",
      "Iteration: 809000, loss: 0.1122875731752063, gradient norm: 0.04843036797752191\n",
      "Iteration: 810000, loss: 0.11228310330832973, gradient norm: 0.1672211297357781\n",
      "Iteration: 811000, loss: 0.11227866180426721, gradient norm: 0.17466616728214737\n",
      "Iteration: 812000, loss: 0.11227422967448307, gradient norm: 0.13897973510151818\n",
      "Iteration: 813000, loss: 0.11226999779659053, gradient norm: 0.23065588444913473\n",
      "Iteration: 814000, loss: 0.11226535108721813, gradient norm: 0.02475456696891423\n",
      "Iteration: 815000, loss: 0.11226121152577721, gradient norm: 0.037536111897315025\n",
      "Iteration: 816000, loss: 0.1122567862784302, gradient norm: 0.007493872477119698\n",
      "Iteration: 817000, loss: 0.11225248055765956, gradient norm: 0.00101831971809408\n",
      "Iteration: 818000, loss: 0.1122481105494347, gradient norm: 0.04546014731155752\n",
      "Iteration: 819000, loss: 0.11224381213805724, gradient norm: 0.005465717803823939\n",
      "Iteration: 820000, loss: 0.11223953189747994, gradient norm: 0.015595088132878117\n",
      "Iteration: 821000, loss: 0.11223530541315498, gradient norm: 0.022958167558283032\n",
      "Iteration: 822000, loss: 0.1122310735558954, gradient norm: 0.07377893699313065\n",
      "Iteration: 823000, loss: 0.11222672637613249, gradient norm: 0.056059788241959715\n",
      "Iteration: 824000, loss: 0.11222256356202422, gradient norm: 0.03539419060992797\n",
      "Iteration: 825000, loss: 0.11221833326657522, gradient norm: 0.014067425044842872\n",
      "Iteration: 826000, loss: 0.11221416262360016, gradient norm: 0.0343131261214786\n",
      "Iteration: 827000, loss: 0.11220991803526531, gradient norm: 0.020874626159404507\n",
      "Iteration: 828000, loss: 0.11220580278841356, gradient norm: 0.012910489891558635\n",
      "Iteration: 829000, loss: 0.11220162875345405, gradient norm: 0.0080509252746459\n",
      "Iteration: 830000, loss: 0.11219747596733487, gradient norm: 0.04014089353219861\n",
      "Iteration: 831000, loss: 0.1121932778897971, gradient norm: 0.13780829197643676\n",
      "Iteration: 832000, loss: 0.11218934897684549, gradient norm: 0.02784653546406406\n",
      "Iteration: 833000, loss: 0.11218501032277345, gradient norm: 0.03574332850226742\n",
      "Iteration: 834000, loss: 0.11218096528486583, gradient norm: 0.09078156709052684\n",
      "Iteration: 835000, loss: 0.11217693657308954, gradient norm: 0.0601314627660174\n",
      "Iteration: 836000, loss: 0.11217276777896952, gradient norm: 0.07121054446951192\n",
      "Iteration: 837000, loss: 0.11216870524254657, gradient norm: 0.01001144537689115\n",
      "Iteration: 838000, loss: 0.11216467133461165, gradient norm: 0.03337427965977698\n",
      "Iteration: 839000, loss: 0.11216058403943223, gradient norm: 0.10516980048149477\n",
      "Iteration: 840000, loss: 0.1121566221100259, gradient norm: 0.023387452459949536\n",
      "Iteration: 841000, loss: 0.11215258096379256, gradient norm: 0.0353335452769524\n",
      "Iteration: 842000, loss: 0.11214850168004024, gradient norm: 0.00915322639227534\n",
      "Iteration: 843000, loss: 0.11214450071400595, gradient norm: 0.04159365061525768\n",
      "Iteration: 844000, loss: 0.11214054711597467, gradient norm: 0.09455844222675361\n",
      "Iteration: 845000, loss: 0.11213649438452176, gradient norm: 0.005118810060068916\n",
      "Iteration: 846000, loss: 0.11213251381095553, gradient norm: 0.015192838918349256\n",
      "Iteration: 847000, loss: 0.1121284795274068, gradient norm: 0.015366798915435196\n",
      "Iteration: 848000, loss: 0.11212458804876482, gradient norm: 0.05714056096833005\n",
      "Iteration: 849000, loss: 0.11212055531737337, gradient norm: 0.003599041135752917\n",
      "Iteration: 850000, loss: 0.11211662505212218, gradient norm: 0.09667014262846708\n",
      "Iteration: 851000, loss: 0.1121126941186187, gradient norm: 0.13288224403849563\n",
      "Iteration: 852000, loss: 0.11210860994375026, gradient norm: 0.012068888287283358\n",
      "Iteration: 853000, loss: 0.1121048213184177, gradient norm: 0.0018250057758985134\n",
      "Iteration: 854000, loss: 0.11210081730641158, gradient norm: 0.10617884135276534\n",
      "Iteration: 855000, loss: 0.1120969522100872, gradient norm: 0.0017482746247071248\n",
      "Iteration: 856000, loss: 0.11209300736168697, gradient norm: 0.0948333277074024\n",
      "Iteration: 857000, loss: 0.11208908425350611, gradient norm: 0.004410732830748032\n",
      "Iteration: 858000, loss: 0.1120851500707403, gradient norm: 0.12334891366525991\n",
      "Iteration: 859000, loss: 0.11208123445302569, gradient norm: 0.006898716911389389\n",
      "Iteration: 860000, loss: 0.11207735491507859, gradient norm: 0.003903820501787981\n",
      "Iteration: 861000, loss: 0.11207352759507794, gradient norm: 0.022085921245413635\n",
      "Iteration: 862000, loss: 0.11206951004395652, gradient norm: 0.03125589716921894\n",
      "Iteration: 863000, loss: 0.11206555282928175, gradient norm: 0.013753429145227189\n",
      "Iteration: 864000, loss: 0.11206178513656899, gradient norm: 0.08863584763107842\n",
      "Iteration: 865000, loss: 0.11205789230460435, gradient norm: 0.029483708772632088\n",
      "Iteration: 866000, loss: 0.11205394295493723, gradient norm: 0.002763972196597174\n",
      "Iteration: 867000, loss: 0.1120501069948574, gradient norm: 0.012626636372001203\n",
      "Iteration: 868000, loss: 0.11204625141108819, gradient norm: 0.04441566708155054\n",
      "Iteration: 869000, loss: 0.11204235929543509, gradient norm: 0.03162327794460036\n",
      "Iteration: 870000, loss: 0.11203851849622018, gradient norm: 0.04536190021853658\n",
      "Iteration: 871000, loss: 0.11203458404132155, gradient norm: 0.005381523603070142\n",
      "Iteration: 872000, loss: 0.11203075030635462, gradient norm: 0.027272112866702076\n",
      "Iteration: 873000, loss: 0.11202691536352517, gradient norm: 0.038490949994436766\n",
      "Iteration: 874000, loss: 0.11202307983786619, gradient norm: 0.03457600991525436\n",
      "Iteration: 875000, loss: 0.11201918131845606, gradient norm: 0.004188022992684981\n",
      "Iteration: 876000, loss: 0.11201538205525151, gradient norm: 0.08350287869740057\n",
      "Iteration: 877000, loss: 0.11201147748422514, gradient norm: 0.0815282939630555\n",
      "Iteration: 878000, loss: 0.1120076865239638, gradient norm: 0.0008989740450191282\n",
      "Iteration: 879000, loss: 0.1120038563195147, gradient norm: 0.0009807371349720173\n",
      "Iteration: 880000, loss: 0.11199997611805207, gradient norm: 0.10424813867847114\n",
      "Iteration: 881000, loss: 0.11199617498675474, gradient norm: 0.10266005283489393\n",
      "Iteration: 882000, loss: 0.11199236259531918, gradient norm: 0.06677066585520107\n",
      "Iteration: 883000, loss: 0.11198844926185997, gradient norm: 0.01783631754871024\n",
      "Iteration: 884000, loss: 0.11198474671694873, gradient norm: 0.02445627581523359\n",
      "Iteration: 885000, loss: 0.11198076712121258, gradient norm: 0.017259202958156316\n",
      "Iteration: 886000, loss: 0.11197705682656331, gradient norm: 0.14182797024097057\n",
      "Iteration: 887000, loss: 0.11197310976524678, gradient norm: 0.23106213656559182\n",
      "Iteration: 888000, loss: 0.11196935003653342, gradient norm: 0.006420414713258991\n",
      "Iteration: 889000, loss: 0.11196549583878604, gradient norm: 0.007332171880726888\n",
      "Iteration: 890000, loss: 0.11196163693723588, gradient norm: 0.14288094655189007\n",
      "Iteration: 891000, loss: 0.1119577939863861, gradient norm: 0.019841658220364555\n",
      "Iteration: 892000, loss: 0.11195366186164224, gradient norm: 0.034996769527139156\n",
      "Iteration: 893000, loss: 0.1119498449824446, gradient norm: 0.036334909876138556\n",
      "Iteration: 894000, loss: 0.1119457033088558, gradient norm: 0.05857639027623852\n",
      "Iteration: 895000, loss: 0.11194169122721248, gradient norm: 0.0392440085995933\n",
      "Iteration: 896000, loss: 0.111937319167245, gradient norm: 0.006157235547445509\n",
      "Iteration: 897000, loss: 0.11193293478954894, gradient norm: 0.07099626917902374\n",
      "Iteration: 898000, loss: 0.1119285013816696, gradient norm: 0.013399799517074732\n",
      "Iteration: 899000, loss: 0.11192409336658565, gradient norm: 0.014603625265650715\n",
      "Iteration: 900000, loss: 0.1119196206366239, gradient norm: 0.06637496644748306\n",
      "Iteration: 901000, loss: 0.11191534361811986, gradient norm: 0.0019363163922014722\n",
      "Iteration: 902000, loss: 0.11191101188041913, gradient norm: 0.0057062898826147086\n",
      "Iteration: 903000, loss: 0.11190679761407224, gradient norm: 0.051736156957811565\n",
      "Iteration: 904000, loss: 0.1119025568154333, gradient norm: 0.025349671688955036\n",
      "Iteration: 905000, loss: 0.11189833956961034, gradient norm: 0.008659052594720442\n",
      "Iteration: 906000, loss: 0.11189421363613972, gradient norm: 0.07659188062266674\n",
      "Iteration: 907000, loss: 0.11189013026888397, gradient norm: 0.02206725774696309\n",
      "Iteration: 908000, loss: 0.11188608211893043, gradient norm: 0.13129965188453943\n",
      "Iteration: 909000, loss: 0.11188196998908831, gradient norm: 0.006103420472071132\n",
      "Iteration: 910000, loss: 0.11187798716129714, gradient norm: 0.047468804209230435\n",
      "Iteration: 911000, loss: 0.11187388093294905, gradient norm: 0.043702094928945834\n",
      "Iteration: 912000, loss: 0.11187002752693152, gradient norm: 0.06887901965087113\n",
      "Iteration: 913000, loss: 0.11186596798566299, gradient norm: 0.06460627014148114\n",
      "Iteration: 914000, loss: 0.11186198009371878, gradient norm: 0.021573308553147263\n",
      "Iteration: 915000, loss: 0.11185811992324639, gradient norm: 0.02562351992331275\n",
      "Iteration: 916000, loss: 0.11185414447652743, gradient norm: 0.002056241515338217\n",
      "Iteration: 917000, loss: 0.11185030099812102, gradient norm: 0.041164380262348414\n",
      "Iteration: 918000, loss: 0.11184650043405946, gradient norm: 0.020712623417442046\n",
      "Iteration: 919000, loss: 0.11184250036095186, gradient norm: 0.20349151966360765\n",
      "Iteration: 920000, loss: 0.11183861167849075, gradient norm: 0.01423714144115343\n",
      "Iteration: 921000, loss: 0.11183480302623665, gradient norm: 0.0017139669055207722\n",
      "Iteration: 922000, loss: 0.11183104603983296, gradient norm: 0.055586406905111053\n",
      "Iteration: 923000, loss: 0.11182714055298001, gradient norm: 0.000840218180517043\n",
      "Iteration: 924000, loss: 0.11182330445336619, gradient norm: 0.06628275739741721\n",
      "Iteration: 925000, loss: 0.11181958891061507, gradient norm: 0.03809107666199666\n",
      "Iteration: 926000, loss: 0.11181566755824858, gradient norm: 0.032919896516905967\n",
      "Iteration: 927000, loss: 0.11181185320934356, gradient norm: 0.009188369598471953\n",
      "Iteration: 928000, loss: 0.11180809003418778, gradient norm: 0.10163892814008763\n",
      "Iteration: 929000, loss: 0.11180431723044744, gradient norm: 0.11689064103194095\n",
      "Iteration: 930000, loss: 0.11180045322796127, gradient norm: 0.008819314063457588\n",
      "Iteration: 931000, loss: 0.1117966245615226, gradient norm: 0.10896218264305087\n",
      "Iteration: 932000, loss: 0.11179294523104841, gradient norm: 0.053709239653844054\n",
      "Iteration: 933000, loss: 0.11178908092589847, gradient norm: 0.013942220121091186\n",
      "Iteration: 934000, loss: 0.11178531931648447, gradient norm: 0.0040095639799982465\n",
      "Iteration: 935000, loss: 0.11178146428563751, gradient norm: 0.1290527856529144\n",
      "Iteration: 936000, loss: 0.11177777264741465, gradient norm: 0.07396873513760789\n",
      "Iteration: 937000, loss: 0.11177389160439245, gradient norm: 0.008193462992512253\n",
      "Iteration: 938000, loss: 0.11177012061604062, gradient norm: 0.007923010733242856\n",
      "Iteration: 939000, loss: 0.11176628189525595, gradient norm: 0.057821294314373876\n",
      "Iteration: 940000, loss: 0.11176240010650544, gradient norm: 0.01364795541943314\n",
      "Iteration: 941000, loss: 0.11175865404527656, gradient norm: 0.08944102223789598\n",
      "Iteration: 942000, loss: 0.11175481327968202, gradient norm: 0.026110653054333702\n",
      "Iteration: 943000, loss: 0.11175089686343632, gradient norm: 0.20427047324152298\n",
      "Iteration: 944000, loss: 0.11174710834075699, gradient norm: 0.11577103708858803\n",
      "Iteration: 945000, loss: 0.11174326895036554, gradient norm: 0.31450813447777304\n",
      "Iteration: 946000, loss: 0.11173937358444823, gradient norm: 0.07875286698607942\n",
      "Iteration: 947000, loss: 0.11173551784438439, gradient norm: 0.07831890879225482\n",
      "Iteration: 948000, loss: 0.1117315177658978, gradient norm: 0.06201994067841001\n",
      "Iteration: 949000, loss: 0.11172760743810811, gradient norm: 0.019615271573448466\n",
      "Iteration: 950000, loss: 0.11172370594291775, gradient norm: 0.04500547569576146\n",
      "Iteration: 951000, loss: 0.1117197438386098, gradient norm: 0.011669800353262021\n",
      "Iteration: 952000, loss: 0.11171582556682322, gradient norm: 0.018538899965376783\n",
      "Iteration: 953000, loss: 0.1117119482419727, gradient norm: 0.025512151180363353\n",
      "Iteration: 954000, loss: 0.11170790606134415, gradient norm: 0.16502473888931993\n",
      "Iteration: 955000, loss: 0.11170385244642023, gradient norm: 0.13115175064430515\n",
      "Iteration: 956000, loss: 0.11170001616849291, gradient norm: 0.012887912861813738\n",
      "Iteration: 957000, loss: 0.1116959340718056, gradient norm: 0.004418936664175297\n",
      "Iteration: 958000, loss: 0.11169202778434635, gradient norm: 0.004803351195407497\n",
      "Iteration: 959000, loss: 0.11168794761224714, gradient norm: 0.268364026621486\n",
      "Iteration: 960000, loss: 0.11168398960902937, gradient norm: 0.022732167898390953\n",
      "Iteration: 961000, loss: 0.11167976207939517, gradient norm: 0.06075003548610177\n",
      "Iteration: 962000, loss: 0.11167561071840479, gradient norm: 0.008019197596804075\n",
      "Iteration: 963000, loss: 0.11167160506374091, gradient norm: 0.018312130053352485\n",
      "Iteration: 964000, loss: 0.11166727759692187, gradient norm: 0.0468373642554323\n",
      "Iteration: 965000, loss: 0.11166320744099195, gradient norm: 0.024904422316467542\n",
      "Iteration: 966000, loss: 0.11165905854733083, gradient norm: 0.26773614318073335\n",
      "Iteration: 967000, loss: 0.1116548553333998, gradient norm: 0.04359760589512719\n",
      "Iteration: 968000, loss: 0.11165079613295067, gradient norm: 0.16180298827127884\n",
      "Iteration: 969000, loss: 0.1116466307110826, gradient norm: 0.007814967907867553\n",
      "Iteration: 970000, loss: 0.11164261623349966, gradient norm: 0.07832778811555215\n",
      "Iteration: 971000, loss: 0.11163842272486504, gradient norm: 0.0332995328459991\n",
      "Iteration: 972000, loss: 0.11163447596569256, gradient norm: 0.13670876662198017\n",
      "Iteration: 973000, loss: 0.11163030005464303, gradient norm: 0.004089990070136265\n",
      "Iteration: 974000, loss: 0.11162639004397358, gradient norm: 0.05318099464344418\n",
      "Iteration: 975000, loss: 0.1116222765868211, gradient norm: 0.1751618907334273\n",
      "Iteration: 976000, loss: 0.11161840292492875, gradient norm: 0.02238999049763261\n",
      "Iteration: 977000, loss: 0.11161429761974856, gradient norm: 0.0039640781425025005\n",
      "Iteration: 978000, loss: 0.11161047002101003, gradient norm: 0.13572604345148695\n",
      "Iteration: 979000, loss: 0.11160648153900682, gradient norm: 0.00662390715680325\n",
      "Iteration: 980000, loss: 0.11160256345571748, gradient norm: 0.16355739499372976\n",
      "Iteration: 981000, loss: 0.11159852265630067, gradient norm: 0.035100280543321455\n",
      "Iteration: 982000, loss: 0.11159478869323339, gradient norm: 0.020782019128932377\n",
      "Iteration: 983000, loss: 0.11159082188412461, gradient norm: 0.04597061107368244\n",
      "Iteration: 984000, loss: 0.11158690743771695, gradient norm: 0.039227954579750764\n",
      "Iteration: 985000, loss: 0.1115830318342961, gradient norm: 0.01763814548035527\n",
      "Iteration: 986000, loss: 0.11157920724998517, gradient norm: 0.03876023694421846\n",
      "Iteration: 987000, loss: 0.1115753707018584, gradient norm: 0.019742154867688363\n",
      "Iteration: 988000, loss: 0.1115715199080339, gradient norm: 0.13145447466585294\n",
      "Iteration: 989000, loss: 0.11156766656487939, gradient norm: 0.01847701646576792\n",
      "Iteration: 990000, loss: 0.11156383751141369, gradient norm: 0.08397421535867863\n",
      "Iteration: 991000, loss: 0.11156009366005121, gradient norm: 0.062239050756839054\n",
      "Iteration: 992000, loss: 0.11155628512410314, gradient norm: 0.024371135540152824\n",
      "Iteration: 993000, loss: 0.11155245690696092, gradient norm: 0.08566941701383693\n",
      "Iteration: 994000, loss: 0.11154876018602772, gradient norm: 0.0429368982478096\n",
      "Iteration: 995000, loss: 0.1115449489021432, gradient norm: 0.0028298268381859565\n",
      "Iteration: 996000, loss: 0.1115412102209198, gradient norm: 0.0381311298610784\n",
      "Iteration: 997000, loss: 0.11153747124476399, gradient norm: 0.00563846756068392\n",
      "Iteration: 998000, loss: 0.11153382914736115, gradient norm: 0.008677670674355627\n",
      "Iteration: 999000, loss: 0.1115301276320404, gradient norm: 0.11514142285414698\n",
      "Iteration: 1000000, loss: 0.11152634746775227, gradient norm: 0.04089551625258277\n",
      "Iteration: 1001000, loss: 0.11152266015128935, gradient norm: 0.03935066400429689\n",
      "Iteration: 1002000, loss: 0.11151897764717018, gradient norm: 0.010171057235564608\n",
      "Iteration: 1003000, loss: 0.11151536072269692, gradient norm: 0.03972848383978015\n",
      "Iteration: 1004000, loss: 0.11151165149708069, gradient norm: 0.0635957132575988\n",
      "Iteration: 1005000, loss: 0.11150800895576608, gradient norm: 0.06213491730447999\n",
      "Iteration: 1006000, loss: 0.1115044091235913, gradient norm: 0.008597034018104781\n",
      "Iteration: 1007000, loss: 0.11150068773463752, gradient norm: 0.07495663979693037\n",
      "Iteration: 1008000, loss: 0.11149714113288971, gradient norm: 0.02309703389204542\n",
      "Iteration: 1009000, loss: 0.11149351263525938, gradient norm: 0.004477945758624398\n",
      "Iteration: 1010000, loss: 0.1114898958999232, gradient norm: 0.1489555825729448\n",
      "Iteration: 1011000, loss: 0.11148629303890635, gradient norm: 0.05597858662174291\n",
      "Iteration: 1012000, loss: 0.11148272290218122, gradient norm: 0.0036296562933522884\n",
      "Iteration: 1013000, loss: 0.11147920310886034, gradient norm: 0.0947823322311597\n",
      "Iteration: 1014000, loss: 0.11147551100045651, gradient norm: 0.008439327621697697\n",
      "Iteration: 1015000, loss: 0.11147214164778603, gradient norm: 0.03330397146057543\n",
      "Iteration: 1016000, loss: 0.11146842942055885, gradient norm: 0.05046500352545509\n",
      "Iteration: 1017000, loss: 0.11146496237942458, gradient norm: 0.08083473535441403\n",
      "Iteration: 1018000, loss: 0.11146146903655199, gradient norm: 0.046031615417815835\n",
      "Iteration: 1019000, loss: 0.11145786946936519, gradient norm: 0.021133427157288675\n",
      "Iteration: 1020000, loss: 0.11145435783449507, gradient norm: 0.022187456607683894\n",
      "Iteration: 1021000, loss: 0.11145094086825041, gradient norm: 0.057166614506024595\n",
      "Iteration: 1022000, loss: 0.1114474189551095, gradient norm: 0.04233209301741336\n",
      "Iteration: 1023000, loss: 0.11144379061391971, gradient norm: 0.015025763254043945\n",
      "Iteration: 1024000, loss: 0.11144048493364349, gradient norm: 0.04440934862089172\n",
      "Iteration: 1025000, loss: 0.11143686576721318, gradient norm: 0.07480373674769353\n",
      "Iteration: 1026000, loss: 0.1114335210968389, gradient norm: 0.046004625169445784\n",
      "Iteration: 1027000, loss: 0.11143007957353063, gradient norm: 0.01634826446793046\n",
      "Iteration: 1028000, loss: 0.11142659679737833, gradient norm: 0.02270111930574417\n",
      "Iteration: 1029000, loss: 0.11142317073967459, gradient norm: 0.18906653670237472\n",
      "Iteration: 1030000, loss: 0.11141977659089053, gradient norm: 0.020871117592695686\n",
      "Iteration: 1031000, loss: 0.11141631027814375, gradient norm: 0.04458141265365934\n",
      "Iteration: 1032000, loss: 0.11141295802139019, gradient norm: 0.05625041523857639\n",
      "Iteration: 1033000, loss: 0.1114095529027428, gradient norm: 0.15185109983531717\n",
      "Iteration: 1034000, loss: 0.11140604837097501, gradient norm: 0.05260821752989466\n",
      "Iteration: 1035000, loss: 0.1114027114351446, gradient norm: 0.0028797910641235555\n",
      "Iteration: 1036000, loss: 0.11139932907627534, gradient norm: 0.18866609205063328\n",
      "Iteration: 1037000, loss: 0.11139594936698759, gradient norm: 0.02656722443708399\n",
      "Iteration: 1038000, loss: 0.11139261576220533, gradient norm: 0.032171066934722506\n",
      "Iteration: 1039000, loss: 0.11138925581204398, gradient norm: 0.0067948273889623\n",
      "Iteration: 1040000, loss: 0.11138593034253799, gradient norm: 0.013901036382617705\n",
      "Iteration: 1041000, loss: 0.11138250728255335, gradient norm: 0.01600098804549952\n",
      "Iteration: 1042000, loss: 0.11137923102384069, gradient norm: 0.04888245924881889\n",
      "Iteration: 1043000, loss: 0.11137591209787213, gradient norm: 0.18756794236987856\n",
      "Iteration: 1044000, loss: 0.11137253627716982, gradient norm: 0.10948096680704687\n",
      "Iteration: 1045000, loss: 0.11136921232725512, gradient norm: 0.0642412870414918\n",
      "Iteration: 1046000, loss: 0.11136593612808313, gradient norm: 0.03598764576749662\n",
      "Iteration: 1047000, loss: 0.11136260688045176, gradient norm: 0.03889717466296508\n",
      "Iteration: 1048000, loss: 0.11135932912303997, gradient norm: 0.06213244640249926\n",
      "Iteration: 1049000, loss: 0.11135611415662146, gradient norm: 0.12372528467913019\n",
      "Iteration: 1050000, loss: 0.1113526448799003, gradient norm: 0.008690360430881372\n",
      "Iteration: 1051000, loss: 0.11134953150172786, gradient norm: 0.26025293870784216\n",
      "Iteration: 1052000, loss: 0.1113462694679325, gradient norm: 0.005360684335642847\n",
      "Iteration: 1053000, loss: 0.11134297759470314, gradient norm: 0.03393214326602834\n",
      "Iteration: 1054000, loss: 0.11133972540109945, gradient norm: 0.018026008816046067\n",
      "Iteration: 1055000, loss: 0.11133647818960242, gradient norm: 0.05095233159677703\n",
      "Iteration: 1056000, loss: 0.1113332708585122, gradient norm: 0.08790194039233278\n",
      "Iteration: 1057000, loss: 0.11133007871536216, gradient norm: 0.14621974350522213\n",
      "Iteration: 1058000, loss: 0.11132673597781349, gradient norm: 0.01835028577479221\n",
      "Iteration: 1059000, loss: 0.11132364955095517, gradient norm: 0.10497476112459589\n",
      "Iteration: 1060000, loss: 0.11132039652536653, gradient norm: 0.014476739634603183\n",
      "Iteration: 1061000, loss: 0.11131721960443046, gradient norm: 0.16219262683742042\n",
      "Iteration: 1062000, loss: 0.11131396977568897, gradient norm: 0.10559960871423264\n",
      "Iteration: 1063000, loss: 0.11131082128128204, gradient norm: 0.25226669102680904\n",
      "Iteration: 1064000, loss: 0.11130759904520986, gradient norm: 0.026741948040847296\n",
      "Iteration: 1065000, loss: 0.11130442269822563, gradient norm: 0.037371590426893336\n",
      "Iteration: 1066000, loss: 0.11130128062015354, gradient norm: 0.003258890280727128\n",
      "Iteration: 1067000, loss: 0.11129818736423809, gradient norm: 0.05575306590885244\n",
      "Iteration: 1068000, loss: 0.11129490036283061, gradient norm: 0.017849792160821817\n",
      "Iteration: 1069000, loss: 0.11129182198523024, gradient norm: 0.046376350098247865\n",
      "Iteration: 1070000, loss: 0.11128871046088985, gradient norm: 0.05268513822443364\n",
      "Iteration: 1071000, loss: 0.11128550364779252, gradient norm: 0.06885004582019176\n",
      "Iteration: 1072000, loss: 0.11128240810235086, gradient norm: 0.20245291221642334\n",
      "Iteration: 1073000, loss: 0.11127932818435655, gradient norm: 0.02788871485453506\n",
      "Iteration: 1074000, loss: 0.11127623251988399, gradient norm: 0.23765460257751694\n",
      "Iteration: 1075000, loss: 0.11127312335464848, gradient norm: 0.08731003420569954\n",
      "Iteration: 1076000, loss: 0.11126995957626756, gradient norm: 0.01439492246807083\n",
      "Iteration: 1077000, loss: 0.11126688729759336, gradient norm: 0.11162744647442654\n",
      "Iteration: 1078000, loss: 0.11126378198104302, gradient norm: 0.020752275080222246\n",
      "Iteration: 1079000, loss: 0.1112608100325787, gradient norm: 0.05058232471583104\n",
      "Iteration: 1080000, loss: 0.11125755043645194, gradient norm: 0.03330999038136963\n",
      "Iteration: 1081000, loss: 0.11125457454678948, gradient norm: 0.028509721107992977\n",
      "Iteration: 1082000, loss: 0.11125151641440426, gradient norm: 0.0030430922659267436\n",
      "Iteration: 1083000, loss: 0.11124853204984629, gradient norm: 0.07310139892983529\n",
      "Iteration: 1084000, loss: 0.11124535899213933, gradient norm: 0.09382726530434696\n",
      "Iteration: 1085000, loss: 0.11124229894242782, gradient norm: 0.017631457816630527\n",
      "Iteration: 1086000, loss: 0.1112393478276689, gradient norm: 0.004136938984896671\n",
      "Iteration: 1087000, loss: 0.11123633142923044, gradient norm: 0.015872821501939537\n",
      "Iteration: 1088000, loss: 0.11123328697247402, gradient norm: 0.03518150088181657\n",
      "Iteration: 1089000, loss: 0.11123023266390593, gradient norm: 0.04210114012499311\n",
      "Iteration: 1090000, loss: 0.11122725428765838, gradient norm: 0.042677284933386454\n",
      "Iteration: 1091000, loss: 0.1112241777732875, gradient norm: 0.26026012066884546\n",
      "Iteration: 1092000, loss: 0.1112212583941197, gradient norm: 0.03871845859437363\n",
      "Iteration: 1093000, loss: 0.11121821525967254, gradient norm: 0.04278989628867704\n",
      "Iteration: 1094000, loss: 0.11121518390658468, gradient norm: 0.06678625341124195\n",
      "Iteration: 1095000, loss: 0.11121224261452463, gradient norm: 0.00620095713977849\n",
      "Iteration: 1096000, loss: 0.11120930284822526, gradient norm: 0.1293831217963404\n",
      "Iteration: 1097000, loss: 0.11120626665149283, gradient norm: 0.0430827689243487\n",
      "Iteration: 1098000, loss: 0.11120330596412313, gradient norm: 0.014416848983312414\n",
      "Iteration: 1099000, loss: 0.11120037225624037, gradient norm: 0.006476468741789677\n",
      "Iteration: 1100000, loss: 0.11119742656414261, gradient norm: 0.0454540528733113\n",
      "Iteration: 1101000, loss: 0.11119445463671031, gradient norm: 0.1784823017299864\n",
      "Iteration: 1102000, loss: 0.11119153106405719, gradient norm: 0.03208698483927859\n",
      "Iteration: 1103000, loss: 0.11118856929593984, gradient norm: 0.028764847762955972\n",
      "Iteration: 1104000, loss: 0.11118565063133294, gradient norm: 0.04831115545567356\n",
      "Iteration: 1105000, loss: 0.11118265063936573, gradient norm: 0.024479539909322775\n",
      "Iteration: 1106000, loss: 0.11117981916921692, gradient norm: 0.06371753995267813\n",
      "Iteration: 1107000, loss: 0.11117687525708049, gradient norm: 0.059226997707535124\n",
      "Iteration: 1108000, loss: 0.11117390259695979, gradient norm: 0.25417294097397697\n",
      "Iteration: 1109000, loss: 0.11117113905509471, gradient norm: 0.028219062051644882\n",
      "Iteration: 1110000, loss: 0.11116819496856799, gradient norm: 0.0035763181862280154\n",
      "Iteration: 1111000, loss: 0.11116523836204568, gradient norm: 0.10217568263163926\n",
      "Iteration: 1112000, loss: 0.11116237596005203, gradient norm: 0.16873950516090608\n",
      "Iteration: 1113000, loss: 0.11115951576926432, gradient norm: 0.012124288575692609\n",
      "Iteration: 1114000, loss: 0.11115667956735097, gradient norm: 0.03410994300011757\n",
      "Iteration: 1115000, loss: 0.11115370545472908, gradient norm: 0.13245920124998997\n",
      "Iteration: 1116000, loss: 0.11115087964285048, gradient norm: 0.1748971089727302\n",
      "Iteration: 1117000, loss: 0.1111479929080825, gradient norm: 0.06282457635167353\n",
      "Iteration: 1118000, loss: 0.11114511138692078, gradient norm: 0.10830166076763019\n",
      "Iteration: 1119000, loss: 0.11114233317103722, gradient norm: 0.0033842196650976037\n",
      "Iteration: 1120000, loss: 0.11113944647264537, gradient norm: 0.006041835666010746\n",
      "Iteration: 1121000, loss: 0.11113664420074693, gradient norm: 0.1252024397715286\n",
      "Iteration: 1122000, loss: 0.11113372402660428, gradient norm: 0.052363044331027966\n",
      "Iteration: 1123000, loss: 0.11113097325455788, gradient norm: 0.021025294076927548\n",
      "Iteration: 1124000, loss: 0.11112809309205787, gradient norm: 0.06464975622208848\n",
      "Iteration: 1125000, loss: 0.11112536405505301, gradient norm: 0.08641688733480173\n",
      "Iteration: 1126000, loss: 0.11112246828778595, gradient norm: 0.26856309928280275\n",
      "Iteration: 1127000, loss: 0.11111966896205994, gradient norm: 0.12258946820352586\n",
      "Iteration: 1128000, loss: 0.11111690510931307, gradient norm: 0.01920362487138029\n",
      "Iteration: 1129000, loss: 0.11111411722927435, gradient norm: 0.15174220302243555\n",
      "Iteration: 1130000, loss: 0.11111131643163473, gradient norm: 0.1727085441938866\n",
      "Iteration: 1131000, loss: 0.11110852242511887, gradient norm: 0.043006064899472646\n",
      "Iteration: 1132000, loss: 0.11110571916215446, gradient norm: 0.13308603153555254\n",
      "Iteration: 1133000, loss: 0.11110298950624903, gradient norm: 0.005405610481665437\n",
      "Iteration: 1134000, loss: 0.11110020606370095, gradient norm: 0.09590038257740792\n",
      "Iteration: 1135000, loss: 0.11109744932327596, gradient norm: 0.08593363332291297\n",
      "Iteration: 1136000, loss: 0.11109473689903693, gradient norm: 0.013659500596954236\n",
      "Iteration: 1137000, loss: 0.11109184273905308, gradient norm: 0.11896288707299328\n",
      "Iteration: 1138000, loss: 0.11108916874428895, gradient norm: 0.0683862939847088\n",
      "Iteration: 1139000, loss: 0.11108646939034199, gradient norm: 0.028559467339876982\n",
      "Iteration: 1140000, loss: 0.11108366446609139, gradient norm: 0.004098266990057202\n",
      "Iteration: 1141000, loss: 0.11108104165176648, gradient norm: 0.18541513808663246\n",
      "Iteration: 1142000, loss: 0.11107828511789561, gradient norm: 0.02738069980185243\n",
      "Iteration: 1143000, loss: 0.11107554279511415, gradient norm: 0.1525942603735174\n",
      "Iteration: 1144000, loss: 0.1110728101816965, gradient norm: 0.28670220212779507\n",
      "Iteration: 1145000, loss: 0.1110701134559178, gradient norm: 0.02452832372829578\n",
      "Iteration: 1146000, loss: 0.11106737665133457, gradient norm: 0.06897672765593137\n",
      "Iteration: 1147000, loss: 0.11106469205436005, gradient norm: 0.044223962127559115\n",
      "Iteration: 1148000, loss: 0.11106192298046706, gradient norm: 0.1859573705073879\n",
      "Iteration: 1149000, loss: 0.11105934899631249, gradient norm: 0.2427892783575896\n",
      "Iteration: 1150000, loss: 0.11105668093391192, gradient norm: 0.025441084042646108\n",
      "Iteration: 1151000, loss: 0.11105395812773233, gradient norm: 0.31551159655239763\n",
      "Iteration: 1152000, loss: 0.11105128613419352, gradient norm: 0.03462618916776253\n",
      "Iteration: 1153000, loss: 0.11104862806993103, gradient norm: 0.024155202156080524\n",
      "Iteration: 1154000, loss: 0.11104590027642548, gradient norm: 0.0820976705926918\n",
      "Iteration: 1155000, loss: 0.11104333705501207, gradient norm: 0.06319715718767117\n",
      "Iteration: 1156000, loss: 0.11104055400788769, gradient norm: 0.09077185468575497\n",
      "Iteration: 1157000, loss: 0.11103802209985199, gradient norm: 0.019276446305416882\n",
      "Iteration: 1158000, loss: 0.11103538502784206, gradient norm: 0.06311558952230156\n",
      "Iteration: 1159000, loss: 0.11103267660563394, gradient norm: 0.1207347087160564\n",
      "Iteration: 1160000, loss: 0.11102998925517718, gradient norm: 0.0038944815622592114\n",
      "Iteration: 1161000, loss: 0.11102744622349704, gradient norm: 0.03249866136518894\n",
      "Iteration: 1162000, loss: 0.11102477574674974, gradient norm: 0.013335319454306958\n",
      "Iteration: 1163000, loss: 0.11102224752699849, gradient norm: 0.05957214716107712\n",
      "Iteration: 1164000, loss: 0.1110194388843496, gradient norm: 0.04618278929975392\n",
      "Iteration: 1165000, loss: 0.11101701011414786, gradient norm: 0.017828784622298036\n",
      "Iteration: 1166000, loss: 0.1110143531811325, gradient norm: 0.1401226654753259\n",
      "Iteration: 1167000, loss: 0.11101167514887954, gradient norm: 0.16392680347841226\n",
      "Iteration: 1168000, loss: 0.11100918113318292, gradient norm: 0.10713945320704188\n",
      "Iteration: 1169000, loss: 0.11100652388995237, gradient norm: 0.10939021309814745\n",
      "Iteration: 1170000, loss: 0.11100393823044055, gradient norm: 0.15458720979173293\n",
      "Iteration: 1171000, loss: 0.111001403618794, gradient norm: 0.012908297725969936\n",
      "Iteration: 1172000, loss: 0.11099876486510053, gradient norm: 0.03472651496400719\n",
      "Iteration: 1173000, loss: 0.11099622646821873, gradient norm: 0.22902036901456857\n",
      "Iteration: 1174000, loss: 0.11099365379109458, gradient norm: 0.028941850876824545\n",
      "Iteration: 1175000, loss: 0.1109910968722843, gradient norm: 0.06396219688495246\n",
      "Iteration: 1176000, loss: 0.11098849563414916, gradient norm: 0.03433079412740638\n",
      "Iteration: 1177000, loss: 0.11098599146086559, gradient norm: 0.17633661469559295\n",
      "Iteration: 1178000, loss: 0.11098341091324757, gradient norm: 0.0796419479740661\n",
      "Iteration: 1179000, loss: 0.11098085337973129, gradient norm: 0.15837812505284726\n",
      "Iteration: 1180000, loss: 0.11097836162840641, gradient norm: 0.022187048060926497\n",
      "Iteration: 1181000, loss: 0.11097578088642913, gradient norm: 0.0668303500524804\n",
      "Iteration: 1182000, loss: 0.11097322934783406, gradient norm: 0.09744600302459364\n",
      "Iteration: 1183000, loss: 0.11097080610033112, gradient norm: 0.13832537019275035\n",
      "Iteration: 1184000, loss: 0.11096818323388646, gradient norm: 0.24425522673492972\n",
      "Iteration: 1185000, loss: 0.11096569977515364, gradient norm: 0.013663522344821634\n",
      "Iteration: 1186000, loss: 0.11096321627810637, gradient norm: 0.06304655514183283\n",
      "Iteration: 1187000, loss: 0.11096069700044901, gradient norm: 0.03782058990377318\n",
      "Iteration: 1188000, loss: 0.11095822652076863, gradient norm: 0.043617727846023115\n",
      "Iteration: 1189000, loss: 0.11095571926010114, gradient norm: 0.11490324094868452\n",
      "Iteration: 1190000, loss: 0.11095316797465335, gradient norm: 0.009959681437158432\n",
      "Iteration: 1191000, loss: 0.11095076403999866, gradient norm: 0.042653242086739894\n",
      "Iteration: 1192000, loss: 0.11094824419805312, gradient norm: 0.07341402372052265\n",
      "Iteration: 1193000, loss: 0.11094572561442427, gradient norm: 0.24716820326663524\n",
      "Iteration: 1194000, loss: 0.11094332120915992, gradient norm: 0.01637655095939529\n",
      "Iteration: 1195000, loss: 0.11094089203851785, gradient norm: 0.06773371859032312\n",
      "Iteration: 1196000, loss: 0.11093835284231143, gradient norm: 0.24099988879768866\n",
      "Iteration: 1197000, loss: 0.1109359690412515, gradient norm: 0.14211871648333194\n",
      "Iteration: 1198000, loss: 0.11093340488497117, gradient norm: 0.0466503142783536\n",
      "Iteration: 1199000, loss: 0.1109310820710911, gradient norm: 0.07869450034058136\n",
      "Iteration: 1200000, loss: 0.11092852211637166, gradient norm: 0.18295805459215297\n",
      "Iteration: 1201000, loss: 0.11092614910366767, gradient norm: 0.04111036645818997\n",
      "Iteration: 1202000, loss: 0.11092366488831268, gradient norm: 0.21036805470134626\n",
      "Iteration: 1203000, loss: 0.11092125102445552, gradient norm: 0.01232279950545675\n",
      "Iteration: 1204000, loss: 0.11091880527591798, gradient norm: 0.13280672918377365\n",
      "Iteration: 1205000, loss: 0.11091643778648938, gradient norm: 0.03845219004946679\n",
      "Iteration: 1206000, loss: 0.11091397881936658, gradient norm: 0.14719076278801685\n",
      "Iteration: 1207000, loss: 0.11091157725866072, gradient norm: 0.032180881333217747\n",
      "Iteration: 1208000, loss: 0.11090913661064562, gradient norm: 0.05582135669043172\n",
      "Iteration: 1209000, loss: 0.11090671009044867, gradient norm: 0.016929249071167527\n",
      "Iteration: 1210000, loss: 0.11090438916598962, gradient norm: 0.00859894198857799\n",
      "Iteration: 1211000, loss: 0.11090198724188279, gradient norm: 0.06838017258860822\n",
      "Iteration: 1212000, loss: 0.11089953360256459, gradient norm: 0.2038403813868305\n",
      "Iteration: 1213000, loss: 0.11089714877553118, gradient norm: 0.016034709536744882\n",
      "Iteration: 1214000, loss: 0.11089479967214094, gradient norm: 0.01737551657017565\n",
      "Iteration: 1215000, loss: 0.11089240363690119, gradient norm: 0.025107399400597485\n",
      "Iteration: 1216000, loss: 0.11089004789727332, gradient norm: 0.04716076859090039\n",
      "Iteration: 1217000, loss: 0.11088766654952027, gradient norm: 0.049480346066959724\n",
      "Iteration: 1218000, loss: 0.11088525675788653, gradient norm: 0.15004354257926417\n",
      "Iteration: 1219000, loss: 0.11088298754931526, gradient norm: 0.2279576860255935\n",
      "Iteration: 1220000, loss: 0.11088054562004067, gradient norm: 0.2115615598844071\n",
      "Iteration: 1221000, loss: 0.11087819896334085, gradient norm: 0.10891830410378346\n",
      "Iteration: 1222000, loss: 0.11087590269356008, gradient norm: 0.02250462093199879\n",
      "Iteration: 1223000, loss: 0.11087348805620101, gradient norm: 0.04681298257642581\n",
      "Iteration: 1224000, loss: 0.11087115492755657, gradient norm: 0.04024194596854481\n",
      "Iteration: 1225000, loss: 0.11086885324459056, gradient norm: 0.0546898321186779\n",
      "Iteration: 1226000, loss: 0.11086651252782269, gradient norm: 0.1602008192432932\n",
      "Iteration: 1227000, loss: 0.11086406474738039, gradient norm: 0.027002780914681692\n",
      "Iteration: 1228000, loss: 0.11086187959628677, gradient norm: 0.031336756749566604\n",
      "Iteration: 1229000, loss: 0.1108593865715032, gradient norm: 0.053141938842583045\n",
      "Iteration: 1230000, loss: 0.11085716383550621, gradient norm: 0.02426933756189548\n",
      "Iteration: 1231000, loss: 0.11085477986200142, gradient norm: 0.18638884080595378\n",
      "Iteration: 1232000, loss: 0.11085241354318615, gradient norm: 0.009750457895928115\n",
      "Iteration: 1233000, loss: 0.11085011311331357, gradient norm: 0.0272336856753483\n",
      "Iteration: 1234000, loss: 0.11084759954265508, gradient norm: 0.09491062136520789\n",
      "Iteration: 1235000, loss: 0.11084528242561234, gradient norm: 0.0374091524065863\n",
      "Iteration: 1236000, loss: 0.11084273418570857, gradient norm: 0.02108838798961149\n",
      "Iteration: 1237000, loss: 0.11084031716808293, gradient norm: 0.05980428788328025\n",
      "Iteration: 1238000, loss: 0.11083790125982393, gradient norm: 0.09162429966374894\n",
      "Iteration: 1239000, loss: 0.11083537298828613, gradient norm: 0.047956418522014814\n",
      "Iteration: 1240000, loss: 0.11083302462304377, gradient norm: 0.13764489254906465\n",
      "Iteration: 1241000, loss: 0.11083041045419759, gradient norm: 0.05442805838987988\n",
      "Iteration: 1242000, loss: 0.11082805206573311, gradient norm: 0.21518418423524815\n",
      "Iteration: 1243000, loss: 0.11082561903874318, gradient norm: 0.021037983530382973\n",
      "Iteration: 1244000, loss: 0.11082317251847972, gradient norm: 0.0388644676889163\n",
      "Iteration: 1245000, loss: 0.11082072428512914, gradient norm: 0.12528546370486884\n",
      "Iteration: 1246000, loss: 0.1108182762177866, gradient norm: 0.02844748446383926\n",
      "Iteration: 1247000, loss: 0.1108159028409974, gradient norm: 0.28956805314664735\n",
      "Iteration: 1248000, loss: 0.11081350163929693, gradient norm: 0.016146304538882515\n",
      "Iteration: 1249000, loss: 0.11081110013750699, gradient norm: 0.2541653750178389\n",
      "Iteration: 1250000, loss: 0.11080869512355623, gradient norm: 0.026197369751130135\n",
      "Iteration: 1251000, loss: 0.11080628604610414, gradient norm: 0.018513988002936535\n",
      "Iteration: 1252000, loss: 0.11080393536001983, gradient norm: 0.014331772494008913\n",
      "Iteration: 1253000, loss: 0.11080163039002534, gradient norm: 0.1317649842188433\n",
      "Iteration: 1254000, loss: 0.1107991442223863, gradient norm: 0.04295183173384793\n",
      "Iteration: 1255000, loss: 0.11079687453062599, gradient norm: 0.017499234621285432\n",
      "Iteration: 1256000, loss: 0.11079455248802876, gradient norm: 0.024095092748570215\n",
      "Iteration: 1257000, loss: 0.11079218766677847, gradient norm: 0.08157315487801338\n",
      "Iteration: 1258000, loss: 0.11079002460435362, gradient norm: 0.008891261316750264\n",
      "Iteration: 1259000, loss: 0.11078735368484179, gradient norm: 0.03840553916978239\n",
      "Iteration: 1260000, loss: 0.11078535499910143, gradient norm: 0.05069202659123819\n",
      "Iteration: 1261000, loss: 0.11078281767947516, gradient norm: 0.019666737284481255\n",
      "Iteration: 1262000, loss: 0.110780668436409, gradient norm: 0.1624181947404977\n",
      "Iteration: 1263000, loss: 0.11077825396702044, gradient norm: 0.10404695145433683\n",
      "Iteration: 1264000, loss: 0.11077592811145189, gradient norm: 0.006226218992799177\n",
      "Iteration: 1265000, loss: 0.11077368092468072, gradient norm: 0.22691786198903463\n",
      "Iteration: 1266000, loss: 0.11077137215324491, gradient norm: 0.08620082807044703\n",
      "Iteration: 1267000, loss: 0.11076916749571414, gradient norm: 0.08767216044589442\n",
      "Iteration: 1268000, loss: 0.11076670455516162, gradient norm: 0.06251474127253667\n",
      "Iteration: 1269000, loss: 0.11076452030772278, gradient norm: 0.056662860598775414\n",
      "Iteration: 1270000, loss: 0.11076221425446123, gradient norm: 0.02664920428351445\n",
      "Iteration: 1271000, loss: 0.11076004389290174, gradient norm: 0.045418363050302904\n",
      "Iteration: 1272000, loss: 0.11075765883800909, gradient norm: 0.01681987405083938\n",
      "Iteration: 1273000, loss: 0.11075540544674889, gradient norm: 0.0070879971486691095\n",
      "Iteration: 1274000, loss: 0.11075318679748451, gradient norm: 0.3149870154277172\n",
      "Iteration: 1275000, loss: 0.11075089041099041, gradient norm: 0.05231863936769028\n",
      "Iteration: 1276000, loss: 0.11074860205733338, gradient norm: 0.043749408800882544\n",
      "Iteration: 1277000, loss: 0.11074635124922326, gradient norm: 0.01676792068698318\n",
      "Iteration: 1278000, loss: 0.1107441568946697, gradient norm: 0.021448895420063237\n",
      "Iteration: 1279000, loss: 0.11074187335573424, gradient norm: 0.039868090082782366\n",
      "Iteration: 1280000, loss: 0.11073963439347469, gradient norm: 0.014577923385798644\n",
      "Iteration: 1281000, loss: 0.1107373913746287, gradient norm: 0.016422226093416082\n",
      "Iteration: 1282000, loss: 0.11073517871690813, gradient norm: 0.32601946829005535\n",
      "Iteration: 1283000, loss: 0.1107328996398985, gradient norm: 0.030808370526771853\n",
      "Iteration: 1284000, loss: 0.11073068369058803, gradient norm: 0.044104609846247186\n",
      "Iteration: 1285000, loss: 0.11072851615181294, gradient norm: 0.09182308345120656\n",
      "Iteration: 1286000, loss: 0.1107261890604347, gradient norm: 0.027282273067955876\n",
      "Iteration: 1287000, loss: 0.11072396505958565, gradient norm: 0.0472616220851244\n",
      "Iteration: 1288000, loss: 0.11072181546059044, gradient norm: 0.038965679859195\n",
      "Iteration: 1289000, loss: 0.1107195498386711, gradient norm: 0.07581040943680185\n",
      "Iteration: 1290000, loss: 0.11071731183077867, gradient norm: 0.1391984618650514\n",
      "Iteration: 1291000, loss: 0.11071516903250772, gradient norm: 0.241130682297488\n",
      "Iteration: 1292000, loss: 0.11071292667631966, gradient norm: 0.07571781885007986\n",
      "Iteration: 1293000, loss: 0.11071064368738252, gradient norm: 0.028743276466020626\n",
      "Iteration: 1294000, loss: 0.11070844192806475, gradient norm: 0.023480067608728446\n",
      "Iteration: 1295000, loss: 0.11070623705480698, gradient norm: 0.05822591277629157\n",
      "Iteration: 1296000, loss: 0.11070405142834511, gradient norm: 0.004351751229902389\n",
      "Iteration: 1297000, loss: 0.11070197183799581, gradient norm: 0.06045860816089743\n",
      "Iteration: 1298000, loss: 0.11069952781899445, gradient norm: 0.12304015024367376\n",
      "Iteration: 1299000, loss: 0.11069736971252184, gradient norm: 0.010866027000871482\n",
      "Iteration: 1300000, loss: 0.11069515685968341, gradient norm: 0.05091287471399189\n",
      "Iteration: 1301000, loss: 0.11069304241456598, gradient norm: 0.11200032030062586\n",
      "Iteration: 1302000, loss: 0.1106907220470384, gradient norm: 0.005097478263406539\n",
      "Iteration: 1303000, loss: 0.11068855254503228, gradient norm: 0.24745918314300744\n",
      "Iteration: 1304000, loss: 0.11068637031660938, gradient norm: 0.031215234906385177\n",
      "Iteration: 1305000, loss: 0.11068408678622779, gradient norm: 0.12810512053764928\n",
      "Iteration: 1306000, loss: 0.11068184301506041, gradient norm: 0.0915806995482685\n",
      "Iteration: 1307000, loss: 0.11067963480284436, gradient norm: 0.045023622611158526\n",
      "Iteration: 1308000, loss: 0.1106774191293707, gradient norm: 0.014155172250540143\n",
      "Iteration: 1309000, loss: 0.1106751181406237, gradient norm: 0.0179941038114857\n",
      "Iteration: 1310000, loss: 0.11067290120132485, gradient norm: 0.019055296456023774\n",
      "Iteration: 1311000, loss: 0.11067060170735599, gradient norm: 0.0502085355302832\n",
      "Iteration: 1312000, loss: 0.1106682954759308, gradient norm: 0.024244686205593266\n",
      "Iteration: 1313000, loss: 0.11066606686639478, gradient norm: 0.06578640464621943\n",
      "Iteration: 1314000, loss: 0.11066376415652973, gradient norm: 0.049003000543640304\n",
      "Iteration: 1315000, loss: 0.1106614284052339, gradient norm: 0.024079446279073186\n",
      "Iteration: 1316000, loss: 0.11065912692941073, gradient norm: 0.04689578036315108\n",
      "Iteration: 1317000, loss: 0.11065664621044437, gradient norm: 0.01560202230474281\n",
      "Iteration: 1318000, loss: 0.11065444655081916, gradient norm: 0.09003390059797575\n",
      "Iteration: 1319000, loss: 0.11065204677309495, gradient norm: 0.1169245447804025\n",
      "Iteration: 1320000, loss: 0.11064961812112319, gradient norm: 0.1979694474295565\n",
      "Iteration: 1321000, loss: 0.11064715590249005, gradient norm: 0.212148804303665\n",
      "Iteration: 1322000, loss: 0.11064478701864193, gradient norm: 0.06575379389708401\n",
      "Iteration: 1323000, loss: 0.11064236801074148, gradient norm: 0.01757059826575833\n",
      "Iteration: 1324000, loss: 0.11064001371439167, gradient norm: 0.11245670963527679\n",
      "Iteration: 1325000, loss: 0.11063744826908611, gradient norm: 0.022715865937905904\n",
      "Iteration: 1326000, loss: 0.11063504761324786, gradient norm: 0.21787507306801804\n",
      "Iteration: 1327000, loss: 0.1106327340657686, gradient norm: 0.037687438594623035\n",
      "Iteration: 1328000, loss: 0.11063021651377077, gradient norm: 0.0602648664953192\n",
      "Iteration: 1329000, loss: 0.1106278414902026, gradient norm: 0.02409180380183853\n",
      "Iteration: 1330000, loss: 0.11062539743040728, gradient norm: 0.07209458143458729\n",
      "Iteration: 1331000, loss: 0.11062294877349872, gradient norm: 0.004293818736640698\n",
      "Iteration: 1332000, loss: 0.1106206261496294, gradient norm: 0.2203696643459705\n",
      "Iteration: 1333000, loss: 0.11061829516762961, gradient norm: 0.0050885090196958315\n",
      "Iteration: 1334000, loss: 0.11061587375716356, gradient norm: 0.04136404165145515\n",
      "Iteration: 1335000, loss: 0.11061348371612681, gradient norm: 0.3147182128260672\n",
      "Iteration: 1336000, loss: 0.11061119643726079, gradient norm: 0.03262147528851095\n",
      "Iteration: 1337000, loss: 0.11060874547773476, gradient norm: 0.16237209926987822\n",
      "Iteration: 1338000, loss: 0.11060652476244107, gradient norm: 0.03444454994030451\n",
      "Iteration: 1339000, loss: 0.1106041473868564, gradient norm: 0.08156586210046983\n",
      "Iteration: 1340000, loss: 0.11060198101739038, gradient norm: 0.05332391686584071\n",
      "Iteration: 1341000, loss: 0.11059951156677968, gradient norm: 0.03230992845406368\n",
      "Iteration: 1342000, loss: 0.11059731998748763, gradient norm: 0.11416546270175182\n",
      "Iteration: 1343000, loss: 0.11059499410728227, gradient norm: 0.01930320117163721\n",
      "Iteration: 1344000, loss: 0.11059281456069689, gradient norm: 0.07124998747383764\n",
      "Iteration: 1345000, loss: 0.1105904935508091, gradient norm: 0.06803000544096127\n",
      "Iteration: 1346000, loss: 0.11058827055560022, gradient norm: 0.030757757234580182\n",
      "Iteration: 1347000, loss: 0.1105860691470357, gradient norm: 0.1224930917593423\n",
      "Iteration: 1348000, loss: 0.11058377820421071, gradient norm: 0.021746139149072655\n",
      "Iteration: 1349000, loss: 0.11058160323193128, gradient norm: 0.1930862488905865\n",
      "Iteration: 1350000, loss: 0.11057940283411584, gradient norm: 0.189252130820082\n",
      "Iteration: 1351000, loss: 0.11057713505932883, gradient norm: 0.0667797011103476\n",
      "Iteration: 1352000, loss: 0.11057510066364237, gradient norm: 0.017627890167595757\n",
      "Iteration: 1353000, loss: 0.11057280595889209, gradient norm: 0.032527194019030946\n",
      "Iteration: 1354000, loss: 0.11057064198631111, gradient norm: 0.30021831523389053\n",
      "Iteration: 1355000, loss: 0.11056844704156134, gradient norm: 0.030091940872621455\n",
      "Iteration: 1356000, loss: 0.11056625804646895, gradient norm: 0.0017857491111853054\n",
      "Iteration: 1357000, loss: 0.11056428430898117, gradient norm: 0.10903815213776846\n",
      "Iteration: 1358000, loss: 0.11056189140085797, gradient norm: 0.13618275010072858\n",
      "Iteration: 1359000, loss: 0.11055988309118497, gradient norm: 0.012433443655453364\n",
      "Iteration: 1360000, loss: 0.11055772833167697, gradient norm: 0.05949691905547652\n",
      "Iteration: 1361000, loss: 0.11055563650060424, gradient norm: 0.15800981803280967\n",
      "Iteration: 1362000, loss: 0.11055335112317329, gradient norm: 0.21255212982734203\n",
      "Iteration: 1363000, loss: 0.11055137019336886, gradient norm: 0.11270641089071803\n",
      "Iteration: 1364000, loss: 0.11054920154310756, gradient norm: 0.16554927554524285\n",
      "Iteration: 1365000, loss: 0.11054704992015019, gradient norm: 0.0243575265020902\n",
      "Iteration: 1366000, loss: 0.11054498088216036, gradient norm: 0.31907670898156626\n",
      "Iteration: 1367000, loss: 0.11054280708436429, gradient norm: 0.07513266949266095\n",
      "Iteration: 1368000, loss: 0.11054073493789744, gradient norm: 0.0964551440187497\n",
      "Iteration: 1369000, loss: 0.11053869034860557, gradient norm: 0.0019741356852907344\n",
      "Iteration: 1370000, loss: 0.11053657261499768, gradient norm: 0.06498742404954613\n",
      "Iteration: 1371000, loss: 0.11053449558467536, gradient norm: 0.13028164604298115\n",
      "Iteration: 1372000, loss: 0.11053233460691886, gradient norm: 0.05405148191863322\n",
      "Iteration: 1373000, loss: 0.11053032772719057, gradient norm: 0.043407278947054095\n",
      "Iteration: 1374000, loss: 0.11052817994788784, gradient norm: 0.019380893551865567\n",
      "Iteration: 1375000, loss: 0.11052614320283115, gradient norm: 0.007146397168230239\n",
      "Iteration: 1376000, loss: 0.11052409573628656, gradient norm: 0.017544557954416874\n",
      "Iteration: 1377000, loss: 0.11052198842874347, gradient norm: 0.15723009939119714\n",
      "Iteration: 1378000, loss: 0.11052005006965669, gradient norm: 0.03962346844149087\n",
      "Iteration: 1379000, loss: 0.1105178523111017, gradient norm: 0.04855992370410322\n",
      "Iteration: 1380000, loss: 0.11051590315285545, gradient norm: 0.08611590432119369\n",
      "Iteration: 1381000, loss: 0.110513764062322, gradient norm: 0.18377749053370807\n",
      "Iteration: 1382000, loss: 0.11051171734528355, gradient norm: 0.01773945803323899\n",
      "Iteration: 1383000, loss: 0.11050971476341197, gradient norm: 0.13437418099965484\n",
      "Iteration: 1384000, loss: 0.11050759839209981, gradient norm: 0.01471101263517209\n",
      "Iteration: 1385000, loss: 0.1105056463711821, gradient norm: 0.007351346630136244\n",
      "Iteration: 1386000, loss: 0.11050353010468643, gradient norm: 0.023366700875986565\n",
      "Iteration: 1387000, loss: 0.11050155678783688, gradient norm: 0.12722881043219691\n",
      "Iteration: 1388000, loss: 0.11049959641109619, gradient norm: 0.1175923845890052\n",
      "Iteration: 1389000, loss: 0.1104974397520218, gradient norm: 0.03863724046653684\n",
      "Iteration: 1390000, loss: 0.11049556757991401, gradient norm: 0.02564316149662676\n",
      "Iteration: 1391000, loss: 0.11049343440205443, gradient norm: 0.08659093221575491\n",
      "Iteration: 1392000, loss: 0.1104914796096593, gradient norm: 0.156042173235733\n",
      "Iteration: 1393000, loss: 0.11048950558896198, gradient norm: 0.0544292681559582\n",
      "Iteration: 1394000, loss: 0.11048741684344288, gradient norm: 0.08140736288132523\n",
      "Iteration: 1395000, loss: 0.11048535945674792, gradient norm: 0.17302619159267352\n",
      "Iteration: 1396000, loss: 0.11048346144331987, gradient norm: 0.0066089268022364615\n",
      "Iteration: 1397000, loss: 0.11048147781803636, gradient norm: 0.29022705130111026\n",
      "Iteration: 1398000, loss: 0.11047936672206421, gradient norm: 0.16202173617034105\n",
      "Iteration: 1399000, loss: 0.11047758068557005, gradient norm: 0.03272345194244374\n",
      "Iteration: 1400000, loss: 0.11047550595278705, gradient norm: 0.021043704305329834\n",
      "Iteration: 1401000, loss: 0.11047346000679255, gradient norm: 0.2557367823439531\n",
      "Iteration: 1402000, loss: 0.11047150614198549, gradient norm: 0.059395075021334176\n",
      "Iteration: 1403000, loss: 0.11046944781696985, gradient norm: 0.1148570043393815\n",
      "Iteration: 1404000, loss: 0.1104676918294492, gradient norm: 0.08799165283771083\n",
      "Iteration: 1405000, loss: 0.11046546964405074, gradient norm: 0.008750595521226017\n",
      "Iteration: 1406000, loss: 0.11046366053036374, gradient norm: 0.032611790775142264\n",
      "Iteration: 1407000, loss: 0.11046168573833244, gradient norm: 0.0033574161310098425\n",
      "Iteration: 1408000, loss: 0.11045967138760614, gradient norm: 0.029673642148404185\n",
      "Iteration: 1409000, loss: 0.11045773403558454, gradient norm: 0.020304385323046306\n",
      "Iteration: 1410000, loss: 0.11045572730357722, gradient norm: 0.03468915609868012\n",
      "Iteration: 1411000, loss: 0.11045384262203796, gradient norm: 0.022110207481244516\n",
      "Iteration: 1412000, loss: 0.11045180868825331, gradient norm: 0.03390354141060892\n",
      "Iteration: 1413000, loss: 0.1104498958259864, gradient norm: 0.07254731470833768\n",
      "Iteration: 1414000, loss: 0.11044785882967295, gradient norm: 0.005277178721377235\n",
      "Iteration: 1415000, loss: 0.11044601449586876, gradient norm: 0.024961722098697434\n",
      "Iteration: 1416000, loss: 0.11044397033977978, gradient norm: 0.07120869594171762\n",
      "Iteration: 1417000, loss: 0.11044207439533411, gradient norm: 0.07424885007870782\n",
      "Iteration: 1418000, loss: 0.11044013077829859, gradient norm: 0.028560015178042002\n",
      "Iteration: 1419000, loss: 0.11043818409909607, gradient norm: 0.01346764216785504\n",
      "Iteration: 1420000, loss: 0.11043633129225588, gradient norm: 0.01927871375366856\n",
      "Iteration: 1421000, loss: 0.11043429582059822, gradient norm: 0.18356987744556916\n",
      "Iteration: 1422000, loss: 0.11043241473446082, gradient norm: 0.04128867309889289\n",
      "Iteration: 1423000, loss: 0.11043041798244406, gradient norm: 0.06971041088208318\n",
      "Iteration: 1424000, loss: 0.11042853714371449, gradient norm: 0.1707400218513989\n",
      "Iteration: 1425000, loss: 0.11042664390623225, gradient norm: 0.19977113590175058\n",
      "Iteration: 1426000, loss: 0.11042460215546634, gradient norm: 0.26458037917838256\n",
      "Iteration: 1427000, loss: 0.11042273055653999, gradient norm: 0.2903786542141359\n",
      "Iteration: 1428000, loss: 0.11042086434369477, gradient norm: 0.10460035076382744\n",
      "Iteration: 1429000, loss: 0.11041891580599304, gradient norm: 0.14575903094320272\n",
      "Iteration: 1430000, loss: 0.11041704429886297, gradient norm: 0.04158643298616138\n",
      "Iteration: 1431000, loss: 0.11041508914413745, gradient norm: 0.023865983892834532\n",
      "Iteration: 1432000, loss: 0.1104131010620473, gradient norm: 0.015223269345876748\n",
      "Iteration: 1433000, loss: 0.11041135532148494, gradient norm: 0.07615032024059638\n",
      "Iteration: 1434000, loss: 0.11040942514279167, gradient norm: 0.4178344385056732\n",
      "Iteration: 1435000, loss: 0.11040743956562121, gradient norm: 0.02331942088522356\n",
      "Iteration: 1436000, loss: 0.1104056114973381, gradient norm: 0.09405590963809468\n",
      "Iteration: 1437000, loss: 0.11040371743217009, gradient norm: 0.22217912070050344\n",
      "Iteration: 1438000, loss: 0.11040168958964336, gradient norm: 0.002948544858005833\n",
      "Iteration: 1439000, loss: 0.11039996365423564, gradient norm: 0.05074048481619147\n",
      "Iteration: 1440000, loss: 0.11039792990813617, gradient norm: 0.013922923487711802\n",
      "Iteration: 1441000, loss: 0.1103961640356074, gradient norm: 0.3919467104711195\n",
      "Iteration: 1442000, loss: 0.11039418768028617, gradient norm: 0.06607892550922637\n",
      "Iteration: 1443000, loss: 0.11039238574226837, gradient norm: 0.07082809978939832\n",
      "Iteration: 1444000, loss: 0.11039036787142899, gradient norm: 0.020049360680312565\n",
      "Iteration: 1445000, loss: 0.11038859818315035, gradient norm: 0.012358398604207807\n",
      "Iteration: 1446000, loss: 0.11038671545571047, gradient norm: 0.11410539464566992\n",
      "Iteration: 1447000, loss: 0.11038480723361849, gradient norm: 0.07332811798934938\n",
      "Iteration: 1448000, loss: 0.11038281783047676, gradient norm: 0.01387153343704808\n",
      "Iteration: 1449000, loss: 0.11038106492325851, gradient norm: 0.07634440216622275\n",
      "Iteration: 1450000, loss: 0.11037917635921396, gradient norm: 0.27377213389340127\n",
      "Iteration: 1451000, loss: 0.11037731665109542, gradient norm: 0.007706876067316552\n",
      "Iteration: 1452000, loss: 0.11037531271652683, gradient norm: 0.022726613627685355\n",
      "Iteration: 1453000, loss: 0.11037363449945892, gradient norm: 0.019634074587412162\n",
      "Iteration: 1454000, loss: 0.11037166986111004, gradient norm: 0.15630386884466996\n",
      "Iteration: 1455000, loss: 0.11036982671624063, gradient norm: 0.0036771534108475676\n",
      "Iteration: 1456000, loss: 0.11036803642921081, gradient norm: 0.02238711200511341\n",
      "Iteration: 1457000, loss: 0.11036602197562538, gradient norm: 0.08186487007691147\n",
      "Iteration: 1458000, loss: 0.11036424408410235, gradient norm: 0.22928541586203044\n",
      "Iteration: 1459000, loss: 0.11036239242383968, gradient norm: 0.03372076995759449\n",
      "Iteration: 1460000, loss: 0.11036055343055669, gradient norm: 0.04203105272822387\n",
      "Iteration: 1461000, loss: 0.1103586456407605, gradient norm: 0.10138986944340152\n",
      "Iteration: 1462000, loss: 0.11035685691660872, gradient norm: 0.09471652933301399\n",
      "Iteration: 1463000, loss: 0.11035498708499299, gradient norm: 0.12593985291420728\n",
      "Iteration: 1464000, loss: 0.11035304018182522, gradient norm: 0.023574673013485148\n",
      "Iteration: 1465000, loss: 0.1103512378488168, gradient norm: 0.06751396905422688\n",
      "Iteration: 1466000, loss: 0.11034943401675898, gradient norm: 0.1419865202537077\n",
      "Iteration: 1467000, loss: 0.11034743025924684, gradient norm: 0.38840394834202513\n",
      "Iteration: 1468000, loss: 0.11034565065890169, gradient norm: 0.045484184146228464\n",
      "Iteration: 1469000, loss: 0.11034382632224231, gradient norm: 0.21516041881087342\n",
      "Iteration: 1470000, loss: 0.11034208448045424, gradient norm: 0.08598589243627304\n",
      "Iteration: 1471000, loss: 0.11034006481339734, gradient norm: 0.03464562568408329\n",
      "Iteration: 1472000, loss: 0.11033832181358309, gradient norm: 0.011230647515482223\n",
      "Iteration: 1473000, loss: 0.11033650198331615, gradient norm: 0.14061016298485426\n",
      "Iteration: 1474000, loss: 0.11033456260379423, gradient norm: 0.3045611427845935\n",
      "Iteration: 1475000, loss: 0.11033275366472801, gradient norm: 0.013202992201116707\n",
      "Iteration: 1476000, loss: 0.11033098319822339, gradient norm: 0.023329942131837183\n",
      "Iteration: 1477000, loss: 0.11032910307688745, gradient norm: 0.1355564895964569\n",
      "Iteration: 1478000, loss: 0.11032713437184338, gradient norm: 0.025642684276988088\n",
      "Iteration: 1479000, loss: 0.11032550546976384, gradient norm: 0.39374268479280544\n",
      "Iteration: 1480000, loss: 0.11032354752672052, gradient norm: 0.0293007945219947\n",
      "Iteration: 1481000, loss: 0.11032176143352372, gradient norm: 0.02533668082825903\n",
      "Iteration: 1482000, loss: 0.1103199434178989, gradient norm: 0.029899906676665186\n",
      "Iteration: 1483000, loss: 0.11031805502510536, gradient norm: 0.03538623978336717\n",
      "Iteration: 1484000, loss: 0.11031631368419888, gradient norm: 0.0418082444751972\n",
      "Iteration: 1485000, loss: 0.11031440056861866, gradient norm: 0.007591680368491363\n",
      "Iteration: 1486000, loss: 0.11031260659517207, gradient norm: 0.08059597843314568\n",
      "Iteration: 1487000, loss: 0.11031097836683398, gradient norm: 0.0387646909299883\n",
      "Iteration: 1488000, loss: 0.11030893478785733, gradient norm: 0.03629920215431336\n",
      "Iteration: 1489000, loss: 0.11030724657988616, gradient norm: 0.10286733814636286\n",
      "Iteration: 1490000, loss: 0.11030532258436992, gradient norm: 0.006420487255852701\n",
      "Iteration: 1491000, loss: 0.11030361582364834, gradient norm: 0.047269890616093525\n",
      "Iteration: 1492000, loss: 0.1103017503004402, gradient norm: 0.14901308665534366\n",
      "Iteration: 1493000, loss: 0.11029995589940779, gradient norm: 0.10458707933828691\n",
      "Iteration: 1494000, loss: 0.11029815909251414, gradient norm: 0.12157173413291682\n",
      "Iteration: 1495000, loss: 0.11029628474417252, gradient norm: 0.09307139828574804\n",
      "Iteration: 1496000, loss: 0.11029467520077589, gradient norm: 0.10280419288437151\n",
      "Iteration: 1497000, loss: 0.1102926617932964, gradient norm: 0.2742445978863487\n",
      "Iteration: 1498000, loss: 0.11029082978097414, gradient norm: 0.12018596521600662\n",
      "Iteration: 1499000, loss: 0.11028910745078423, gradient norm: 0.08151759519355484\n",
      "Iteration: 1500000, loss: 0.11028737815654481, gradient norm: 0.03266565400433246\n",
      "Iteration: 1501000, loss: 0.1102854853799449, gradient norm: 0.08236181215198277\n",
      "Iteration: 1502000, loss: 0.11028377952547094, gradient norm: 0.013184994145605838\n",
      "Iteration: 1503000, loss: 0.11028205889859058, gradient norm: 0.03814155455049985\n",
      "Iteration: 1504000, loss: 0.11028018737098211, gradient norm: 0.020982022797442216\n",
      "Iteration: 1505000, loss: 0.1102783335998034, gradient norm: 0.005443255065182855\n",
      "Iteration: 1506000, loss: 0.11027662369406292, gradient norm: 0.07572143910040405\n",
      "Iteration: 1507000, loss: 0.11027486130537091, gradient norm: 0.055516892146951956\n",
      "Iteration: 1508000, loss: 0.11027303286444896, gradient norm: 0.023835662543177878\n",
      "Iteration: 1509000, loss: 0.1102713114107843, gradient norm: 0.00048071346889162254\n",
      "Iteration: 1510000, loss: 0.11026951255280183, gradient norm: 0.012320831726824844\n",
      "Iteration: 1511000, loss: 0.11026779381932837, gradient norm: 0.042011779349421505\n",
      "Iteration: 1512000, loss: 0.1102658603485736, gradient norm: 0.007615628907346229\n",
      "Iteration: 1513000, loss: 0.11026421726110565, gradient norm: 0.1313261301883221\n",
      "Iteration: 1514000, loss: 0.11026239980957395, gradient norm: 0.015208376802979155\n",
      "Iteration: 1515000, loss: 0.11026066061161416, gradient norm: 0.006249527511885666\n",
      "Iteration: 1516000, loss: 0.11025879063022924, gradient norm: 0.003004661383098906\n",
      "Iteration: 1517000, loss: 0.11025707135194679, gradient norm: 0.011329675673270886\n",
      "Iteration: 1518000, loss: 0.11025544302918817, gradient norm: 0.05849249346533576\n",
      "Iteration: 1519000, loss: 0.11025351726955768, gradient norm: 0.19051217426209943\n",
      "Iteration: 1520000, loss: 0.11025175161176422, gradient norm: 0.04710762339109223\n",
      "Iteration: 1521000, loss: 0.11025005275963695, gradient norm: 0.19732276592832665\n",
      "Iteration: 1522000, loss: 0.11024820980338644, gradient norm: 0.00986854739093138\n",
      "Iteration: 1523000, loss: 0.11024645711793099, gradient norm: 0.08067208390929793\n",
      "Iteration: 1524000, loss: 0.11024479302283056, gradient norm: 0.2722095802091638\n",
      "Iteration: 1525000, loss: 0.11024299635337957, gradient norm: 0.020363285260471365\n",
      "Iteration: 1526000, loss: 0.11024120326428258, gradient norm: 0.0032493410636425094\n",
      "Iteration: 1527000, loss: 0.11023946615625317, gradient norm: 0.20664552981148313\n",
      "Iteration: 1528000, loss: 0.11023783668064487, gradient norm: 0.02848786983049091\n",
      "Iteration: 1529000, loss: 0.1102360098006812, gradient norm: 0.11646698464408237\n",
      "Iteration: 1530000, loss: 0.11023413002271742, gradient norm: 0.05439498587047283\n",
      "Iteration: 1531000, loss: 0.11023253245117672, gradient norm: 0.38286920226530147\n",
      "Iteration: 1532000, loss: 0.11023078110925524, gradient norm: 0.049051362534967176\n",
      "Iteration: 1533000, loss: 0.11022904908939683, gradient norm: 0.09819047728880877\n",
      "Iteration: 1534000, loss: 0.11022720354216495, gradient norm: 0.1071505441287601\n",
      "Iteration: 1535000, loss: 0.1102255270298947, gradient norm: 0.05571763317310017\n",
      "Iteration: 1536000, loss: 0.11022369820104119, gradient norm: 0.02626371437320353\n",
      "Iteration: 1537000, loss: 0.1102221976843426, gradient norm: 0.12233650055712977\n",
      "Iteration: 1538000, loss: 0.11022018139144711, gradient norm: 0.02081101094746239\n",
      "Iteration: 1539000, loss: 0.11021856330264374, gradient norm: 0.02993962909725363\n",
      "Iteration: 1540000, loss: 0.11021688633560504, gradient norm: 0.091087403184547\n",
      "Iteration: 1541000, loss: 0.11021503542679144, gradient norm: 0.018939282535478573\n",
      "Iteration: 1542000, loss: 0.1102133319746782, gradient norm: 0.057867858667291834\n",
      "Iteration: 1543000, loss: 0.1102116942643353, gradient norm: 0.28261742099845516\n",
      "Iteration: 1544000, loss: 0.1102100139044043, gradient norm: 0.16703531461159557\n",
      "Iteration: 1545000, loss: 0.110208129535465, gradient norm: 0.27407839821914365\n",
      "Iteration: 1546000, loss: 0.11020634048787854, gradient norm: 0.048574111389689596\n",
      "Iteration: 1547000, loss: 0.11020479627025045, gradient norm: 0.4488736841513524\n",
      "Iteration: 1548000, loss: 0.1102031209488859, gradient norm: 0.412957670461412\n",
      "Iteration: 1549000, loss: 0.11020127627095402, gradient norm: 0.0011674347010692185\n",
      "Iteration: 1550000, loss: 0.11019964144465529, gradient norm: 0.034710969005894717\n",
      "Iteration: 1551000, loss: 0.11019795776916434, gradient norm: 0.04729138686238515\n",
      "Iteration: 1552000, loss: 0.11019601059876023, gradient norm: 0.14427714504706943\n",
      "Iteration: 1553000, loss: 0.11019444384285255, gradient norm: 0.02208115816744056\n",
      "Iteration: 1554000, loss: 0.11019284561754931, gradient norm: 0.09375165078171131\n",
      "Iteration: 1555000, loss: 0.11019105367638707, gradient norm: 0.012225470050832174\n",
      "Iteration: 1556000, loss: 0.11018925623645465, gradient norm: 0.02554389913493621\n",
      "Iteration: 1557000, loss: 0.11018778585335912, gradient norm: 0.08603001578281586\n",
      "Iteration: 1558000, loss: 0.110185973543405, gradient norm: 0.002407078736805889\n",
      "Iteration: 1559000, loss: 0.11018421346600558, gradient norm: 0.02832012664081974\n",
      "Iteration: 1560000, loss: 0.11018259516328999, gradient norm: 0.0022852484188747085\n",
      "Iteration: 1561000, loss: 0.11018090743104128, gradient norm: 0.00252913676081996\n",
      "Iteration: 1562000, loss: 0.11017920384592578, gradient norm: 0.0020004263898286347\n",
      "Iteration: 1563000, loss: 0.11017757000253897, gradient norm: 0.013268660137954656\n",
      "Iteration: 1564000, loss: 0.1101758132246314, gradient norm: 0.019937801962558177\n",
      "Iteration: 1565000, loss: 0.11017411060506323, gradient norm: 0.024480328133813123\n",
      "Iteration: 1566000, loss: 0.11017251207745656, gradient norm: 0.062189456831746856\n",
      "Iteration: 1567000, loss: 0.11017070349823234, gradient norm: 0.01598444251451126\n",
      "Iteration: 1568000, loss: 0.11016912668557435, gradient norm: 0.009734998442218066\n",
      "Iteration: 1569000, loss: 0.1101673932085788, gradient norm: 0.022833653464834947\n",
      "Iteration: 1570000, loss: 0.11016572478684734, gradient norm: 0.017383424938217776\n",
      "Iteration: 1571000, loss: 0.11016399599959091, gradient norm: 0.017629300389983706\n",
      "Iteration: 1572000, loss: 0.11016222811330713, gradient norm: 0.387962935811738\n",
      "Iteration: 1573000, loss: 0.11016072337834831, gradient norm: 0.03599773360246872\n",
      "Iteration: 1574000, loss: 0.11015909220968786, gradient norm: 0.029604972111235334\n",
      "Iteration: 1575000, loss: 0.11015725447025063, gradient norm: 0.003072059860629047\n",
      "Iteration: 1576000, loss: 0.11015566507129344, gradient norm: 0.05003175863086514\n",
      "Iteration: 1577000, loss: 0.11015390249601043, gradient norm: 0.04979526674481642\n",
      "Iteration: 1578000, loss: 0.11015233572288007, gradient norm: 0.0006653547050700314\n",
      "Iteration: 1579000, loss: 0.11015071660569066, gradient norm: 0.0746282241886227\n",
      "Iteration: 1580000, loss: 0.110148974373358, gradient norm: 0.09912918033039833\n",
      "Iteration: 1581000, loss: 0.11014720175191463, gradient norm: 0.3095894751246396\n",
      "Iteration: 1582000, loss: 0.11014562619128694, gradient norm: 0.0025604573973781204\n",
      "Iteration: 1583000, loss: 0.11014404177886429, gradient norm: 0.11390733052461044\n",
      "Iteration: 1584000, loss: 0.11014234877616105, gradient norm: 0.029005860972042188\n",
      "Iteration: 1585000, loss: 0.11014056223905343, gradient norm: 0.03047228930517941\n",
      "Iteration: 1586000, loss: 0.1101390167793812, gradient norm: 0.2236511834733391\n",
      "Iteration: 1587000, loss: 0.11013742628600895, gradient norm: 0.012562206913328506\n",
      "Iteration: 1588000, loss: 0.11013574026685714, gradient norm: 0.43384451796716483\n",
      "Iteration: 1589000, loss: 0.11013400572985445, gradient norm: 0.01115743820214054\n",
      "Iteration: 1590000, loss: 0.11013240130967153, gradient norm: 0.05361332213717242\n",
      "Iteration: 1591000, loss: 0.11013086215782027, gradient norm: 0.09904789986097932\n",
      "Iteration: 1592000, loss: 0.11012914312562279, gradient norm: 0.008727875037446556\n",
      "Iteration: 1593000, loss: 0.11012734624423108, gradient norm: 0.30665763939488144\n",
      "Iteration: 1594000, loss: 0.1101258925099851, gradient norm: 0.00839137514391668\n",
      "Iteration: 1595000, loss: 0.11012417159632484, gradient norm: 0.023378412985959267\n",
      "Iteration: 1596000, loss: 0.11012253323549459, gradient norm: 0.019322422030458107\n",
      "Iteration: 1597000, loss: 0.11012099824812921, gradient norm: 0.18371816116810197\n",
      "Iteration: 1598000, loss: 0.11011920927161722, gradient norm: 0.010443942171344376\n",
      "Iteration: 1599000, loss: 0.11011774035999695, gradient norm: 0.024347427750213455\n",
      "Iteration: 1600000, loss: 0.1101159363522509, gradient norm: 0.0017172823936562058\n",
      "Iteration: 1601000, loss: 0.11011439422705874, gradient norm: 0.028717132871412338\n",
      "Iteration: 1602000, loss: 0.11011276893933832, gradient norm: 0.08572798718883573\n",
      "Iteration: 1603000, loss: 0.11011107693378666, gradient norm: 0.028566361183608228\n",
      "Iteration: 1604000, loss: 0.11010956313016536, gradient norm: 0.03761333758605829\n",
      "Iteration: 1605000, loss: 0.11010781320967136, gradient norm: 0.3768477119243021\n",
      "Iteration: 1606000, loss: 0.11010624572769846, gradient norm: 0.02978244453854236\n",
      "Iteration: 1607000, loss: 0.1101045695074741, gradient norm: 0.06107317075005455\n",
      "Iteration: 1608000, loss: 0.11010305368246011, gradient norm: 0.006211321428234505\n",
      "Iteration: 1609000, loss: 0.11010139449273544, gradient norm: 0.42885712430203926\n",
      "Iteration: 1610000, loss: 0.11009967584537025, gradient norm: 0.004151124998129645\n",
      "Iteration: 1611000, loss: 0.11009829560201975, gradient norm: 0.03234949853287486\n",
      "Iteration: 1612000, loss: 0.11009649826021818, gradient norm: 0.0269792282428868\n",
      "Iteration: 1613000, loss: 0.11009490678686357, gradient norm: 0.0018417904009675374\n",
      "Iteration: 1614000, loss: 0.11009330609385251, gradient norm: 0.007652091810096143\n",
      "Iteration: 1615000, loss: 0.11009177566499224, gradient norm: 0.08439505677449324\n",
      "Iteration: 1616000, loss: 0.11009000773053479, gradient norm: 0.013716110155366924\n",
      "Iteration: 1617000, loss: 0.11008859704513108, gradient norm: 0.018775763131236845\n",
      "Iteration: 1618000, loss: 0.11008680041501163, gradient norm: 0.004457805691364482\n",
      "Iteration: 1619000, loss: 0.11008527290153922, gradient norm: 0.03687686131047912\n",
      "Iteration: 1620000, loss: 0.11008358438657345, gradient norm: 0.06066036614144853\n",
      "Iteration: 1621000, loss: 0.11008215042847264, gradient norm: 0.09678215442294227\n",
      "Iteration: 1622000, loss: 0.11008043275967734, gradient norm: 0.1540025266242096\n",
      "Iteration: 1623000, loss: 0.1100788334104271, gradient norm: 0.0034406006570259504\n",
      "Iteration: 1624000, loss: 0.1100772675942123, gradient norm: 0.021778725443349786\n",
      "Iteration: 1625000, loss: 0.11007570620681052, gradient norm: 0.34846353542639297\n",
      "Iteration: 1626000, loss: 0.11007410927368946, gradient norm: 0.05631648248301798\n",
      "Iteration: 1627000, loss: 0.11007245290593953, gradient norm: 0.1908546664239849\n",
      "Iteration: 1628000, loss: 0.11007083725214184, gradient norm: 0.015659013226297213\n",
      "Iteration: 1629000, loss: 0.11006942296027185, gradient norm: 0.2968543231914664\n",
      "Iteration: 1630000, loss: 0.11006774237045738, gradient norm: 0.10492541700452232\n",
      "Iteration: 1631000, loss: 0.11006607573136737, gradient norm: 0.04961833731527221\n",
      "Iteration: 1632000, loss: 0.11006445931727564, gradient norm: 0.01153629291475457\n",
      "Iteration: 1633000, loss: 0.1100630416582949, gradient norm: 0.05239618385146686\n",
      "Iteration: 1634000, loss: 0.11006134462424327, gradient norm: 0.13685726152457336\n",
      "Iteration: 1635000, loss: 0.11005976681081782, gradient norm: 0.03346323515498653\n",
      "Iteration: 1636000, loss: 0.11005818154477645, gradient norm: 0.01476716895635073\n",
      "Iteration: 1637000, loss: 0.11005668918818619, gradient norm: 0.13561696920437283\n",
      "Iteration: 1638000, loss: 0.1100550671461765, gradient norm: 0.05603639165272708\n",
      "Iteration: 1639000, loss: 0.11005345987209843, gradient norm: 0.1700750114267032\n",
      "Iteration: 1640000, loss: 0.11005190126306613, gradient norm: 0.11287017168204386\n",
      "Iteration: 1641000, loss: 0.11005030919522542, gradient norm: 0.006807665255786946\n",
      "Iteration: 1642000, loss: 0.11004873049123617, gradient norm: 0.03838762420763582\n",
      "Iteration: 1643000, loss: 0.11004722503964492, gradient norm: 0.011888680123033114\n",
      "Iteration: 1644000, loss: 0.11004560655161019, gradient norm: 0.2235861658011172\n",
      "Iteration: 1645000, loss: 0.11004396611261184, gradient norm: 0.0013801588103161176\n",
      "Iteration: 1646000, loss: 0.11004261893578052, gradient norm: 0.019950314349186475\n",
      "Iteration: 1647000, loss: 0.11004085987279724, gradient norm: 0.02903069305248237\n",
      "Iteration: 1648000, loss: 0.11003929954769208, gradient norm: 0.014499270085156878\n",
      "Iteration: 1649000, loss: 0.1100378035464079, gradient norm: 0.002323912020705954\n",
      "Iteration: 1650000, loss: 0.11003622303809607, gradient norm: 0.0008857194179898829\n",
      "Iteration: 1651000, loss: 0.11003474983546792, gradient norm: 0.053450794937592895\n",
      "Iteration: 1652000, loss: 0.11003313913197363, gradient norm: 0.2745065951851263\n",
      "Iteration: 1653000, loss: 0.11003160702385785, gradient norm: 0.015064281329097463\n",
      "Iteration: 1654000, loss: 0.11003003613647036, gradient norm: 0.04014853864991628\n",
      "Iteration: 1655000, loss: 0.11002859595730825, gradient norm: 0.02349724863102241\n",
      "Iteration: 1656000, loss: 0.11002694066153877, gradient norm: 0.00806884155353888\n",
      "Iteration: 1657000, loss: 0.11002533572264446, gradient norm: 0.2461339795864134\n",
      "Iteration: 1658000, loss: 0.11002399089509089, gradient norm: 0.30794425907878914\n",
      "Iteration: 1659000, loss: 0.11002237604724667, gradient norm: 0.1403429849335413\n",
      "Iteration: 1660000, loss: 0.11002077643286202, gradient norm: 0.029998624286068584\n",
      "Iteration: 1661000, loss: 0.11001940626982655, gradient norm: 0.08070610300752415\n",
      "Iteration: 1662000, loss: 0.11001757386334246, gradient norm: 0.027619928404973466\n",
      "Iteration: 1663000, loss: 0.11001622633922384, gradient norm: 0.0021425160614188246\n",
      "Iteration: 1664000, loss: 0.11001476584534435, gradient norm: 0.42647329567713604\n",
      "Iteration: 1665000, loss: 0.11001310223309906, gradient norm: 0.5141779538880135\n",
      "Iteration: 1666000, loss: 0.11001156846833166, gradient norm: 0.32007751132197404\n",
      "Iteration: 1667000, loss: 0.1100100681137928, gradient norm: 0.006800808133341799\n",
      "Iteration: 1668000, loss: 0.1100086280333818, gradient norm: 0.02116879343285096\n",
      "Iteration: 1669000, loss: 0.11000705314295227, gradient norm: 0.15464504107740845\n",
      "Iteration: 1670000, loss: 0.11000549635272816, gradient norm: 0.01200962680617303\n",
      "Iteration: 1671000, loss: 0.11000401575577422, gradient norm: 0.00467896355782785\n",
      "Iteration: 1672000, loss: 0.11000246021542157, gradient norm: 0.011145075850378829\n",
      "Iteration: 1673000, loss: 0.11000095585821154, gradient norm: 0.0015567804902259267\n",
      "Iteration: 1674000, loss: 0.10999946727654937, gradient norm: 0.040928299834503946\n",
      "Iteration: 1675000, loss: 0.10999785116662131, gradient norm: 0.0019305305014685604\n",
      "Iteration: 1676000, loss: 0.10999635020816036, gradient norm: 0.05283188045223511\n",
      "Iteration: 1677000, loss: 0.1099950024338715, gradient norm: 0.13324673473320214\n",
      "Iteration: 1678000, loss: 0.10999334804106568, gradient norm: 0.08164029440171414\n",
      "Iteration: 1679000, loss: 0.1099918305958193, gradient norm: 0.14992513916545674\n",
      "Iteration: 1680000, loss: 0.10999031255310816, gradient norm: 0.002532849079646567\n",
      "Iteration: 1681000, loss: 0.10998877151137174, gradient norm: 0.031161438204732172\n",
      "Iteration: 1682000, loss: 0.10998737491191589, gradient norm: 0.012519171225403674\n",
      "Iteration: 1683000, loss: 0.10998584929655056, gradient norm: 0.004524000852334423\n",
      "Iteration: 1684000, loss: 0.10998442228516446, gradient norm: 0.10259194763928049\n",
      "Iteration: 1685000, loss: 0.10998269754104342, gradient norm: 0.1400440658954255\n",
      "Iteration: 1686000, loss: 0.10998148318209296, gradient norm: 0.038787058343895946\n",
      "Iteration: 1687000, loss: 0.10997979109012207, gradient norm: 0.008616772220092335\n",
      "Iteration: 1688000, loss: 0.10997842476018611, gradient norm: 0.07843664549337004\n",
      "Iteration: 1689000, loss: 0.1099768081193485, gradient norm: 0.009515795351094798\n",
      "Iteration: 1690000, loss: 0.10997523951623811, gradient norm: 0.04307140155138951\n",
      "Iteration: 1691000, loss: 0.10997385628994626, gradient norm: 0.01551637436461686\n",
      "Iteration: 1692000, loss: 0.10997232110428808, gradient norm: 0.014343690593610856\n",
      "Iteration: 1693000, loss: 0.10997082401416902, gradient norm: 0.03005359753375611\n",
      "Iteration: 1694000, loss: 0.10996937840262545, gradient norm: 0.004152458595651922\n",
      "Iteration: 1695000, loss: 0.10996783747326695, gradient norm: 0.0032975276505706383\n",
      "Iteration: 1696000, loss: 0.10996639180732262, gradient norm: 0.030101045064406372\n",
      "Iteration: 1697000, loss: 0.10996489859521051, gradient norm: 0.20208929354530983\n",
      "Iteration: 1698000, loss: 0.10996334037124726, gradient norm: 0.04875526400385028\n",
      "Iteration: 1699000, loss: 0.10996188744081295, gradient norm: 0.5876058672852014\n",
      "Iteration: 1700000, loss: 0.10996041901942044, gradient norm: 0.0008259649951258579\n",
      "Iteration: 1701000, loss: 0.10995891382906602, gradient norm: 0.016773888375269507\n",
      "Iteration: 1702000, loss: 0.10995742793357147, gradient norm: 0.28329199574730257\n",
      "Iteration: 1703000, loss: 0.10995597084817658, gradient norm: 0.270520586854027\n",
      "Iteration: 1704000, loss: 0.10995458120446298, gradient norm: 0.028596690856161205\n",
      "Iteration: 1705000, loss: 0.109953015356069, gradient norm: 0.33523162992663624\n",
      "Iteration: 1706000, loss: 0.10995153767098112, gradient norm: 0.02781236701849345\n",
      "Iteration: 1707000, loss: 0.10995016426732283, gradient norm: 0.04692389947650439\n",
      "Iteration: 1708000, loss: 0.10994854793647689, gradient norm: 0.014048140625833419\n",
      "Iteration: 1709000, loss: 0.1099472582092276, gradient norm: 0.008661552361768092\n",
      "Iteration: 1710000, loss: 0.10994553851688284, gradient norm: 0.00295540847340796\n",
      "Iteration: 1711000, loss: 0.10994430451964862, gradient norm: 0.027026650095308975\n",
      "Iteration: 1712000, loss: 0.10994273895557129, gradient norm: 0.1243928784332506\n",
      "Iteration: 1713000, loss: 0.10994146357603958, gradient norm: 0.06005739009391977\n",
      "Iteration: 1714000, loss: 0.1099397310218263, gradient norm: 0.14467585033831573\n",
      "Iteration: 1715000, loss: 0.1099383010447229, gradient norm: 0.009266515109252358\n",
      "Iteration: 1716000, loss: 0.10993690780354089, gradient norm: 0.1843623989854807\n",
      "Iteration: 1717000, loss: 0.10993556376705894, gradient norm: 0.255969899106619\n",
      "Iteration: 1718000, loss: 0.10993391916096852, gradient norm: 0.002010647967578435\n",
      "Iteration: 1719000, loss: 0.10993257569291849, gradient norm: 0.06748367730553655\n",
      "Iteration: 1720000, loss: 0.10993115322963667, gradient norm: 0.003987210405389778\n",
      "Iteration: 1721000, loss: 0.1099295911271017, gradient norm: 0.03915220557128446\n",
      "Iteration: 1722000, loss: 0.10992823931646821, gradient norm: 0.008364932633672545\n",
      "Iteration: 1723000, loss: 0.10992680990945099, gradient norm: 0.010895914226970033\n",
      "Iteration: 1724000, loss: 0.1099252446433073, gradient norm: 0.0018755614249106115\n",
      "Iteration: 1725000, loss: 0.10992382901757547, gradient norm: 0.07929844756097447\n",
      "Iteration: 1726000, loss: 0.10992243623629136, gradient norm: 0.02357655809013106\n",
      "Iteration: 1727000, loss: 0.10992097643293243, gradient norm: 0.014404968715116546\n",
      "Iteration: 1728000, loss: 0.10991966216760242, gradient norm: 0.1502229253611173\n",
      "Iteration: 1729000, loss: 0.10991796200566913, gradient norm: 0.25638460378087047\n",
      "Iteration: 1730000, loss: 0.10991672381628102, gradient norm: 0.009681311825397061\n",
      "Iteration: 1731000, loss: 0.109915247455039, gradient norm: 0.0038387951424206225\n",
      "Iteration: 1732000, loss: 0.10991380779955434, gradient norm: 0.03085580366514792\n",
      "Iteration: 1733000, loss: 0.10991230210155854, gradient norm: 0.002958610129571525\n",
      "Iteration: 1734000, loss: 0.10991095388830321, gradient norm: 0.015126339567087985\n",
      "Iteration: 1735000, loss: 0.10990951491171538, gradient norm: 0.014365493107410098\n",
      "Iteration: 1736000, loss: 0.10990798873005682, gradient norm: 0.060573960618634\n",
      "Iteration: 1737000, loss: 0.10990665058066719, gradient norm: 0.29506830903813475\n",
      "Iteration: 1738000, loss: 0.10990531689511447, gradient norm: 0.0878973278573108\n",
      "Iteration: 1739000, loss: 0.10990372893083031, gradient norm: 0.017144338866020993\n",
      "Iteration: 1740000, loss: 0.10990234830217441, gradient norm: 0.08603390848060723\n",
      "Iteration: 1741000, loss: 0.10990098802104917, gradient norm: 0.05029566752416626\n",
      "Iteration: 1742000, loss: 0.10989955314645622, gradient norm: 0.4269708906860912\n",
      "Iteration: 1743000, loss: 0.10989803462413722, gradient norm: 0.018585267219287226\n",
      "Iteration: 1744000, loss: 0.10989659776607316, gradient norm: 0.09657519048643051\n",
      "Iteration: 1745000, loss: 0.10989541904194625, gradient norm: 0.2823112176282631\n",
      "Iteration: 1746000, loss: 0.10989380622446998, gradient norm: 0.2732320166461572\n",
      "Iteration: 1747000, loss: 0.10989245630179377, gradient norm: 0.03467968429087278\n",
      "Iteration: 1748000, loss: 0.10989106204854761, gradient norm: 0.0034787322068253147\n",
      "Iteration: 1749000, loss: 0.1098896306085644, gradient norm: 0.2192521369314048\n",
      "Iteration: 1750000, loss: 0.10988826966226613, gradient norm: 0.0034285260599848967\n",
      "Iteration: 1751000, loss: 0.10988676777630185, gradient norm: 0.019025930327254477\n",
      "Iteration: 1752000, loss: 0.1098853661581673, gradient norm: 0.0062477721312533905\n",
      "Iteration: 1753000, loss: 0.10988412207175109, gradient norm: 0.12397418589698897\n",
      "Iteration: 1754000, loss: 0.10988257416699074, gradient norm: 0.009680762594309953\n",
      "Iteration: 1755000, loss: 0.10988119493942884, gradient norm: 0.3301880296512336\n",
      "Iteration: 1756000, loss: 0.10987974398230935, gradient norm: 0.00893156963444012\n",
      "Iteration: 1757000, loss: 0.10987848614254929, gradient norm: 0.015896076087507267\n",
      "Iteration: 1758000, loss: 0.10987694696763327, gradient norm: 0.010166241062206989\n",
      "Iteration: 1759000, loss: 0.10987556515791982, gradient norm: 0.0042918753042175345\n",
      "Iteration: 1760000, loss: 0.10987423770974186, gradient norm: 0.02668854382646078\n",
      "Iteration: 1761000, loss: 0.10987285965423525, gradient norm: 0.0058324806228250495\n",
      "Iteration: 1762000, loss: 0.10987145895687257, gradient norm: 0.0061869833085853206\n",
      "Iteration: 1763000, loss: 0.10987023637217269, gradient norm: 0.04294807777784381\n",
      "Iteration: 1764000, loss: 0.10986879226669582, gradient norm: 0.034481864997686094\n",
      "Iteration: 1765000, loss: 0.10986720558258796, gradient norm: 0.004592501765815242\n",
      "Iteration: 1766000, loss: 0.10986602035692258, gradient norm: 0.08034560635980502\n",
      "Iteration: 1767000, loss: 0.10986441964715016, gradient norm: 0.0009584383046834638\n",
      "Iteration: 1768000, loss: 0.10986321779452236, gradient norm: 0.007055692132861323\n",
      "Iteration: 1769000, loss: 0.10986180941925391, gradient norm: 0.005343048599939358\n",
      "Iteration: 1770000, loss: 0.10986044196719964, gradient norm: 0.004598389163517689\n",
      "Iteration: 1771000, loss: 0.109859018050215, gradient norm: 0.32795039040406715\n",
      "Iteration: 1772000, loss: 0.1098577157831947, gradient norm: 0.022522019182998364\n",
      "Iteration: 1773000, loss: 0.10985621321800683, gradient norm: 0.0028853833396231977\n",
      "Iteration: 1774000, loss: 0.10985510950473494, gradient norm: 0.06280710799964574\n",
      "Iteration: 1775000, loss: 0.10985336457789234, gradient norm: 0.02351795991191563\n",
      "Iteration: 1776000, loss: 0.10985232590630445, gradient norm: 0.0015438294166198081\n",
      "Iteration: 1777000, loss: 0.10985080516361886, gradient norm: 0.06831946632232444\n",
      "Iteration: 1778000, loss: 0.10984943485170189, gradient norm: 0.16257530832587677\n",
      "Iteration: 1779000, loss: 0.10984812497169494, gradient norm: 0.01374335508737922\n",
      "Iteration: 1780000, loss: 0.1098466579101755, gradient norm: 0.02931518878644979\n",
      "Iteration: 1781000, loss: 0.10984528502867431, gradient norm: 0.00369641787510789\n",
      "Iteration: 1782000, loss: 0.10984404935355349, gradient norm: 0.0016991472253630826\n",
      "Iteration: 1783000, loss: 0.10984258091268784, gradient norm: 0.06823416014383982\n",
      "Iteration: 1784000, loss: 0.10984129489255408, gradient norm: 0.004469787013884876\n",
      "Iteration: 1785000, loss: 0.10984005803285841, gradient norm: 0.004588815835065196\n",
      "Iteration: 1786000, loss: 0.10983864175316915, gradient norm: 0.009075829137135678\n",
      "Iteration: 1787000, loss: 0.10983711211867524, gradient norm: 0.002145230568388815\n",
      "Iteration: 1788000, loss: 0.10983590925444647, gradient norm: 0.009648194822934403\n",
      "Iteration: 1789000, loss: 0.10983468371835448, gradient norm: 0.20601512053585766\n",
      "Iteration: 1790000, loss: 0.10983306802445972, gradient norm: 0.014629583130554036\n",
      "Iteration: 1791000, loss: 0.10983183702156106, gradient norm: 0.030869948168674978\n",
      "Iteration: 1792000, loss: 0.10983058386826369, gradient norm: 0.1380886532283836\n",
      "Iteration: 1793000, loss: 0.10982912994497443, gradient norm: 0.06935967408190304\n",
      "Iteration: 1794000, loss: 0.10982776845237771, gradient norm: 0.07424623448006448\n",
      "Iteration: 1795000, loss: 0.10982644030755594, gradient norm: 0.012521217761470521\n",
      "Iteration: 1796000, loss: 0.10982501195018739, gradient norm: 0.06900098840460303\n",
      "Iteration: 1797000, loss: 0.10982376490947024, gradient norm: 0.04175562779580727\n",
      "Iteration: 1798000, loss: 0.10982240506087333, gradient norm: 0.2627519626800914\n",
      "Iteration: 1799000, loss: 0.10982125612678652, gradient norm: 0.02550358069262324\n",
      "Iteration: 1800000, loss: 0.1098196192618468, gradient norm: 0.06817522540726713\n",
      "Iteration: 1801000, loss: 0.10981849931906819, gradient norm: 0.005858268951246451\n",
      "Iteration: 1802000, loss: 0.10981712794436174, gradient norm: 0.030217828795475767\n",
      "Iteration: 1803000, loss: 0.10981581407373256, gradient norm: 0.35672103318664067\n",
      "Iteration: 1804000, loss: 0.10981434557517118, gradient norm: 0.007304594895059778\n",
      "Iteration: 1805000, loss: 0.10981319432814578, gradient norm: 0.010723421746416793\n",
      "Iteration: 1806000, loss: 0.10981164439037817, gradient norm: 0.015738766733496633\n",
      "Iteration: 1807000, loss: 0.10981043919688278, gradient norm: 0.04616860605183957\n",
      "Iteration: 1808000, loss: 0.1098091185042343, gradient norm: 0.004210330349248797\n",
      "Iteration: 1809000, loss: 0.10980777300275713, gradient norm: 0.020733593665213938\n",
      "Iteration: 1810000, loss: 0.10980645534696856, gradient norm: 0.005559603338801348\n",
      "Iteration: 1811000, loss: 0.10980516331975372, gradient norm: 0.46829101851755534\n",
      "Iteration: 1812000, loss: 0.10980387465608535, gradient norm: 0.0022006022225513596\n",
      "Iteration: 1813000, loss: 0.10980249307998341, gradient norm: 0.010857401505154643\n",
      "Iteration: 1814000, loss: 0.10980136760843329, gradient norm: 0.2862973901719654\n",
      "Iteration: 1815000, loss: 0.10979975720083075, gradient norm: 0.018397433328032883\n",
      "Iteration: 1816000, loss: 0.10979865920218501, gradient norm: 0.1397590387265108\n",
      "Iteration: 1817000, loss: 0.1097972754599673, gradient norm: 0.11737011753987843\n",
      "Iteration: 1818000, loss: 0.10979594015717013, gradient norm: 0.10531934339333497\n",
      "Iteration: 1819000, loss: 0.10979442536477915, gradient norm: 0.020971852957334614\n",
      "Iteration: 1820000, loss: 0.10979348431764338, gradient norm: 0.027624015930326942\n",
      "Iteration: 1821000, loss: 0.10979196379172544, gradient norm: 0.20400654223478296\n",
      "Iteration: 1822000, loss: 0.10979069163682122, gradient norm: 0.09607224711918987\n",
      "Iteration: 1823000, loss: 0.10978936169048964, gradient norm: 0.015095632197192278\n",
      "Iteration: 1824000, loss: 0.10978798861958255, gradient norm: 0.028581152807172654\n",
      "Iteration: 1825000, loss: 0.10978678953522508, gradient norm: 0.07611434178011882\n",
      "Iteration: 1826000, loss: 0.10978539873458039, gradient norm: 0.08442824965362415\n",
      "Iteration: 1827000, loss: 0.10978414005345985, gradient norm: 0.015259470892437345\n",
      "Iteration: 1828000, loss: 0.10978290770069615, gradient norm: 0.4203313904756023\n",
      "Iteration: 1829000, loss: 0.10978143445577224, gradient norm: 0.0007508701822029704\n",
      "Iteration: 1830000, loss: 0.10978024998003415, gradient norm: 0.0033020638142390907\n",
      "Iteration: 1831000, loss: 0.10977917329391279, gradient norm: 0.0700606809422524\n",
      "Iteration: 1832000, loss: 0.10977748574184525, gradient norm: 0.0023415724835191606\n",
      "Iteration: 1833000, loss: 0.10977641588901636, gradient norm: 0.056743579930942284\n",
      "Iteration: 1834000, loss: 0.10977499035538968, gradient norm: 0.005756985826432461\n",
      "Iteration: 1835000, loss: 0.10977382343776348, gradient norm: 0.008108828167849955\n",
      "Iteration: 1836000, loss: 0.10977239445936289, gradient norm: 0.018001628642079076\n",
      "Iteration: 1837000, loss: 0.10977111849808566, gradient norm: 0.07414858245743769\n",
      "Iteration: 1838000, loss: 0.10976987171349493, gradient norm: 0.01426965557394705\n",
      "Iteration: 1839000, loss: 0.10976854671308318, gradient norm: 0.010572319412870157\n",
      "Iteration: 1840000, loss: 0.10976738453819469, gradient norm: 0.12033898975941056\n",
      "Iteration: 1841000, loss: 0.10976599996407473, gradient norm: 0.0711556292612558\n",
      "Iteration: 1842000, loss: 0.10976476597374099, gradient norm: 0.049587069766081585\n",
      "Iteration: 1843000, loss: 0.10976339364655019, gradient norm: 0.009822845393165691\n",
      "Iteration: 1844000, loss: 0.10976222271070818, gradient norm: 0.10983629306372134\n",
      "Iteration: 1845000, loss: 0.10976073385903015, gradient norm: 0.19784704597944527\n",
      "Iteration: 1846000, loss: 0.10975972013611823, gradient norm: 0.30825476052025574\n",
      "Iteration: 1847000, loss: 0.10975817172414586, gradient norm: 0.05848138484144902\n",
      "Iteration: 1848000, loss: 0.10975720080162778, gradient norm: 0.010261035484095522\n",
      "Iteration: 1849000, loss: 0.10975580802495181, gradient norm: 0.01998696615232244\n",
      "Iteration: 1850000, loss: 0.1097544561346482, gradient norm: 0.4032924208348277\n",
      "Iteration: 1851000, loss: 0.10975323928006034, gradient norm: 0.010530356147854067\n",
      "Iteration: 1852000, loss: 0.10975193084484108, gradient norm: 0.0006909531909147407\n",
      "Iteration: 1853000, loss: 0.10975069861992243, gradient norm: 0.015389758317797696\n",
      "Iteration: 1854000, loss: 0.10974944043075271, gradient norm: 0.2809900157646963\n",
      "Iteration: 1855000, loss: 0.1097482738585591, gradient norm: 0.013526372319006937\n",
      "Iteration: 1856000, loss: 0.10974689795868905, gradient norm: 0.0010859279591320977\n",
      "Iteration: 1857000, loss: 0.10974564318880976, gradient norm: 0.006345726258847616\n",
      "Iteration: 1858000, loss: 0.10974440371921719, gradient norm: 0.025958082964396185\n",
      "Iteration: 1859000, loss: 0.10974310393747536, gradient norm: 0.3512009237177404\n",
      "Iteration: 1860000, loss: 0.10974191228206004, gradient norm: 0.03389002366030938\n",
      "Iteration: 1861000, loss: 0.10974061272950245, gradient norm: 0.29526647562705105\n",
      "Iteration: 1862000, loss: 0.10973933420563133, gradient norm: 0.007059073030390301\n",
      "Iteration: 1863000, loss: 0.10973802013839207, gradient norm: 0.0031953397591325252\n",
      "Iteration: 1864000, loss: 0.10973686947338716, gradient norm: 0.0021503210454125953\n",
      "Iteration: 1865000, loss: 0.10973553176781053, gradient norm: 0.007846554322833283\n",
      "Iteration: 1866000, loss: 0.10973435309736203, gradient norm: 0.23103759018966294\n",
      "Iteration: 1867000, loss: 0.1097330907974023, gradient norm: 0.010304574100314118\n",
      "Iteration: 1868000, loss: 0.10973183559476256, gradient norm: 0.06609392378748115\n",
      "Iteration: 1869000, loss: 0.1097305216898069, gradient norm: 0.004713498879341262\n",
      "Iteration: 1870000, loss: 0.10972939631209554, gradient norm: 0.046635454056446875\n",
      "Iteration: 1871000, loss: 0.1097280251390408, gradient norm: 0.19463814999517118\n",
      "Iteration: 1872000, loss: 0.10972679402221791, gradient norm: 0.010521438180800853\n",
      "Iteration: 1873000, loss: 0.10972556387904697, gradient norm: 0.0009213854711301152\n",
      "Iteration: 1874000, loss: 0.10972434081290697, gradient norm: 0.010827108221263246\n",
      "Iteration: 1875000, loss: 0.10972311693865754, gradient norm: 0.13974214899373338\n",
      "Iteration: 1876000, loss: 0.10972182143947788, gradient norm: 0.1509377490925065\n",
      "Iteration: 1877000, loss: 0.10972058422260945, gradient norm: 0.15320380617549023\n",
      "Iteration: 1878000, loss: 0.10971931820797433, gradient norm: 0.021679635798885974\n",
      "Iteration: 1879000, loss: 0.1097180177052214, gradient norm: 0.018257226157557725\n",
      "Iteration: 1880000, loss: 0.10971695036231123, gradient norm: 0.024194644839029427\n",
      "Iteration: 1881000, loss: 0.10971554034746026, gradient norm: 0.03824328138501655\n",
      "Iteration: 1882000, loss: 0.1097146698857149, gradient norm: 0.09867486089556186\n",
      "Iteration: 1883000, loss: 0.10971318099831395, gradient norm: 0.002298860121365403\n",
      "Iteration: 1884000, loss: 0.10971188412503954, gradient norm: 0.04891689257016079\n",
      "Iteration: 1885000, loss: 0.10971074859079706, gradient norm: 0.14156614883363905\n",
      "Iteration: 1886000, loss: 0.10970947238964626, gradient norm: 0.0020703632380560154\n",
      "Iteration: 1887000, loss: 0.10970820831665719, gradient norm: 0.28818225463800995\n",
      "Iteration: 1888000, loss: 0.10970702307649341, gradient norm: 0.0004354114575160537\n",
      "Iteration: 1889000, loss: 0.10970584387761383, gradient norm: 0.020707997732786212\n",
      "Iteration: 1890000, loss: 0.10970453163191951, gradient norm: 0.018109125235599613\n",
      "Iteration: 1891000, loss: 0.10970332352757634, gradient norm: 0.0039272634909774465\n",
      "Iteration: 1892000, loss: 0.10970207855673374, gradient norm: 0.006410678690029235\n",
      "Iteration: 1893000, loss: 0.10970099731922037, gradient norm: 0.02900831926676225\n",
      "Iteration: 1894000, loss: 0.10969970661862696, gradient norm: 0.0030051571653580762\n",
      "Iteration: 1895000, loss: 0.10969852621548554, gradient norm: 0.05856697383856541\n",
      "Iteration: 1896000, loss: 0.10969735670968067, gradient norm: 0.24175588993771308\n",
      "Iteration: 1897000, loss: 0.10969584925347503, gradient norm: 0.0010062779156977217\n",
      "Iteration: 1898000, loss: 0.1096948157895162, gradient norm: 0.08181904209652474\n",
      "Iteration: 1899000, loss: 0.10969379539820943, gradient norm: 0.06636917693111845\n",
      "Iteration: 1900000, loss: 0.10969235074226745, gradient norm: 0.027490348468707192\n",
      "Iteration: 1901000, loss: 0.10969109363752243, gradient norm: 0.02303490997126051\n",
      "Iteration: 1902000, loss: 0.10969003429156035, gradient norm: 0.008050944272787016\n",
      "Iteration: 1903000, loss: 0.10968881564754585, gradient norm: 0.019341215963072495\n",
      "Iteration: 1904000, loss: 0.10968751949676613, gradient norm: 0.2878999990687201\n",
      "Iteration: 1905000, loss: 0.10968639416804107, gradient norm: 0.05483669637046461\n",
      "Iteration: 1906000, loss: 0.10968498635716785, gradient norm: 0.1274133759023691\n",
      "Iteration: 1907000, loss: 0.10968401169941937, gradient norm: 0.33271530185778264\n",
      "Iteration: 1908000, loss: 0.10968260028103215, gradient norm: 0.006266795578831521\n",
      "Iteration: 1909000, loss: 0.10968162634802868, gradient norm: 0.059478110720838154\n",
      "Iteration: 1910000, loss: 0.10968018651746408, gradient norm: 7.469802224586488e-05\n",
      "Iteration: 1911000, loss: 0.10967926014106218, gradient norm: 0.3531457032361433\n",
      "Iteration: 1912000, loss: 0.1096777987447971, gradient norm: 0.021176092227928246\n",
      "Iteration: 1913000, loss: 0.1096766831637679, gradient norm: 0.004679190656052729\n",
      "Iteration: 1914000, loss: 0.10967555359051631, gradient norm: 0.2038703047407336\n",
      "Iteration: 1915000, loss: 0.10967433018380787, gradient norm: 0.02850321426369368\n",
      "Iteration: 1916000, loss: 0.1096730748509755, gradient norm: 0.019071590751341922\n",
      "Iteration: 1917000, loss: 0.10967189032016594, gradient norm: 0.003131095143072319\n",
      "Iteration: 1918000, loss: 0.1096706700011103, gradient norm: 0.003972730346334408\n",
      "Iteration: 1919000, loss: 0.10966948736904814, gradient norm: 0.0017880495591189544\n",
      "Iteration: 1920000, loss: 0.10966841403814294, gradient norm: 0.01112158961427435\n",
      "Iteration: 1921000, loss: 0.10966698555925804, gradient norm: 0.2994525490154444\n",
      "Iteration: 1922000, loss: 0.10966603732993464, gradient norm: 0.002083745901243098\n",
      "Iteration: 1923000, loss: 0.10966482481036453, gradient norm: 0.03810936549344522\n",
      "Iteration: 1924000, loss: 0.10966373192907466, gradient norm: 0.06898516377860096\n",
      "Iteration: 1925000, loss: 0.10966217407786741, gradient norm: 0.0007843267031252389\n",
      "Iteration: 1926000, loss: 0.10966130428549968, gradient norm: 0.014963626537298456\n",
      "Iteration: 1927000, loss: 0.10966004835169516, gradient norm: 0.014288232303564727\n",
      "Iteration: 1928000, loss: 0.10965882291538674, gradient norm: 0.19287697363062298\n",
      "Iteration: 1929000, loss: 0.10965767858539366, gradient norm: 0.00659919564619653\n",
      "Iteration: 1930000, loss: 0.10965651249849255, gradient norm: 0.002492836932030753\n",
      "Iteration: 1931000, loss: 0.10965542523356599, gradient norm: 0.0213696111052676\n",
      "Iteration: 1932000, loss: 0.10965405108641195, gradient norm: 0.07217561486069415\n",
      "Iteration: 1933000, loss: 0.1096530427517389, gradient norm: 0.031937913216170226\n",
      "Iteration: 1934000, loss: 0.10965173126308571, gradient norm: 0.12312493099301773\n",
      "Iteration: 1935000, loss: 0.10965059408766975, gradient norm: 0.0026565537695189736\n",
      "Iteration: 1936000, loss: 0.10964942919522187, gradient norm: 0.05907735671079929\n",
      "Iteration: 1937000, loss: 0.10964822305946856, gradient norm: 0.11196823533139684\n",
      "Iteration: 1938000, loss: 0.10964709487097729, gradient norm: 0.0017629456608208639\n",
      "Iteration: 1939000, loss: 0.10964589911927668, gradient norm: 0.011523119417625982\n",
      "Iteration: 1940000, loss: 0.10964468278426187, gradient norm: 0.035132919992877994\n",
      "Iteration: 1941000, loss: 0.10964347815473434, gradient norm: 0.4286694997322307\n",
      "Iteration: 1942000, loss: 0.10964250410137856, gradient norm: 0.37942126050600766\n",
      "Iteration: 1943000, loss: 0.10964136156779827, gradient norm: 0.36647318464191586\n",
      "Iteration: 1944000, loss: 0.10964002982424166, gradient norm: 0.030033440188586707\n",
      "Iteration: 1945000, loss: 0.10963907113756412, gradient norm: 0.01139494479064981\n",
      "Iteration: 1946000, loss: 0.10963755828728582, gradient norm: 0.056731491735511314\n",
      "Iteration: 1947000, loss: 0.10963682968821352, gradient norm: 0.13582733632513272\n",
      "Iteration: 1948000, loss: 0.10963526698477273, gradient norm: 0.08367466226658801\n",
      "Iteration: 1949000, loss: 0.10963442537929612, gradient norm: 0.022590836972689814\n",
      "Iteration: 1950000, loss: 0.10963302376484457, gradient norm: 0.012321437196866984\n",
      "Iteration: 1951000, loss: 0.10963202496528059, gradient norm: 0.5638011297390974\n",
      "Iteration: 1952000, loss: 0.10963073564217474, gradient norm: 0.0042937231272038535\n",
      "Iteration: 1953000, loss: 0.10962982128249607, gradient norm: 0.04124789378836979\n",
      "Iteration: 1954000, loss: 0.10962844433698714, gradient norm: 0.14654769641483964\n",
      "Iteration: 1955000, loss: 0.10962718423469045, gradient norm: 0.0506028524080508\n",
      "Iteration: 1956000, loss: 0.10962625878761148, gradient norm: 0.03541015957265952\n",
      "Iteration: 1957000, loss: 0.109625056828282, gradient norm: 0.17892649053925494\n",
      "Iteration: 1958000, loss: 0.10962379599877096, gradient norm: 0.0007925627108315914\n",
      "Iteration: 1959000, loss: 0.10962268579493997, gradient norm: 0.030025132912853077\n",
      "Iteration: 1960000, loss: 0.10962158378989595, gradient norm: 0.0016964471155852235\n",
      "Iteration: 1961000, loss: 0.10962069060121735, gradient norm: 0.10397636545986871\n",
      "Iteration: 1962000, loss: 0.1096190934365571, gradient norm: 0.008197202499312708\n",
      "Iteration: 1963000, loss: 0.10961832241758611, gradient norm: 0.10864150204311034\n",
      "Iteration: 1964000, loss: 0.10961682666581735, gradient norm: 0.017823370558040456\n",
      "Iteration: 1965000, loss: 0.10961599089786912, gradient norm: 0.1451601144189324\n",
      "Iteration: 1966000, loss: 0.10961461656999769, gradient norm: 0.005079960497771839\n",
      "Iteration: 1967000, loss: 0.10961351349037438, gradient norm: 0.046045788979191205\n",
      "Iteration: 1968000, loss: 0.10961249460419037, gradient norm: 0.003175968183192901\n",
      "Iteration: 1969000, loss: 0.10961126516778989, gradient norm: 0.004563058419421691\n",
      "Iteration: 1970000, loss: 0.10961008940764994, gradient norm: 0.0024866005198001554\n",
      "Iteration: 1971000, loss: 0.10960900967927287, gradient norm: 0.08436756457670878\n",
      "Iteration: 1972000, loss: 0.10960796423780275, gradient norm: 0.36750043012968053\n",
      "Iteration: 1973000, loss: 0.10960663775980241, gradient norm: 0.0021551171596676834\n",
      "Iteration: 1974000, loss: 0.10960559425780243, gradient norm: 0.48891324617199156\n",
      "Iteration: 1975000, loss: 0.10960438229531724, gradient norm: 0.0952681097101848\n",
      "Iteration: 1976000, loss: 0.10960352562258471, gradient norm: 0.06872160817613562\n",
      "Iteration: 1977000, loss: 0.10960209086246227, gradient norm: 0.043873426061594605\n",
      "Iteration: 1978000, loss: 0.10960108887677354, gradient norm: 0.11059822365595477\n",
      "Iteration: 1979000, loss: 0.1095999050635469, gradient norm: 0.15639135720308842\n",
      "Iteration: 1980000, loss: 0.10959882604464065, gradient norm: 0.0034235785158975565\n",
      "Iteration: 1981000, loss: 0.10959758251710851, gradient norm: 0.014930662433776908\n",
      "Iteration: 1982000, loss: 0.10959649344244603, gradient norm: 0.013915612381120136\n",
      "Iteration: 1983000, loss: 0.10959538136592153, gradient norm: 0.014226373943348828\n",
      "Iteration: 1984000, loss: 0.1095942564343767, gradient norm: 0.05080359251609981\n",
      "Iteration: 1985000, loss: 0.10959318384906382, gradient norm: 0.0946588749049107\n",
      "Iteration: 1986000, loss: 0.10959203848777194, gradient norm: 0.10381122334398417\n",
      "Iteration: 1987000, loss: 0.1095908951655764, gradient norm: 0.021660892987129476\n",
      "Iteration: 1988000, loss: 0.10958961323501268, gradient norm: 0.0387110295686396\n",
      "Iteration: 1989000, loss: 0.10958876305921592, gradient norm: 0.032250643365542335\n",
      "Iteration: 1990000, loss: 0.1095873538563802, gradient norm: 0.01886788806869632\n",
      "Iteration: 1991000, loss: 0.10958642323914512, gradient norm: 0.0685783356752046\n",
      "Iteration: 1992000, loss: 0.1095853701729033, gradient norm: 0.38434011348404484\n",
      "Iteration: 1993000, loss: 0.10958415061389479, gradient norm: 0.005974906828807037\n",
      "Iteration: 1994000, loss: 0.10958292189909224, gradient norm: 0.012653265347651715\n",
      "Iteration: 1995000, loss: 0.10958203630000593, gradient norm: 0.16136525707932117\n",
      "Iteration: 1996000, loss: 0.10958066158241833, gradient norm: 0.0023840372920646857\n",
      "Iteration: 1997000, loss: 0.10957978033642489, gradient norm: 0.02594986390704673\n",
      "Iteration: 1998000, loss: 0.10957845463330505, gradient norm: 0.027605816983127206\n",
      "Iteration: 1999000, loss: 0.10957767928777838, gradient norm: 0.16250139669329552\n",
      "Iteration: 2000000, loss: 0.10957617329160736, gradient norm: 0.023689560456412673\n",
      "Iteration: 2001000, loss: 0.10957525772431953, gradient norm: 0.009153915015424752\n",
      "Iteration: 2002000, loss: 0.10957416205772588, gradient norm: 0.019455987090712574\n",
      "Iteration: 2003000, loss: 0.10957291967666798, gradient norm: 0.23128508954591498\n",
      "Iteration: 2004000, loss: 0.1095719029260321, gradient norm: 0.005296039313072781\n",
      "Iteration: 2005000, loss: 0.10957101178117903, gradient norm: 0.11572358729616301\n",
      "Iteration: 2006000, loss: 0.10956962326699919, gradient norm: 0.03929160064500242\n",
      "Iteration: 2007000, loss: 0.10956854186766804, gradient norm: 0.025066467945762037\n",
      "Iteration: 2008000, loss: 0.10956747792542759, gradient norm: 0.007331883740431539\n",
      "Iteration: 2009000, loss: 0.10956631979032022, gradient norm: 0.0762282904709396\n",
      "Iteration: 2010000, loss: 0.1095651765399795, gradient norm: 0.007207442507392994\n",
      "Iteration: 2011000, loss: 0.10956421159400731, gradient norm: 0.11774678674409571\n",
      "Iteration: 2012000, loss: 0.10956300499439134, gradient norm: 0.28032251915464285\n",
      "Iteration: 2013000, loss: 0.10956193244218135, gradient norm: 0.2434665054460514\n",
      "Iteration: 2014000, loss: 0.10956075712949771, gradient norm: 0.027639469354435044\n",
      "Iteration: 2015000, loss: 0.10955986818047037, gradient norm: 0.21764311964436464\n",
      "Iteration: 2016000, loss: 0.10955849959813065, gradient norm: 0.02360009770205012\n",
      "Iteration: 2017000, loss: 0.10955754911139605, gradient norm: 0.15947913262800759\n",
      "Iteration: 2018000, loss: 0.10955638689870514, gradient norm: 0.04334749163856065\n",
      "Iteration: 2019000, loss: 0.10955536440183436, gradient norm: 0.010052890949003277\n",
      "Iteration: 2020000, loss: 0.10955420591212732, gradient norm: 0.20899452112809075\n",
      "Iteration: 2021000, loss: 0.1095531682386015, gradient norm: 0.10109977791095394\n",
      "Iteration: 2022000, loss: 0.10955193811465327, gradient norm: 0.017064445758858602\n",
      "Iteration: 2023000, loss: 0.10955103079726213, gradient norm: 0.17968525308135763\n",
      "Iteration: 2024000, loss: 0.10954981140276733, gradient norm: 0.0006344011173374148\n",
      "Iteration: 2025000, loss: 0.10954874136827727, gradient norm: 0.004672048743075437\n",
      "Iteration: 2026000, loss: 0.10954771183807771, gradient norm: 0.01687401492428963\n",
      "Iteration: 2027000, loss: 0.10954650277895508, gradient norm: 0.4846459504872487\n",
      "Iteration: 2028000, loss: 0.10954556628086035, gradient norm: 0.3055893762817749\n",
      "Iteration: 2029000, loss: 0.10954434186075537, gradient norm: 0.0041959491154736185\n",
      "Iteration: 2030000, loss: 0.10954331772287795, gradient norm: 0.20864637984171872\n",
      "Iteration: 2031000, loss: 0.10954213777033528, gradient norm: 0.026243523696763964\n",
      "Iteration: 2032000, loss: 0.10954119056497891, gradient norm: 0.019978498525474307\n",
      "Iteration: 2033000, loss: 0.10954003840069125, gradient norm: 0.0021475883081068857\n",
      "Iteration: 2034000, loss: 0.10953901242834797, gradient norm: 0.14324316177898708\n",
      "Iteration: 2035000, loss: 0.10953762752734164, gradient norm: 0.18887243333321138\n",
      "Iteration: 2036000, loss: 0.10953693947169846, gradient norm: 0.14109808086905262\n",
      "Iteration: 2037000, loss: 0.10953561907479144, gradient norm: 0.23543491714322468\n",
      "Iteration: 2038000, loss: 0.10953452021800035, gradient norm: 0.008142329759824736\n",
      "Iteration: 2039000, loss: 0.1095336029875875, gradient norm: 0.013430232887737684\n",
      "Iteration: 2040000, loss: 0.10953231176424051, gradient norm: 0.009471036122102497\n",
      "Iteration: 2041000, loss: 0.10953144471237294, gradient norm: 0.018857978887383393\n",
      "Iteration: 2042000, loss: 0.10953021109006744, gradient norm: 0.15804224028599778\n",
      "Iteration: 2043000, loss: 0.10952909092043686, gradient norm: 0.015347058066298797\n",
      "Iteration: 2044000, loss: 0.10952816544214367, gradient norm: 0.02351493626407318\n",
      "Iteration: 2045000, loss: 0.10952693301355348, gradient norm: 0.0029025902089878314\n",
      "Iteration: 2046000, loss: 0.10952587027749221, gradient norm: 0.23596701985672947\n",
      "Iteration: 2047000, loss: 0.10952492217492826, gradient norm: 0.006436493683009088\n",
      "Iteration: 2048000, loss: 0.10952370345479634, gradient norm: 0.027570665622463374\n",
      "Iteration: 2049000, loss: 0.109522772392329, gradient norm: 0.20299337684583876\n",
      "Iteration: 2050000, loss: 0.10952146406870765, gradient norm: 0.16127885029720737\n",
      "Iteration: 2051000, loss: 0.10952067155006734, gradient norm: 0.4634311105686735\n",
      "Iteration: 2052000, loss: 0.10951932277895475, gradient norm: 0.005620688499176097\n",
      "Iteration: 2053000, loss: 0.10951841658395275, gradient norm: 0.02698549884870269\n",
      "Iteration: 2054000, loss: 0.10951725948222069, gradient norm: 0.08399843519547791\n",
      "Iteration: 2055000, loss: 0.10951622610404117, gradient norm: 0.0343045863413202\n",
      "Iteration: 2056000, loss: 0.10951511670597831, gradient norm: 0.0918000861258364\n",
      "Iteration: 2057000, loss: 0.10951416330674602, gradient norm: 0.05869570652770643\n",
      "Iteration: 2058000, loss: 0.10951285806158129, gradient norm: 0.003710870461354392\n",
      "Iteration: 2059000, loss: 0.1095120225121025, gradient norm: 0.3276654067653212\n",
      "Iteration: 2060000, loss: 0.10951079098100663, gradient norm: 0.004581473927072762\n",
      "Iteration: 2061000, loss: 0.10950976697784975, gradient norm: 0.01709673522085393\n",
      "Iteration: 2062000, loss: 0.10950883359374768, gradient norm: 0.324427238195133\n",
      "Iteration: 2063000, loss: 0.10950760268647065, gradient norm: 0.035368434337132514\n",
      "Iteration: 2064000, loss: 0.10950650322008292, gradient norm: 0.003621567905574905\n",
      "Iteration: 2065000, loss: 0.10950557124757193, gradient norm: 0.09293682939989285\n",
      "Iteration: 2066000, loss: 0.10950433933938512, gradient norm: 0.007584299799927765\n",
      "Iteration: 2067000, loss: 0.10950347161488909, gradient norm: 0.39358944626893094\n",
      "Iteration: 2068000, loss: 0.10950228952240842, gradient norm: 0.004581114585555182\n",
      "Iteration: 2069000, loss: 0.10950130822368936, gradient norm: 0.002825854623421637\n",
      "Iteration: 2070000, loss: 0.10950012647204602, gradient norm: 0.009605237737255704\n",
      "Iteration: 2071000, loss: 0.10949920600040414, gradient norm: 0.033372382784928004\n",
      "Iteration: 2072000, loss: 0.10949815280432754, gradient norm: 0.043650464984752446\n",
      "Iteration: 2073000, loss: 0.10949691909182604, gradient norm: 0.02135068400567178\n",
      "Iteration: 2074000, loss: 0.1094960328489519, gradient norm: 0.20966559858098796\n",
      "Iteration: 2075000, loss: 0.10949493597164184, gradient norm: 0.13468973419401029\n",
      "Iteration: 2076000, loss: 0.1094937699269981, gradient norm: 0.0005415189082892784\n",
      "Iteration: 2077000, loss: 0.10949281286909655, gradient norm: 0.0040594154780705695\n",
      "Iteration: 2078000, loss: 0.10949168507360733, gradient norm: 0.0386188775192158\n",
      "Iteration: 2079000, loss: 0.10949074212244636, gradient norm: 0.0498303079381674\n",
      "Iteration: 2080000, loss: 0.1094896458205351, gradient norm: 0.5377037817418698\n",
      "Iteration: 2081000, loss: 0.10948850625231199, gradient norm: 0.0021508637273393513\n",
      "Iteration: 2082000, loss: 0.10948768561801135, gradient norm: 0.05685576263094124\n",
      "Iteration: 2083000, loss: 0.10948633048799379, gradient norm: 0.002479188236552598\n",
      "Iteration: 2084000, loss: 0.10948542827818261, gradient norm: 0.017539492636928448\n",
      "Iteration: 2085000, loss: 0.1094844920801387, gradient norm: 0.08257761135582116\n",
      "Iteration: 2086000, loss: 0.10948320500943595, gradient norm: 0.08927817481816279\n",
      "Iteration: 2087000, loss: 0.10948241662003033, gradient norm: 0.6317511192508708\n",
      "Iteration: 2088000, loss: 0.1094812024659687, gradient norm: 0.0037019833846535367\n",
      "Iteration: 2089000, loss: 0.10948028458916884, gradient norm: 0.00716302509615207\n",
      "Iteration: 2090000, loss: 0.10947924372464965, gradient norm: 0.021416770284840774\n",
      "Iteration: 2091000, loss: 0.10947812860445984, gradient norm: 0.052656597887702994\n",
      "Iteration: 2092000, loss: 0.10947700639359174, gradient norm: 0.021688710728809672\n",
      "Iteration: 2093000, loss: 0.10947601781624303, gradient norm: 0.005386025916647522\n",
      "Iteration: 2094000, loss: 0.10947504754066265, gradient norm: 0.03927570715275731\n",
      "Iteration: 2095000, loss: 0.10947387535799663, gradient norm: 0.19229183278580264\n",
      "Iteration: 2096000, loss: 0.10947293196869293, gradient norm: 0.278077949860317\n",
      "Iteration: 2097000, loss: 0.10947186880542042, gradient norm: 0.027682309038195976\n",
      "Iteration: 2098000, loss: 0.10947083076880698, gradient norm: 0.07778585973687419\n",
      "Iteration: 2099000, loss: 0.10946979997109865, gradient norm: 0.041999460075253986\n",
      "Iteration: 2100000, loss: 0.10946871281631812, gradient norm: 0.002106540195789884\n",
      "Iteration: 2101000, loss: 0.10946770828090033, gradient norm: 0.005239772035873373\n",
      "Iteration: 2102000, loss: 0.10946674801898137, gradient norm: 0.0025139462324008078\n",
      "Iteration: 2103000, loss: 0.10946565205216925, gradient norm: 0.0011012406491336887\n",
      "Iteration: 2104000, loss: 0.10946467626365011, gradient norm: 0.018715640842844904\n",
      "Iteration: 2105000, loss: 0.10946357687745199, gradient norm: 0.0014261182876495638\n",
      "Iteration: 2106000, loss: 0.1094626633794473, gradient norm: 0.08744587244689686\n",
      "Iteration: 2107000, loss: 0.109461534732053, gradient norm: 0.1883048588915948\n",
      "Iteration: 2108000, loss: 0.10946046886973784, gradient norm: 0.18021813423761057\n",
      "Iteration: 2109000, loss: 0.10945951389564047, gradient norm: 0.009542737198093206\n",
      "Iteration: 2110000, loss: 0.10945836974090276, gradient norm: 0.004056915902817146\n",
      "Iteration: 2111000, loss: 0.10945748236519005, gradient norm: 0.049054707591449795\n",
      "Iteration: 2112000, loss: 0.1094565024359305, gradient norm: 0.059918379827689494\n",
      "Iteration: 2113000, loss: 0.10945539769439067, gradient norm: 0.14334108338250812\n",
      "Iteration: 2114000, loss: 0.10945422157505323, gradient norm: 0.0363789132087519\n",
      "Iteration: 2115000, loss: 0.10945337022242378, gradient norm: 0.07414449555855793\n",
      "Iteration: 2116000, loss: 0.10945229934533103, gradient norm: 0.05621310358452906\n",
      "Iteration: 2117000, loss: 0.10945123860627251, gradient norm: 0.24445471815182085\n",
      "Iteration: 2118000, loss: 0.10945038561898514, gradient norm: 0.11344106139925528\n",
      "Iteration: 2119000, loss: 0.10944908863227842, gradient norm: 0.020127484184421725\n",
      "Iteration: 2120000, loss: 0.1094481395044588, gradient norm: 0.05670760415336743\n",
      "Iteration: 2121000, loss: 0.10944733110379565, gradient norm: 0.056605343781881\n",
      "Iteration: 2122000, loss: 0.10944614626024941, gradient norm: 0.1028279455447231\n",
      "Iteration: 2123000, loss: 0.10944507922409807, gradient norm: 0.0038987271618685713\n",
      "Iteration: 2124000, loss: 0.10944415403677392, gradient norm: 0.006848136299111321\n",
      "Iteration: 2125000, loss: 0.10944317061388506, gradient norm: 0.06152963353402982\n",
      "Iteration: 2126000, loss: 0.10944202676951491, gradient norm: 0.003545829158391983\n",
      "Iteration: 2127000, loss: 0.10944103298114331, gradient norm: 0.0022477778999997916\n",
      "Iteration: 2128000, loss: 0.10944018885491424, gradient norm: 0.07574085420777606\n",
      "Iteration: 2129000, loss: 0.10943896834596879, gradient norm: 0.00530295305649192\n",
      "Iteration: 2130000, loss: 0.10943797158880823, gradient norm: 0.35678474307919317\n",
      "Iteration: 2131000, loss: 0.10943721852620827, gradient norm: 0.09230439827145367\n",
      "Iteration: 2132000, loss: 0.10943596480927742, gradient norm: 0.1508404294998832\n",
      "Iteration: 2133000, loss: 0.10943496643290411, gradient norm: 0.19261204796313625\n",
      "Iteration: 2134000, loss: 0.1094340059188045, gradient norm: 0.007531659535487732\n",
      "Iteration: 2135000, loss: 0.10943306903349259, gradient norm: 0.28630440358746606\n",
      "Iteration: 2136000, loss: 0.10943185071106802, gradient norm: 0.011219557425836128\n",
      "Iteration: 2137000, loss: 0.10943108221487904, gradient norm: 0.5820050690291251\n",
      "Iteration: 2138000, loss: 0.10943000534148595, gradient norm: 0.01259056599831159\n",
      "Iteration: 2139000, loss: 0.10942889003935108, gradient norm: 0.0011427730643555401\n",
      "Iteration: 2140000, loss: 0.10942792915895631, gradient norm: 0.008036207309811808\n",
      "Iteration: 2141000, loss: 0.10942711276707624, gradient norm: 0.004801029905049919\n",
      "Iteration: 2142000, loss: 0.1094261777394039, gradient norm: 0.07962983604991243\n",
      "Iteration: 2143000, loss: 0.10942499872270298, gradient norm: 0.07780180669369073\n",
      "Iteration: 2144000, loss: 0.10942396231828443, gradient norm: 0.07112461040545495\n",
      "Iteration: 2145000, loss: 0.10942284873800281, gradient norm: 0.37653256029712007\n",
      "Iteration: 2146000, loss: 0.10942196951452128, gradient norm: 0.004088163190468498\n",
      "Iteration: 2147000, loss: 0.10942107056207967, gradient norm: 0.044347772833997115\n",
      "Iteration: 2148000, loss: 0.109420159260306, gradient norm: 0.28875240029288146\n",
      "Iteration: 2149000, loss: 0.1094189481268864, gradient norm: 0.12612996146678812\n",
      "Iteration: 2150000, loss: 0.10941794698686005, gradient norm: 0.0024357854035109675\n",
      "Iteration: 2151000, loss: 0.10941692561343286, gradient norm: 0.1555085181329567\n",
      "Iteration: 2152000, loss: 0.10941601119340205, gradient norm: 0.004790516622041605\n",
      "Iteration: 2153000, loss: 0.10941509363092536, gradient norm: 0.04358017781433074\n",
      "Iteration: 2154000, loss: 0.10941397707697138, gradient norm: 0.4418498996825206\n",
      "Iteration: 2155000, loss: 0.10941313553988909, gradient norm: 0.1292232564515135\n",
      "Iteration: 2156000, loss: 0.10941205409035473, gradient norm: 0.005888664978653469\n",
      "Iteration: 2157000, loss: 0.10941096236940002, gradient norm: 0.031182015495562918\n",
      "Iteration: 2158000, loss: 0.10941005558026166, gradient norm: 0.022632479546787678\n",
      "Iteration: 2159000, loss: 0.10940901893429139, gradient norm: 0.0031608691733299925\n",
      "Iteration: 2160000, loss: 0.10940810463661046, gradient norm: 0.00777199851134625\n",
      "Iteration: 2161000, loss: 0.10940714197345451, gradient norm: 0.004222827658320462\n",
      "Iteration: 2162000, loss: 0.10940611340461666, gradient norm: 0.30747981291592535\n",
      "Iteration: 2163000, loss: 0.10940508086845994, gradient norm: 0.2559724605328623\n",
      "Iteration: 2164000, loss: 0.10940401027102525, gradient norm: 0.07265795289999898\n",
      "Iteration: 2165000, loss: 0.10940328578303553, gradient norm: 0.03358548816182987\n",
      "Iteration: 2166000, loss: 0.10940214465303706, gradient norm: 0.007932507950477693\n",
      "Iteration: 2167000, loss: 0.1094010901011649, gradient norm: 0.007090315536167892\n",
      "Iteration: 2168000, loss: 0.10940029980766187, gradient norm: 0.03355935444708318\n",
      "Iteration: 2169000, loss: 0.10939911852464644, gradient norm: 0.019765899209777837\n",
      "Iteration: 2170000, loss: 0.10939825934178207, gradient norm: 0.007038820650123496\n",
      "Iteration: 2171000, loss: 0.10939725971344726, gradient norm: 0.0008767667341914691\n",
      "Iteration: 2172000, loss: 0.10939626889842165, gradient norm: 0.02175444287906144\n",
      "Iteration: 2173000, loss: 0.10939517783443935, gradient norm: 0.1668554485582553\n",
      "Iteration: 2174000, loss: 0.10939440276943582, gradient norm: 0.030782501103171232\n",
      "Iteration: 2175000, loss: 0.10939335839598184, gradient norm: 0.09370057070039858\n",
      "Iteration: 2176000, loss: 0.10939231823962504, gradient norm: 0.041955192692064214\n",
      "Iteration: 2177000, loss: 0.10939117714016913, gradient norm: 0.26959991471420225\n",
      "Iteration: 2178000, loss: 0.10939051545806265, gradient norm: 0.002476824240283232\n",
      "Iteration: 2179000, loss: 0.10938936560543149, gradient norm: 0.3229110117997884\n",
      "Iteration: 2180000, loss: 0.10938840316417235, gradient norm: 0.024516445232632945\n",
      "Iteration: 2181000, loss: 0.10938751174940153, gradient norm: 0.2135208102959685\n",
      "Iteration: 2182000, loss: 0.10938639577624563, gradient norm: 0.005759700875214884\n",
      "Iteration: 2183000, loss: 0.10938570091485553, gradient norm: 0.18086325896415523\n",
      "Iteration: 2184000, loss: 0.10938443293827643, gradient norm: 0.0031896755511736257\n",
      "Iteration: 2185000, loss: 0.10938343375476083, gradient norm: 0.022951535706896595\n",
      "Iteration: 2186000, loss: 0.10938269073452163, gradient norm: 0.1413273548846108\n",
      "Iteration: 2187000, loss: 0.10938158827648799, gradient norm: 0.10889723313258907\n",
      "Iteration: 2188000, loss: 0.10938058730144844, gradient norm: 0.0009893998746846834\n",
      "Iteration: 2189000, loss: 0.10937966246525017, gradient norm: 0.038479793165479996\n",
      "Iteration: 2190000, loss: 0.10937860033463652, gradient norm: 0.2525752967743686\n",
      "Iteration: 2191000, loss: 0.10937785605100162, gradient norm: 0.10309858247860307\n",
      "Iteration: 2192000, loss: 0.10937670004363824, gradient norm: 0.026319745097176977\n",
      "Iteration: 2193000, loss: 0.10937563089562968, gradient norm: 0.26954202796771093\n",
      "Iteration: 2194000, loss: 0.10937490493127144, gradient norm: 0.010010932577143982\n",
      "Iteration: 2195000, loss: 0.10937377940719549, gradient norm: 0.003532660878049496\n",
      "Iteration: 2196000, loss: 0.10937293997970735, gradient norm: 0.0347627888062463\n",
      "Iteration: 2197000, loss: 0.10937175084044903, gradient norm: 0.10178834448254906\n",
      "Iteration: 2198000, loss: 0.10937102081380792, gradient norm: 0.08997299879841164\n",
      "Iteration: 2199000, loss: 0.10936984323274111, gradient norm: 0.03635492412760543\n",
      "Iteration: 2200000, loss: 0.10936902899592711, gradient norm: 0.054324332826825764\n",
      "Iteration: 2201000, loss: 0.10936807972306452, gradient norm: 0.1736158398696724\n",
      "Iteration: 2202000, loss: 0.10936689503738169, gradient norm: 0.05450992893863586\n",
      "Iteration: 2203000, loss: 0.10936613996835776, gradient norm: 0.2200179842285624\n",
      "Iteration: 2204000, loss: 0.10936505723849158, gradient norm: 0.11426967654511062\n",
      "Iteration: 2205000, loss: 0.10936407902411283, gradient norm: 0.05051274797652999\n",
      "Iteration: 2206000, loss: 0.1093630750231622, gradient norm: 0.021182268992789636\n",
      "Iteration: 2207000, loss: 0.10936227839299463, gradient norm: 0.1682746916710934\n",
      "Iteration: 2208000, loss: 0.10936112480330204, gradient norm: 0.11921070164819969\n",
      "Iteration: 2209000, loss: 0.10936029167352825, gradient norm: 0.35266138676587605\n",
      "Iteration: 2210000, loss: 0.10935927410683763, gradient norm: 0.03504035990334192\n",
      "Iteration: 2211000, loss: 0.1093583463905841, gradient norm: 0.35556978766382175\n",
      "Iteration: 2212000, loss: 0.10935740157734827, gradient norm: 0.30062167102580095\n",
      "Iteration: 2213000, loss: 0.10935637120148174, gradient norm: 0.514436738818783\n",
      "Iteration: 2214000, loss: 0.10935543822779621, gradient norm: 0.10886186706701359\n",
      "Iteration: 2215000, loss: 0.10935438921392923, gradient norm: 0.04176222529523429\n",
      "Iteration: 2216000, loss: 0.10935351327044893, gradient norm: 0.06972519655233526\n",
      "Iteration: 2217000, loss: 0.10935249363816328, gradient norm: 0.003893551291880458\n",
      "Iteration: 2218000, loss: 0.1093517082192553, gradient norm: 0.05751193521271381\n",
      "Iteration: 2219000, loss: 0.10935044948201093, gradient norm: 0.008975425921511867\n",
      "Iteration: 2220000, loss: 0.10934962772674842, gradient norm: 0.2886078861057053\n",
      "Iteration: 2221000, loss: 0.10934872249076219, gradient norm: 0.3208933602878285\n",
      "Iteration: 2222000, loss: 0.10934778580737725, gradient norm: 0.017172246083849374\n",
      "Iteration: 2223000, loss: 0.10934673999241235, gradient norm: 0.0320125719834576\n",
      "Iteration: 2224000, loss: 0.10934579501789134, gradient norm: 0.1157473825165002\n",
      "Iteration: 2225000, loss: 0.10934494402534951, gradient norm: 0.20495705540805956\n",
      "Iteration: 2226000, loss: 0.10934388994128204, gradient norm: 0.004202317256990199\n",
      "Iteration: 2227000, loss: 0.10934280771579609, gradient norm: 0.05445843468409897\n",
      "Iteration: 2228000, loss: 0.10934196331851229, gradient norm: 0.09747240714433536\n",
      "Iteration: 2229000, loss: 0.1093410936040056, gradient norm: 0.020744200144677722\n",
      "Iteration: 2230000, loss: 0.10934008574726435, gradient norm: 0.15155866147855704\n",
      "Iteration: 2231000, loss: 0.10933915330829908, gradient norm: 0.059593824893890174\n",
      "Iteration: 2232000, loss: 0.10933826356956351, gradient norm: 0.15820909394807278\n",
      "Iteration: 2233000, loss: 0.1093370653917599, gradient norm: 0.3087117948089982\n",
      "Iteration: 2234000, loss: 0.10933630409140159, gradient norm: 0.009573821131648888\n",
      "Iteration: 2235000, loss: 0.109335329848885, gradient norm: 0.0970908952628491\n",
      "Iteration: 2236000, loss: 0.10933437383201962, gradient norm: 0.0853522275380457\n",
      "Iteration: 2237000, loss: 0.10933351086380287, gradient norm: 0.17128241431565516\n",
      "Iteration: 2238000, loss: 0.10933237069119625, gradient norm: 0.010386179661190542\n",
      "Iteration: 2239000, loss: 0.10933157932873945, gradient norm: 0.06883097469736804\n",
      "Iteration: 2240000, loss: 0.10933062549040819, gradient norm: 0.016312262543483697\n",
      "Iteration: 2241000, loss: 0.10932960800050275, gradient norm: 0.1400911358997422\n",
      "Iteration: 2242000, loss: 0.10932865687445445, gradient norm: 0.007134768066473427\n",
      "Iteration: 2243000, loss: 0.10932774466679536, gradient norm: 0.059519487870979444\n",
      "Iteration: 2244000, loss: 0.1093268358699918, gradient norm: 0.03374119222140363\n",
      "Iteration: 2245000, loss: 0.10932590667515461, gradient norm: 0.15901989658595103\n",
      "Iteration: 2246000, loss: 0.10932490512564273, gradient norm: 0.6470037290343006\n",
      "Iteration: 2247000, loss: 0.10932398386467239, gradient norm: 0.08804257804117728\n",
      "Iteration: 2248000, loss: 0.10932292299577295, gradient norm: 0.015133456617464016\n",
      "Iteration: 2249000, loss: 0.10932215461989743, gradient norm: 0.18391392109477764\n",
      "Iteration: 2250000, loss: 0.10932103108608615, gradient norm: 0.20579371615165173\n",
      "Iteration: 2251000, loss: 0.10932010987411872, gradient norm: 0.11902077204177833\n",
      "Iteration: 2252000, loss: 0.10931923925994372, gradient norm: 0.03481939353115171\n",
      "Iteration: 2253000, loss: 0.1093183681728768, gradient norm: 0.16279223780328583\n",
      "Iteration: 2254000, loss: 0.10931734847020838, gradient norm: 0.038462292986605424\n",
      "Iteration: 2255000, loss: 0.10931620358345276, gradient norm: 0.04692103216877073\n",
      "Iteration: 2256000, loss: 0.1093155305601762, gradient norm: 0.37145271942195984\n",
      "Iteration: 2257000, loss: 0.10931452843213414, gradient norm: 0.10557603130697081\n",
      "Iteration: 2258000, loss: 0.10931354512942983, gradient norm: 0.015755875442257542\n",
      "Iteration: 2259000, loss: 0.10931266156948825, gradient norm: 0.06376917770220213\n",
      "Iteration: 2260000, loss: 0.10931166500527655, gradient norm: 0.0010088429458948115\n",
      "Iteration: 2261000, loss: 0.10931080520909746, gradient norm: 0.010631378424131786\n",
      "Iteration: 2262000, loss: 0.10930978546139695, gradient norm: 0.009925076347773336\n",
      "Iteration: 2263000, loss: 0.1093088758139155, gradient norm: 0.004875014668086114\n",
      "Iteration: 2264000, loss: 0.10930797373653717, gradient norm: 0.03332232091625318\n",
      "Iteration: 2265000, loss: 0.10930691759585, gradient norm: 0.2236189807870774\n",
      "Iteration: 2266000, loss: 0.10930614246308497, gradient norm: 0.006592398234895458\n",
      "Iteration: 2267000, loss: 0.10930510178235772, gradient norm: 0.029507436403495253\n",
      "Iteration: 2268000, loss: 0.10930439488069715, gradient norm: 0.10919596795299821\n",
      "Iteration: 2269000, loss: 0.10930320224871484, gradient norm: 0.39101409825240396\n",
      "Iteration: 2270000, loss: 0.10930234567571899, gradient norm: 0.009426595169670204\n",
      "Iteration: 2271000, loss: 0.10930149539216023, gradient norm: 0.09240512296404092\n",
      "Iteration: 2272000, loss: 0.10930062092200242, gradient norm: 0.05282210782901019\n",
      "Iteration: 2273000, loss: 0.10929947872352151, gradient norm: 0.44942845372260426\n",
      "Iteration: 2274000, loss: 0.10929872470281768, gradient norm: 0.018144393559734007\n",
      "Iteration: 2275000, loss: 0.10929762075896636, gradient norm: 0.019133173931739782\n",
      "Iteration: 2276000, loss: 0.10929679726711947, gradient norm: 0.14915442360242584\n",
      "Iteration: 2277000, loss: 0.10929591043293183, gradient norm: 0.1491161424790474\n",
      "Iteration: 2278000, loss: 0.10929481613409166, gradient norm: 0.004165064219259914\n",
      "Iteration: 2279000, loss: 0.10929416580330673, gradient norm: 0.3056366279006922\n",
      "Iteration: 2280000, loss: 0.10929295474637497, gradient norm: 0.06303904347207083\n",
      "Iteration: 2281000, loss: 0.10929231087984494, gradient norm: 0.10550952816044938\n",
      "Iteration: 2282000, loss: 0.10929117804791194, gradient norm: 0.11252125795745467\n",
      "Iteration: 2283000, loss: 0.10929027974451207, gradient norm: 0.21275569143603176\n",
      "Iteration: 2284000, loss: 0.10928948265339382, gradient norm: 0.026898455928683455\n",
      "Iteration: 2285000, loss: 0.10928827828716763, gradient norm: 0.08304076226741793\n",
      "Iteration: 2286000, loss: 0.10928759131930867, gradient norm: 0.16665190857571507\n",
      "Iteration: 2287000, loss: 0.10928654746597224, gradient norm: 0.07896198601863623\n",
      "Iteration: 2288000, loss: 0.10928573595612931, gradient norm: 0.00608391421205425\n",
      "Iteration: 2289000, loss: 0.10928483754122191, gradient norm: 0.13390658645950668\n",
      "Iteration: 2290000, loss: 0.10928382911685898, gradient norm: 0.10335556821658946\n",
      "Iteration: 2291000, loss: 0.10928289950826384, gradient norm: 0.053868497312555864\n",
      "Iteration: 2292000, loss: 0.10928199871638651, gradient norm: 0.0196478127429744\n",
      "Iteration: 2293000, loss: 0.10928116744193599, gradient norm: 0.010759415803330395\n",
      "Iteration: 2294000, loss: 0.10928018606681787, gradient norm: 0.0019135882436009258\n",
      "Iteration: 2295000, loss: 0.10927926534278458, gradient norm: 0.28348524234159217\n",
      "Iteration: 2296000, loss: 0.10927841732316489, gradient norm: 0.19744275842956605\n",
      "Iteration: 2297000, loss: 0.10927756983961563, gradient norm: 0.2304164069450603\n",
      "Iteration: 2298000, loss: 0.10927653591842378, gradient norm: 0.026459828899623002\n",
      "Iteration: 2299000, loss: 0.10927559229026958, gradient norm: 0.00762375293764667\n",
      "Iteration: 2300000, loss: 0.10927480210528476, gradient norm: 0.05116240269429725\n",
      "Iteration: 2301000, loss: 0.10927383584624756, gradient norm: 0.017737181314936765\n",
      "Iteration: 2302000, loss: 0.10927291291787418, gradient norm: 0.12331882044421727\n",
      "Iteration: 2303000, loss: 0.10927203069358822, gradient norm: 0.06552540875266635\n",
      "Iteration: 2304000, loss: 0.10927104531926758, gradient norm: 0.08724776322403764\n",
      "Iteration: 2305000, loss: 0.10927028086850864, gradient norm: 0.07537179774903924\n",
      "Iteration: 2306000, loss: 0.10926933548119891, gradient norm: 0.14883771106426064\n",
      "Iteration: 2307000, loss: 0.10926840130895528, gradient norm: 0.005683193447905911\n",
      "Iteration: 2308000, loss: 0.10926738380763427, gradient norm: 0.30838257155622767\n",
      "Iteration: 2309000, loss: 0.10926657548067864, gradient norm: 0.46926513145215376\n",
      "Iteration: 2310000, loss: 0.10926577063467727, gradient norm: 0.08069760213137646\n",
      "Iteration: 2311000, loss: 0.10926466496699376, gradient norm: 0.0548837467434091\n",
      "Iteration: 2312000, loss: 0.10926389955143026, gradient norm: 0.06050732600916857\n",
      "Iteration: 2313000, loss: 0.10926291497709101, gradient norm: 0.13777935672214472\n",
      "Iteration: 2314000, loss: 0.10926212957498287, gradient norm: 0.011219317562732879\n",
      "Iteration: 2315000, loss: 0.10926123156010674, gradient norm: 0.030102571441641617\n",
      "Iteration: 2316000, loss: 0.10926028359910595, gradient norm: 0.20604913024048258\n",
      "Iteration: 2317000, loss: 0.10925926042396228, gradient norm: 0.07317132495359618\n",
      "Iteration: 2318000, loss: 0.10925855748028801, gradient norm: 0.011388806249520055\n",
      "Iteration: 2319000, loss: 0.10925752030153663, gradient norm: 0.13360900343556792\n",
      "Iteration: 2320000, loss: 0.10925675728538721, gradient norm: 0.15354879734853832\n",
      "Iteration: 2321000, loss: 0.10925566921269085, gradient norm: 0.0506287456785785\n",
      "Iteration: 2322000, loss: 0.10925499549757617, gradient norm: 0.0395048681295039\n",
      "Iteration: 2323000, loss: 0.10925406043045727, gradient norm: 0.010573509933484125\n",
      "Iteration: 2324000, loss: 0.10925300642126223, gradient norm: 0.38982706119805466\n",
      "Iteration: 2325000, loss: 0.10925215936935875, gradient norm: 0.15534325197747062\n",
      "Iteration: 2326000, loss: 0.10925138191865517, gradient norm: 0.012349421765440403\n",
      "Iteration: 2327000, loss: 0.10925035615817781, gradient norm: 0.07857945425959349\n",
      "Iteration: 2328000, loss: 0.10924954710589958, gradient norm: 0.1011774627178927\n",
      "Iteration: 2329000, loss: 0.10924868823280451, gradient norm: 0.44984192222530445\n",
      "Iteration: 2330000, loss: 0.10924775127000834, gradient norm: 0.010328050799753564\n",
      "Iteration: 2331000, loss: 0.10924690299499784, gradient norm: 0.08058377340242516\n",
      "Iteration: 2332000, loss: 0.10924598979526402, gradient norm: 0.04971439872208137\n",
      "Iteration: 2333000, loss: 0.1092452161909136, gradient norm: 0.06731671307143135\n",
      "Iteration: 2334000, loss: 0.1092441280803917, gradient norm: 0.04218041977357276\n",
      "Iteration: 2335000, loss: 0.10924325561655773, gradient norm: 0.04213554212090388\n",
      "Iteration: 2336000, loss: 0.10924250475016752, gradient norm: 0.0160461440992833\n",
      "Iteration: 2337000, loss: 0.10924155290088057, gradient norm: 0.014255078427930537\n",
      "Iteration: 2338000, loss: 0.10924087566397606, gradient norm: 0.10048283132055576\n",
      "Iteration: 2339000, loss: 0.10923965042763051, gradient norm: 0.026482049335890424\n",
      "Iteration: 2340000, loss: 0.10923889358860266, gradient norm: 0.004608614683011581\n",
      "Iteration: 2341000, loss: 0.10923819682107674, gradient norm: 0.08088933154048503\n",
      "Iteration: 2342000, loss: 0.10923708346862301, gradient norm: 0.017483877905563775\n",
      "Iteration: 2343000, loss: 0.10923629654570056, gradient norm: 0.06400986522747232\n",
      "Iteration: 2344000, loss: 0.10923535232517602, gradient norm: 0.21595149618797072\n",
      "Iteration: 2345000, loss: 0.1092347105678984, gradient norm: 0.044831436224950665\n",
      "Iteration: 2346000, loss: 0.10923358995937117, gradient norm: 0.06858153218626013\n",
      "Iteration: 2347000, loss: 0.10923284234849734, gradient norm: 0.03101682989476976\n",
      "Iteration: 2348000, loss: 0.10923188172467792, gradient norm: 0.11028221841838179\n",
      "Iteration: 2349000, loss: 0.10923098882798492, gradient norm: 0.00919984575014291\n",
      "Iteration: 2350000, loss: 0.10923023803966862, gradient norm: 0.3191217498122468\n",
      "Iteration: 2351000, loss: 0.1092294633771969, gradient norm: 0.0803365764432008\n",
      "Iteration: 2352000, loss: 0.10922824591361323, gradient norm: 0.05779260797204173\n",
      "Iteration: 2353000, loss: 0.10922756826725853, gradient norm: 0.09290425609264365\n",
      "Iteration: 2354000, loss: 0.10922661913890883, gradient norm: 0.1062776818225062\n",
      "Iteration: 2355000, loss: 0.10922593926499116, gradient norm: 0.050189795005079925\n",
      "Iteration: 2356000, loss: 0.10922506245719125, gradient norm: 0.030829726317573115\n",
      "Iteration: 2357000, loss: 0.10922392593109644, gradient norm: 0.03205115303965005\n",
      "Iteration: 2358000, loss: 0.10922322676353334, gradient norm: 0.0029312043259316282\n",
      "Iteration: 2359000, loss: 0.10922241404157801, gradient norm: 0.010170188202887308\n",
      "Iteration: 2360000, loss: 0.10922147925271548, gradient norm: 0.06203956065968033\n",
      "Iteration: 2361000, loss: 0.1092205514297702, gradient norm: 0.017791416446056637\n",
      "Iteration: 2362000, loss: 0.1092196999127889, gradient norm: 0.33355800839476135\n",
      "Iteration: 2363000, loss: 0.10921903151811728, gradient norm: 0.26387887834111806\n",
      "Iteration: 2364000, loss: 0.10921795286160062, gradient norm: 0.2909199462281271\n",
      "Iteration: 2365000, loss: 0.1092170518905736, gradient norm: 0.0427921203920669\n",
      "Iteration: 2366000, loss: 0.10921640376187022, gradient norm: 0.03908454675323894\n",
      "Iteration: 2367000, loss: 0.10921533937052727, gradient norm: 0.10694506713540872\n",
      "Iteration: 2368000, loss: 0.10921460361378516, gradient norm: 0.08066939982103498\n",
      "Iteration: 2369000, loss: 0.10921371892711665, gradient norm: 0.1426741958288987\n",
      "Iteration: 2370000, loss: 0.10921275647414251, gradient norm: 0.020651318136518042\n",
      "Iteration: 2371000, loss: 0.10921201050986308, gradient norm: 0.007574844653250117\n",
      "Iteration: 2372000, loss: 0.10921119146726277, gradient norm: 0.0967277687400006\n",
      "Iteration: 2373000, loss: 0.10921021676075873, gradient norm: 0.15247641051053099\n",
      "Iteration: 2374000, loss: 0.10920938661982567, gradient norm: 0.14924590888419545\n",
      "Iteration: 2375000, loss: 0.10920855320724138, gradient norm: 0.01223962375673348\n",
      "Iteration: 2376000, loss: 0.10920778353892967, gradient norm: 0.08489879873018401\n",
      "Iteration: 2377000, loss: 0.10920677370522594, gradient norm: 0.12281604810199015\n",
      "Iteration: 2378000, loss: 0.10920595571093164, gradient norm: 0.27103944012287\n",
      "Iteration: 2379000, loss: 0.109205097083609, gradient norm: 0.5387999067558205\n",
      "Iteration: 2380000, loss: 0.10920434956834765, gradient norm: 0.4917369552412538\n",
      "Iteration: 2381000, loss: 0.10920336282256142, gradient norm: 0.09977030169313583\n",
      "Iteration: 2382000, loss: 0.10920258911253766, gradient norm: 0.12629639534196008\n",
      "Iteration: 2383000, loss: 0.10920170742300192, gradient norm: 0.010568309335955421\n",
      "Iteration: 2384000, loss: 0.10920085653073174, gradient norm: 0.01929469777447776\n",
      "Iteration: 2385000, loss: 0.1092000050096171, gradient norm: 0.037625119431500355\n",
      "Iteration: 2386000, loss: 0.10919913100259138, gradient norm: 0.18751554522475744\n",
      "Iteration: 2387000, loss: 0.10919834818637801, gradient norm: 0.09247402560744912\n",
      "Iteration: 2388000, loss: 0.10919742596072389, gradient norm: 0.06835065736550863\n",
      "Iteration: 2389000, loss: 0.10919662315836638, gradient norm: 0.1202027980227812\n",
      "Iteration: 2390000, loss: 0.1091957658313859, gradient norm: 0.035968763699169556\n",
      "Iteration: 2391000, loss: 0.10919491908669864, gradient norm: 0.050403059306323\n",
      "Iteration: 2392000, loss: 0.1091940046442762, gradient norm: 0.12360467111275791\n",
      "Iteration: 2393000, loss: 0.10919339799624787, gradient norm: 0.17715273352751887\n",
      "Iteration: 2394000, loss: 0.10919228835580597, gradient norm: 0.030925006135720887\n",
      "Iteration: 2395000, loss: 0.10919147533749052, gradient norm: 0.014570757232301394\n",
      "Iteration: 2396000, loss: 0.10919079569891478, gradient norm: 0.29186653772717774\n",
      "Iteration: 2397000, loss: 0.10918970861666281, gradient norm: 0.07532131881182409\n",
      "Iteration: 2398000, loss: 0.10918904226421201, gradient norm: 0.024576751786189885\n",
      "Iteration: 2399000, loss: 0.10918826105193515, gradient norm: 0.0160072236506054\n",
      "Iteration: 2400000, loss: 0.10918737282952884, gradient norm: 0.08502465669914606\n",
      "Iteration: 2401000, loss: 0.10918643060730525, gradient norm: 0.014783087275451114\n",
      "Iteration: 2402000, loss: 0.10918556320469727, gradient norm: 0.10571702319598147\n",
      "Iteration: 2403000, loss: 0.1091849500438646, gradient norm: 0.014972001724814903\n",
      "Iteration: 2404000, loss: 0.1091839337821909, gradient norm: 0.1317966467572293\n",
      "Iteration: 2405000, loss: 0.10918335930881057, gradient norm: 0.048163867735435816\n",
      "Iteration: 2406000, loss: 0.10918211783353292, gradient norm: 0.0699615457807967\n",
      "Iteration: 2407000, loss: 0.10918153144461762, gradient norm: 0.005956491839018751\n",
      "Iteration: 2408000, loss: 0.10918057071734089, gradient norm: 0.05208985669568987\n",
      "Iteration: 2409000, loss: 0.10917988505432737, gradient norm: 0.13256841204535255\n",
      "Iteration: 2410000, loss: 0.1091788484501227, gradient norm: 0.10546802729452827\n",
      "Iteration: 2411000, loss: 0.10917829959387458, gradient norm: 0.15107568210146372\n",
      "Iteration: 2412000, loss: 0.10917718635206065, gradient norm: 0.029597843072137114\n",
      "Iteration: 2413000, loss: 0.10917649799873268, gradient norm: 0.0930014722277289\n",
      "Iteration: 2414000, loss: 0.10917574180723792, gradient norm: 0.05428428564214795\n",
      "Iteration: 2415000, loss: 0.10917483468251683, gradient norm: 0.26279327278792236\n",
      "Iteration: 2416000, loss: 0.10917404412971658, gradient norm: 0.2763554627500975\n",
      "Iteration: 2417000, loss: 0.10917306081574432, gradient norm: 0.04867794536815711\n",
      "Iteration: 2418000, loss: 0.10917237993841317, gradient norm: 0.400720509545081\n",
      "Iteration: 2419000, loss: 0.10917146149473625, gradient norm: 0.5196106788341218\n",
      "Iteration: 2420000, loss: 0.10917079984531466, gradient norm: 0.01956835858958941\n",
      "Iteration: 2421000, loss: 0.10916975967545015, gradient norm: 0.3786731639462099\n",
      "Iteration: 2422000, loss: 0.10916913711490742, gradient norm: 0.02922994689475836\n",
      "Iteration: 2423000, loss: 0.1091681319129033, gradient norm: 0.1981478854030317\n",
      "Iteration: 2424000, loss: 0.10916734425777201, gradient norm: 0.01273411548153011\n",
      "Iteration: 2425000, loss: 0.10916658034220168, gradient norm: 0.017135944801245473\n",
      "Iteration: 2426000, loss: 0.10916571804343603, gradient norm: 0.40869634464240373\n",
      "Iteration: 2427000, loss: 0.10916503046059706, gradient norm: 0.0872686066775482\n",
      "Iteration: 2428000, loss: 0.10916401782645324, gradient norm: 0.0560810870671268\n",
      "Iteration: 2429000, loss: 0.10916331547654386, gradient norm: 0.010203617897672598\n",
      "Iteration: 2430000, loss: 0.10916245846601104, gradient norm: 0.0018907653748751667\n",
      "Iteration: 2431000, loss: 0.10916157868614405, gradient norm: 0.025937995619684528\n",
      "Iteration: 2432000, loss: 0.10916085271261258, gradient norm: 0.03351675336241276\n",
      "Iteration: 2433000, loss: 0.109159917123649, gradient norm: 0.15406282876524374\n",
      "Iteration: 2434000, loss: 0.10915918402168842, gradient norm: 0.08193700099221826\n",
      "Iteration: 2435000, loss: 0.10915835442613944, gradient norm: 0.11923896417487105\n",
      "Iteration: 2436000, loss: 0.10915753498751295, gradient norm: 0.009430051163479322\n",
      "Iteration: 2437000, loss: 0.10915665266401803, gradient norm: 0.21447286253444148\n",
      "Iteration: 2438000, loss: 0.10915602669305804, gradient norm: 0.03424928070612622\n",
      "Iteration: 2439000, loss: 0.10915496974515194, gradient norm: 0.03893938447970068\n",
      "Iteration: 2440000, loss: 0.10915427969742783, gradient norm: 0.039139066312485926\n",
      "Iteration: 2441000, loss: 0.10915350008568166, gradient norm: 0.036615654358478716\n",
      "Iteration: 2442000, loss: 0.10915251948695712, gradient norm: 0.24195558712602286\n",
      "Iteration: 2443000, loss: 0.10915199239074143, gradient norm: 0.13384681684792574\n",
      "Iteration: 2444000, loss: 0.10915093932115814, gradient norm: 0.01843491011225605\n",
      "Iteration: 2445000, loss: 0.10915021172973143, gradient norm: 0.2389953600495466\n",
      "Iteration: 2446000, loss: 0.10914937291150466, gradient norm: 0.014573969196629325\n",
      "Iteration: 2447000, loss: 0.10914853258379321, gradient norm: 0.009542066948256168\n",
      "Iteration: 2448000, loss: 0.10914773345822318, gradient norm: 0.011405388687692317\n",
      "Iteration: 2449000, loss: 0.10914692862574289, gradient norm: 0.09958549837581859\n",
      "Iteration: 2450000, loss: 0.10914613132459894, gradient norm: 0.22502812157418117\n",
      "Iteration: 2451000, loss: 0.10914541363036338, gradient norm: 0.1956127560195189\n",
      "Iteration: 2452000, loss: 0.10914449846172615, gradient norm: 0.04361370988471069\n",
      "Iteration: 2453000, loss: 0.10914386737067741, gradient norm: 0.2544051985340074\n",
      "Iteration: 2454000, loss: 0.10914277401778148, gradient norm: 0.07776169562276748\n",
      "Iteration: 2455000, loss: 0.1091420455968854, gradient norm: 0.04963268247382892\n",
      "Iteration: 2456000, loss: 0.10914135513439499, gradient norm: 0.24630436651274454\n",
      "Iteration: 2457000, loss: 0.10914056179759013, gradient norm: 0.004934591161556386\n",
      "Iteration: 2458000, loss: 0.10913968235085593, gradient norm: 0.24499799793974347\n",
      "Iteration: 2459000, loss: 0.10913907321506065, gradient norm: 0.06721014264544964\n",
      "Iteration: 2460000, loss: 0.10913801197140273, gradient norm: 0.025975215062445597\n",
      "Iteration: 2461000, loss: 0.10913724813443167, gradient norm: 0.12806128017854568\n",
      "Iteration: 2462000, loss: 0.1091366089868699, gradient norm: 0.2276961335184787\n",
      "Iteration: 2463000, loss: 0.10913562130435282, gradient norm: 0.09935407381329389\n",
      "Iteration: 2464000, loss: 0.10913494305278676, gradient norm: 0.07937417200061155\n",
      "Iteration: 2465000, loss: 0.10913413675567607, gradient norm: 0.16579492908779286\n",
      "Iteration: 2466000, loss: 0.10913335865280614, gradient norm: 0.13578153416029357\n",
      "Iteration: 2467000, loss: 0.10913243605535644, gradient norm: 0.03439657830493247\n",
      "Iteration: 2468000, loss: 0.10913156043777962, gradient norm: 0.007440921524996919\n",
      "Iteration: 2469000, loss: 0.10913094342170074, gradient norm: 0.3690108660909809\n",
      "Iteration: 2470000, loss: 0.10913003173769838, gradient norm: 0.004814706631374514\n",
      "Iteration: 2471000, loss: 0.10912936829677489, gradient norm: 0.3107250932776804\n",
      "Iteration: 2472000, loss: 0.10912843641630557, gradient norm: 0.05065263236588629\n",
      "Iteration: 2473000, loss: 0.10912787323477256, gradient norm: 0.5822954171493072\n",
      "Iteration: 2474000, loss: 0.10912694466000067, gradient norm: 0.12185258335712176\n",
      "Iteration: 2475000, loss: 0.1091261260838831, gradient norm: 0.018262856771133023\n",
      "Iteration: 2476000, loss: 0.10912530163732945, gradient norm: 0.13890485622391424\n",
      "Iteration: 2477000, loss: 0.10912439943433583, gradient norm: 0.028158087888185922\n",
      "Iteration: 2478000, loss: 0.10912388395501486, gradient norm: 0.21954938749898806\n",
      "Iteration: 2479000, loss: 0.10912296223602597, gradient norm: 0.39714302886335273\n",
      "Iteration: 2480000, loss: 0.10912209293428261, gradient norm: 0.6580236486711533\n",
      "Iteration: 2481000, loss: 0.10912133942742133, gradient norm: 0.045582842815195895\n",
      "Iteration: 2482000, loss: 0.10912054841544701, gradient norm: 0.03521563386251141\n",
      "Iteration: 2483000, loss: 0.10911988053449592, gradient norm: 0.15264285254728596\n",
      "Iteration: 2484000, loss: 0.10911881586101652, gradient norm: 0.02565980362592602\n",
      "Iteration: 2485000, loss: 0.10911824128656658, gradient norm: 0.014133933141233316\n",
      "Iteration: 2486000, loss: 0.10911736280329178, gradient norm: 0.017542386843764232\n",
      "Iteration: 2487000, loss: 0.10911663375974977, gradient norm: 0.06842303519862723\n",
      "Iteration: 2488000, loss: 0.10911584716693834, gradient norm: 0.22934534898071285\n",
      "Iteration: 2489000, loss: 0.1091152122600224, gradient norm: 0.06060383257493567\n",
      "Iteration: 2490000, loss: 0.10911413766484403, gradient norm: 0.028078689021028176\n",
      "Iteration: 2491000, loss: 0.10911342781692052, gradient norm: 0.024520350470272716\n",
      "Iteration: 2492000, loss: 0.10911275063748481, gradient norm: 0.03179305804872709\n",
      "Iteration: 2493000, loss: 0.1091119138351748, gradient norm: 0.5519278328785786\n",
      "Iteration: 2494000, loss: 0.10911118053022975, gradient norm: 0.04349001279756711\n",
      "Iteration: 2495000, loss: 0.10911019675271615, gradient norm: 0.26047297074936177\n",
      "Iteration: 2496000, loss: 0.10910970353937191, gradient norm: 0.007062960045914054\n",
      "Iteration: 2497000, loss: 0.10910875752660852, gradient norm: 0.05604662060658464\n",
      "Iteration: 2498000, loss: 0.10910791840109299, gradient norm: 0.01964748980136889\n",
      "Iteration: 2499000, loss: 0.10910737125701507, gradient norm: 0.0800925126784566\n",
      "Iteration: 2500000, loss: 0.10910639485694969, gradient norm: 0.019464532562441062\n",
      "Iteration: 2501000, loss: 0.10910554519943329, gradient norm: 0.2576318563713997\n",
      "Iteration: 2502000, loss: 0.10910500977194972, gradient norm: 0.044723283889921535\n",
      "Iteration: 2503000, loss: 0.10910411890914912, gradient norm: 0.12365606490631835\n",
      "Iteration: 2504000, loss: 0.10910326329162882, gradient norm: 0.07578724818634089\n",
      "Iteration: 2505000, loss: 0.10910250151218753, gradient norm: 0.18743607536231546\n",
      "Iteration: 2506000, loss: 0.10910183818368599, gradient norm: 0.2543677567889946\n",
      "Iteration: 2507000, loss: 0.10910095463830877, gradient norm: 0.06399216635141067\n",
      "Iteration: 2508000, loss: 0.10910018939769435, gradient norm: 0.04436839910138398\n",
      "Iteration: 2509000, loss: 0.10909948269594319, gradient norm: 0.08104848378928445\n",
      "Iteration: 2510000, loss: 0.10909861666841406, gradient norm: 0.1318013604801399\n",
      "Iteration: 2511000, loss: 0.10909797142872085, gradient norm: 0.002167375371715202\n",
      "Iteration: 2512000, loss: 0.1090970735252231, gradient norm: 0.018542240847937787\n",
      "Iteration: 2513000, loss: 0.10909636711549306, gradient norm: 0.03184197914496753\n",
      "Iteration: 2514000, loss: 0.10909556588938298, gradient norm: 0.016909203936060727\n",
      "Iteration: 2515000, loss: 0.10909472113761419, gradient norm: 0.3606676121441471\n",
      "Iteration: 2516000, loss: 0.10909409865025348, gradient norm: 0.09693366123665212\n",
      "Iteration: 2517000, loss: 0.10909321486899394, gradient norm: 0.14665422343986725\n",
      "Iteration: 2518000, loss: 0.10909235704787144, gradient norm: 0.007779129956132312\n",
      "Iteration: 2519000, loss: 0.10909178398329775, gradient norm: 0.3664224948157738\n",
      "Iteration: 2520000, loss: 0.10909105449842713, gradient norm: 0.5331458537192502\n",
      "Iteration: 2521000, loss: 0.10909006373653299, gradient norm: 0.12958551266461366\n",
      "Iteration: 2522000, loss: 0.10908952842720514, gradient norm: 0.08759433068545977\n",
      "Iteration: 2523000, loss: 0.10908859600353787, gradient norm: 0.05019455826473387\n",
      "Iteration: 2524000, loss: 0.10908787110418093, gradient norm: 0.014091460038902373\n",
      "Iteration: 2525000, loss: 0.10908722818371479, gradient norm: 0.03531448795222588\n",
      "Iteration: 2526000, loss: 0.10908627726076733, gradient norm: 0.053238955257303155\n",
      "Iteration: 2527000, loss: 0.1090854647766249, gradient norm: 0.0942524605167919\n",
      "Iteration: 2528000, loss: 0.10908486845611899, gradient norm: 0.1171192458227233\n",
      "Iteration: 2529000, loss: 0.10908409547478617, gradient norm: 0.2093020603454356\n",
      "Iteration: 2530000, loss: 0.10908317115360128, gradient norm: 0.02120577137768685\n",
      "Iteration: 2531000, loss: 0.10908250282130962, gradient norm: 0.16231915119371731\n",
      "Iteration: 2532000, loss: 0.1090818845241944, gradient norm: 0.05561605777779104\n",
      "Iteration: 2533000, loss: 0.10908099499946058, gradient norm: 0.444841342688711\n",
      "Iteration: 2534000, loss: 0.10908008096416492, gradient norm: 0.30336977848865204\n",
      "Iteration: 2535000, loss: 0.1090796345141008, gradient norm: 0.28916497265951885\n",
      "Iteration: 2536000, loss: 0.10907867327425098, gradient norm: 0.058162749275362725\n",
      "Iteration: 2537000, loss: 0.10907787101288852, gradient norm: 0.1196714227806441\n",
      "Iteration: 2538000, loss: 0.10907709368798457, gradient norm: 0.005105111602081022\n",
      "Iteration: 2539000, loss: 0.10907652474618831, gradient norm: 0.009908840691185009\n",
      "Iteration: 2540000, loss: 0.1090756500749878, gradient norm: 0.13710806137796983\n",
      "Iteration: 2541000, loss: 0.10907480219648241, gradient norm: 0.06846128099521619\n",
      "Iteration: 2542000, loss: 0.10907421530104575, gradient norm: 0.07240725489448753\n",
      "Iteration: 2543000, loss: 0.1090733082803717, gradient norm: 0.008751006286459856\n",
      "Iteration: 2544000, loss: 0.1090728124489658, gradient norm: 0.18716413440055002\n",
      "Iteration: 2545000, loss: 0.1090717240851295, gradient norm: 0.061593009440388445\n",
      "Iteration: 2546000, loss: 0.10907109028267586, gradient norm: 0.05261281483823313\n",
      "Iteration: 2547000, loss: 0.10907041553980427, gradient norm: 0.15130315050712448\n",
      "Iteration: 2548000, loss: 0.10906966045519687, gradient norm: 0.028866421642805954\n",
      "Iteration: 2549000, loss: 0.10906872541744067, gradient norm: 0.02533630853714765\n",
      "Iteration: 2550000, loss: 0.10906811948530662, gradient norm: 0.2261859211787904\n",
      "Iteration: 2551000, loss: 0.10906722314424312, gradient norm: 0.08184563760187462\n",
      "Iteration: 2552000, loss: 0.10906662058750655, gradient norm: 0.05635344748512079\n",
      "Iteration: 2553000, loss: 0.10906591361144942, gradient norm: 0.035183087885810226\n",
      "Iteration: 2554000, loss: 0.10906497798672388, gradient norm: 0.06986568589422028\n",
      "Iteration: 2555000, loss: 0.10906428648360152, gradient norm: 0.21480753990505178\n",
      "Iteration: 2556000, loss: 0.10906360422188459, gradient norm: 0.06453962788143044\n",
      "Iteration: 2557000, loss: 0.10906282630794173, gradient norm: 0.0029256787368041165\n",
      "Iteration: 2558000, loss: 0.10906218353492585, gradient norm: 0.014577590317039906\n",
      "Iteration: 2559000, loss: 0.10906114811617347, gradient norm: 0.11542198065309117\n",
      "Iteration: 2560000, loss: 0.10906055266714282, gradient norm: 0.016075748006062186\n",
      "Iteration: 2561000, loss: 0.1090597956417256, gradient norm: 0.047687214238985524\n",
      "Iteration: 2562000, loss: 0.10905912218464017, gradient norm: 0.07325226846699136\n",
      "Iteration: 2563000, loss: 0.10905823598621565, gradient norm: 0.3195876293702606\n",
      "Iteration: 2564000, loss: 0.10905755473659687, gradient norm: 0.07895179946261285\n",
      "Iteration: 2565000, loss: 0.10905682283968536, gradient norm: 0.025194005439780603\n",
      "Iteration: 2566000, loss: 0.109056009205575, gradient norm: 0.2802920305100932\n",
      "Iteration: 2567000, loss: 0.10905535048892452, gradient norm: 0.05511959885332652\n",
      "Iteration: 2568000, loss: 0.10905462379963453, gradient norm: 0.12460671474106116\n",
      "Iteration: 2569000, loss: 0.10905384078615839, gradient norm: 0.21400280431510288\n",
      "Iteration: 2570000, loss: 0.1090529420696026, gradient norm: 0.05465793078066481\n",
      "Iteration: 2571000, loss: 0.10905229425956285, gradient norm: 0.00988291853270144\n",
      "Iteration: 2572000, loss: 0.10905159798012591, gradient norm: 0.17859694880102556\n",
      "Iteration: 2573000, loss: 0.10905094503351546, gradient norm: 0.20840788069936503\n",
      "Iteration: 2574000, loss: 0.1090501032766619, gradient norm: 0.5303591001998522\n",
      "Iteration: 2575000, loss: 0.10904927630811989, gradient norm: 0.05750671772995824\n",
      "Iteration: 2576000, loss: 0.1090487611265279, gradient norm: 0.13730235708112032\n",
      "Iteration: 2577000, loss: 0.10904770496346194, gradient norm: 0.13501734091630155\n",
      "Iteration: 2578000, loss: 0.10904724376559452, gradient norm: 0.10696250329311008\n",
      "Iteration: 2579000, loss: 0.10904635102936866, gradient norm: 0.0020436510638801855\n",
      "Iteration: 2580000, loss: 0.10904573894474413, gradient norm: 0.028799916892931838\n",
      "Iteration: 2581000, loss: 0.10904482357924619, gradient norm: 0.2407980500942488\n",
      "Iteration: 2582000, loss: 0.1090441523314759, gradient norm: 0.07439778585615427\n",
      "Iteration: 2583000, loss: 0.10904346836514944, gradient norm: 0.04761413395156935\n",
      "Iteration: 2584000, loss: 0.10904278806699551, gradient norm: 0.0469366315109699\n",
      "Iteration: 2585000, loss: 0.10904194109806449, gradient norm: 0.009484035415458131\n",
      "Iteration: 2586000, loss: 0.10904124334433703, gradient norm: 0.07350932834733974\n",
      "Iteration: 2587000, loss: 0.10904043542092842, gradient norm: 0.012127260490682133\n",
      "Iteration: 2588000, loss: 0.1090398722242665, gradient norm: 0.15168865800649858\n",
      "Iteration: 2589000, loss: 0.10903899153472128, gradient norm: 0.04233492138562249\n",
      "Iteration: 2590000, loss: 0.10903846742145806, gradient norm: 0.14326144742486938\n",
      "Iteration: 2591000, loss: 0.10903739648236652, gradient norm: 0.04018194527021961\n",
      "Iteration: 2592000, loss: 0.10903689913186167, gradient norm: 0.291273229012468\n",
      "Iteration: 2593000, loss: 0.10903602245289051, gradient norm: 0.011959492654329582\n",
      "Iteration: 2594000, loss: 0.10903538619682018, gradient norm: 0.09199412580537727\n",
      "Iteration: 2595000, loss: 0.10903460025374506, gradient norm: 0.09509998519476405\n",
      "Iteration: 2596000, loss: 0.10903389909946277, gradient norm: 0.031187892483223446\n",
      "Iteration: 2597000, loss: 0.1090331587842847, gradient norm: 0.28537886276505875\n",
      "Iteration: 2598000, loss: 0.10903238729294396, gradient norm: 0.01571613607893596\n",
      "Iteration: 2599000, loss: 0.10903181349142699, gradient norm: 0.10288459663867182\n",
      "Iteration: 2600000, loss: 0.10903084378039714, gradient norm: 0.018511100655181214\n",
      "Iteration: 2601000, loss: 0.10903026192143853, gradient norm: 0.1531333362868657\n",
      "Iteration: 2602000, loss: 0.10902945317137944, gradient norm: 0.015869820847475884\n",
      "Iteration: 2603000, loss: 0.10902880325522563, gradient norm: 0.0966213095371308\n",
      "Iteration: 2604000, loss: 0.10902812719895673, gradient norm: 0.1086230431894881\n",
      "Iteration: 2605000, loss: 0.10902734939343661, gradient norm: 0.13390578492373598\n",
      "Iteration: 2606000, loss: 0.10902666188196848, gradient norm: 0.29287543407405103\n",
      "Iteration: 2607000, loss: 0.10902582782975889, gradient norm: 0.025520625540029724\n",
      "Iteration: 2608000, loss: 0.1090250985432919, gradient norm: 0.05240118103782955\n",
      "Iteration: 2609000, loss: 0.10902449287633503, gradient norm: 0.4377352668340068\n",
      "Iteration: 2610000, loss: 0.10902367128521644, gradient norm: 0.05701999489613837\n",
      "Iteration: 2611000, loss: 0.10902299290844635, gradient norm: 0.0371427378509455\n",
      "Iteration: 2612000, loss: 0.10902247168333683, gradient norm: 0.13478306310614172\n",
      "Iteration: 2613000, loss: 0.10902137003479345, gradient norm: 0.22573418004554166\n",
      "Iteration: 2614000, loss: 0.1090207452018918, gradient norm: 0.5779492303530386\n",
      "Iteration: 2615000, loss: 0.10902020260808985, gradient norm: 0.07422612370473647\n",
      "Iteration: 2616000, loss: 0.10901936860492092, gradient norm: 0.029143178580047092\n",
      "Iteration: 2617000, loss: 0.10901862049169664, gradient norm: 0.6006008839657144\n",
      "Iteration: 2618000, loss: 0.10901792907869236, gradient norm: 0.45391074008833415\n",
      "Iteration: 2619000, loss: 0.10901714021171335, gradient norm: 0.020797043435342204\n",
      "Iteration: 2620000, loss: 0.10901647997847372, gradient norm: 0.10997312335247729\n",
      "Iteration: 2621000, loss: 0.10901588268250323, gradient norm: 0.31329891803204246\n",
      "Iteration: 2622000, loss: 0.10901488429509783, gradient norm: 0.2358114028425208\n",
      "Iteration: 2623000, loss: 0.10901436152511979, gradient norm: 0.15156713494861707\n",
      "Iteration: 2624000, loss: 0.1090136317409023, gradient norm: 0.01352641519920685\n",
      "Iteration: 2625000, loss: 0.10901281466791138, gradient norm: 0.2206997298279458\n",
      "Iteration: 2626000, loss: 0.10901218357453234, gradient norm: 0.07059908027206636\n",
      "Iteration: 2627000, loss: 0.10901141995085356, gradient norm: 0.07614149137892508\n",
      "Iteration: 2628000, loss: 0.10901067849286586, gradient norm: 0.17003186358530048\n",
      "Iteration: 2629000, loss: 0.10901015286708216, gradient norm: 0.232025817787721\n",
      "Iteration: 2630000, loss: 0.10900922483660998, gradient norm: 0.0362220652818255\n",
      "Iteration: 2631000, loss: 0.10900850380778479, gradient norm: 0.14322487574437393\n",
      "Iteration: 2632000, loss: 0.1090078976721011, gradient norm: 0.09151637282298433\n",
      "Iteration: 2633000, loss: 0.10900713961704946, gradient norm: 0.3942604701240535\n",
      "Iteration: 2634000, loss: 0.10900639712350471, gradient norm: 0.15953316710929377\n",
      "Iteration: 2635000, loss: 0.10900575549736835, gradient norm: 0.3738610741270524\n",
      "Iteration: 2636000, loss: 0.10900503169824444, gradient norm: 0.044167012609646165\n",
      "Iteration: 2637000, loss: 0.10900427121613165, gradient norm: 0.04152908557284188\n",
      "Iteration: 2638000, loss: 0.10900356401168622, gradient norm: 0.03247416573991476\n",
      "Iteration: 2639000, loss: 0.10900305039937129, gradient norm: 0.0990089615286815\n",
      "Iteration: 2640000, loss: 0.10900196988555237, gradient norm: 0.28146886899796303\n",
      "Iteration: 2641000, loss: 0.1090014594548781, gradient norm: 0.6258623735399202\n",
      "Iteration: 2642000, loss: 0.10900082913306591, gradient norm: 0.03351573152921891\n",
      "Iteration: 2643000, loss: 0.10900001967652247, gradient norm: 0.08288489419247481\n",
      "Iteration: 2644000, loss: 0.10899931278302563, gradient norm: 0.10234864451555015\n",
      "Iteration: 2645000, loss: 0.10899855535754273, gradient norm: 0.11545047211616685\n",
      "Iteration: 2646000, loss: 0.10899791237376681, gradient norm: 0.05588274756724012\n",
      "Iteration: 2647000, loss: 0.10899720867939337, gradient norm: 0.24337013372294516\n",
      "Iteration: 2648000, loss: 0.10899650142381337, gradient norm: 0.10803056418376286\n",
      "Iteration: 2649000, loss: 0.10899575696589395, gradient norm: 0.05432659080802296\n",
      "Iteration: 2650000, loss: 0.10899520639250007, gradient norm: 0.10472070125395427\n",
      "Iteration: 2651000, loss: 0.10899434631745072, gradient norm: 0.03593576650961107\n",
      "Iteration: 2652000, loss: 0.10899369118732262, gradient norm: 0.22256275147086535\n",
      "Iteration: 2653000, loss: 0.10899285032243035, gradient norm: 0.23824489225423356\n",
      "Iteration: 2654000, loss: 0.10899232951678195, gradient norm: 0.012018118336719882\n",
      "Iteration: 2655000, loss: 0.10899163888590171, gradient norm: 0.07317507778721592\n",
      "Iteration: 2656000, loss: 0.1089907513989474, gradient norm: 0.003893525096861467\n",
      "Iteration: 2657000, loss: 0.10899013483073662, gradient norm: 0.0853782424879612\n",
      "Iteration: 2658000, loss: 0.10898959232504236, gradient norm: 0.14048475890166184\n",
      "Iteration: 2659000, loss: 0.1089886213842368, gradient norm: 0.2986528657310903\n",
      "Iteration: 2660000, loss: 0.10898803987767046, gradient norm: 0.04173706014991166\n",
      "Iteration: 2661000, loss: 0.10898734567131353, gradient norm: 0.03240632976316961\n",
      "Iteration: 2662000, loss: 0.10898675132520759, gradient norm: 0.09469210088680101\n",
      "Iteration: 2663000, loss: 0.10898589486649282, gradient norm: 0.2182593476224817\n",
      "Iteration: 2664000, loss: 0.10898520053301694, gradient norm: 0.009332451679908546\n",
      "Iteration: 2665000, loss: 0.10898461399793213, gradient norm: 0.02053658175916945\n",
      "Iteration: 2666000, loss: 0.10898375419569915, gradient norm: 0.015243219494014207\n",
      "Iteration: 2667000, loss: 0.10898317164553226, gradient norm: 0.026568514968363096\n",
      "Iteration: 2668000, loss: 0.10898243574734454, gradient norm: 0.009203993898342198\n",
      "Iteration: 2669000, loss: 0.10898175820839762, gradient norm: 0.014001680957126457\n",
      "Iteration: 2670000, loss: 0.10898103188340402, gradient norm: 0.00888089528481992\n",
      "Iteration: 2671000, loss: 0.1089804515885025, gradient norm: 0.09414324332825108\n",
      "Iteration: 2672000, loss: 0.10897963439313921, gradient norm: 0.14769488856046842\n",
      "Iteration: 2673000, loss: 0.10897890027841806, gradient norm: 0.02327356950248109\n",
      "Iteration: 2674000, loss: 0.1089783001481288, gradient norm: 0.0928741449608753\n",
      "Iteration: 2675000, loss: 0.10897756535126384, gradient norm: 0.011433655091909013\n",
      "Iteration: 2676000, loss: 0.10897688198135629, gradient norm: 0.21692859341884255\n",
      "Iteration: 2677000, loss: 0.1089762601274538, gradient norm: 0.3558593515123088\n",
      "Iteration: 2678000, loss: 0.10897542537936622, gradient norm: 0.1110344825238923\n",
      "Iteration: 2679000, loss: 0.10897466624559328, gradient norm: 0.03946683063912336\n",
      "Iteration: 2680000, loss: 0.10897418179804706, gradient norm: 0.037750077508066006\n",
      "Iteration: 2681000, loss: 0.10897345321618011, gradient norm: 0.17268677858255177\n",
      "Iteration: 2682000, loss: 0.10897266584038424, gradient norm: 0.11811277430918962\n",
      "Iteration: 2683000, loss: 0.10897211188429995, gradient norm: 0.14156617049809136\n",
      "Iteration: 2684000, loss: 0.10897123978980035, gradient norm: 0.45489758052846146\n",
      "Iteration: 2685000, loss: 0.10897062496732533, gradient norm: 0.11216857158094301\n",
      "Iteration: 2686000, loss: 0.10896995413653682, gradient norm: 0.09200472590874723\n",
      "Iteration: 2687000, loss: 0.10896917478189695, gradient norm: 0.15251997672798484\n",
      "Iteration: 2688000, loss: 0.10896857681515996, gradient norm: 0.19371560162532067\n",
      "Iteration: 2689000, loss: 0.10896804304748199, gradient norm: 0.39362477878887975\n",
      "Iteration: 2690000, loss: 0.10896715587224413, gradient norm: 0.10417287483452174\n",
      "Iteration: 2691000, loss: 0.10896641284030872, gradient norm: 0.01450823515759724\n",
      "Iteration: 2692000, loss: 0.10896581658970689, gradient norm: 0.3632140830438995\n",
      "Iteration: 2693000, loss: 0.10896523481560494, gradient norm: 0.17615451860090856\n",
      "Iteration: 2694000, loss: 0.10896443355211292, gradient norm: 0.042449991609432136\n",
      "Iteration: 2695000, loss: 0.10896371970077837, gradient norm: 0.291419081787323\n",
      "Iteration: 2696000, loss: 0.10896306974584616, gradient norm: 0.009545193934243018\n",
      "Iteration: 2697000, loss: 0.10896231111732718, gradient norm: 0.10675587060702872\n",
      "Iteration: 2698000, loss: 0.10896181322485181, gradient norm: 0.564951835466345\n",
      "Iteration: 2699000, loss: 0.10896104734244003, gradient norm: 0.3345907953941412\n",
      "Iteration: 2700000, loss: 0.10896033665249175, gradient norm: 0.027076695526419274\n",
      "Iteration: 2701000, loss: 0.10895961019734636, gradient norm: 0.1298139984198247\n",
      "Iteration: 2702000, loss: 0.10895898778433528, gradient norm: 0.16844563086086642\n",
      "Iteration: 2703000, loss: 0.10895829869628199, gradient norm: 0.08876104560727086\n",
      "Iteration: 2704000, loss: 0.10895753723996976, gradient norm: 0.026867218552231655\n",
      "Iteration: 2705000, loss: 0.10895689071900995, gradient norm: 0.054777704677111336\n",
      "Iteration: 2706000, loss: 0.1089562994539506, gradient norm: 0.2589796405120382\n",
      "Iteration: 2707000, loss: 0.10895558989261753, gradient norm: 0.021297748260813057\n",
      "Iteration: 2708000, loss: 0.10895475412155158, gradient norm: 0.054174176769312625\n",
      "Iteration: 2709000, loss: 0.10895424575480593, gradient norm: 0.01591793194440147\n",
      "Iteration: 2710000, loss: 0.10895359135167271, gradient norm: 0.06338530583372252\n",
      "Iteration: 2711000, loss: 0.10895272384365623, gradient norm: 0.07669451516673771\n",
      "Iteration: 2712000, loss: 0.10895224385869637, gradient norm: 0.28331321732085013\n",
      "Iteration: 2713000, loss: 0.10895140957560841, gradient norm: 0.37350531051814817\n",
      "Iteration: 2714000, loss: 0.10895082710978223, gradient norm: 0.020189527691011365\n",
      "Iteration: 2715000, loss: 0.10895017660062095, gradient norm: 0.0009996579300998929\n",
      "Iteration: 2716000, loss: 0.10894932604600442, gradient norm: 0.12601385227955483\n",
      "Iteration: 2717000, loss: 0.10894890476151577, gradient norm: 0.2651549020208891\n",
      "Iteration: 2718000, loss: 0.1089479855039755, gradient norm: 0.04155653814201204\n",
      "Iteration: 2719000, loss: 0.1089474002989844, gradient norm: 0.11185680950524603\n",
      "Iteration: 2720000, loss: 0.10894675748243852, gradient norm: 0.13964818690416805\n",
      "Iteration: 2721000, loss: 0.10894599472084289, gradient norm: 0.0023049566887552494\n",
      "Iteration: 2722000, loss: 0.10894551417702013, gradient norm: 0.09277794660603969\n",
      "Iteration: 2723000, loss: 0.1089446063965975, gradient norm: 0.08948304507911795\n",
      "Iteration: 2724000, loss: 0.10894401141451565, gradient norm: 0.09297174214410615\n",
      "Iteration: 2725000, loss: 0.10894339840532732, gradient norm: 0.024900715713838617\n",
      "Iteration: 2726000, loss: 0.10894271673025435, gradient norm: 0.12035185863756404\n",
      "Iteration: 2727000, loss: 0.10894189995189235, gradient norm: 0.052510480353309415\n",
      "Iteration: 2728000, loss: 0.10894139629368495, gradient norm: 0.2981195263292313\n",
      "Iteration: 2729000, loss: 0.10894070070723852, gradient norm: 0.14223270256063658\n",
      "Iteration: 2730000, loss: 0.10893992446545697, gradient norm: 0.07495106502262976\n",
      "Iteration: 2731000, loss: 0.10893937769632538, gradient norm: 0.6433928998917884\n",
      "Iteration: 2732000, loss: 0.10893863102826797, gradient norm: 0.010564641027558108\n",
      "Iteration: 2733000, loss: 0.10893802557949113, gradient norm: 0.21064549330121357\n",
      "Iteration: 2734000, loss: 0.10893722281145135, gradient norm: 0.019009625175424396\n",
      "Iteration: 2735000, loss: 0.10893666808282729, gradient norm: 0.03930351210481\n",
      "Iteration: 2736000, loss: 0.10893602529185928, gradient norm: 0.0645160786875048\n",
      "Iteration: 2737000, loss: 0.10893527341461909, gradient norm: 0.5401101464732484\n",
      "Iteration: 2738000, loss: 0.1089346705941744, gradient norm: 0.06176640042522534\n",
      "Iteration: 2739000, loss: 0.10893401836933819, gradient norm: 0.17135697551741336\n",
      "Iteration: 2740000, loss: 0.10893320291074278, gradient norm: 0.019596906259889902\n",
      "Iteration: 2741000, loss: 0.10893269301496894, gradient norm: 0.08216685995719049\n",
      "Iteration: 2742000, loss: 0.10893189404108511, gradient norm: 0.015497400671210254\n",
      "Iteration: 2743000, loss: 0.10893134724009751, gradient norm: 0.0038271551979997954\n",
      "Iteration: 2744000, loss: 0.10893057577042566, gradient norm: 0.01597225411673021\n",
      "Iteration: 2745000, loss: 0.10893001328105972, gradient norm: 0.017803880212797513\n",
      "Iteration: 2746000, loss: 0.10892926611451263, gradient norm: 0.02426278290695404\n",
      "Iteration: 2747000, loss: 0.10892863337045344, gradient norm: 0.14344792976458504\n",
      "Iteration: 2748000, loss: 0.10892796167847456, gradient norm: 0.028056327566545487\n",
      "Iteration: 2749000, loss: 0.10892737268980955, gradient norm: 0.009628485691259446\n",
      "Iteration: 2750000, loss: 0.10892656033604756, gradient norm: 0.11267459548578397\n",
      "Iteration: 2751000, loss: 0.10892606379280877, gradient norm: 0.06389958462982914\n",
      "Iteration: 2752000, loss: 0.1089252837581317, gradient norm: 0.08517218189021562\n",
      "Iteration: 2753000, loss: 0.10892459994775329, gradient norm: 0.21623634849154905\n",
      "Iteration: 2754000, loss: 0.10892398898243658, gradient norm: 0.014967133175214514\n",
      "Iteration: 2755000, loss: 0.10892332573738686, gradient norm: 0.20889658753130835\n",
      "Iteration: 2756000, loss: 0.10892268403222495, gradient norm: 0.14527402999014646\n",
      "Iteration: 2757000, loss: 0.10892205280833772, gradient norm: 0.08710232665182445\n",
      "Iteration: 2758000, loss: 0.1089212549124587, gradient norm: 0.09292560205688173\n",
      "Iteration: 2759000, loss: 0.10892074317842423, gradient norm: 0.4661297454692803\n",
      "Iteration: 2760000, loss: 0.10891994572534097, gradient norm: 0.4659686286115932\n",
      "Iteration: 2761000, loss: 0.10891936296059293, gradient norm: 0.08820050691239283\n",
      "Iteration: 2762000, loss: 0.10891864617697897, gradient norm: 0.03298977796482529\n",
      "Iteration: 2763000, loss: 0.10891802988609729, gradient norm: 0.052587130776119186\n",
      "Iteration: 2764000, loss: 0.10891735569146259, gradient norm: 0.3486317068342645\n",
      "Iteration: 2765000, loss: 0.10891679290652183, gradient norm: 0.09222550043472467\n",
      "Iteration: 2766000, loss: 0.10891593061617842, gradient norm: 0.09881355473646043\n",
      "Iteration: 2767000, loss: 0.10891539789324818, gradient norm: 0.3233424047013703\n",
      "Iteration: 2768000, loss: 0.10891491198515492, gradient norm: 0.1035096270271057\n",
      "Iteration: 2769000, loss: 0.10891393771036008, gradient norm: 0.005717269450565677\n",
      "Iteration: 2770000, loss: 0.1089135102910999, gradient norm: 0.2778209416258826\n",
      "Iteration: 2771000, loss: 0.10891268800471961, gradient norm: 0.060834519860366136\n",
      "Iteration: 2772000, loss: 0.10891208909846736, gradient norm: 0.2252291514689686\n",
      "Iteration: 2773000, loss: 0.10891142009411996, gradient norm: 0.022057043772365382\n",
      "Iteration: 2774000, loss: 0.10891080587259946, gradient norm: 0.03278744675940498\n",
      "Iteration: 2775000, loss: 0.10891012927594904, gradient norm: 0.2585790943326529\n",
      "Iteration: 2776000, loss: 0.10890958265459336, gradient norm: 0.024778021602049727\n",
      "Iteration: 2777000, loss: 0.10890881025019948, gradient norm: 0.03986478559040848\n",
      "Iteration: 2778000, loss: 0.10890811182185715, gradient norm: 0.3543275852753058\n",
      "Iteration: 2779000, loss: 0.1089075311385484, gradient norm: 0.017731411161469805\n",
      "Iteration: 2780000, loss: 0.10890687691209529, gradient norm: 0.45514224517882956\n",
      "Iteration: 2781000, loss: 0.10890621230263238, gradient norm: 0.1588668839402831\n",
      "Iteration: 2782000, loss: 0.10890545078471008, gradient norm: 0.21793750242873866\n",
      "Iteration: 2783000, loss: 0.10890498402056255, gradient norm: 0.008425806997553515\n",
      "Iteration: 2784000, loss: 0.10890433410815965, gradient norm: 0.055016536764986884\n",
      "Iteration: 2785000, loss: 0.10890349672276076, gradient norm: 0.07822844870570404\n",
      "Iteration: 2786000, loss: 0.10890296608803471, gradient norm: 0.22746587090107076\n",
      "Iteration: 2787000, loss: 0.10890222294599435, gradient norm: 0.08741013936623158\n",
      "Iteration: 2788000, loss: 0.10890167532579195, gradient norm: 0.033128912798900685\n",
      "Iteration: 2789000, loss: 0.10890105356266513, gradient norm: 0.00818164767180407\n",
      "Iteration: 2790000, loss: 0.10890037770585356, gradient norm: 0.015240548213731836\n",
      "Iteration: 2791000, loss: 0.10889973574981975, gradient norm: 0.31822774955405986\n",
      "Iteration: 2792000, loss: 0.10889907759865533, gradient norm: 0.12893457340151074\n",
      "Iteration: 2793000, loss: 0.10889837935438253, gradient norm: 0.00394568252246085\n",
      "Iteration: 2794000, loss: 0.1088978375768248, gradient norm: 0.0492764898646703\n",
      "Iteration: 2795000, loss: 0.1088970102812104, gradient norm: 0.0453931121154571\n",
      "Iteration: 2796000, loss: 0.10889655107344555, gradient norm: 0.05271929356479504\n",
      "Iteration: 2797000, loss: 0.10889570239781299, gradient norm: 0.16897245088980156\n",
      "Iteration: 2798000, loss: 0.10889518850947334, gradient norm: 0.13005197306051777\n",
      "Iteration: 2799000, loss: 0.10889449018310182, gradient norm: 0.03975013245200776\n",
      "Iteration: 2800000, loss: 0.10889406370265015, gradient norm: 0.07688509056519935\n",
      "Iteration: 2801000, loss: 0.10889307232591693, gradient norm: 0.01599974241096272\n",
      "Iteration: 2802000, loss: 0.10889260551221454, gradient norm: 0.14287112801240445\n",
      "Iteration: 2803000, loss: 0.10889198409386978, gradient norm: 0.45118102917987823\n",
      "Iteration: 2804000, loss: 0.10889119319726698, gradient norm: 0.5791857754775479\n",
      "Iteration: 2805000, loss: 0.10889066986638989, gradient norm: 0.03423426436884581\n",
      "Iteration: 2806000, loss: 0.10888992161842344, gradient norm: 0.04433271808879888\n",
      "Iteration: 2807000, loss: 0.10888936710107315, gradient norm: 0.022694771237801305\n",
      "Iteration: 2808000, loss: 0.10888876774333368, gradient norm: 0.0241336490439907\n",
      "Iteration: 2809000, loss: 0.10888801520931744, gradient norm: 0.027015936232399427\n",
      "Iteration: 2810000, loss: 0.1088874108826727, gradient norm: 0.15885762842152437\n",
      "Iteration: 2811000, loss: 0.10888677521256848, gradient norm: 0.019394073496462178\n",
      "Iteration: 2812000, loss: 0.1088861498160143, gradient norm: 0.5660677664066093\n",
      "Iteration: 2813000, loss: 0.10888554911750996, gradient norm: 0.09662793667314237\n",
      "Iteration: 2814000, loss: 0.10888481584404681, gradient norm: 0.02439872153776059\n",
      "Iteration: 2815000, loss: 0.10888416267502046, gradient norm: 0.4144900062261264\n",
      "Iteration: 2816000, loss: 0.1088836597710355, gradient norm: 0.08852082066169951\n",
      "Iteration: 2817000, loss: 0.1088829238485728, gradient norm: 0.02627626893488834\n",
      "Iteration: 2818000, loss: 0.10888223176576853, gradient norm: 0.1316966156014377\n",
      "Iteration: 2819000, loss: 0.10888158014607804, gradient norm: 0.025054516739242503\n",
      "Iteration: 2820000, loss: 0.10888109862051887, gradient norm: 0.09981415798137615\n",
      "Iteration: 2821000, loss: 0.10888027495150672, gradient norm: 0.17331012470091894\n",
      "Iteration: 2822000, loss: 0.10887974608601235, gradient norm: 0.04586179743261038\n",
      "Iteration: 2823000, loss: 0.10887917589840376, gradient norm: 0.09200102857583506\n",
      "Iteration: 2824000, loss: 0.10887829838414324, gradient norm: 0.13071914637278653\n",
      "Iteration: 2825000, loss: 0.1088778888976212, gradient norm: 0.019119994864663595\n",
      "Iteration: 2826000, loss: 0.10887715231272238, gradient norm: 0.002788829556441459\n",
      "Iteration: 2827000, loss: 0.10887652567319357, gradient norm: 0.010306633527544884\n",
      "Iteration: 2828000, loss: 0.1088758980872912, gradient norm: 0.010643217230931085\n",
      "Iteration: 2829000, loss: 0.10887526960077608, gradient norm: 0.010103330085434492\n",
      "Iteration: 2830000, loss: 0.10887460957975116, gradient norm: 0.019639767169273844\n",
      "Iteration: 2831000, loss: 0.10887401028533546, gradient norm: 0.6168912829829701\n",
      "Iteration: 2832000, loss: 0.10887343514776049, gradient norm: 0.07688431893660146\n",
      "Iteration: 2833000, loss: 0.1088725754891076, gradient norm: 0.29197711700545936\n",
      "Iteration: 2834000, loss: 0.10887226973791392, gradient norm: 0.15247085984536937\n",
      "Iteration: 2835000, loss: 0.10887130846601949, gradient norm: 0.1359979881836067\n",
      "Iteration: 2836000, loss: 0.10887078311401134, gradient norm: 0.048168033455359686\n",
      "Iteration: 2837000, loss: 0.10887025533085434, gradient norm: 0.12946216593800303\n",
      "Iteration: 2838000, loss: 0.10886963492043843, gradient norm: 0.07329016171111831\n",
      "Iteration: 2839000, loss: 0.10886873358900007, gradient norm: 0.24151754137029308\n",
      "Iteration: 2840000, loss: 0.10886840165153307, gradient norm: 0.3967175451266972\n",
      "Iteration: 2841000, loss: 0.10886743832580957, gradient norm: 0.06170785268440566\n",
      "Iteration: 2842000, loss: 0.10886709463806271, gradient norm: 0.11615397688501962\n",
      "Iteration: 2843000, loss: 0.10886634741046429, gradient norm: 0.020016683970099463\n",
      "Iteration: 2844000, loss: 0.1088656658402117, gradient norm: 0.07503107707351611\n",
      "Iteration: 2845000, loss: 0.10886514647580688, gradient norm: 0.08358053882603578\n",
      "Iteration: 2846000, loss: 0.10886452371831327, gradient norm: 0.41009489336149546\n",
      "Iteration: 2847000, loss: 0.10886371429032905, gradient norm: 0.1977485562288012\n",
      "Iteration: 2848000, loss: 0.10886323849431903, gradient norm: 0.1932315367943061\n",
      "Iteration: 2849000, loss: 0.1088624572043168, gradient norm: 0.03054304973028877\n",
      "Iteration: 2850000, loss: 0.10886193782196983, gradient norm: 0.04023690963278133\n",
      "Iteration: 2851000, loss: 0.10886130278784259, gradient norm: 0.02383086751230493\n",
      "Iteration: 2852000, loss: 0.10886064881145484, gradient norm: 0.034265539639649165\n",
      "Iteration: 2853000, loss: 0.1088601627025831, gradient norm: 0.034974271949734764\n",
      "Iteration: 2854000, loss: 0.1088593691214876, gradient norm: 0.057244212089665965\n",
      "Iteration: 2855000, loss: 0.10885883073459106, gradient norm: 0.041497172205712994\n",
      "Iteration: 2856000, loss: 0.10885809948431081, gradient norm: 0.2140039513225652\n",
      "Iteration: 2857000, loss: 0.10885742593672013, gradient norm: 0.14309816911807885\n",
      "Iteration: 2858000, loss: 0.10885693510478939, gradient norm: 0.054976781266429256\n",
      "Iteration: 2859000, loss: 0.10885631635092638, gradient norm: 0.032996882794462445\n",
      "Iteration: 2860000, loss: 0.10885563003735321, gradient norm: 0.006101094203534392\n",
      "Iteration: 2861000, loss: 0.108854954703113, gradient norm: 0.07725390068198429\n",
      "Iteration: 2862000, loss: 0.1088543829074946, gradient norm: 0.0683161149329125\n",
      "Iteration: 2863000, loss: 0.10885382714878256, gradient norm: 0.4194136189762349\n",
      "Iteration: 2864000, loss: 0.10885305265131753, gradient norm: 0.021371059582736208\n",
      "Iteration: 2865000, loss: 0.10885249171031824, gradient norm: 0.018254932675976308\n",
      "Iteration: 2866000, loss: 0.10885189640990993, gradient norm: 0.019678208510676594\n",
      "Iteration: 2867000, loss: 0.10885124624443542, gradient norm: 0.04478104261864265\n",
      "Iteration: 2868000, loss: 0.10885066129169525, gradient norm: 0.23941735005421083\n",
      "Iteration: 2869000, loss: 0.1088499578149641, gradient norm: 0.15650021778067194\n",
      "Iteration: 2870000, loss: 0.10884936822798048, gradient norm: 0.07605433588081134\n",
      "Iteration: 2871000, loss: 0.10884865249027771, gradient norm: 0.008588888828017607\n",
      "Iteration: 2872000, loss: 0.1088481002784007, gradient norm: 0.016776736497789865\n",
      "Iteration: 2873000, loss: 0.10884753319336023, gradient norm: 0.0331234362962356\n",
      "Iteration: 2874000, loss: 0.10884687198071308, gradient norm: 0.04454497903686254\n",
      "Iteration: 2875000, loss: 0.1088462691223951, gradient norm: 0.13457453258552768\n",
      "Iteration: 2876000, loss: 0.10884548763204345, gradient norm: 0.046249653561161654\n",
      "Iteration: 2877000, loss: 0.10884513382865098, gradient norm: 0.5590003194722868\n",
      "Iteration: 2878000, loss: 0.10884437222560138, gradient norm: 0.3570117651705253\n",
      "Iteration: 2879000, loss: 0.10884372260008911, gradient norm: 0.0945832296875128\n",
      "Iteration: 2880000, loss: 0.10884315312599614, gradient norm: 0.10211465521506256\n",
      "Iteration: 2881000, loss: 0.10884240232150207, gradient norm: 0.6200536651382047\n",
      "Iteration: 2882000, loss: 0.10884179532805154, gradient norm: 0.37925660591728355\n",
      "Iteration: 2883000, loss: 0.10884125098738084, gradient norm: 0.2597237708049104\n",
      "Iteration: 2884000, loss: 0.10884055409164456, gradient norm: 0.22361206159137315\n",
      "Iteration: 2885000, loss: 0.10883994960612264, gradient norm: 0.05371924610400163\n",
      "Iteration: 2886000, loss: 0.10883935374926894, gradient norm: 0.016596984298787296\n",
      "Iteration: 2887000, loss: 0.10883872190190114, gradient norm: 0.033323262036048636\n",
      "Iteration: 2888000, loss: 0.10883821419843878, gradient norm: 0.3998619846104226\n",
      "Iteration: 2889000, loss: 0.10883744726532377, gradient norm: 0.31591455680028735\n",
      "Iteration: 2890000, loss: 0.10883682620900419, gradient norm: 0.0840877986015453\n",
      "Iteration: 2891000, loss: 0.1088362867364287, gradient norm: 0.009520554672035749\n",
      "Iteration: 2892000, loss: 0.10883563059190035, gradient norm: 0.10023849735683717\n",
      "Iteration: 2893000, loss: 0.10883510194219399, gradient norm: 0.07615851169212302\n",
      "Iteration: 2894000, loss: 0.10883431867221914, gradient norm: 0.05600037150994102\n",
      "Iteration: 2895000, loss: 0.10883377545953034, gradient norm: 0.10832478421930636\n",
      "Iteration: 2896000, loss: 0.10883311109562047, gradient norm: 0.003201269038851146\n",
      "Iteration: 2897000, loss: 0.10883256505308715, gradient norm: 0.03419237807564212\n",
      "Iteration: 2898000, loss: 0.10883183805573687, gradient norm: 0.0927782447313626\n",
      "Iteration: 2899000, loss: 0.10883124390166818, gradient norm: 0.18532518459369407\n",
      "Iteration: 2900000, loss: 0.10883079850911065, gradient norm: 0.04109189545737454\n",
      "Iteration: 2901000, loss: 0.10883000979606074, gradient norm: 0.5226070355376687\n",
      "Iteration: 2902000, loss: 0.10882938991373418, gradient norm: 0.051229827063778995\n",
      "Iteration: 2903000, loss: 0.10882887401870756, gradient norm: 0.02088520316614903\n",
      "Iteration: 2904000, loss: 0.10882812649147104, gradient norm: 0.22876770224640086\n",
      "Iteration: 2905000, loss: 0.10882767420752103, gradient norm: 0.09994769792106256\n",
      "Iteration: 2906000, loss: 0.10882692506552895, gradient norm: 0.26917833975335376\n",
      "Iteration: 2907000, loss: 0.10882630487545507, gradient norm: 0.07963554287405397\n",
      "Iteration: 2908000, loss: 0.1088257914502232, gradient norm: 0.20556292804144385\n",
      "Iteration: 2909000, loss: 0.10882500977914124, gradient norm: 0.2646110381849177\n",
      "Iteration: 2910000, loss: 0.10882449233495481, gradient norm: 0.1364835812618768\n",
      "Iteration: 2911000, loss: 0.10882382243344381, gradient norm: 0.019940327230065284\n",
      "Iteration: 2912000, loss: 0.10882323666379544, gradient norm: 0.0012218324908535336\n",
      "Iteration: 2913000, loss: 0.10882268871511486, gradient norm: 0.09959092378847882\n",
      "Iteration: 2914000, loss: 0.10882207970483333, gradient norm: 0.14262917328734587\n",
      "Iteration: 2915000, loss: 0.10882129323226834, gradient norm: 0.07516706774780962\n",
      "Iteration: 2916000, loss: 0.1088207835895724, gradient norm: 0.280243656433188\n",
      "Iteration: 2917000, loss: 0.10882018610010452, gradient norm: 0.20157650108927444\n",
      "Iteration: 2918000, loss: 0.10881945610448616, gradient norm: 0.009438311495398616\n",
      "Iteration: 2919000, loss: 0.10881894118947814, gradient norm: 0.17523347732373856\n",
      "Iteration: 2920000, loss: 0.10881841777508931, gradient norm: 0.09613127535386981\n",
      "Iteration: 2921000, loss: 0.10881761275394511, gradient norm: 0.040997684066593126\n",
      "Iteration: 2922000, loss: 0.10881703621660042, gradient norm: 0.16408801230856704\n",
      "Iteration: 2923000, loss: 0.10881650955330087, gradient norm: 0.08578549772408069\n",
      "Iteration: 2924000, loss: 0.10881582648723954, gradient norm: 0.09472623053635183\n",
      "Iteration: 2925000, loss: 0.10881515802319669, gradient norm: 0.08911973775728331\n",
      "Iteration: 2926000, loss: 0.10881460966150466, gradient norm: 0.032722543925620164\n",
      "Iteration: 2927000, loss: 0.10881402736263539, gradient norm: 0.019332137512573382\n",
      "Iteration: 2928000, loss: 0.10881335772064807, gradient norm: 0.5294649050868061\n",
      "Iteration: 2929000, loss: 0.10881290935109592, gradient norm: 0.11929181083976409\n",
      "Iteration: 2930000, loss: 0.1088120091410798, gradient norm: 0.004861745628888995\n",
      "Iteration: 2931000, loss: 0.1088116863562305, gradient norm: 0.40179906212984406\n",
      "Iteration: 2932000, loss: 0.10881081459488601, gradient norm: 0.0928153489941858\n",
      "Iteration: 2933000, loss: 0.10881025108698301, gradient norm: 0.18244239645930876\n",
      "Iteration: 2934000, loss: 0.10880974936291306, gradient norm: 0.020990869436676808\n",
      "Iteration: 2935000, loss: 0.10880914800490682, gradient norm: 0.27202512008130103\n",
      "Iteration: 2936000, loss: 0.10880846182682559, gradient norm: 0.22021671301132698\n",
      "Iteration: 2937000, loss: 0.10880789022636735, gradient norm: 0.002049784589847991\n",
      "Iteration: 2938000, loss: 0.1088072010137289, gradient norm: 0.03303171016292584\n",
      "Iteration: 2939000, loss: 0.10880670743131335, gradient norm: 0.14510054112329768\n",
      "Iteration: 2940000, loss: 0.10880600574378464, gradient norm: 0.46214543265998304\n",
      "Iteration: 2941000, loss: 0.1088054675935674, gradient norm: 0.026286804777724285\n",
      "Iteration: 2942000, loss: 0.10880484193017939, gradient norm: 0.0641070412021734\n",
      "Iteration: 2943000, loss: 0.10880415360496187, gradient norm: 0.05949815868450485\n",
      "Iteration: 2944000, loss: 0.10880357819864156, gradient norm: 0.24578115857852195\n",
      "Iteration: 2945000, loss: 0.1088029255346593, gradient norm: 0.11740833798457576\n",
      "Iteration: 2946000, loss: 0.10880243295426735, gradient norm: 0.05005816298111004\n",
      "Iteration: 2947000, loss: 0.10880163488040778, gradient norm: 0.04618221542951781\n",
      "Iteration: 2948000, loss: 0.10880116017114486, gradient norm: 0.006308790855001917\n",
      "Iteration: 2949000, loss: 0.1088006403805907, gradient norm: 0.32370444545494836\n",
      "Iteration: 2950000, loss: 0.10879986467653088, gradient norm: 0.04001899104648989\n",
      "Iteration: 2951000, loss: 0.1087993722165738, gradient norm: 0.11959713290695922\n",
      "Iteration: 2952000, loss: 0.10879863817779548, gradient norm: 0.36482719775694356\n",
      "Iteration: 2953000, loss: 0.1087980345851544, gradient norm: 0.03470427193569813\n",
      "Iteration: 2954000, loss: 0.1087975523267748, gradient norm: 0.021915525252488223\n",
      "Iteration: 2955000, loss: 0.108796787742941, gradient norm: 0.30233394841846867\n",
      "Iteration: 2956000, loss: 0.10879634045804153, gradient norm: 0.006390669467682458\n",
      "Iteration: 2957000, loss: 0.10879562495595078, gradient norm: 0.15608166757720235\n",
      "Iteration: 2958000, loss: 0.10879508049307217, gradient norm: 0.19797706992083683\n",
      "Iteration: 2959000, loss: 0.10879439959975006, gradient norm: 0.08722400234234823\n",
      "Iteration: 2960000, loss: 0.10879388423582105, gradient norm: 0.0964593947416939\n",
      "Iteration: 2961000, loss: 0.10879318106215391, gradient norm: 0.05293799773974666\n",
      "Iteration: 2962000, loss: 0.10879256890116049, gradient norm: 0.018950234307230623\n",
      "Iteration: 2963000, loss: 0.10879202874257336, gradient norm: 0.2107971112077082\n",
      "Iteration: 2964000, loss: 0.10879138811170543, gradient norm: 0.1119372974892759\n",
      "Iteration: 2965000, loss: 0.1087908167725105, gradient norm: 0.010338692202441461\n",
      "Iteration: 2966000, loss: 0.10879020634851347, gradient norm: 0.021896705483543222\n",
      "Iteration: 2967000, loss: 0.10878968763375577, gradient norm: 0.27856972872013747\n",
      "Iteration: 2968000, loss: 0.10878891799035251, gradient norm: 0.06991717409089533\n",
      "Iteration: 2969000, loss: 0.10878836314322271, gradient norm: 0.14674031349317687\n",
      "Iteration: 2970000, loss: 0.10878777433889282, gradient norm: 0.04797447818990568\n",
      "Iteration: 2971000, loss: 0.10878715696889947, gradient norm: 0.040498206208668286\n",
      "Iteration: 2972000, loss: 0.10878650988015619, gradient norm: 0.15088238716029231\n",
      "Iteration: 2973000, loss: 0.10878600673145414, gradient norm: 0.5024559700952218\n",
      "Iteration: 2974000, loss: 0.10878527646608191, gradient norm: 0.008158732006977702\n",
      "Iteration: 2975000, loss: 0.10878483917017819, gradient norm: 0.058413720253415395\n",
      "Iteration: 2976000, loss: 0.1087840813835612, gradient norm: 0.614041181542274\n",
      "Iteration: 2977000, loss: 0.1087835360263328, gradient norm: 0.09527328024773858\n",
      "Iteration: 2978000, loss: 0.10878282701404052, gradient norm: 0.263253718976198\n",
      "Iteration: 2979000, loss: 0.10878239449549049, gradient norm: 0.026453984370451673\n",
      "Iteration: 2980000, loss: 0.10878169798354301, gradient norm: 0.024149334634009707\n",
      "Iteration: 2981000, loss: 0.10878106128776537, gradient norm: 0.04864214757679573\n",
      "Iteration: 2982000, loss: 0.10878070885541052, gradient norm: 0.27183826584775\n",
      "Iteration: 2983000, loss: 0.10877979503926352, gradient norm: 0.005733299599890893\n",
      "Iteration: 2984000, loss: 0.10877935913075375, gradient norm: 0.00885602248831641\n",
      "Iteration: 2985000, loss: 0.10877868164950066, gradient norm: 0.04166991018237779\n",
      "Iteration: 2986000, loss: 0.10877824075943099, gradient norm: 0.23610872889902393\n",
      "Iteration: 2987000, loss: 0.10877747297908727, gradient norm: 0.20403270268513102\n",
      "Iteration: 2988000, loss: 0.10877695862563684, gradient norm: 0.055912846517501974\n",
      "Iteration: 2989000, loss: 0.10877628616764204, gradient norm: 0.2634077081393708\n",
      "Iteration: 2990000, loss: 0.10877568360095177, gradient norm: 0.05056448550284904\n",
      "Iteration: 2991000, loss: 0.10877506919889188, gradient norm: 0.4520583410045254\n",
      "Iteration: 2992000, loss: 0.10877436809429357, gradient norm: 0.3202101522993912\n",
      "Iteration: 2993000, loss: 0.10877392142176075, gradient norm: 0.3444400381370997\n",
      "Iteration: 2994000, loss: 0.10877335389223994, gradient norm: 0.28966035284183655\n",
      "Iteration: 2995000, loss: 0.10877256424769491, gradient norm: 0.12417862240413659\n",
      "Iteration: 2996000, loss: 0.1087721076006531, gradient norm: 0.015512199466846241\n",
      "Iteration: 2997000, loss: 0.1087714375492026, gradient norm: 0.4092953663698353\n",
      "Iteration: 2998000, loss: 0.10877091556811817, gradient norm: 0.275802974082088\n",
      "Iteration: 2999000, loss: 0.1087701810327915, gradient norm: 0.07703540390446485\n",
      "Iteration: 3000000, loss: 0.10876974674502556, gradient norm: 0.2973033935719348\n",
      "Iteration: 3001000, loss: 0.10876901766622171, gradient norm: 0.10545513266993707\n",
      "Iteration: 3002000, loss: 0.10876836694445229, gradient norm: 0.21555270253706818\n",
      "Iteration: 3003000, loss: 0.10876793987703255, gradient norm: 0.01913508976889378\n",
      "Iteration: 3004000, loss: 0.10876727974461892, gradient norm: 0.03988529520079564\n",
      "Iteration: 3005000, loss: 0.1087666175946097, gradient norm: 0.04162835569743516\n",
      "Iteration: 3006000, loss: 0.10876607521499278, gradient norm: 0.06166150629413317\n",
      "Iteration: 3007000, loss: 0.1087654683874966, gradient norm: 0.06711384957402267\n",
      "Iteration: 3008000, loss: 0.10876477380087761, gradient norm: 0.02043548933648298\n",
      "Iteration: 3009000, loss: 0.10876423076663738, gradient norm: 0.0033378344987646893\n",
      "Iteration: 3010000, loss: 0.1087636781905587, gradient norm: 0.040302354090786995\n",
      "Iteration: 3011000, loss: 0.10876308459052825, gradient norm: 0.25435638709806385\n",
      "Iteration: 3012000, loss: 0.10876243002091687, gradient norm: 0.294967348493801\n",
      "Iteration: 3013000, loss: 0.10876175479730836, gradient norm: 0.03627526429110813\n",
      "Iteration: 3014000, loss: 0.1087611761041939, gradient norm: 0.0850200755837328\n",
      "Iteration: 3015000, loss: 0.10876058446189038, gradient norm: 0.1875341368582427\n",
      "Iteration: 3016000, loss: 0.1087601309498842, gradient norm: 0.10254747543400493\n",
      "Iteration: 3017000, loss: 0.10875946225747962, gradient norm: 0.02762550153099243\n",
      "Iteration: 3018000, loss: 0.10875869071063034, gradient norm: 0.01209657500459646\n",
      "Iteration: 3019000, loss: 0.10875829514309883, gradient norm: 0.038405327874905845\n",
      "Iteration: 3020000, loss: 0.1087575866472742, gradient norm: 0.09208110719595718\n",
      "Iteration: 3021000, loss: 0.10875703070031477, gradient norm: 0.0710682385409422\n",
      "Iteration: 3022000, loss: 0.10875631862912419, gradient norm: 0.11027533313641268\n",
      "Iteration: 3023000, loss: 0.1087560090354849, gradient norm: 0.07224270395708063\n",
      "Iteration: 3024000, loss: 0.10875509111056321, gradient norm: 0.01869174152135714\n",
      "Iteration: 3025000, loss: 0.1087547572492868, gradient norm: 0.03279379480410381\n",
      "Iteration: 3026000, loss: 0.10875395661363663, gradient norm: 0.37388384725640544\n",
      "Iteration: 3027000, loss: 0.10875337713160836, gradient norm: 0.01598701093967748\n",
      "Iteration: 3028000, loss: 0.10875286544925246, gradient norm: 0.23774568898448153\n",
      "Iteration: 3029000, loss: 0.10875223883037398, gradient norm: 0.05514987586704791\n",
      "Iteration: 3030000, loss: 0.10875159442193598, gradient norm: 0.36859702605200706\n",
      "Iteration: 3031000, loss: 0.1087508908154695, gradient norm: 0.11521618992176198\n",
      "Iteration: 3032000, loss: 0.10875059019568388, gradient norm: 0.22434357792879178\n",
      "Iteration: 3033000, loss: 0.10874974563955138, gradient norm: 0.06504976764991882\n",
      "Iteration: 3034000, loss: 0.10874927368101826, gradient norm: 0.1256837473586205\n",
      "Iteration: 3035000, loss: 0.10874850028498556, gradient norm: 0.11164814061340363\n",
      "Iteration: 3036000, loss: 0.10874802093837548, gradient norm: 0.058032407584612526\n",
      "Iteration: 3037000, loss: 0.10874760652554087, gradient norm: 0.06356051212916021\n",
      "Iteration: 3038000, loss: 0.10874681246658638, gradient norm: 0.3928133463063875\n",
      "Iteration: 3039000, loss: 0.10874603928749232, gradient norm: 0.07967766173234576\n",
      "Iteration: 3040000, loss: 0.10874562068294584, gradient norm: 0.013705249759180941\n",
      "Iteration: 3041000, loss: 0.10874513693198716, gradient norm: 0.21067056731035205\n",
      "Iteration: 3042000, loss: 0.10874428692242791, gradient norm: 0.03169075300153723\n",
      "Iteration: 3043000, loss: 0.10874384231485121, gradient norm: 0.011526926767184746\n",
      "Iteration: 3044000, loss: 0.10874323742469258, gradient norm: 0.026532335417010867\n",
      "Iteration: 3045000, loss: 0.10874268949313458, gradient norm: 0.2281805413009355\n",
      "Iteration: 3046000, loss: 0.10874196156470156, gradient norm: 0.07984371005009708\n",
      "Iteration: 3047000, loss: 0.10874130675646802, gradient norm: 0.00910176574759077\n",
      "Iteration: 3048000, loss: 0.1087407710374639, gradient norm: 0.01347588019058346\n",
      "Iteration: 3049000, loss: 0.10874023874356387, gradient norm: 0.059382356612091486\n",
      "Iteration: 3050000, loss: 0.10873970860407567, gradient norm: 0.014064029467418745\n",
      "Iteration: 3051000, loss: 0.10873891252739923, gradient norm: 0.012413774645875274\n",
      "Iteration: 3052000, loss: 0.10873843938147645, gradient norm: 0.024248611356351688\n",
      "Iteration: 3053000, loss: 0.10873773542930404, gradient norm: 0.01427698924654652\n",
      "Iteration: 3054000, loss: 0.10873720643889329, gradient norm: 0.1627233780140507\n",
      "Iteration: 3055000, loss: 0.10873650029327178, gradient norm: 0.07464339736247895\n",
      "Iteration: 3056000, loss: 0.10873595714281835, gradient norm: 0.24674114148060503\n",
      "Iteration: 3057000, loss: 0.10873547994409961, gradient norm: 0.026519901960836322\n",
      "Iteration: 3058000, loss: 0.10873456009008217, gradient norm: 0.19030869479893647\n",
      "Iteration: 3059000, loss: 0.10873415272747544, gradient norm: 0.034847015800245636\n",
      "Iteration: 3060000, loss: 0.10873344651772429, gradient norm: 0.011213387667026715\n",
      "Iteration: 3061000, loss: 0.10873289745768784, gradient norm: 0.005426798147455684\n",
      "Iteration: 3062000, loss: 0.10873239223644654, gradient norm: 0.39870919310368\n",
      "Iteration: 3063000, loss: 0.10873152957553128, gradient norm: 0.22741400910052678\n",
      "Iteration: 3064000, loss: 0.10873110322870627, gradient norm: 0.01904469427319009\n",
      "Iteration: 3065000, loss: 0.10873042967989334, gradient norm: 0.03359264233692474\n",
      "Iteration: 3066000, loss: 0.10872975386324513, gradient norm: 0.06534042363370833\n",
      "Iteration: 3067000, loss: 0.10872913890518629, gradient norm: 0.06677080370291547\n",
      "Iteration: 3068000, loss: 0.10872850285965029, gradient norm: 0.014218249491815254\n",
      "Iteration: 3069000, loss: 0.10872795145372771, gradient norm: 0.0273162247414635\n",
      "Iteration: 3070000, loss: 0.10872714077687172, gradient norm: 0.18942882156950977\n",
      "Iteration: 3071000, loss: 0.10872640826324104, gradient norm: 0.21447126433314687\n",
      "Iteration: 3072000, loss: 0.10872566286341347, gradient norm: 0.04880260443246222\n",
      "Iteration: 3073000, loss: 0.10872494134840449, gradient norm: 0.013840598747050771\n",
      "Iteration: 3074000, loss: 0.10872383414541364, gradient norm: 0.4769915207409916\n",
      "Iteration: 3075000, loss: 0.10872254997152968, gradient norm: 0.016741740110651618\n",
      "Iteration: 3076000, loss: 0.10872141422875505, gradient norm: 0.04107872094917926\n",
      "Iteration: 3077000, loss: 0.10872020702646089, gradient norm: 0.12959845438202452\n",
      "Iteration: 3078000, loss: 0.10871903696454055, gradient norm: 0.01109115696305613\n",
      "Iteration: 3079000, loss: 0.10871825328562065, gradient norm: 0.11567323387269726\n",
      "Iteration: 3080000, loss: 0.10871715682800936, gradient norm: 0.4098405259554674\n",
      "Iteration: 3081000, loss: 0.10871618980238404, gradient norm: 0.006969205844309761\n",
      "Iteration: 3082000, loss: 0.10871548324155296, gradient norm: 0.36346951848358944\n",
      "Iteration: 3083000, loss: 0.10871457418047925, gradient norm: 0.2235377963193001\n",
      "Iteration: 3084000, loss: 0.10871371714540035, gradient norm: 0.0019513081688574477\n",
      "Iteration: 3085000, loss: 0.10871309718473245, gradient norm: 0.3087646085248806\n",
      "Iteration: 3086000, loss: 0.10871205884863301, gradient norm: 0.011426914740735596\n",
      "Iteration: 3087000, loss: 0.10871155015768824, gradient norm: 0.0893339764977743\n",
      "Iteration: 3088000, loss: 0.10871066187272622, gradient norm: 0.3910750229388243\n",
      "Iteration: 3089000, loss: 0.10870997957284699, gradient norm: 0.11233506280171733\n",
      "Iteration: 3090000, loss: 0.10870915349611146, gradient norm: 0.11240202887495128\n",
      "Iteration: 3091000, loss: 0.10870850238306505, gradient norm: 0.228705570889604\n",
      "Iteration: 3092000, loss: 0.1087076774369607, gradient norm: 0.5020617568984089\n",
      "Iteration: 3093000, loss: 0.1087069934315139, gradient norm: 0.015312271429419908\n",
      "Iteration: 3094000, loss: 0.10870624966583753, gradient norm: 0.600396610715628\n",
      "Iteration: 3095000, loss: 0.10870553566876989, gradient norm: 0.010027051333233812\n",
      "Iteration: 3096000, loss: 0.10870486549566621, gradient norm: 0.00022071718493177285\n",
      "Iteration: 3097000, loss: 0.10870423053001817, gradient norm: 0.21566709355546412\n",
      "Iteration: 3098000, loss: 0.1087033209045809, gradient norm: 0.16467363291860906\n",
      "Iteration: 3099000, loss: 0.1087027740026513, gradient norm: 0.01299940349499262\n",
      "Iteration: 3100000, loss: 0.10870204768748858, gradient norm: 0.48423476036185464\n",
      "Iteration: 3101000, loss: 0.10870130858364399, gradient norm: 0.244262290771311\n",
      "Iteration: 3102000, loss: 0.10870067736744492, gradient norm: 0.03842131626155427\n",
      "Iteration: 3103000, loss: 0.10869985733993268, gradient norm: 0.016384707967228015\n",
      "Iteration: 3104000, loss: 0.10869925277910716, gradient norm: 0.015134418106635203\n",
      "Iteration: 3105000, loss: 0.1086987077138252, gradient norm: 0.22067926262351908\n",
      "Iteration: 3106000, loss: 0.1086978324703328, gradient norm: 0.14104611945612203\n",
      "Iteration: 3107000, loss: 0.1086972648969705, gradient norm: 0.4693786658249602\n",
      "Iteration: 3108000, loss: 0.10869643162280107, gradient norm: 0.7355588227369738\n",
      "Iteration: 3109000, loss: 0.10869571141133227, gradient norm: 0.019699645246027717\n",
      "Iteration: 3110000, loss: 0.10869523453603398, gradient norm: 0.12347858207073979\n",
      "Iteration: 3111000, loss: 0.10869431874108587, gradient norm: 0.05671997408970139\n",
      "Iteration: 3112000, loss: 0.10869393555896179, gradient norm: 0.07293165613747463\n",
      "Iteration: 3113000, loss: 0.10869290180141135, gradient norm: 0.1559617485644026\n",
      "Iteration: 3114000, loss: 0.10869229562085105, gradient norm: 0.033176189934177744\n",
      "Iteration: 3115000, loss: 0.1086918227971853, gradient norm: 0.18366365240840044\n",
      "Iteration: 3116000, loss: 0.10869091593349836, gradient norm: 0.09653544556512013\n",
      "Iteration: 3117000, loss: 0.10869036745257216, gradient norm: 0.030685092297745645\n",
      "Iteration: 3118000, loss: 0.10868967660560162, gradient norm: 0.5487800259078807\n",
      "Iteration: 3119000, loss: 0.10868904591008365, gradient norm: 0.06395192638032264\n",
      "Iteration: 3120000, loss: 0.1086882506210631, gradient norm: 0.06797388322135187\n",
      "Iteration: 3121000, loss: 0.10868754819261656, gradient norm: 0.03226018688402041\n",
      "Iteration: 3122000, loss: 0.10868701694439319, gradient norm: 0.13080804841510962\n",
      "Iteration: 3123000, loss: 0.1086862866561024, gradient norm: 0.1478781868613735\n",
      "Iteration: 3124000, loss: 0.10868562886151094, gradient norm: 0.6570857777204727\n",
      "Iteration: 3125000, loss: 0.10868501354023306, gradient norm: 0.3281138414576044\n",
      "Iteration: 3126000, loss: 0.10868420672213236, gradient norm: 0.09626352854647029\n",
      "Iteration: 3127000, loss: 0.10868380769007005, gradient norm: 0.13016266888708652\n",
      "Iteration: 3128000, loss: 0.10868292496760176, gradient norm: 0.12262612524788061\n",
      "Iteration: 3129000, loss: 0.10868222753041971, gradient norm: 0.026773155091796532\n",
      "Iteration: 3130000, loss: 0.10868151855634409, gradient norm: 0.007800636587695338\n",
      "Iteration: 3131000, loss: 0.10868114555066152, gradient norm: 0.07695050618293817\n",
      "Iteration: 3132000, loss: 0.10868024571043446, gradient norm: 0.16337628743469398\n",
      "Iteration: 3133000, loss: 0.10867948566114426, gradient norm: 0.031515802353359317\n",
      "Iteration: 3134000, loss: 0.1086790449151604, gradient norm: 0.10075218844930393\n",
      "Iteration: 3135000, loss: 0.10867828106916838, gradient norm: 0.6170705574619842\n",
      "Iteration: 3136000, loss: 0.10867756460978115, gradient norm: 0.008515400887659915\n",
      "Iteration: 3137000, loss: 0.10867706654456484, gradient norm: 0.06314652060893476\n",
      "Iteration: 3138000, loss: 0.1086762458639873, gradient norm: 0.3035358026879348\n",
      "Iteration: 3139000, loss: 0.10867565999378487, gradient norm: 0.0139535987725759\n",
      "Iteration: 3140000, loss: 0.1086749928967919, gradient norm: 0.07459290851389369\n",
      "Iteration: 3141000, loss: 0.10867430149571422, gradient norm: 0.2970972093162468\n",
      "Iteration: 3142000, loss: 0.10867364402389292, gradient norm: 0.026041086428474906\n",
      "Iteration: 3143000, loss: 0.10867298150905935, gradient norm: 0.016294493343067318\n",
      "Iteration: 3144000, loss: 0.10867244127406563, gradient norm: 0.16696544098753405\n",
      "Iteration: 3145000, loss: 0.1086716389682932, gradient norm: 0.013471418569580756\n",
      "Iteration: 3146000, loss: 0.10867087992492126, gradient norm: 0.05609610697113602\n",
      "Iteration: 3147000, loss: 0.10867035867887964, gradient norm: 0.0055596132717577705\n",
      "Iteration: 3148000, loss: 0.10866977377274253, gradient norm: 0.08060570024931216\n",
      "Iteration: 3149000, loss: 0.10866888666721261, gradient norm: 0.011499995508893274\n",
      "Iteration: 3150000, loss: 0.1086683477382697, gradient norm: 0.07454466065127818\n",
      "Iteration: 3151000, loss: 0.10866758483586389, gradient norm: 0.034180223403779374\n",
      "Iteration: 3152000, loss: 0.1086669788196885, gradient norm: 0.10523940970106141\n",
      "Iteration: 3153000, loss: 0.10866613819418088, gradient norm: 0.35510523889843126\n",
      "Iteration: 3154000, loss: 0.10866543332870258, gradient norm: 0.02086471182426953\n",
      "Iteration: 3155000, loss: 0.10866467214259434, gradient norm: 0.050584425623630684\n",
      "Iteration: 3156000, loss: 0.10866379565053248, gradient norm: 0.47602085864185406\n",
      "Iteration: 3157000, loss: 0.10866291846648425, gradient norm: 0.507524580413817\n",
      "Iteration: 3158000, loss: 0.10866162269922655, gradient norm: 0.02015299917635723\n",
      "Iteration: 3159000, loss: 0.10866057156180253, gradient norm: 0.7177321912559966\n",
      "Iteration: 3160000, loss: 0.10865949398442082, gradient norm: 0.04333138079619386\n",
      "Iteration: 3161000, loss: 0.10865858066191975, gradient norm: 0.007421303682284514\n",
      "Iteration: 3162000, loss: 0.10865771445633954, gradient norm: 0.06639664268432009\n",
      "Iteration: 3163000, loss: 0.10865681388669182, gradient norm: 0.08997778334160925\n",
      "Iteration: 3164000, loss: 0.10865586807923261, gradient norm: 0.11655581084005727\n",
      "Iteration: 3165000, loss: 0.10865501285681524, gradient norm: 0.11646416472243314\n",
      "Iteration: 3166000, loss: 0.1086542961997875, gradient norm: 0.02835682014753948\n",
      "Iteration: 3167000, loss: 0.10865348296590135, gradient norm: 0.03662959818045308\n",
      "Iteration: 3168000, loss: 0.10865276824110223, gradient norm: 0.5118062823500112\n",
      "Iteration: 3169000, loss: 0.10865199630110377, gradient norm: 0.012105665257173142\n",
      "Iteration: 3170000, loss: 0.10865123242626801, gradient norm: 0.009045409012713654\n",
      "Iteration: 3171000, loss: 0.10865051546721809, gradient norm: 0.010758207662097183\n",
      "Iteration: 3172000, loss: 0.10864971108645452, gradient norm: 0.03773684882800008\n",
      "Iteration: 3173000, loss: 0.10864919170792382, gradient norm: 0.06751421048111034\n",
      "Iteration: 3174000, loss: 0.10864852131658985, gradient norm: 0.03192457016011176\n",
      "Iteration: 3175000, loss: 0.10864765206960916, gradient norm: 0.17086844841856985\n",
      "Iteration: 3176000, loss: 0.10864692702212576, gradient norm: 0.254023152544437\n",
      "Iteration: 3177000, loss: 0.10864623533561135, gradient norm: 0.03357893598839771\n",
      "Iteration: 3178000, loss: 0.10864561697864143, gradient norm: 0.05915390499064971\n",
      "Iteration: 3179000, loss: 0.10864504555557317, gradient norm: 0.03948570028214708\n",
      "Iteration: 3180000, loss: 0.10864420329453985, gradient norm: 0.03027894366060238\n",
      "Iteration: 3181000, loss: 0.10864369372514354, gradient norm: 0.24229207087326954\n",
      "Iteration: 3182000, loss: 0.10864292374591865, gradient norm: 0.015291222924574046\n",
      "Iteration: 3183000, loss: 0.10864222934868334, gradient norm: 0.2291566435211258\n",
      "Iteration: 3184000, loss: 0.10864175608744671, gradient norm: 0.214131217935965\n",
      "Iteration: 3185000, loss: 0.10864094209323996, gradient norm: 0.009200961952570164\n",
      "Iteration: 3186000, loss: 0.10864037369412004, gradient norm: 0.001628118249059711\n",
      "Iteration: 3187000, loss: 0.10863975363378779, gradient norm: 0.6832967396048693\n",
      "Iteration: 3188000, loss: 0.10863919206692145, gradient norm: 0.08045649537812447\n",
      "Iteration: 3189000, loss: 0.10863842946717639, gradient norm: 0.29738368952236355\n",
      "Iteration: 3190000, loss: 0.10863768985923436, gradient norm: 0.3422673885721362\n",
      "Iteration: 3191000, loss: 0.10863729946813377, gradient norm: 0.042731950078983644\n",
      "Iteration: 3192000, loss: 0.10863650377706671, gradient norm: 0.023634613574547735\n",
      "Iteration: 3193000, loss: 0.10863585157765962, gradient norm: 0.003540208240497044\n",
      "Iteration: 3194000, loss: 0.10863523010046697, gradient norm: 0.31392005390848565\n",
      "Iteration: 3195000, loss: 0.10863471832413947, gradient norm: 0.1252602909989296\n",
      "Iteration: 3196000, loss: 0.10863403903781563, gradient norm: 0.25267100758441124\n",
      "Iteration: 3197000, loss: 0.10863326437339413, gradient norm: 0.047616920969620766\n",
      "Iteration: 3198000, loss: 0.10863282348369808, gradient norm: 0.052712864286059764\n",
      "Iteration: 3199000, loss: 0.10863206576208466, gradient norm: 0.30267353971169253\n",
      "Iteration: 3200000, loss: 0.10863162821627148, gradient norm: 0.10660045598572265\n",
      "Iteration: 3201000, loss: 0.10863076828953606, gradient norm: 0.03139584091125076\n",
      "Iteration: 3202000, loss: 0.10863029254674887, gradient norm: 0.013992852464907484\n",
      "Iteration: 3203000, loss: 0.10862985692065703, gradient norm: 0.0940218431214713\n",
      "Iteration: 3204000, loss: 0.10862889556658445, gradient norm: 0.09318590970243044\n",
      "Iteration: 3205000, loss: 0.10862845658668177, gradient norm: 0.5085973430946195\n",
      "Iteration: 3206000, loss: 0.10862783676571024, gradient norm: 0.00967573393599036\n",
      "Iteration: 3207000, loss: 0.10862718281033426, gradient norm: 0.04484774846591604\n",
      "Iteration: 3208000, loss: 0.10862664122590213, gradient norm: 0.08202756924767403\n",
      "Iteration: 3209000, loss: 0.10862601235867675, gradient norm: 0.2106117637687881\n",
      "Iteration: 3210000, loss: 0.10862521733078938, gradient norm: 0.32420265390057246\n",
      "Iteration: 3211000, loss: 0.1086248001320238, gradient norm: 0.4958786072667219\n",
      "Iteration: 3212000, loss: 0.10862398843737243, gradient norm: 0.23172564596498973\n",
      "Iteration: 3213000, loss: 0.10862372780567663, gradient norm: 0.14094150953152668\n",
      "Iteration: 3214000, loss: 0.1086228445104718, gradient norm: 0.1491786779249616\n",
      "Iteration: 3215000, loss: 0.10862228371105381, gradient norm: 0.5229720830889589\n",
      "Iteration: 3216000, loss: 0.10862180227014034, gradient norm: 0.18302553007329822\n",
      "Iteration: 3217000, loss: 0.10862101856594113, gradient norm: 0.047873455127160194\n",
      "Iteration: 3218000, loss: 0.1086203722016219, gradient norm: 0.04957235417347578\n",
      "Iteration: 3219000, loss: 0.10861987891409862, gradient norm: 0.43674763413848056\n",
      "Iteration: 3220000, loss: 0.1086192287556625, gradient norm: 0.09864774094070876\n",
      "Iteration: 3221000, loss: 0.10861864224121967, gradient norm: 0.026410414572749442\n",
      "Iteration: 3222000, loss: 0.10861799731254361, gradient norm: 0.12557586912800522\n",
      "Iteration: 3223000, loss: 0.10861767887729129, gradient norm: 0.2512514870710884\n",
      "Iteration: 3224000, loss: 0.10861680784920918, gradient norm: 0.09027001842362978\n",
      "Iteration: 3225000, loss: 0.10861615265575933, gradient norm: 0.2858547605185702\n",
      "Iteration: 3226000, loss: 0.10861554141571828, gradient norm: 0.3859223487164971\n",
      "Iteration: 3227000, loss: 0.10861511234434525, gradient norm: 0.45578775705994845\n",
      "Iteration: 3228000, loss: 0.10861444310062976, gradient norm: 0.14603432391667806\n",
      "Iteration: 3229000, loss: 0.10861384900916954, gradient norm: 0.3357534241040687\n",
      "Iteration: 3230000, loss: 0.10861327165670397, gradient norm: 0.12051764863178929\n",
      "Iteration: 3231000, loss: 0.10861244553622838, gradient norm: 0.009995223265590717\n",
      "Iteration: 3232000, loss: 0.10861207662666927, gradient norm: 0.07749040703683888\n",
      "Iteration: 3233000, loss: 0.10861140364938425, gradient norm: 0.10007226279537362\n",
      "Iteration: 3234000, loss: 0.10861075465025977, gradient norm: 0.053200020953796945\n",
      "Iteration: 3235000, loss: 0.10861025910745627, gradient norm: 0.29116341060753614\n",
      "Iteration: 3236000, loss: 0.10860957484951941, gradient norm: 0.5279189890938221\n",
      "Iteration: 3237000, loss: 0.10860900169626082, gradient norm: 0.045661896596885576\n",
      "Iteration: 3238000, loss: 0.10860842122019142, gradient norm: 0.06212419609279225\n",
      "Iteration: 3239000, loss: 0.10860776844721003, gradient norm: 0.3975394668389528\n",
      "Iteration: 3240000, loss: 0.10860723773917491, gradient norm: 0.17468637429514114\n",
      "Iteration: 3241000, loss: 0.10860659110364645, gradient norm: 0.11650231268178474\n",
      "Iteration: 3242000, loss: 0.10860604461344434, gradient norm: 0.06756755393740321\n",
      "Iteration: 3243000, loss: 0.10860544315507127, gradient norm: 0.17800360853553948\n",
      "Iteration: 3244000, loss: 0.10860467828768046, gradient norm: 0.04006167574015941\n",
      "Iteration: 3245000, loss: 0.10860415129533549, gradient norm: 0.031234530398976536\n",
      "Iteration: 3246000, loss: 0.10860378635006754, gradient norm: 0.18151979315969605\n",
      "Iteration: 3247000, loss: 0.10860288638247527, gradient norm: 0.4469791362060578\n",
      "Iteration: 3248000, loss: 0.10860239639742508, gradient norm: 0.014198160113330036\n",
      "Iteration: 3249000, loss: 0.1086018113533502, gradient norm: 0.062571266854516\n",
      "Iteration: 3250000, loss: 0.10860125872876523, gradient norm: 0.029658518788802296\n",
      "Iteration: 3251000, loss: 0.10860058703075344, gradient norm: 0.007042342432956834\n",
      "Iteration: 3252000, loss: 0.10860000225667303, gradient norm: 0.05184715242871266\n",
      "Iteration: 3253000, loss: 0.10859942564936699, gradient norm: 0.3031787907582049\n",
      "Iteration: 3254000, loss: 0.1085988263380717, gradient norm: 0.02820375574041742\n",
      "Iteration: 3255000, loss: 0.10859828146997438, gradient norm: 0.09498151681764176\n",
      "Iteration: 3256000, loss: 0.10859759914234472, gradient norm: 0.06259292790889688\n",
      "Iteration: 3257000, loss: 0.10859702285221778, gradient norm: 0.1181689624620517\n",
      "Iteration: 3258000, loss: 0.10859647229684004, gradient norm: 0.23098935036805504\n",
      "Iteration: 3259000, loss: 0.10859577312665075, gradient norm: 0.019832224838465213\n",
      "Iteration: 3260000, loss: 0.1085952930355984, gradient norm: 0.026275171734356517\n",
      "Iteration: 3261000, loss: 0.10859459608425945, gradient norm: 0.023540279526045597\n",
      "Iteration: 3262000, loss: 0.10859409919510568, gradient norm: 0.5926484623643105\n",
      "Iteration: 3263000, loss: 0.10859342974560868, gradient norm: 0.02774951536784179\n",
      "Iteration: 3264000, loss: 0.10859282664593434, gradient norm: 0.3395675538408466\n",
      "Iteration: 3265000, loss: 0.10859236255650986, gradient norm: 0.058149029252334415\n",
      "Iteration: 3266000, loss: 0.10859157503770316, gradient norm: 0.04181839348522645\n",
      "Iteration: 3267000, loss: 0.10859105383340344, gradient norm: 0.05607737090925031\n",
      "Iteration: 3268000, loss: 0.10859050353285539, gradient norm: 0.03975531687905386\n",
      "Iteration: 3269000, loss: 0.10858998341618266, gradient norm: 0.09195065406623124\n",
      "Iteration: 3270000, loss: 0.10858921306428863, gradient norm: 0.3002213439131551\n",
      "Iteration: 3271000, loss: 0.10858872252564807, gradient norm: 0.024770149150179846\n",
      "Iteration: 3272000, loss: 0.10858803338919958, gradient norm: 0.10348160563425193\n",
      "Iteration: 3273000, loss: 0.10858746627635658, gradient norm: 0.02279058023422862\n",
      "Iteration: 3274000, loss: 0.10858695006847459, gradient norm: 0.05114914876143494\n",
      "Iteration: 3275000, loss: 0.10858621447093011, gradient norm: 0.038117315010010394\n",
      "Iteration: 3276000, loss: 0.10858569028614808, gradient norm: 0.2763146751175614\n",
      "Iteration: 3277000, loss: 0.10858509438771125, gradient norm: 0.013695512564642276\n",
      "Iteration: 3278000, loss: 0.1085844774547568, gradient norm: 0.010657645936296886\n",
      "Iteration: 3279000, loss: 0.10858396789379128, gradient norm: 0.022073981469174435\n",
      "Iteration: 3280000, loss: 0.10858327577387775, gradient norm: 0.034198219339492596\n",
      "Iteration: 3281000, loss: 0.10858283708285309, gradient norm: 0.04565719711952855\n",
      "Iteration: 3282000, loss: 0.10858222042639915, gradient norm: 0.36296472237910066\n",
      "Iteration: 3283000, loss: 0.10858134393602512, gradient norm: 0.029664208849939918\n",
      "Iteration: 3284000, loss: 0.10858091907483079, gradient norm: 0.09032374005881602\n",
      "Iteration: 3285000, loss: 0.10858038459445499, gradient norm: 0.006681760248731621\n",
      "Iteration: 3286000, loss: 0.10857966733498083, gradient norm: 0.20495404923046137\n",
      "Iteration: 3287000, loss: 0.10857936118569318, gradient norm: 0.17749923209026625\n",
      "Iteration: 3288000, loss: 0.10857840935042916, gradient norm: 0.0007283243359167277\n",
      "Iteration: 3289000, loss: 0.10857806313984418, gradient norm: 0.09388237895991002\n",
      "Iteration: 3290000, loss: 0.10857729722861181, gradient norm: 0.03086206707793198\n",
      "Iteration: 3291000, loss: 0.10857691426641836, gradient norm: 0.31536269759999996\n",
      "Iteration: 3292000, loss: 0.10857615118638624, gradient norm: 0.0344040153449056\n",
      "Iteration: 3293000, loss: 0.10857565723127953, gradient norm: 0.3027089355861848\n",
      "Iteration: 3294000, loss: 0.10857488045813914, gradient norm: 0.0952135701717897\n",
      "Iteration: 3295000, loss: 0.10857447436367058, gradient norm: 0.17914325269767117\n",
      "Iteration: 3296000, loss: 0.10857361883393063, gradient norm: 0.03800940205328747\n",
      "Iteration: 3297000, loss: 0.10857317602978503, gradient norm: 0.05856831907102839\n",
      "Iteration: 3298000, loss: 0.10857269100913644, gradient norm: 0.03790381936005229\n",
      "Iteration: 3299000, loss: 0.10857195773156353, gradient norm: 0.004108082063877918\n",
      "Iteration: 3300000, loss: 0.10857127161687197, gradient norm: 0.3421314313392362\n",
      "Iteration: 3301000, loss: 0.10857080284480017, gradient norm: 0.03944371005748524\n",
      "Iteration: 3302000, loss: 0.10857012701892285, gradient norm: 0.011748400673718856\n",
      "Iteration: 3303000, loss: 0.10856937292394447, gradient norm: 0.019436593196889486\n",
      "Iteration: 3304000, loss: 0.10856886643825187, gradient norm: 0.08144506227848963\n",
      "Iteration: 3305000, loss: 0.10856830705826596, gradient norm: 0.1498253617293245\n",
      "Iteration: 3306000, loss: 0.10856739608292296, gradient norm: 0.05778882159720967\n",
      "Iteration: 3307000, loss: 0.10856697226273831, gradient norm: 0.3557756711131064\n",
      "Iteration: 3308000, loss: 0.10856612297420214, gradient norm: 0.015905251817114812\n",
      "Iteration: 3309000, loss: 0.10856559610594202, gradient norm: 0.12509621544453142\n",
      "Iteration: 3310000, loss: 0.10856500398308067, gradient norm: 0.05325318847639193\n",
      "Iteration: 3311000, loss: 0.10856422727042866, gradient norm: 0.4163832603043191\n",
      "Iteration: 3312000, loss: 0.10856360375593573, gradient norm: 0.05621662342359368\n",
      "Iteration: 3313000, loss: 0.10856295520614896, gradient norm: 0.23450861446711851\n",
      "Iteration: 3314000, loss: 0.10856226417851295, gradient norm: 0.0724467930037993\n",
      "Iteration: 3315000, loss: 0.10856171011313126, gradient norm: 0.19689756145507217\n",
      "Iteration: 3316000, loss: 0.10856107166318468, gradient norm: 0.08330472994870129\n",
      "Iteration: 3317000, loss: 0.10856029947761962, gradient norm: 0.4755618033266271\n",
      "Iteration: 3318000, loss: 0.10855991851584824, gradient norm: 0.04360176969809823\n",
      "Iteration: 3319000, loss: 0.10855916595761873, gradient norm: 0.1513424856351883\n",
      "Iteration: 3320000, loss: 0.10855835179623441, gradient norm: 0.03896403618796875\n",
      "Iteration: 3321000, loss: 0.10855790859022259, gradient norm: 0.045888621862086684\n",
      "Iteration: 3322000, loss: 0.10855727217708255, gradient norm: 0.011303365737158276\n",
      "Iteration: 3323000, loss: 0.10855675087535292, gradient norm: 0.04345216336875525\n",
      "Iteration: 3324000, loss: 0.10855583423640908, gradient norm: 0.03503807275130767\n",
      "Iteration: 3325000, loss: 0.10855532645456761, gradient norm: 0.31548142926242945\n",
      "Iteration: 3326000, loss: 0.10855481856474752, gradient norm: 0.0864995869611716\n",
      "Iteration: 3327000, loss: 0.10855430480427296, gradient norm: 0.7042256649300247\n",
      "Iteration: 3328000, loss: 0.10855356846953129, gradient norm: 0.5524465390799562\n",
      "Iteration: 3329000, loss: 0.10855267570579352, gradient norm: 0.365766266073287\n",
      "Iteration: 3330000, loss: 0.10855222876536555, gradient norm: 0.07654028696672321\n",
      "Iteration: 3331000, loss: 0.10855166898071968, gradient norm: 0.061147244978701006\n",
      "Iteration: 3332000, loss: 0.1085510666109086, gradient norm: 0.24656129090433707\n",
      "Iteration: 3333000, loss: 0.10855030155831621, gradient norm: 0.0026250663552198667\n",
      "Iteration: 3334000, loss: 0.10854979227306612, gradient norm: 0.07765857476466065\n",
      "Iteration: 3335000, loss: 0.1085491680873516, gradient norm: 0.31704872282236657\n",
      "Iteration: 3336000, loss: 0.10854844101106098, gradient norm: 0.6486794742619739\n",
      "Iteration: 3337000, loss: 0.10854789633595482, gradient norm: 0.1706646484678238\n",
      "Iteration: 3338000, loss: 0.10854718908960101, gradient norm: 0.05370892943336819\n",
      "Iteration: 3339000, loss: 0.10854664594154101, gradient norm: 0.07854680554046416\n",
      "Iteration: 3340000, loss: 0.10854603222969345, gradient norm: 0.004726134081436983\n",
      "Iteration: 3341000, loss: 0.10854559510779348, gradient norm: 0.20086058087143713\n",
      "Iteration: 3342000, loss: 0.10854460404309364, gradient norm: 0.01910637905716489\n",
      "Iteration: 3343000, loss: 0.10854421169220362, gradient norm: 0.08898141695004859\n",
      "Iteration: 3344000, loss: 0.10854352715747019, gradient norm: 0.6124118581816634\n",
      "Iteration: 3345000, loss: 0.1085428912481035, gradient norm: 0.06084914289013152\n",
      "Iteration: 3346000, loss: 0.10854229933335444, gradient norm: 0.590287522724908\n",
      "Iteration: 3347000, loss: 0.10854165636602875, gradient norm: 0.07837614820854494\n",
      "Iteration: 3348000, loss: 0.10854107614649211, gradient norm: 0.07958269498478822\n",
      "Iteration: 3349000, loss: 0.10854026436124382, gradient norm: 0.22618395511088077\n",
      "Iteration: 3350000, loss: 0.10853987417996346, gradient norm: 0.06287441115796197\n",
      "Iteration: 3351000, loss: 0.10853922492448442, gradient norm: 0.1663688630584414\n",
      "Iteration: 3352000, loss: 0.10853856834786536, gradient norm: 0.3134808918083907\n",
      "Iteration: 3353000, loss: 0.10853782068059274, gradient norm: 0.025074993655258802\n",
      "Iteration: 3354000, loss: 0.10853731742646391, gradient norm: 0.036771346776163026\n",
      "Iteration: 3355000, loss: 0.10853655176021115, gradient norm: 0.006897739052025379\n",
      "Iteration: 3356000, loss: 0.10853619243360886, gradient norm: 0.05233798449558339\n",
      "Iteration: 3357000, loss: 0.10853538011216392, gradient norm: 0.005907323668201313\n",
      "Iteration: 3358000, loss: 0.10853473824974853, gradient norm: 0.09437861179883952\n",
      "Iteration: 3359000, loss: 0.10853422322653342, gradient norm: 0.006168389647937668\n",
      "Iteration: 3360000, loss: 0.10853356697323868, gradient norm: 0.014225887026001695\n",
      "Iteration: 3361000, loss: 0.10853289951608704, gradient norm: 0.10005312537784385\n",
      "Iteration: 3362000, loss: 0.10853226706257146, gradient norm: 0.17267136760521923\n",
      "Iteration: 3363000, loss: 0.10853173864372741, gradient norm: 0.0021848162375432464\n",
      "Iteration: 3364000, loss: 0.10853107197409176, gradient norm: 0.03383158798340064\n",
      "Iteration: 3365000, loss: 0.10853049154892873, gradient norm: 0.04332386279893909\n",
      "Iteration: 3366000, loss: 0.10852982818574046, gradient norm: 0.3520954311856384\n",
      "Iteration: 3367000, loss: 0.10852912871628348, gradient norm: 0.13102626939161435\n",
      "Iteration: 3368000, loss: 0.10852862194216822, gradient norm: 0.13960753295807113\n",
      "Iteration: 3369000, loss: 0.1085278996272923, gradient norm: 0.0923976979412098\n",
      "Iteration: 3370000, loss: 0.10852725560060238, gradient norm: 0.0888901264422937\n",
      "Iteration: 3371000, loss: 0.10852679679348537, gradient norm: 0.012685967785201865\n",
      "Iteration: 3372000, loss: 0.10852593252745385, gradient norm: 0.2136029639213628\n",
      "Iteration: 3373000, loss: 0.10852536802833355, gradient norm: 0.00939282536162351\n",
      "Iteration: 3374000, loss: 0.10852478622269303, gradient norm: 0.4972001880110832\n",
      "Iteration: 3375000, loss: 0.10852432373859532, gradient norm: 0.13391026204566323\n",
      "Iteration: 3376000, loss: 0.1085234443969543, gradient norm: 0.06816006291410603\n",
      "Iteration: 3377000, loss: 0.10852283560355667, gradient norm: 0.07245769104308411\n",
      "Iteration: 3378000, loss: 0.10852228925836539, gradient norm: 0.04948902252966977\n",
      "Iteration: 3379000, loss: 0.1085217844846904, gradient norm: 0.264388638576089\n",
      "Iteration: 3380000, loss: 0.10852102112968492, gradient norm: 0.15420283836551266\n",
      "Iteration: 3381000, loss: 0.10852036335505798, gradient norm: 0.08483374612607146\n",
      "Iteration: 3382000, loss: 0.10851969566591012, gradient norm: 0.04212633864117789\n",
      "Iteration: 3383000, loss: 0.10851924542592448, gradient norm: 0.4272514296567221\n",
      "Iteration: 3384000, loss: 0.10851843909658215, gradient norm: 0.03472681438049446\n",
      "Iteration: 3385000, loss: 0.10851777656639143, gradient norm: 0.08308032418871784\n",
      "Iteration: 3386000, loss: 0.1085173587788971, gradient norm: 0.29240337154514495\n",
      "Iteration: 3387000, loss: 0.10851652540291945, gradient norm: 0.0045235859360464046\n",
      "Iteration: 3388000, loss: 0.10851607357830788, gradient norm: 0.2747598108626888\n",
      "Iteration: 3389000, loss: 0.10851539406787222, gradient norm: 0.3087343134843945\n",
      "Iteration: 3390000, loss: 0.10851466559914152, gradient norm: 0.02577204801635844\n",
      "Iteration: 3391000, loss: 0.10851408937357863, gradient norm: 0.06565467749614559\n",
      "Iteration: 3392000, loss: 0.10851349700741511, gradient norm: 0.007596039306365537\n",
      "Iteration: 3393000, loss: 0.10851298978095757, gradient norm: 0.137721443700139\n",
      "Iteration: 3394000, loss: 0.10851215615007961, gradient norm: 0.7975121281044029\n",
      "Iteration: 3395000, loss: 0.10851155890916488, gradient norm: 0.012161727828897602\n",
      "Iteration: 3396000, loss: 0.10851096287604647, gradient norm: 0.046479449380018856\n",
      "Iteration: 3397000, loss: 0.10851031338742417, gradient norm: 0.008570565660206966\n",
      "Iteration: 3398000, loss: 0.10850970621404096, gradient norm: 0.03432108506349196\n",
      "Iteration: 3399000, loss: 0.10850919033585271, gradient norm: 0.2748530358679946\n",
      "Iteration: 3400000, loss: 0.10850836248479725, gradient norm: 0.32904687126281257\n",
      "Iteration: 3401000, loss: 0.10850783778481767, gradient norm: 0.5067518132898763\n",
      "Iteration: 3402000, loss: 0.10850724717911188, gradient norm: 0.013295000719126574\n",
      "Iteration: 3403000, loss: 0.10850660950024164, gradient norm: 0.04873617120542373\n",
      "Iteration: 3404000, loss: 0.10850601908398033, gradient norm: 0.015994792814434697\n",
      "Iteration: 3405000, loss: 0.1085052837701994, gradient norm: 0.096323926040143\n",
      "Iteration: 3406000, loss: 0.10850467878593013, gradient norm: 0.10507023491860934\n",
      "Iteration: 3407000, loss: 0.10850394617939307, gradient norm: 0.14522779440538816\n",
      "Iteration: 3408000, loss: 0.10850344881487613, gradient norm: 0.004270226569569289\n",
      "Iteration: 3409000, loss: 0.1085028987308517, gradient norm: 0.03503525416305219\n",
      "Iteration: 3410000, loss: 0.1085020797538107, gradient norm: 0.052946344264910364\n",
      "Iteration: 3411000, loss: 0.1085015612571545, gradient norm: 0.08527380002017267\n",
      "Iteration: 3412000, loss: 0.10850085884475781, gradient norm: 0.41888619606031796\n",
      "Iteration: 3413000, loss: 0.10850045595771604, gradient norm: 0.2893922252391315\n",
      "Iteration: 3414000, loss: 0.1084995661041994, gradient norm: 0.3331527168751803\n",
      "Iteration: 3415000, loss: 0.10849906754251705, gradient norm: 0.04089921382495183\n",
      "Iteration: 3416000, loss: 0.10849830048974712, gradient norm: 0.031324792600621564\n",
      "Iteration: 3417000, loss: 0.10849782223313674, gradient norm: 0.06015897360281835\n",
      "Iteration: 3418000, loss: 0.10849715965158503, gradient norm: 0.18678452152115704\n",
      "Iteration: 3419000, loss: 0.10849650817314922, gradient norm: 0.283041979335551\n",
      "Iteration: 3420000, loss: 0.1084958319753389, gradient norm: 0.15980257975689424\n",
      "Iteration: 3421000, loss: 0.10849512485872342, gradient norm: 0.30106736563010683\n",
      "Iteration: 3422000, loss: 0.10849465881233386, gradient norm: 0.014944912543826898\n",
      "Iteration: 3423000, loss: 0.10849392068797035, gradient norm: 0.02680593439812381\n",
      "Iteration: 3424000, loss: 0.10849343319500407, gradient norm: 0.019448467260183643\n",
      "Iteration: 3425000, loss: 0.10849274223532185, gradient norm: 0.04322272461118573\n",
      "Iteration: 3426000, loss: 0.10849201189920925, gradient norm: 0.11798076345129996\n",
      "Iteration: 3427000, loss: 0.10849130829810022, gradient norm: 0.11389760103639718\n",
      "Iteration: 3428000, loss: 0.10849093388357108, gradient norm: 0.0356205930155891\n",
      "Iteration: 3429000, loss: 0.10849030623257991, gradient norm: 0.12485199500506727\n",
      "Iteration: 3430000, loss: 0.10848943534130706, gradient norm: 0.06742130970493919\n",
      "Iteration: 3431000, loss: 0.10848899770790743, gradient norm: 0.27242097029690443\n",
      "Iteration: 3432000, loss: 0.10848819592912427, gradient norm: 0.0070651349133710045\n",
      "Iteration: 3433000, loss: 0.10848776666326176, gradient norm: 0.04548486806758721\n",
      "Iteration: 3434000, loss: 0.10848710639718234, gradient norm: 0.11014029966331962\n",
      "Iteration: 3435000, loss: 0.1084863665230992, gradient norm: 0.021313776674653354\n",
      "Iteration: 3436000, loss: 0.1084857825492591, gradient norm: 0.08399462595597162\n",
      "Iteration: 3437000, loss: 0.10848508283421447, gradient norm: 0.1049196815141648\n",
      "Iteration: 3438000, loss: 0.10848456497016667, gradient norm: 0.4378314659890408\n",
      "Iteration: 3439000, loss: 0.10848383925834695, gradient norm: 0.025637272254603344\n",
      "Iteration: 3440000, loss: 0.10848328177953245, gradient norm: 0.02043046663525002\n",
      "Iteration: 3441000, loss: 0.10848259872173938, gradient norm: 0.027287598332941008\n",
      "Iteration: 3442000, loss: 0.10848207470033613, gradient norm: 0.02186962866719796\n",
      "Iteration: 3443000, loss: 0.10848128759660122, gradient norm: 0.0792504629576483\n",
      "Iteration: 3444000, loss: 0.10848082085942602, gradient norm: 0.06758899645875131\n",
      "Iteration: 3445000, loss: 0.10848008698501854, gradient norm: 0.000980194784223275\n",
      "Iteration: 3446000, loss: 0.10847950725226599, gradient norm: 0.02273619841287591\n",
      "Iteration: 3447000, loss: 0.10847888738870412, gradient norm: 0.017809676736602824\n",
      "Iteration: 3448000, loss: 0.10847823170240145, gradient norm: 0.025820327961086277\n",
      "Iteration: 3449000, loss: 0.10847770344090893, gradient norm: 0.12178033733085818\n",
      "Iteration: 3450000, loss: 0.10847699734737223, gradient norm: 0.10758637744087064\n",
      "Iteration: 3451000, loss: 0.10847634587996687, gradient norm: 0.029135114233238748\n",
      "Iteration: 3452000, loss: 0.10847559648703718, gradient norm: 0.10611463271634886\n",
      "Iteration: 3453000, loss: 0.10847514490285129, gradient norm: 0.521544880259109\n",
      "Iteration: 3454000, loss: 0.10847451438350776, gradient norm: 0.14612463718638974\n",
      "Iteration: 3455000, loss: 0.10847376260408728, gradient norm: 0.08718259953320787\n",
      "Iteration: 3456000, loss: 0.10847321906053645, gradient norm: 0.18263274271952468\n",
      "Iteration: 3457000, loss: 0.10847249933420604, gradient norm: 0.14543143074066486\n",
      "Iteration: 3458000, loss: 0.10847215683505554, gradient norm: 0.06812108195396616\n",
      "Iteration: 3459000, loss: 0.10847142556581259, gradient norm: 0.09672411262713387\n",
      "Iteration: 3460000, loss: 0.10847063532026728, gradient norm: 0.23914627735658173\n",
      "Iteration: 3461000, loss: 0.10847000056356819, gradient norm: 0.12869676123251403\n",
      "Iteration: 3462000, loss: 0.10846952581579115, gradient norm: 0.09725239887887979\n",
      "Iteration: 3463000, loss: 0.10846868454519872, gradient norm: 0.08689577412022524\n",
      "Iteration: 3464000, loss: 0.10846816499778268, gradient norm: 0.05372133439821138\n",
      "Iteration: 3465000, loss: 0.10846777632240866, gradient norm: 0.14117870817214986\n",
      "Iteration: 3466000, loss: 0.1084670219289524, gradient norm: 0.08917936584312582\n",
      "Iteration: 3467000, loss: 0.10846619254246101, gradient norm: 0.06359327284690929\n",
      "Iteration: 3468000, loss: 0.10846564807684417, gradient norm: 0.05309639704695974\n",
      "Iteration: 3469000, loss: 0.10846502349716004, gradient norm: 0.002438500454536148\n",
      "Iteration: 3470000, loss: 0.10846447034617819, gradient norm: 0.23285830059188595\n",
      "Iteration: 3471000, loss: 0.1084639461506791, gradient norm: 0.18373473031243748\n",
      "Iteration: 3472000, loss: 0.10846311229678196, gradient norm: 0.022711183744105624\n",
      "Iteration: 3473000, loss: 0.10846259417734205, gradient norm: 0.16531185592623898\n",
      "Iteration: 3474000, loss: 0.10846190677697333, gradient norm: 0.08620557028305853\n",
      "Iteration: 3475000, loss: 0.10846132310324015, gradient norm: 0.009885677865845675\n",
      "Iteration: 3476000, loss: 0.10846058351074415, gradient norm: 0.36785058952049887\n",
      "Iteration: 3477000, loss: 0.10846018113982475, gradient norm: 0.21303007876436125\n",
      "Iteration: 3478000, loss: 0.10845934571888569, gradient norm: 0.03849270858464379\n",
      "Iteration: 3479000, loss: 0.10845893716858847, gradient norm: 0.25287675330217263\n",
      "Iteration: 3480000, loss: 0.10845807127050429, gradient norm: 0.019253903454914706\n",
      "Iteration: 3481000, loss: 0.10845762896134009, gradient norm: 0.19761381228966005\n",
      "Iteration: 3482000, loss: 0.10845684917874629, gradient norm: 0.45325824825656275\n",
      "Iteration: 3483000, loss: 0.10845628423920714, gradient norm: 0.0756836775403165\n",
      "Iteration: 3484000, loss: 0.10845569601620605, gradient norm: 0.1264510522176276\n",
      "Iteration: 3485000, loss: 0.1084549831176105, gradient norm: 0.016519478320178124\n",
      "Iteration: 3486000, loss: 0.10845443714291267, gradient norm: 0.05358885798424534\n",
      "Iteration: 3487000, loss: 0.10845382615443531, gradient norm: 0.16279889894112262\n",
      "Iteration: 3488000, loss: 0.10845313120520918, gradient norm: 0.025475920322740556\n",
      "Iteration: 3489000, loss: 0.10845246990865011, gradient norm: 0.09485881069379018\n",
      "Iteration: 3490000, loss: 0.10845210345964383, gradient norm: 0.7352889005506452\n",
      "Iteration: 3491000, loss: 0.10845115203112984, gradient norm: 0.083873255954055\n",
      "Iteration: 3492000, loss: 0.10845075082237234, gradient norm: 0.01215647208232516\n",
      "Iteration: 3493000, loss: 0.10845001358527163, gradient norm: 0.0338914263059647\n",
      "Iteration: 3494000, loss: 0.10844954889247381, gradient norm: 0.15845830217336093\n",
      "Iteration: 3495000, loss: 0.10844877827852205, gradient norm: 0.16997086097140587\n",
      "Iteration: 3496000, loss: 0.10844832711433498, gradient norm: 0.31387938257107867\n",
      "Iteration: 3497000, loss: 0.10844747622921332, gradient norm: 0.17578647091079974\n",
      "Iteration: 3498000, loss: 0.10844687136340536, gradient norm: 0.5257886165320411\n",
      "Iteration: 3499000, loss: 0.10844634854104382, gradient norm: 0.22175261237239854\n",
      "Iteration: 3500000, loss: 0.1084456245083525, gradient norm: 0.018218164960375885\n",
      "Iteration: 3501000, loss: 0.10844502659497535, gradient norm: 0.01768873693834772\n",
      "Iteration: 3502000, loss: 0.10844455688878978, gradient norm: 0.11787692284678633\n",
      "Iteration: 3503000, loss: 0.10844379091802517, gradient norm: 0.06888612804662063\n",
      "Iteration: 3504000, loss: 0.10844322129011859, gradient norm: 0.01915901368100674\n",
      "Iteration: 3505000, loss: 0.10844254629842473, gradient norm: 0.014957320294021097\n",
      "Iteration: 3506000, loss: 0.108442018852133, gradient norm: 0.804161150626157\n",
      "Iteration: 3507000, loss: 0.10844130969939561, gradient norm: 0.0068088134182056155\n",
      "Iteration: 3508000, loss: 0.10844069241052558, gradient norm: 0.009251106838145794\n",
      "Iteration: 3509000, loss: 0.10844009447877077, gradient norm: 0.44460458738634606\n",
      "Iteration: 3510000, loss: 0.10843955887674141, gradient norm: 0.39794736353694665\n",
      "Iteration: 3511000, loss: 0.10843882703897975, gradient norm: 0.009206455668572278\n",
      "Iteration: 3512000, loss: 0.10843817536417234, gradient norm: 0.024811184809863968\n",
      "Iteration: 3513000, loss: 0.10843774938486866, gradient norm: 0.11768637171285634\n",
      "Iteration: 3514000, loss: 0.10843690385103685, gradient norm: 0.04687752326571541\n",
      "Iteration: 3515000, loss: 0.10843642403813482, gradient norm: 0.16666908543040646\n",
      "Iteration: 3516000, loss: 0.10843576398616327, gradient norm: 0.06626543136940374\n",
      "Iteration: 3517000, loss: 0.10843508290325257, gradient norm: 0.5861453538778906\n",
      "Iteration: 3518000, loss: 0.10843445640593029, gradient norm: 0.12004726303510704\n",
      "Iteration: 3519000, loss: 0.1084339900882741, gradient norm: 0.08692943213465011\n",
      "Iteration: 3520000, loss: 0.10843330692333814, gradient norm: 0.6474284729352225\n",
      "Iteration: 3521000, loss: 0.1084326001546791, gradient norm: 0.05509335339673456\n",
      "Iteration: 3522000, loss: 0.10843217524313781, gradient norm: 0.05804125175093559\n",
      "Iteration: 3523000, loss: 0.1084314553562265, gradient norm: 0.07497543753417876\n",
      "Iteration: 3524000, loss: 0.1084307418679127, gradient norm: 0.17564148682829325\n",
      "Iteration: 3525000, loss: 0.10843024265123627, gradient norm: 0.05900275374345305\n",
      "Iteration: 3526000, loss: 0.10842953314072971, gradient norm: 0.02236388942769372\n",
      "Iteration: 3527000, loss: 0.10842898961321171, gradient norm: 0.030968474815181005\n",
      "Iteration: 3528000, loss: 0.10842828887770106, gradient norm: 0.041300494558560975\n",
      "Iteration: 3529000, loss: 0.10842773814841493, gradient norm: 0.005451599331493782\n",
      "Iteration: 3530000, loss: 0.1084271655226627, gradient norm: 0.2295655316578902\n",
      "Iteration: 3531000, loss: 0.10842641665575864, gradient norm: 0.04389978723180141\n",
      "Iteration: 3532000, loss: 0.1084260139730041, gradient norm: 0.307028535899797\n",
      "Iteration: 3533000, loss: 0.10842515511423263, gradient norm: 0.04842283493083689\n",
      "Iteration: 3534000, loss: 0.10842460637623512, gradient norm: 0.025965035365417135\n",
      "Iteration: 3535000, loss: 0.10842400836056856, gradient norm: 0.11524273684971055\n",
      "Iteration: 3536000, loss: 0.1084233618556755, gradient norm: 0.20423405300843145\n",
      "Iteration: 3537000, loss: 0.10842289300192004, gradient norm: 0.04222311804768442\n",
      "Iteration: 3538000, loss: 0.10842212489942218, gradient norm: 0.07776856207537762\n",
      "Iteration: 3539000, loss: 0.10842154267675685, gradient norm: 0.19024269870326158\n",
      "Iteration: 3540000, loss: 0.10842092511629281, gradient norm: 0.21587848304141888\n",
      "Iteration: 3541000, loss: 0.10842030883181604, gradient norm: 0.1566935888099038\n",
      "Iteration: 3542000, loss: 0.10841966493729653, gradient norm: 0.2085516781493923\n",
      "Iteration: 3543000, loss: 0.10841897920310441, gradient norm: 0.12442822477709084\n",
      "Iteration: 3544000, loss: 0.10841850641433294, gradient norm: 0.17052209922676145\n",
      "Iteration: 3545000, loss: 0.10841779066247982, gradient norm: 0.27528529970883814\n",
      "Iteration: 3546000, loss: 0.10841728651315051, gradient norm: 0.015175462720162838\n",
      "Iteration: 3547000, loss: 0.10841659638718401, gradient norm: 0.0797536923965202\n",
      "Iteration: 3548000, loss: 0.10841592990384108, gradient norm: 0.015946292798625084\n",
      "Iteration: 3549000, loss: 0.10841529890491748, gradient norm: 0.04484596230180787\n",
      "Iteration: 3550000, loss: 0.10841477482364988, gradient norm: 0.5479264640199286\n",
      "Iteration: 3551000, loss: 0.10841408638081973, gradient norm: 0.3962969852649805\n",
      "Iteration: 3552000, loss: 0.10841355322116017, gradient norm: 0.10476031421185245\n",
      "Iteration: 3553000, loss: 0.1084129269064103, gradient norm: 0.41833490202453755\n",
      "Iteration: 3554000, loss: 0.10841232594901686, gradient norm: 0.5199143320151679\n",
      "Iteration: 3555000, loss: 0.10841161974335815, gradient norm: 0.025904887256956843\n",
      "Iteration: 3556000, loss: 0.10841102294481447, gradient norm: 0.008088507252943255\n",
      "Iteration: 3557000, loss: 0.10841048686876642, gradient norm: 0.042251536616384255\n",
      "Iteration: 3558000, loss: 0.1084097815660596, gradient norm: 0.012311366762374395\n",
      "Iteration: 3559000, loss: 0.1084091709769298, gradient norm: 0.02834520425095933\n",
      "Iteration: 3560000, loss: 0.10840866642520273, gradient norm: 0.09350606309625065\n",
      "Iteration: 3561000, loss: 0.10840802692644616, gradient norm: 0.19825123631480293\n",
      "Iteration: 3562000, loss: 0.10840721552422297, gradient norm: 0.05526785053710836\n",
      "Iteration: 3563000, loss: 0.10840689747371432, gradient norm: 0.2935907079120895\n",
      "Iteration: 3564000, loss: 0.10840609419368913, gradient norm: 0.6850521816197613\n",
      "Iteration: 3565000, loss: 0.10840555035105749, gradient norm: 0.2127860540096815\n",
      "Iteration: 3566000, loss: 0.1084047696656557, gradient norm: 0.3695833116029489\n",
      "Iteration: 3567000, loss: 0.10840438459263974, gradient norm: 0.22267314130081767\n",
      "Iteration: 3568000, loss: 0.10840360960374834, gradient norm: 0.05515507394842411\n",
      "Iteration: 3569000, loss: 0.10840301432211896, gradient norm: 0.028780228460916425\n",
      "Iteration: 3570000, loss: 0.10840251102531168, gradient norm: 0.6771956213445064\n",
      "Iteration: 3571000, loss: 0.10840179818599562, gradient norm: 0.18735361340815795\n",
      "Iteration: 3572000, loss: 0.10840124427314167, gradient norm: 0.07258368117718302\n",
      "Iteration: 3573000, loss: 0.10840056222825223, gradient norm: 0.0791351097758578\n",
      "Iteration: 3574000, loss: 0.10840001252535478, gradient norm: 0.22439425923511863\n",
      "Iteration: 3575000, loss: 0.10839934810248843, gradient norm: 0.09594465096362159\n",
      "Iteration: 3576000, loss: 0.10839869514149147, gradient norm: 0.013336619329659382\n",
      "Iteration: 3577000, loss: 0.10839816683994917, gradient norm: 0.05283686245598867\n",
      "Iteration: 3578000, loss: 0.10839746282521935, gradient norm: 0.03653539225159794\n",
      "Iteration: 3579000, loss: 0.10839704919011312, gradient norm: 0.030301686499341788\n",
      "Iteration: 3580000, loss: 0.10839619665907677, gradient norm: 0.30021183625247705\n",
      "Iteration: 3581000, loss: 0.10839568456260422, gradient norm: 0.3696328485630383\n",
      "Iteration: 3582000, loss: 0.10839500571784376, gradient norm: 0.03394687182021189\n",
      "Iteration: 3583000, loss: 0.10839437684800164, gradient norm: 0.004950818070352076\n",
      "Iteration: 3584000, loss: 0.10839407693265148, gradient norm: 0.04164548769915539\n",
      "Iteration: 3585000, loss: 0.10839300298194575, gradient norm: 0.4871296306524984\n",
      "Iteration: 3586000, loss: 0.10839262199409501, gradient norm: 0.04490663099773892\n",
      "Iteration: 3587000, loss: 0.1083919539778518, gradient norm: 0.051785351800543035\n",
      "Iteration: 3588000, loss: 0.10839135008817707, gradient norm: 0.43879401389921546\n",
      "Iteration: 3589000, loss: 0.10839075116609545, gradient norm: 0.0675036835115464\n",
      "Iteration: 3590000, loss: 0.10839025407573777, gradient norm: 0.1961074701578089\n",
      "Iteration: 3591000, loss: 0.10838945885118764, gradient norm: 0.08519682117287865\n",
      "Iteration: 3592000, loss: 0.1083887815059507, gradient norm: 0.03668955950974456\n",
      "Iteration: 3593000, loss: 0.10838831259675646, gradient norm: 0.04282547820940844\n",
      "Iteration: 3594000, loss: 0.10838775502367068, gradient norm: 0.011089228241034775\n",
      "Iteration: 3595000, loss: 0.10838704833483039, gradient norm: 0.12028941195436481\n",
      "Iteration: 3596000, loss: 0.10838628395901577, gradient norm: 0.010802490470129001\n",
      "Iteration: 3597000, loss: 0.1083858262351494, gradient norm: 0.5378844791107291\n",
      "Iteration: 3598000, loss: 0.10838513423463947, gradient norm: 0.07049605128134284\n",
      "Iteration: 3599000, loss: 0.10838440151294011, gradient norm: 0.21001334963646331\n",
      "Iteration: 3600000, loss: 0.10838392562905269, gradient norm: 0.01547083528651478\n",
      "Iteration: 3601000, loss: 0.10838319684836792, gradient norm: 0.5368996329866099\n",
      "Iteration: 3602000, loss: 0.1083825796918526, gradient norm: 0.21254570790119925\n",
      "Iteration: 3603000, loss: 0.10838173723554378, gradient norm: 0.5699409105304805\n",
      "Iteration: 3604000, loss: 0.1083810203868559, gradient norm: 0.1548434220297415\n",
      "Iteration: 3605000, loss: 0.10837880123724039, gradient norm: 0.01006157634586359\n",
      "Iteration: 3606000, loss: 0.10836613457382258, gradient norm: 0.11492560680983653\n",
      "Iteration: 3607000, loss: 0.1083431082118314, gradient norm: 0.38512292488891414\n",
      "Iteration: 3608000, loss: 0.10832740229638003, gradient norm: 0.007525883389986397\n",
      "Iteration: 3609000, loss: 0.10831358048583417, gradient norm: 0.06258464526428006\n",
      "Iteration: 3610000, loss: 0.10830172038603422, gradient norm: 0.22362486758624894\n",
      "Iteration: 3611000, loss: 0.10829311962434317, gradient norm: 0.3492374463076689\n",
      "Iteration: 3612000, loss: 0.10828657498012732, gradient norm: 0.4433062292360691\n",
      "Iteration: 3613000, loss: 0.10828095740258459, gradient norm: 0.06580870009986903\n",
      "Iteration: 3614000, loss: 0.10827644420031272, gradient norm: 0.04773860759275529\n",
      "Iteration: 3615000, loss: 0.10827200086806901, gradient norm: 0.17808487395114947\n",
      "Iteration: 3616000, loss: 0.1082680057250833, gradient norm: 0.04195442199970948\n",
      "Iteration: 3617000, loss: 0.10826440237631715, gradient norm: 0.11403649937532917\n",
      "Iteration: 3618000, loss: 0.10826053675990005, gradient norm: 0.02648634337733905\n",
      "Iteration: 3619000, loss: 0.10825734337236324, gradient norm: 0.12512463635940416\n",
      "Iteration: 3620000, loss: 0.10825410335112409, gradient norm: 0.16583343649970256\n",
      "Iteration: 3621000, loss: 0.10825122347842091, gradient norm: 0.08480865149268045\n",
      "Iteration: 3622000, loss: 0.1082485272285298, gradient norm: 0.07240499081584321\n",
      "Iteration: 3623000, loss: 0.1082459115976102, gradient norm: 0.0551404244768608\n",
      "Iteration: 3624000, loss: 0.10824376370210707, gradient norm: 0.11756564752192583\n",
      "Iteration: 3625000, loss: 0.10824124350846968, gradient norm: 0.15255613121084965\n",
      "Iteration: 3626000, loss: 0.10823905480952668, gradient norm: 0.22612049603596948\n",
      "Iteration: 3627000, loss: 0.10823697934677279, gradient norm: 0.14722106712284033\n",
      "Iteration: 3628000, loss: 0.10823514569800699, gradient norm: 0.3050338991415289\n",
      "Iteration: 3629000, loss: 0.10823326497640835, gradient norm: 0.10300101547725482\n",
      "Iteration: 3630000, loss: 0.10823129298630352, gradient norm: 0.11901090846057386\n",
      "Iteration: 3631000, loss: 0.10822972632882233, gradient norm: 0.02936585055339712\n",
      "Iteration: 3632000, loss: 0.10822806328443696, gradient norm: 0.02436792388985182\n",
      "Iteration: 3633000, loss: 0.10822639962959343, gradient norm: 0.10541111808653968\n",
      "Iteration: 3634000, loss: 0.10822464429643529, gradient norm: 0.03718197452252896\n",
      "Iteration: 3635000, loss: 0.1082231080122685, gradient norm: 0.08742145862256623\n",
      "Iteration: 3636000, loss: 0.10822164846955772, gradient norm: 0.04794702265976971\n",
      "Iteration: 3637000, loss: 0.10822025723758852, gradient norm: 0.13821678181739822\n",
      "Iteration: 3638000, loss: 0.10821872495188772, gradient norm: 0.1039880813509069\n",
      "Iteration: 3639000, loss: 0.10821730965783888, gradient norm: 0.027672701339429045\n",
      "Iteration: 3640000, loss: 0.10821605790761367, gradient norm: 0.11367888269708112\n",
      "Iteration: 3641000, loss: 0.10821467624195867, gradient norm: 0.0462641948762295\n",
      "Iteration: 3642000, loss: 0.10821317676834637, gradient norm: 0.058100647358992646\n",
      "Iteration: 3643000, loss: 0.10821223557942526, gradient norm: 0.47073091459713634\n",
      "Iteration: 3644000, loss: 0.1082107641159738, gradient norm: 0.2174133037694125\n",
      "Iteration: 3645000, loss: 0.1082094320026373, gradient norm: 0.0633020203221882\n",
      "Iteration: 3646000, loss: 0.1082083523948863, gradient norm: 0.12317463395893063\n",
      "Iteration: 3647000, loss: 0.10820710147974208, gradient norm: 0.04896497370948589\n",
      "Iteration: 3648000, loss: 0.10820584305370963, gradient norm: 0.02327212295113903\n",
      "Iteration: 3649000, loss: 0.10820464345145064, gradient norm: 0.12698425955263357\n",
      "Iteration: 3650000, loss: 0.1082037226605682, gradient norm: 0.3471779096244147\n",
      "Iteration: 3651000, loss: 0.10820259221596316, gradient norm: 0.007652061479587689\n",
      "Iteration: 3652000, loss: 0.1082012599542865, gradient norm: 0.24908453819715043\n",
      "Iteration: 3653000, loss: 0.10820018105596454, gradient norm: 0.45011714670649483\n",
      "Iteration: 3654000, loss: 0.10819922135696655, gradient norm: 0.05448540894306329\n",
      "Iteration: 3655000, loss: 0.10819805372754944, gradient norm: 0.07581742529728919\n",
      "Iteration: 3656000, loss: 0.10819703668381224, gradient norm: 0.11695424403965705\n",
      "Iteration: 3657000, loss: 0.10819592164070807, gradient norm: 0.03181746237799699\n",
      "Iteration: 3658000, loss: 0.10819501505915415, gradient norm: 0.6345971771492004\n",
      "Iteration: 3659000, loss: 0.10819386586438294, gradient norm: 0.0744381279110792\n",
      "Iteration: 3660000, loss: 0.10819296378220025, gradient norm: 0.09917140081264064\n",
      "Iteration: 3661000, loss: 0.10819191067857412, gradient norm: 0.012737449921249611\n",
      "Iteration: 3662000, loss: 0.10819096092652868, gradient norm: 0.03679955224424041\n",
      "Iteration: 3663000, loss: 0.10818987057405002, gradient norm: 0.46927317647372035\n",
      "Iteration: 3664000, loss: 0.10818903470386917, gradient norm: 0.1293165606782775\n",
      "Iteration: 3665000, loss: 0.10818798563248119, gradient norm: 0.06342934326439512\n",
      "Iteration: 3666000, loss: 0.1081870347886637, gradient norm: 0.1201549707171358\n",
      "Iteration: 3667000, loss: 0.10818611357219589, gradient norm: 0.5931025071172429\n",
      "Iteration: 3668000, loss: 0.10818514555170865, gradient norm: 0.048891779756123174\n",
      "Iteration: 3669000, loss: 0.10818423443232678, gradient norm: 0.1385052943175947\n",
      "Iteration: 3670000, loss: 0.1081832795258123, gradient norm: 0.036738978385190364\n",
      "Iteration: 3671000, loss: 0.10818237460649605, gradient norm: 0.01417547501718966\n",
      "Iteration: 3672000, loss: 0.10818163324481239, gradient norm: 0.28872405847500693\n",
      "Iteration: 3673000, loss: 0.10818040367447798, gradient norm: 0.21748080276069925\n",
      "Iteration: 3674000, loss: 0.10817984094292916, gradient norm: 0.193591227937118\n",
      "Iteration: 3675000, loss: 0.10817874726389792, gradient norm: 0.046516256670063344\n",
      "Iteration: 3676000, loss: 0.10817786743288593, gradient norm: 0.12606687434601993\n",
      "Iteration: 3677000, loss: 0.1081769948316023, gradient norm: 0.05458588023329924\n",
      "Iteration: 3678000, loss: 0.1081763423630449, gradient norm: 0.3950007281749776\n",
      "Iteration: 3679000, loss: 0.10817513466863012, gradient norm: 0.21984961402533137\n",
      "Iteration: 3680000, loss: 0.10817453727549631, gradient norm: 0.12017782667986505\n",
      "Iteration: 3681000, loss: 0.10817349009695837, gradient norm: 0.5984416266842947\n",
      "Iteration: 3682000, loss: 0.10817269794637292, gradient norm: 0.16904518556746417\n",
      "Iteration: 3683000, loss: 0.10817194067244727, gradient norm: 0.012788591220483122\n",
      "Iteration: 3684000, loss: 0.10817103792639177, gradient norm: 0.19214706588663016\n",
      "Iteration: 3685000, loss: 0.10817017498617468, gradient norm: 0.18855701495995508\n",
      "Iteration: 3686000, loss: 0.10816929273972314, gradient norm: 0.15054933001071102\n",
      "Iteration: 3687000, loss: 0.10816858477082704, gradient norm: 0.1762853558378131\n",
      "Iteration: 3688000, loss: 0.1081677933893256, gradient norm: 0.33109265644533253\n",
      "Iteration: 3689000, loss: 0.10816691664742074, gradient norm: 0.12464516773536083\n",
      "Iteration: 3690000, loss: 0.10816601141590435, gradient norm: 0.034901676226305474\n",
      "Iteration: 3691000, loss: 0.10816532342164732, gradient norm: 0.10214420863043101\n",
      "Iteration: 3692000, loss: 0.1081643431527154, gradient norm: 0.41889302192589356\n",
      "Iteration: 3693000, loss: 0.10816375419940616, gradient norm: 0.028230940475113823\n",
      "Iteration: 3694000, loss: 0.10816281575228215, gradient norm: 0.08566605280900891\n",
      "Iteration: 3695000, loss: 0.10816206505826632, gradient norm: 0.1580731846402937\n",
      "Iteration: 3696000, loss: 0.10816118942310014, gradient norm: 0.33036501216760694\n",
      "Iteration: 3697000, loss: 0.10816065385052052, gradient norm: 0.07756512655929199\n",
      "Iteration: 3698000, loss: 0.10815958800993526, gradient norm: 0.4659968315184702\n",
      "Iteration: 3699000, loss: 0.10815878459182404, gradient norm: 0.07918327941480965\n",
      "Iteration: 3700000, loss: 0.10815810548803059, gradient norm: 0.333021621258409\n",
      "Iteration: 3701000, loss: 0.10815746608158823, gradient norm: 0.020073710569827936\n",
      "Iteration: 3702000, loss: 0.10815660983588418, gradient norm: 0.05893931704465743\n",
      "Iteration: 3703000, loss: 0.10815575389313871, gradient norm: 0.19545980052422407\n",
      "Iteration: 3704000, loss: 0.10815498702400059, gradient norm: 0.5181214274755359\n",
      "Iteration: 3705000, loss: 0.1081542448084776, gradient norm: 0.13091463821478497\n",
      "Iteration: 3706000, loss: 0.1081535121090867, gradient norm: 0.059369236884680425\n",
      "Iteration: 3707000, loss: 0.10815262213050782, gradient norm: 0.34235848825425713\n",
      "Iteration: 3708000, loss: 0.10815197717057969, gradient norm: 0.02261040701472593\n",
      "Iteration: 3709000, loss: 0.10815128405417503, gradient norm: 0.1576463666254121\n",
      "Iteration: 3710000, loss: 0.10815033325989737, gradient norm: 0.4654598343861471\n",
      "Iteration: 3711000, loss: 0.10814970198775807, gradient norm: 0.07267023957489473\n",
      "Iteration: 3712000, loss: 0.10814901479124486, gradient norm: 0.14846467517184708\n",
      "Iteration: 3713000, loss: 0.10814819325997908, gradient norm: 0.06019402635741318\n",
      "Iteration: 3714000, loss: 0.1081473469532327, gradient norm: 0.10536311262134537\n",
      "Iteration: 3715000, loss: 0.10814684537104928, gradient norm: 0.09053458483693465\n",
      "Iteration: 3716000, loss: 0.10814585242190654, gradient norm: 0.09051785600644695\n",
      "Iteration: 3717000, loss: 0.10814521549531048, gradient norm: 0.23558089419176412\n",
      "Iteration: 3718000, loss: 0.10814454024032065, gradient norm: 0.2160322707461151\n",
      "Iteration: 3719000, loss: 0.10814385841481543, gradient norm: 0.4546361350267957\n",
      "Iteration: 3720000, loss: 0.10814303422410027, gradient norm: 0.18880122616800687\n",
      "Iteration: 3721000, loss: 0.10814225795242288, gradient norm: 0.06696790006770321\n",
      "Iteration: 3722000, loss: 0.10814150683389452, gradient norm: 0.06512187692226043\n",
      "Iteration: 3723000, loss: 0.10814085620226924, gradient norm: 0.12899884584806792\n",
      "Iteration: 3724000, loss: 0.1081401495852997, gradient norm: 0.16356204237663044\n",
      "Iteration: 3725000, loss: 0.10813926813683084, gradient norm: 0.029281140480925515\n",
      "Iteration: 3726000, loss: 0.10813862322296304, gradient norm: 0.008028023433343467\n",
      "Iteration: 3727000, loss: 0.10813794983420164, gradient norm: 0.13403806527156087\n",
      "Iteration: 3728000, loss: 0.1081372320263024, gradient norm: 0.18216883750908572\n",
      "Iteration: 3729000, loss: 0.10813635072707212, gradient norm: 0.37514324785796543\n",
      "Iteration: 3730000, loss: 0.10813584858572009, gradient norm: 0.5852464545018465\n",
      "Iteration: 3731000, loss: 0.10813519245895699, gradient norm: 0.16136130270326848\n",
      "Iteration: 3732000, loss: 0.1081342034688431, gradient norm: 0.11111239318413703\n",
      "Iteration: 3733000, loss: 0.10813356131174423, gradient norm: 0.2992040533766563\n",
      "Iteration: 3734000, loss: 0.1081329171784061, gradient norm: 0.022194153258675395\n",
      "Iteration: 3735000, loss: 0.10813218703405034, gradient norm: 0.2584772836290723\n",
      "Iteration: 3736000, loss: 0.10813152893123407, gradient norm: 0.10087986212051772\n",
      "Iteration: 3737000, loss: 0.10813089526436571, gradient norm: 0.398028745769608\n",
      "Iteration: 3738000, loss: 0.10812988094607023, gradient norm: 0.13056144941783132\n",
      "Iteration: 3739000, loss: 0.10812937499075354, gradient norm: 0.23674932641748822\n",
      "Iteration: 3740000, loss: 0.10812871561130907, gradient norm: 0.1034449777947419\n",
      "Iteration: 3741000, loss: 0.10812791219527948, gradient norm: 0.030813090509811413\n",
      "Iteration: 3742000, loss: 0.1081272674650778, gradient norm: 0.015731024826526566\n",
      "Iteration: 3743000, loss: 0.10812656594477547, gradient norm: 0.19963763054946154\n",
      "Iteration: 3744000, loss: 0.10812588999804594, gradient norm: 0.07193583259238277\n",
      "Iteration: 3745000, loss: 0.10812507861538406, gradient norm: 0.03513651386967632\n",
      "Iteration: 3746000, loss: 0.10812459448647964, gradient norm: 0.28460000777599664\n",
      "Iteration: 3747000, loss: 0.10812357617993637, gradient norm: 0.11171323692303936\n",
      "Iteration: 3748000, loss: 0.10812318290406674, gradient norm: 0.1865795371414503\n",
      "Iteration: 3749000, loss: 0.10812231742809987, gradient norm: 0.47862899890603505\n",
      "Iteration: 3750000, loss: 0.10812168772470972, gradient norm: 0.4416335642061602\n",
      "Iteration: 3751000, loss: 0.1081209395421801, gradient norm: 0.06832436122208259\n",
      "Iteration: 3752000, loss: 0.10812044486960506, gradient norm: 0.2512674537419485\n",
      "Iteration: 3753000, loss: 0.1081195112414154, gradient norm: 0.025885303240790913\n",
      "Iteration: 3754000, loss: 0.10811893164203376, gradient norm: 0.012999629756969335\n",
      "Iteration: 3755000, loss: 0.10811823762739388, gradient norm: 0.12266240863549892\n",
      "Iteration: 3756000, loss: 0.10811745848907217, gradient norm: 0.13323726034234817\n",
      "Iteration: 3757000, loss: 0.108116894981244, gradient norm: 0.10326932940864895\n",
      "Iteration: 3758000, loss: 0.10811637242754621, gradient norm: 0.2478325512757982\n",
      "Iteration: 3759000, loss: 0.1081153322511609, gradient norm: 0.054601768486641\n",
      "Iteration: 3760000, loss: 0.10811477445141668, gradient norm: 0.33012044874628077\n",
      "Iteration: 3761000, loss: 0.1081141954363246, gradient norm: 0.6753296404966794\n",
      "Iteration: 3762000, loss: 0.10811340158203246, gradient norm: 0.03334059804880594\n",
      "Iteration: 3763000, loss: 0.10811273257032082, gradient norm: 0.04530252784233071\n",
      "Iteration: 3764000, loss: 0.10811209955783958, gradient norm: 0.06353827979602773\n",
      "Iteration: 3765000, loss: 0.10811140880767485, gradient norm: 0.20390688598293785\n",
      "Iteration: 3766000, loss: 0.10811071907245369, gradient norm: 0.5442090727557137\n",
      "Iteration: 3767000, loss: 0.10811007791324542, gradient norm: 0.5977546927675945\n",
      "Iteration: 3768000, loss: 0.10810940965084706, gradient norm: 0.2750929255272206\n",
      "Iteration: 3769000, loss: 0.10810866470360599, gradient norm: 0.08386154170086256\n",
      "Iteration: 3770000, loss: 0.10810802569352602, gradient norm: 0.09250987099701434\n",
      "Iteration: 3771000, loss: 0.10810731101982646, gradient norm: 0.23587870771695824\n",
      "Iteration: 3772000, loss: 0.10810658166841208, gradient norm: 0.4548777622397782\n",
      "Iteration: 3773000, loss: 0.10810607145667733, gradient norm: 0.23738764308925472\n",
      "Iteration: 3774000, loss: 0.10810539505979326, gradient norm: 0.05464710591621606\n",
      "Iteration: 3775000, loss: 0.10810458861067267, gradient norm: 0.0969203617310933\n",
      "Iteration: 3776000, loss: 0.10810398060273343, gradient norm: 0.09645742230094831\n",
      "Iteration: 3777000, loss: 0.10810343907937094, gradient norm: 0.13498446083057164\n",
      "Iteration: 3778000, loss: 0.10810246314732859, gradient norm: 0.10006487984242304\n",
      "Iteration: 3779000, loss: 0.10810191439117779, gradient norm: 0.16667290761395898\n",
      "Iteration: 3780000, loss: 0.10810143953357693, gradient norm: 0.5089832850202289\n",
      "Iteration: 3781000, loss: 0.10810059307386971, gradient norm: 0.06744438277555892\n",
      "Iteration: 3782000, loss: 0.1080999928299002, gradient norm: 0.504559570721038\n",
      "Iteration: 3783000, loss: 0.10809936279843922, gradient norm: 0.09828050756853032\n",
      "Iteration: 3784000, loss: 0.10809870569119301, gradient norm: 0.09378624382851086\n",
      "Iteration: 3785000, loss: 0.10809790927156641, gradient norm: 0.031578679422522536\n",
      "Iteration: 3786000, loss: 0.1080972976684254, gradient norm: 0.21617959620425217\n",
      "Iteration: 3787000, loss: 0.10809666618905268, gradient norm: 0.053066113664482356\n",
      "Iteration: 3788000, loss: 0.10809595078600195, gradient norm: 0.04721145775011033\n",
      "Iteration: 3789000, loss: 0.10809543985689105, gradient norm: 0.1558992883017953\n",
      "Iteration: 3790000, loss: 0.1080946080379216, gradient norm: 0.4685885874749706\n",
      "Iteration: 3791000, loss: 0.10809394502881073, gradient norm: 0.3352182698968562\n",
      "Iteration: 3792000, loss: 0.10809335102552625, gradient norm: 0.06612348445743926\n",
      "Iteration: 3793000, loss: 0.10809275004746814, gradient norm: 0.1042779812853146\n",
      "Iteration: 3794000, loss: 0.10809205960255786, gradient norm: 0.11916385441711431\n",
      "Iteration: 3795000, loss: 0.10809133771650935, gradient norm: 0.13492992596182893\n",
      "Iteration: 3796000, loss: 0.10809077847970432, gradient norm: 0.2931813382628755\n",
      "Iteration: 3797000, loss: 0.10809000179181762, gradient norm: 0.05599389969386174\n",
      "Iteration: 3798000, loss: 0.10808960776717189, gradient norm: 0.41791242689920743\n",
      "Iteration: 3799000, loss: 0.10808855484027807, gradient norm: 0.11859902262355852\n",
      "Iteration: 3800000, loss: 0.10808812879486979, gradient norm: 0.0570910303576817\n",
      "Iteration: 3801000, loss: 0.10808744224234917, gradient norm: 0.05516888515419194\n",
      "Iteration: 3802000, loss: 0.10808679355135058, gradient norm: 0.046308658204181646\n",
      "Iteration: 3803000, loss: 0.108086158316454, gradient norm: 0.06590918432245281\n",
      "Iteration: 3804000, loss: 0.10808549374598743, gradient norm: 0.13798257406580497\n",
      "Iteration: 3805000, loss: 0.10808490795173335, gradient norm: 0.06840209588812399\n",
      "Iteration: 3806000, loss: 0.10808407716090758, gradient norm: 0.06796655224175957\n",
      "Iteration: 3807000, loss: 0.10808350369947572, gradient norm: 0.13905170422969704\n",
      "Iteration: 3808000, loss: 0.10808293571118044, gradient norm: 0.07139433320512042\n",
      "Iteration: 3809000, loss: 0.10808224273347763, gradient norm: 0.039382935887872235\n",
      "Iteration: 3810000, loss: 0.10808165090574709, gradient norm: 0.44610258065376657\n",
      "Iteration: 3811000, loss: 0.1080808747716058, gradient norm: 0.10585253400457582\n",
      "Iteration: 3812000, loss: 0.10808030876567312, gradient norm: 0.2628834691198059\n",
      "Iteration: 3813000, loss: 0.1080797617267267, gradient norm: 0.15572738492800647\n",
      "Iteration: 3814000, loss: 0.10807902343711089, gradient norm: 0.06371377576025408\n",
      "Iteration: 3815000, loss: 0.10807826797613561, gradient norm: 0.07908163080560403\n",
      "Iteration: 3816000, loss: 0.10807773257996578, gradient norm: 0.07395971169660127\n",
      "Iteration: 3817000, loss: 0.10807699550190632, gradient norm: 0.054480215636290207\n",
      "Iteration: 3818000, loss: 0.10807646692437464, gradient norm: 0.10821573194839329\n",
      "Iteration: 3819000, loss: 0.10807571920173153, gradient norm: 0.41116007632308366\n",
      "Iteration: 3820000, loss: 0.1080751666855127, gradient norm: 0.04271705248821298\n",
      "Iteration: 3821000, loss: 0.10807442385114771, gradient norm: 0.4458296036696382\n",
      "Iteration: 3822000, loss: 0.10807389183735297, gradient norm: 0.06460777300813951\n",
      "Iteration: 3823000, loss: 0.10807321112203594, gradient norm: 0.08845268231895259\n",
      "Iteration: 3824000, loss: 0.10807260135795224, gradient norm: 0.12317975618916402\n",
      "Iteration: 3825000, loss: 0.10807184681638612, gradient norm: 0.052688854466221305\n",
      "Iteration: 3826000, loss: 0.10807126317987031, gradient norm: 0.24520337187313113\n",
      "Iteration: 3827000, loss: 0.10807071845765306, gradient norm: 0.030592250356142908\n",
      "Iteration: 3828000, loss: 0.10806992902497554, gradient norm: 0.09449692111020495\n",
      "Iteration: 3829000, loss: 0.1080693890583685, gradient norm: 0.2971372056474819\n",
      "Iteration: 3830000, loss: 0.10806863835141399, gradient norm: 0.10454124667252217\n",
      "Iteration: 3831000, loss: 0.10806799143366365, gradient norm: 0.12993561870246595\n",
      "Iteration: 3832000, loss: 0.10806762387200183, gradient norm: 0.009691087893112633\n",
      "Iteration: 3833000, loss: 0.10806668640867995, gradient norm: 0.051551018280536674\n",
      "Iteration: 3834000, loss: 0.10806620482958706, gradient norm: 0.43361746282030583\n",
      "Iteration: 3835000, loss: 0.10806559908522272, gradient norm: 0.030934995606391895\n",
      "Iteration: 3836000, loss: 0.10806483484819597, gradient norm: 0.10645361198203249\n",
      "Iteration: 3837000, loss: 0.1080642805893779, gradient norm: 0.27785098766355304\n",
      "Iteration: 3838000, loss: 0.10806366018546708, gradient norm: 0.1586535994594302\n",
      "Iteration: 3839000, loss: 0.10806292706268575, gradient norm: 0.22897232083603278\n",
      "Iteration: 3840000, loss: 0.10806232748658313, gradient norm: 0.3967971920702589\n",
      "Iteration: 3841000, loss: 0.10806168564450866, gradient norm: 0.047379753799478286\n",
      "Iteration: 3842000, loss: 0.10806112754322655, gradient norm: 0.3945254610478431\n",
      "Iteration: 3843000, loss: 0.10806046177174028, gradient norm: 0.3954753002968642\n",
      "Iteration: 3844000, loss: 0.10805974165860457, gradient norm: 0.10978939420878048\n",
      "Iteration: 3845000, loss: 0.10805928427980606, gradient norm: 0.34298195475151\n",
      "Iteration: 3846000, loss: 0.10805850389499705, gradient norm: 0.7702865815477788\n",
      "Iteration: 3847000, loss: 0.10805790529977756, gradient norm: 0.7065768960050065\n",
      "Iteration: 3848000, loss: 0.10805733810137151, gradient norm: 0.00798104361830749\n",
      "Iteration: 3849000, loss: 0.10805667610294048, gradient norm: 0.11028618587296027\n",
      "Iteration: 3850000, loss: 0.10805596999374636, gradient norm: 0.006927863811483848\n",
      "Iteration: 3851000, loss: 0.108055402029357, gradient norm: 0.09349047557462864\n",
      "Iteration: 3852000, loss: 0.10805487102997127, gradient norm: 0.33267213554031255\n",
      "Iteration: 3853000, loss: 0.10805406143860577, gradient norm: 0.08906441496966344\n",
      "Iteration: 3854000, loss: 0.10805351724591115, gradient norm: 0.10639682578441464\n",
      "Iteration: 3855000, loss: 0.10805282698072358, gradient norm: 0.48245423159666145\n",
      "Iteration: 3856000, loss: 0.10805217839098208, gradient norm: 0.12414040751410252\n",
      "Iteration: 3857000, loss: 0.10805179440185302, gradient norm: 0.11948379951334182\n",
      "Iteration: 3858000, loss: 0.10805093003227507, gradient norm: 0.008973719139414582\n",
      "Iteration: 3859000, loss: 0.10805039168236939, gradient norm: 0.08818594231605265\n",
      "Iteration: 3860000, loss: 0.10804971810740306, gradient norm: 0.30486780405780217\n",
      "Iteration: 3861000, loss: 0.10804904173608251, gradient norm: 0.04966601332839527\n",
      "Iteration: 3862000, loss: 0.10804855343696869, gradient norm: 0.29955636427569077\n",
      "Iteration: 3863000, loss: 0.10804793878526434, gradient norm: 0.11435019823678148\n",
      "Iteration: 3864000, loss: 0.10804700715719086, gradient norm: 0.27913742794844787\n",
      "Iteration: 3865000, loss: 0.10804668512137386, gradient norm: 0.03390974605541975\n",
      "Iteration: 3866000, loss: 0.10804601417319111, gradient norm: 0.1358403080058309\n",
      "Iteration: 3867000, loss: 0.10804528969206549, gradient norm: 0.1402645902985274\n",
      "Iteration: 3868000, loss: 0.10804467413646862, gradient norm: 0.17292803455612357\n",
      "Iteration: 3869000, loss: 0.10804424364396259, gradient norm: 0.016222048442445878\n",
      "Iteration: 3870000, loss: 0.10804344660233213, gradient norm: 0.12910838171199457\n",
      "Iteration: 3871000, loss: 0.10804286992921265, gradient norm: 0.06262520936887649\n",
      "Iteration: 3872000, loss: 0.10804219672705886, gradient norm: 0.03806213055430005\n",
      "Iteration: 3873000, loss: 0.10804166900182387, gradient norm: 0.12964361684262404\n",
      "Iteration: 3874000, loss: 0.10804101768539161, gradient norm: 0.4618696584734023\n",
      "Iteration: 3875000, loss: 0.10804025035323482, gradient norm: 0.010237236204193272\n",
      "Iteration: 3876000, loss: 0.10803971722114313, gradient norm: 0.4942271124060716\n",
      "Iteration: 3877000, loss: 0.10803918361253284, gradient norm: 0.0031771726062659604\n",
      "Iteration: 3878000, loss: 0.10803839316171619, gradient norm: 0.05204392409875626\n",
      "Iteration: 3879000, loss: 0.1080379902177126, gradient norm: 0.028004645220591234\n",
      "Iteration: 3880000, loss: 0.10803730823020928, gradient norm: 0.38910649042760964\n",
      "Iteration: 3881000, loss: 0.10803656097206829, gradient norm: 0.1176520706134615\n",
      "Iteration: 3882000, loss: 0.10803600687315748, gradient norm: 0.041245457155783954\n",
      "Iteration: 3883000, loss: 0.1080353868339993, gradient norm: 0.02778371795820198\n",
      "Iteration: 3884000, loss: 0.1080347718498727, gradient norm: 0.41608810009451447\n",
      "Iteration: 3885000, loss: 0.10803420725826043, gradient norm: 0.5353604360432763\n",
      "Iteration: 3886000, loss: 0.10803351819524294, gradient norm: 0.2002572869278093\n",
      "Iteration: 3887000, loss: 0.10803288872301803, gradient norm: 0.42671519647334405\n",
      "Iteration: 3888000, loss: 0.10803239042122917, gradient norm: 0.03596365929314691\n",
      "Iteration: 3889000, loss: 0.10803160535445967, gradient norm: 0.030547236764110808\n",
      "Iteration: 3890000, loss: 0.10803106919704467, gradient norm: 0.08524602888414079\n",
      "Iteration: 3891000, loss: 0.10803042505615292, gradient norm: 0.018685032274476793\n",
      "Iteration: 3892000, loss: 0.10802980628736705, gradient norm: 0.017073359431399832\n",
      "Iteration: 3893000, loss: 0.10802918766953734, gradient norm: 0.037485296814973756\n",
      "Iteration: 3894000, loss: 0.10802867637417611, gradient norm: 0.09525011330050807\n",
      "Iteration: 3895000, loss: 0.10802787997832945, gradient norm: 0.3266605577264446\n",
      "Iteration: 3896000, loss: 0.10802726002087815, gradient norm: 0.0036148494903605537\n",
      "Iteration: 3897000, loss: 0.10802686455922006, gradient norm: 0.22527466908614624\n",
      "Iteration: 3898000, loss: 0.10802619105376698, gradient norm: 0.04014686804900457\n",
      "Iteration: 3899000, loss: 0.10802539053458801, gradient norm: 0.12925036660389302\n",
      "Iteration: 3900000, loss: 0.10802486241635896, gradient norm: 0.08767483778478355\n",
      "Iteration: 3901000, loss: 0.10802430532666087, gradient norm: 0.013269657241377358\n",
      "Iteration: 3902000, loss: 0.1080237063561126, gradient norm: 0.1591887673428007\n",
      "Iteration: 3903000, loss: 0.10802301900225136, gradient norm: 0.45478981825371284\n",
      "Iteration: 3904000, loss: 0.10802238149487767, gradient norm: 0.40272160292209513\n",
      "Iteration: 3905000, loss: 0.10802173696883076, gradient norm: 0.2465547162307104\n",
      "Iteration: 3906000, loss: 0.10802128042542003, gradient norm: 0.04217088770080124\n",
      "Iteration: 3907000, loss: 0.10802057521955151, gradient norm: 0.21367128276648797\n",
      "Iteration: 3908000, loss: 0.1080198996542704, gradient norm: 0.033699980444619033\n",
      "Iteration: 3909000, loss: 0.10801934711748731, gradient norm: 0.5325494431648747\n",
      "Iteration: 3910000, loss: 0.1080187595382676, gradient norm: 0.03306842149366486\n",
      "Iteration: 3911000, loss: 0.10801814744549829, gradient norm: 0.48756466119274433\n",
      "Iteration: 3912000, loss: 0.10801750307120211, gradient norm: 0.028301575560866668\n",
      "Iteration: 3913000, loss: 0.10801680982391751, gradient norm: 0.12331154395059551\n",
      "Iteration: 3914000, loss: 0.10801630462108099, gradient norm: 0.02285342496695887\n",
      "Iteration: 3915000, loss: 0.1080156412978596, gradient norm: 0.25505410686186025\n",
      "Iteration: 3916000, loss: 0.10801509003582115, gradient norm: 0.1582604767418956\n",
      "Iteration: 3917000, loss: 0.10801445899547724, gradient norm: 0.11105872314797292\n",
      "Iteration: 3918000, loss: 0.10801376122901626, gradient norm: 0.3349505743543667\n",
      "Iteration: 3919000, loss: 0.10801316590048228, gradient norm: 0.0707002885961059\n",
      "Iteration: 3920000, loss: 0.10801251176917918, gradient norm: 0.2345524546271032\n",
      "Iteration: 3921000, loss: 0.10801204255970676, gradient norm: 0.35922797275545026\n",
      "Iteration: 3922000, loss: 0.10801136414878801, gradient norm: 0.04283025989279018\n",
      "Iteration: 3923000, loss: 0.10801087933314242, gradient norm: 0.030398088372742566\n",
      "Iteration: 3924000, loss: 0.10800998144916986, gradient norm: 0.21147238442471633\n",
      "Iteration: 3925000, loss: 0.10800952639489149, gradient norm: 0.08635419043449469\n",
      "Iteration: 3926000, loss: 0.10800898071223762, gradient norm: 0.09735852397094628\n",
      "Iteration: 3927000, loss: 0.10800813682160504, gradient norm: 0.07912580571713164\n",
      "Iteration: 3928000, loss: 0.10800766459867485, gradient norm: 0.5120791455927703\n",
      "Iteration: 3929000, loss: 0.1080070135650248, gradient norm: 0.06496149581061167\n",
      "Iteration: 3930000, loss: 0.10800643594103201, gradient norm: 0.08283544535996919\n",
      "Iteration: 3931000, loss: 0.10800582184425943, gradient norm: 0.05477956427417465\n",
      "Iteration: 3932000, loss: 0.10800522967712234, gradient norm: 0.3000756299024434\n",
      "Iteration: 3933000, loss: 0.10800460294490045, gradient norm: 0.2740758043981602\n",
      "Iteration: 3934000, loss: 0.10800385432165484, gradient norm: 0.0440519637171671\n",
      "Iteration: 3935000, loss: 0.10800330679391247, gradient norm: 0.1442755968006783\n",
      "Iteration: 3936000, loss: 0.10800264207356716, gradient norm: 0.25802967920900555\n",
      "Iteration: 3937000, loss: 0.10800222524218418, gradient norm: 0.08716028347085951\n",
      "Iteration: 3938000, loss: 0.1080013062073777, gradient norm: 0.17898033440064315\n",
      "Iteration: 3939000, loss: 0.1080007980614861, gradient norm: 0.301866155922994\n",
      "Iteration: 3940000, loss: 0.10800018700130912, gradient norm: 0.257429819168022\n",
      "Iteration: 3941000, loss: 0.10799952126235385, gradient norm: 0.008221040426922956\n",
      "Iteration: 3942000, loss: 0.10799888553759641, gradient norm: 0.08395254533397187\n",
      "Iteration: 3943000, loss: 0.10799828448417975, gradient norm: 0.20145925973067147\n",
      "Iteration: 3944000, loss: 0.10799763596631923, gradient norm: 0.2623714603239191\n",
      "Iteration: 3945000, loss: 0.10799680904102059, gradient norm: 0.5386176145038181\n",
      "Iteration: 3946000, loss: 0.10799632232261579, gradient norm: 0.23871416297544282\n",
      "Iteration: 3947000, loss: 0.10799557622336482, gradient norm: 0.4432431253139262\n",
      "Iteration: 3948000, loss: 0.10799489236582195, gradient norm: 0.2738340109253607\n",
      "Iteration: 3949000, loss: 0.10799411041287901, gradient norm: 0.20572884362172825\n",
      "Iteration: 3950000, loss: 0.1079933331600229, gradient norm: 0.07368509605631811\n",
      "Iteration: 3951000, loss: 0.10799260545038947, gradient norm: 0.01700265367302144\n",
      "Iteration: 3952000, loss: 0.10799166805956287, gradient norm: 0.2918027635703207\n",
      "Iteration: 3953000, loss: 0.1079908543833489, gradient norm: 0.015229214055970074\n",
      "Iteration: 3954000, loss: 0.10798960148400857, gradient norm: 0.10594230156202371\n",
      "Iteration: 3955000, loss: 0.10798791299946632, gradient norm: 0.25345609760361837\n",
      "Iteration: 3956000, loss: 0.10798567373611274, gradient norm: 0.3487719920073843\n",
      "Iteration: 3957000, loss: 0.10798140341470963, gradient norm: 0.6360199092243747\n",
      "Iteration: 3958000, loss: 0.10797496076071642, gradient norm: 0.08397105821295456\n",
      "Iteration: 3959000, loss: 0.10796726658344515, gradient norm: 0.21582405857209028\n",
      "Iteration: 3960000, loss: 0.1079600861605079, gradient norm: 0.2925733660691661\n",
      "Iteration: 3961000, loss: 0.10795385444272591, gradient norm: 0.3077574517088335\n",
      "Iteration: 3962000, loss: 0.10794824675218061, gradient norm: 0.4251931720582744\n",
      "Iteration: 3963000, loss: 0.10794338982972009, gradient norm: 0.37527779591821153\n",
      "Iteration: 3964000, loss: 0.10793918618804184, gradient norm: 0.08592452974216595\n",
      "Iteration: 3965000, loss: 0.10793504285777232, gradient norm: 0.43714057582457005\n",
      "Iteration: 3966000, loss: 0.10793171389426774, gradient norm: 0.12076418290491005\n",
      "Iteration: 3967000, loss: 0.10792856185045338, gradient norm: 0.14886704244231969\n",
      "Iteration: 3968000, loss: 0.10792576147394331, gradient norm: 0.12795579698486367\n",
      "Iteration: 3969000, loss: 0.10792309054474819, gradient norm: 0.166243856785808\n",
      "Iteration: 3970000, loss: 0.10792100981187958, gradient norm: 0.05733150789423114\n",
      "Iteration: 3971000, loss: 0.10791906772465398, gradient norm: 0.07180075376469292\n",
      "Iteration: 3972000, loss: 0.107917020373026, gradient norm: 0.2234737064602277\n",
      "Iteration: 3973000, loss: 0.10791532023624927, gradient norm: 0.44570556546266005\n",
      "Iteration: 3974000, loss: 0.10791378058230122, gradient norm: 0.11095649374290063\n",
      "Iteration: 3975000, loss: 0.10791231614493826, gradient norm: 0.04045956266590341\n",
      "Iteration: 3976000, loss: 0.10791083103050333, gradient norm: 0.1111392588721754\n",
      "Iteration: 3977000, loss: 0.1079095582567606, gradient norm: 0.12104550735083763\n",
      "Iteration: 3978000, loss: 0.10790848014124542, gradient norm: 0.03486067146394513\n",
      "Iteration: 3979000, loss: 0.1079070901748559, gradient norm: 0.14487260540120128\n",
      "Iteration: 3980000, loss: 0.10790593308698626, gradient norm: 0.18522971998631457\n",
      "Iteration: 3981000, loss: 0.10790492402829785, gradient norm: 0.24756903271903305\n",
      "Iteration: 3982000, loss: 0.10790378783547129, gradient norm: 0.09524580136386544\n",
      "Iteration: 3983000, loss: 0.10790285952380717, gradient norm: 0.05358490316208854\n",
      "Iteration: 3984000, loss: 0.10790180697013661, gradient norm: 0.042976927694928384\n",
      "Iteration: 3985000, loss: 0.10790071200589074, gradient norm: 0.06516511617013922\n",
      "Iteration: 3986000, loss: 0.107899826320721, gradient norm: 0.11300963507217066\n",
      "Iteration: 3987000, loss: 0.10789892653139076, gradient norm: 0.0539107111547198\n",
      "Iteration: 3988000, loss: 0.10789795015027824, gradient norm: 0.27220548152378854\n",
      "Iteration: 3989000, loss: 0.10789716467691761, gradient norm: 0.22006102352421109\n",
      "Iteration: 3990000, loss: 0.1078960546577732, gradient norm: 0.02125183316841223\n",
      "Iteration: 3991000, loss: 0.10789528278169658, gradient norm: 0.7207676679307576\n",
      "Iteration: 3992000, loss: 0.1078942853434936, gradient norm: 0.3668964943301868\n",
      "Iteration: 3993000, loss: 0.10789355819024428, gradient norm: 0.10535058440549804\n",
      "Iteration: 3994000, loss: 0.10789272116622911, gradient norm: 0.34588306367644844\n",
      "Iteration: 3995000, loss: 0.1078919246276219, gradient norm: 0.29692758585523216\n",
      "Iteration: 3996000, loss: 0.10789079152415194, gradient norm: 0.31609510771847743\n",
      "Iteration: 3997000, loss: 0.1078901571649625, gradient norm: 0.14558471947286883\n",
      "Iteration: 3998000, loss: 0.10788937898869798, gradient norm: 0.09696510405191082\n",
      "Iteration: 3999000, loss: 0.10788848061435297, gradient norm: 0.24464150596564857\n",
      "Iteration: 4000000, loss: 0.10788776217218705, gradient norm: 0.02176116896746305\n",
      "Iteration: 4001000, loss: 0.10788702754765103, gradient norm: 0.15142480071393002\n",
      "Iteration: 4002000, loss: 0.10788598921347953, gradient norm: 0.17354701385420515\n",
      "Iteration: 4003000, loss: 0.10788543267834569, gradient norm: 0.22085537193562252\n",
      "Iteration: 4004000, loss: 0.1078845274267878, gradient norm: 0.024476440381645807\n",
      "Iteration: 4005000, loss: 0.10788381622928066, gradient norm: 0.017486415559794605\n",
      "Iteration: 4006000, loss: 0.10788304412313805, gradient norm: 0.24648416747474777\n",
      "Iteration: 4007000, loss: 0.10788217975514419, gradient norm: 0.06533193692065756\n",
      "Iteration: 4008000, loss: 0.10788153002187992, gradient norm: 0.0033819221768211054\n",
      "Iteration: 4009000, loss: 0.10788070363662014, gradient norm: 0.16816376239251768\n",
      "Iteration: 4010000, loss: 0.10788002086147558, gradient norm: 0.25731542159947396\n",
      "Iteration: 4011000, loss: 0.1078791238038329, gradient norm: 0.34934783442716294\n",
      "Iteration: 4012000, loss: 0.10787855353511389, gradient norm: 0.004388739321645057\n",
      "Iteration: 4013000, loss: 0.10787764961599533, gradient norm: 0.26882074985377225\n",
      "Iteration: 4014000, loss: 0.10787695957202371, gradient norm: 0.09892020176475055\n",
      "Iteration: 4015000, loss: 0.107876205374014, gradient norm: 0.09162335485785547\n",
      "Iteration: 4016000, loss: 0.10787546092445464, gradient norm: 0.08915501366004236\n",
      "Iteration: 4017000, loss: 0.10787476895648647, gradient norm: 0.3157459075928755\n",
      "Iteration: 4018000, loss: 0.10787391018231161, gradient norm: 0.04797036331196303\n",
      "Iteration: 4019000, loss: 0.10787347159463355, gradient norm: 0.42893590160309825\n",
      "Iteration: 4020000, loss: 0.10787245793836249, gradient norm: 0.4040558969029825\n",
      "Iteration: 4021000, loss: 0.10787181235895694, gradient norm: 0.0654603293339336\n",
      "Iteration: 4022000, loss: 0.10787107237309346, gradient norm: 0.16018517372335334\n",
      "Iteration: 4023000, loss: 0.10787046853364952, gradient norm: 0.2422000390729395\n",
      "Iteration: 4024000, loss: 0.107869612936413, gradient norm: 0.21766110202471783\n",
      "Iteration: 4025000, loss: 0.10786890870025724, gradient norm: 0.31412641701965943\n",
      "Iteration: 4026000, loss: 0.10786828115307413, gradient norm: 0.16344586124236207\n",
      "Iteration: 4027000, loss: 0.10786748698803017, gradient norm: 0.25610173628523075\n",
      "Iteration: 4028000, loss: 0.10786681690654012, gradient norm: 0.1868141819614556\n",
      "Iteration: 4029000, loss: 0.10786605197743802, gradient norm: 0.16507972487557993\n",
      "Iteration: 4030000, loss: 0.10786534406418215, gradient norm: 0.12152921303109966\n",
      "Iteration: 4031000, loss: 0.10786469917329525, gradient norm: 0.12881777204208783\n",
      "Iteration: 4032000, loss: 0.10786395777587608, gradient norm: 0.05522503321970483\n",
      "Iteration: 4033000, loss: 0.10786323729208869, gradient norm: 0.42530313528003344\n",
      "Iteration: 4034000, loss: 0.10786251607343038, gradient norm: 0.06884261829917304\n",
      "Iteration: 4035000, loss: 0.107861952625505, gradient norm: 0.14773129174879868\n",
      "Iteration: 4036000, loss: 0.10786117845446869, gradient norm: 0.11058377248818685\n",
      "Iteration: 4037000, loss: 0.10786038301178207, gradient norm: 0.1727535930544684\n",
      "Iteration: 4038000, loss: 0.10785974506029566, gradient norm: 0.00955143440485754\n",
      "Iteration: 4039000, loss: 0.10785904852076202, gradient norm: 0.2454438089119373\n",
      "Iteration: 4040000, loss: 0.10785832160451446, gradient norm: 0.034288441841343296\n",
      "Iteration: 4041000, loss: 0.1078576680700842, gradient norm: 0.008418765007523107\n",
      "Iteration: 4042000, loss: 0.10785701734802317, gradient norm: 0.5562714304852734\n",
      "Iteration: 4043000, loss: 0.10785623618159224, gradient norm: 0.2166024268624029\n",
      "Iteration: 4044000, loss: 0.10785559153102908, gradient norm: 0.04167483046914617\n",
      "Iteration: 4045000, loss: 0.10785497362478087, gradient norm: 0.05824486970126696\n",
      "Iteration: 4046000, loss: 0.1078541802939266, gradient norm: 0.1686544636370829\n",
      "Iteration: 4047000, loss: 0.10785354124913211, gradient norm: 0.2029793065141057\n",
      "Iteration: 4048000, loss: 0.1078527428973353, gradient norm: 0.22859507851392427\n",
      "Iteration: 4049000, loss: 0.10785225140188578, gradient norm: 0.16369096268691227\n",
      "Iteration: 4050000, loss: 0.10785135867347637, gradient norm: 0.08882468431219191\n",
      "Iteration: 4051000, loss: 0.1078508577182991, gradient norm: 0.07774654072287303\n",
      "Iteration: 4052000, loss: 0.10785005641913396, gradient norm: 0.13419682180001002\n",
      "Iteration: 4053000, loss: 0.10784937783687926, gradient norm: 0.17667108138373858\n",
      "Iteration: 4054000, loss: 0.10784878960773703, gradient norm: 0.1754030540404397\n",
      "Iteration: 4055000, loss: 0.10784799254143357, gradient norm: 0.01844209086260523\n",
      "Iteration: 4056000, loss: 0.10784743120298798, gradient norm: 0.17148210543399692\n",
      "Iteration: 4057000, loss: 0.1078466487651786, gradient norm: 0.4453192844597359\n",
      "Iteration: 4058000, loss: 0.1078460658735669, gradient norm: 0.12067673500889292\n",
      "Iteration: 4059000, loss: 0.10784524213162566, gradient norm: 0.07410612462910332\n",
      "Iteration: 4060000, loss: 0.1078447363451956, gradient norm: 0.4259789922955714\n",
      "Iteration: 4061000, loss: 0.10784395890400884, gradient norm: 0.5461131204151967\n",
      "Iteration: 4062000, loss: 0.10784328958359529, gradient norm: 0.14611958172188905\n",
      "Iteration: 4063000, loss: 0.1078426234528628, gradient norm: 0.08527833673197022\n",
      "Iteration: 4064000, loss: 0.10784204583926398, gradient norm: 0.326184901200371\n",
      "Iteration: 4065000, loss: 0.10784118547451982, gradient norm: 0.12129753499962677\n",
      "Iteration: 4066000, loss: 0.10784064499036813, gradient norm: 0.25311122713281037\n",
      "Iteration: 4067000, loss: 0.10784000254253931, gradient norm: 0.2455135863302033\n",
      "Iteration: 4068000, loss: 0.10783921025666271, gradient norm: 0.28155318939260426\n",
      "Iteration: 4069000, loss: 0.1078385981590169, gradient norm: 0.15408581706616503\n",
      "Iteration: 4070000, loss: 0.10783798046718976, gradient norm: 0.14652260722770685\n",
      "Iteration: 4071000, loss: 0.1078372435639156, gradient norm: 0.10881763447536826\n",
      "Iteration: 4072000, loss: 0.1078367177750994, gradient norm: 0.1708113890813132\n",
      "Iteration: 4073000, loss: 0.10783583080962014, gradient norm: 0.3952836875492386\n",
      "Iteration: 4074000, loss: 0.10783531049809815, gradient norm: 0.03241456479158599\n",
      "Iteration: 4075000, loss: 0.1078347104699448, gradient norm: 0.07305170515557861\n",
      "Iteration: 4076000, loss: 0.10783394864178925, gradient norm: 0.049564050994468825\n",
      "Iteration: 4077000, loss: 0.10783336971038611, gradient norm: 0.04554435728227365\n",
      "Iteration: 4078000, loss: 0.1078326499052963, gradient norm: 0.1231595966936296\n",
      "Iteration: 4079000, loss: 0.10783196853284228, gradient norm: 0.2223224978864703\n",
      "Iteration: 4080000, loss: 0.10783122861376176, gradient norm: 0.4642554232450407\n",
      "Iteration: 4081000, loss: 0.10783063344791338, gradient norm: 0.28964310888077105\n",
      "Iteration: 4082000, loss: 0.10783004599989002, gradient norm: 0.18192249298870825\n",
      "Iteration: 4083000, loss: 0.10782935274138672, gradient norm: 0.008283150953342075\n",
      "Iteration: 4084000, loss: 0.10782871021204904, gradient norm: 0.10401236041697377\n",
      "Iteration: 4085000, loss: 0.10782796544471102, gradient norm: 0.0798254452871093\n",
      "Iteration: 4086000, loss: 0.10782728507502641, gradient norm: 0.22189807436229514\n",
      "Iteration: 4087000, loss: 0.1078267445946567, gradient norm: 0.26250773136793504\n",
      "Iteration: 4088000, loss: 0.107826027891558, gradient norm: 0.3207509097099136\n",
      "Iteration: 4089000, loss: 0.10782538452794777, gradient norm: 0.10637811272024707\n",
      "Iteration: 4090000, loss: 0.10782481559548282, gradient norm: 0.02358289780070264\n",
      "Iteration: 4091000, loss: 0.10782405320586998, gradient norm: 0.06499985373499798\n",
      "Iteration: 4092000, loss: 0.1078234552179357, gradient norm: 0.508338507589279\n",
      "Iteration: 4093000, loss: 0.10782287743192347, gradient norm: 0.1369981819051967\n",
      "Iteration: 4094000, loss: 0.10782205127536998, gradient norm: 0.16998653350679185\n",
      "Iteration: 4095000, loss: 0.10782147376133122, gradient norm: 0.1304326851025399\n",
      "Iteration: 4096000, loss: 0.1078208417097371, gradient norm: 0.022224756523855332\n",
      "Iteration: 4097000, loss: 0.10782018342008078, gradient norm: 0.06468403232727542\n",
      "Iteration: 4098000, loss: 0.10781944393862611, gradient norm: 0.04832949981930836\n",
      "Iteration: 4099000, loss: 0.10781892100510683, gradient norm: 0.23069352421578293\n",
      "Iteration: 4100000, loss: 0.10781808161944739, gradient norm: 0.3127209983628615\n",
      "Iteration: 4101000, loss: 0.10781773197954277, gradient norm: 0.17097012977706738\n",
      "Iteration: 4102000, loss: 0.10781695625818218, gradient norm: 0.1997440358813103\n",
      "Iteration: 4103000, loss: 0.10781636178519487, gradient norm: 0.3192919892595473\n",
      "Iteration: 4104000, loss: 0.10781549934606491, gradient norm: 0.5725072541946378\n",
      "Iteration: 4105000, loss: 0.10781490054050952, gradient norm: 0.16256653870177826\n",
      "Iteration: 4106000, loss: 0.1078144264230178, gradient norm: 0.34850190456410624\n",
      "Iteration: 4107000, loss: 0.10781375712072523, gradient norm: 0.17353905301690634\n",
      "Iteration: 4108000, loss: 0.10781307640764362, gradient norm: 0.20206195331474708\n",
      "Iteration: 4109000, loss: 0.10781232267173105, gradient norm: 0.11035151287550098\n",
      "Iteration: 4110000, loss: 0.10781171051651549, gradient norm: 0.10889993868347417\n",
      "Iteration: 4111000, loss: 0.1078112426947619, gradient norm: 0.013506667834115272\n",
      "Iteration: 4112000, loss: 0.1078103704014475, gradient norm: 0.17811461048069727\n",
      "Iteration: 4113000, loss: 0.10780979045080599, gradient norm: 0.1676654375005859\n",
      "Iteration: 4114000, loss: 0.10780923274216998, gradient norm: 0.2334680094245648\n",
      "Iteration: 4115000, loss: 0.10780857777505506, gradient norm: 0.43340702707147066\n",
      "Iteration: 4116000, loss: 0.1078078822864143, gradient norm: 0.2366214196105669\n",
      "Iteration: 4117000, loss: 0.10780734559653823, gradient norm: 0.28461988606614524\n",
      "Iteration: 4118000, loss: 0.10780654205386947, gradient norm: 0.0018453257740108293\n",
      "Iteration: 4119000, loss: 0.10780604718758968, gradient norm: 0.04835880114158447\n",
      "Iteration: 4120000, loss: 0.10780530424853364, gradient norm: 0.48167185638831245\n",
      "Iteration: 4121000, loss: 0.10780468397482915, gradient norm: 0.3840024583532995\n",
      "Iteration: 4122000, loss: 0.10780409554289701, gradient norm: 0.11998499927869923\n",
      "Iteration: 4123000, loss: 0.10780344835875813, gradient norm: 0.07044394795197484\n",
      "Iteration: 4124000, loss: 0.107802758005559, gradient norm: 0.5079631927330663\n",
      "Iteration: 4125000, loss: 0.10780220653692169, gradient norm: 0.10245936604245254\n",
      "Iteration: 4126000, loss: 0.1078015887018142, gradient norm: 0.1581714011423157\n",
      "Iteration: 4127000, loss: 0.10780087119821151, gradient norm: 0.6911663430851965\n",
      "Iteration: 4128000, loss: 0.10780015743440026, gradient norm: 0.4766942749716694\n",
      "Iteration: 4129000, loss: 0.10779964923427289, gradient norm: 0.24906469518614632\n",
      "Iteration: 4130000, loss: 0.1077989048490738, gradient norm: 0.05545714622611699\n",
      "Iteration: 4131000, loss: 0.10779837832349108, gradient norm: 0.07679760225541551\n",
      "Iteration: 4132000, loss: 0.10779773853376068, gradient norm: 0.050379055638066626\n",
      "Iteration: 4133000, loss: 0.10779700948354759, gradient norm: 0.11901579784259067\n",
      "Iteration: 4134000, loss: 0.10779639636622376, gradient norm: 0.008249362288654916\n",
      "Iteration: 4135000, loss: 0.107795813216406, gradient norm: 0.16649236377954005\n",
      "Iteration: 4136000, loss: 0.10779526548129535, gradient norm: 0.16279490929473017\n",
      "Iteration: 4137000, loss: 0.10779442818741419, gradient norm: 0.04301766938959203\n",
      "Iteration: 4138000, loss: 0.10779397469421892, gradient norm: 0.045074592591523695\n",
      "Iteration: 4139000, loss: 0.10779331158395238, gradient norm: 0.469492134237259\n",
      "Iteration: 4140000, loss: 0.10779260691782155, gradient norm: 0.021463582289283096\n",
      "Iteration: 4141000, loss: 0.10779204515921637, gradient norm: 0.34069724948889524\n",
      "Iteration: 4142000, loss: 0.10779141697090719, gradient norm: 0.086805173610055\n",
      "Iteration: 4143000, loss: 0.1077907724098134, gradient norm: 0.5942270468802399\n",
      "Iteration: 4144000, loss: 0.10779013379448245, gradient norm: 0.45788025309109287\n",
      "Iteration: 4145000, loss: 0.10778947479085683, gradient norm: 0.5441101154815492\n",
      "Iteration: 4146000, loss: 0.10778889982120345, gradient norm: 0.06481830221273824\n",
      "Iteration: 4147000, loss: 0.10778816898964706, gradient norm: 0.25292865757571925\n",
      "Iteration: 4148000, loss: 0.10778769148044783, gradient norm: 0.6674047086736525\n",
      "Iteration: 4149000, loss: 0.10778697583948045, gradient norm: 0.2991760812763626\n",
      "Iteration: 4150000, loss: 0.10778647108666871, gradient norm: 0.3313158410508107\n",
      "Iteration: 4151000, loss: 0.10778568605581063, gradient norm: 0.012393157730629812\n",
      "Iteration: 4152000, loss: 0.107785196357328, gradient norm: 0.0726176299567097\n",
      "Iteration: 4153000, loss: 0.10778436554496895, gradient norm: 0.08597968395485117\n",
      "Iteration: 4154000, loss: 0.10778392225324564, gradient norm: 0.07209639520733416\n",
      "Iteration: 4155000, loss: 0.10778329783499732, gradient norm: 0.1059440303326389\n",
      "Iteration: 4156000, loss: 0.10778263826907875, gradient norm: 0.38268262767081523\n",
      "Iteration: 4157000, loss: 0.10778201025526056, gradient norm: 0.07196293842674192\n",
      "Iteration: 4158000, loss: 0.10778133401201422, gradient norm: 0.3110067509425131\n",
      "Iteration: 4159000, loss: 0.10778077240310341, gradient norm: 0.0037875287301886735\n",
      "Iteration: 4160000, loss: 0.10778016197268553, gradient norm: 0.10945503825617547\n",
      "Iteration: 4161000, loss: 0.10777952748145142, gradient norm: 0.13001795162611923\n",
      "Iteration: 4162000, loss: 0.10777886178304046, gradient norm: 0.05244779324268099\n",
      "Iteration: 4163000, loss: 0.10777849182201277, gradient norm: 0.2337556735922117\n",
      "Iteration: 4164000, loss: 0.10777758752369059, gradient norm: 0.19721243786654577\n",
      "Iteration: 4165000, loss: 0.1077769924469402, gradient norm: 0.234280217204772\n",
      "Iteration: 4166000, loss: 0.10777636891491853, gradient norm: 0.028890704938117254\n",
      "Iteration: 4167000, loss: 0.10777590125691056, gradient norm: 0.2505046245769648\n",
      "Iteration: 4168000, loss: 0.10777525124397337, gradient norm: 0.19596764201045497\n",
      "Iteration: 4169000, loss: 0.1077744508416115, gradient norm: 0.01586442250165537\n",
      "Iteration: 4170000, loss: 0.1077738770206032, gradient norm: 0.3982625355823093\n",
      "Iteration: 4171000, loss: 0.10777352493535104, gradient norm: 0.1049942092139379\n",
      "Iteration: 4172000, loss: 0.10777257398181579, gradient norm: 0.18495252740249213\n",
      "Iteration: 4173000, loss: 0.10777211033180273, gradient norm: 0.10675760876624617\n",
      "Iteration: 4174000, loss: 0.10777146290056885, gradient norm: 0.49870210301014156\n",
      "Iteration: 4175000, loss: 0.10777086958832717, gradient norm: 0.08211819982752003\n",
      "Iteration: 4176000, loss: 0.10777030832303212, gradient norm: 0.03272552537873646\n",
      "Iteration: 4177000, loss: 0.10776955750271588, gradient norm: 0.08711628287171032\n",
      "Iteration: 4178000, loss: 0.10776901246707529, gradient norm: 0.09519274329041537\n",
      "Iteration: 4179000, loss: 0.10776848536703094, gradient norm: 0.055403234147220246\n",
      "Iteration: 4180000, loss: 0.10776771237490981, gradient norm: 0.17031288637696898\n",
      "Iteration: 4181000, loss: 0.10776721882615808, gradient norm: 0.25742734916148113\n",
      "Iteration: 4182000, loss: 0.1077664992581124, gradient norm: 0.09784694873948115\n",
      "Iteration: 4183000, loss: 0.10776601042288976, gradient norm: 0.05963815604065585\n",
      "Iteration: 4184000, loss: 0.10776526005938687, gradient norm: 0.3475683685586306\n",
      "Iteration: 4185000, loss: 0.10776471861473103, gradient norm: 0.22151053558091977\n",
      "Iteration: 4186000, loss: 0.10776407372519167, gradient norm: 0.12751231434467256\n",
      "Iteration: 4187000, loss: 0.10776355477145348, gradient norm: 0.17599648884161143\n",
      "Iteration: 4188000, loss: 0.10776286428624506, gradient norm: 0.24594083483441928\n",
      "Iteration: 4189000, loss: 0.10776219264986904, gradient norm: 0.1966314717791699\n",
      "Iteration: 4190000, loss: 0.10776170804359268, gradient norm: 0.47369816523394803\n",
      "Iteration: 4191000, loss: 0.10776109398785852, gradient norm: 0.09474780175555148\n",
      "Iteration: 4192000, loss: 0.10776041700294613, gradient norm: 0.17405154725410293\n",
      "Iteration: 4193000, loss: 0.10775983497086893, gradient norm: 0.16265659459009396\n",
      "Iteration: 4194000, loss: 0.10775914542358236, gradient norm: 0.1878132085074265\n",
      "Iteration: 4195000, loss: 0.10775859437929693, gradient norm: 0.009673219952300605\n",
      "Iteration: 4196000, loss: 0.10775796861158937, gradient norm: 0.03372704349753034\n",
      "Iteration: 4197000, loss: 0.10775736554151892, gradient norm: 0.12408427906571348\n",
      "Iteration: 4198000, loss: 0.10775678714402648, gradient norm: 0.11525173007497831\n",
      "Iteration: 4199000, loss: 0.10775623029244391, gradient norm: 0.267709758960662\n",
      "Iteration: 4200000, loss: 0.10775563850861943, gradient norm: 0.14245137033962688\n",
      "Iteration: 4201000, loss: 0.1077550164323364, gradient norm: 0.09644795032489636\n",
      "Iteration: 4202000, loss: 0.10775434960895054, gradient norm: 0.08898125865999156\n",
      "Iteration: 4203000, loss: 0.10775375950141375, gradient norm: 0.111398380053298\n",
      "Iteration: 4204000, loss: 0.10775308310650956, gradient norm: 0.13476175182001282\n",
      "Iteration: 4205000, loss: 0.10775245457966122, gradient norm: 0.09922683771845481\n",
      "Iteration: 4206000, loss: 0.10775206959319267, gradient norm: 0.14046159784646728\n",
      "Iteration: 4207000, loss: 0.1077512347045531, gradient norm: 0.11657108071275575\n",
      "Iteration: 4208000, loss: 0.10775074252707577, gradient norm: 0.14719775867297855\n",
      "Iteration: 4209000, loss: 0.10775025958763203, gradient norm: 0.12887493273990316\n",
      "Iteration: 4210000, loss: 0.10774944386510137, gradient norm: 0.017154519012338117\n",
      "Iteration: 4211000, loss: 0.10774895864974851, gradient norm: 0.0392897567669728\n",
      "Iteration: 4212000, loss: 0.10774836948230553, gradient norm: 0.7307517720345715\n",
      "Iteration: 4213000, loss: 0.10774762888807453, gradient norm: 0.037050034229068114\n",
      "Iteration: 4214000, loss: 0.10774718385395889, gradient norm: 0.17536574585291761\n",
      "Iteration: 4215000, loss: 0.10774642764784476, gradient norm: 0.053954066750658065\n",
      "Iteration: 4216000, loss: 0.10774590014856558, gradient norm: 0.1508719233837459\n",
      "Iteration: 4217000, loss: 0.10774535664493244, gradient norm: 0.09436322107423492\n",
      "Iteration: 4218000, loss: 0.10774473496356242, gradient norm: 0.6833811843163535\n",
      "Iteration: 4219000, loss: 0.10774405689049975, gradient norm: 0.41761360521347785\n",
      "Iteration: 4220000, loss: 0.10774362517340347, gradient norm: 0.18596699160647912\n",
      "Iteration: 4221000, loss: 0.10774282382755422, gradient norm: 0.14715971032448466\n",
      "Iteration: 4222000, loss: 0.1077422578488947, gradient norm: 0.5240694898453989\n",
      "Iteration: 4223000, loss: 0.10774168350886248, gradient norm: 0.08389161596004664\n",
      "Iteration: 4224000, loss: 0.10774116194153538, gradient norm: 0.28897260076674436\n",
      "Iteration: 4225000, loss: 0.10774043308636928, gradient norm: 0.2174137447208305\n",
      "Iteration: 4226000, loss: 0.10773983806982065, gradient norm: 0.35786712252927166\n",
      "Iteration: 4227000, loss: 0.10773942125184102, gradient norm: 0.2613253186833953\n",
      "Iteration: 4228000, loss: 0.1077386310810024, gradient norm: 0.08315437986505413\n",
      "Iteration: 4229000, loss: 0.10773804276873299, gradient norm: 0.08975955872839529\n",
      "Iteration: 4230000, loss: 0.10773760794140887, gradient norm: 0.056516271982217293\n",
      "Iteration: 4231000, loss: 0.10773698377529627, gradient norm: 0.40927399857326063\n",
      "Iteration: 4232000, loss: 0.10773627371367392, gradient norm: 0.16232683673521103\n",
      "Iteration: 4233000, loss: 0.10773571797670049, gradient norm: 0.010428084364130358\n",
      "Iteration: 4234000, loss: 0.10773509207306896, gradient norm: 0.1255229169230616\n",
      "Iteration: 4235000, loss: 0.10773447593239155, gradient norm: 0.2285121258036528\n",
      "Iteration: 4236000, loss: 0.10773397344295484, gradient norm: 0.1979821351573472\n",
      "Iteration: 4237000, loss: 0.10773336952802157, gradient norm: 0.34683110325577976\n",
      "Iteration: 4238000, loss: 0.10773261766127548, gradient norm: 0.23087879269304348\n",
      "Iteration: 4239000, loss: 0.10773229750943576, gradient norm: 0.10426517982168247\n",
      "Iteration: 4240000, loss: 0.10773141050731463, gradient norm: 0.08333011234142976\n",
      "Iteration: 4241000, loss: 0.10773100897990047, gradient norm: 0.27981040196212403\n",
      "Iteration: 4242000, loss: 0.10773023670766566, gradient norm: 0.19174725081345445\n",
      "Iteration: 4243000, loss: 0.10772989990682907, gradient norm: 0.15916687740456184\n",
      "Iteration: 4244000, loss: 0.10772913878147318, gradient norm: 0.07348500672929033\n",
      "Iteration: 4245000, loss: 0.1077286687818753, gradient norm: 0.11580903031705958\n",
      "Iteration: 4246000, loss: 0.10772789444924166, gradient norm: 0.10014166469464025\n",
      "Iteration: 4247000, loss: 0.1077275153403814, gradient norm: 0.2180068157532479\n",
      "Iteration: 4248000, loss: 0.10772676319017982, gradient norm: 0.6455929518612648\n",
      "Iteration: 4249000, loss: 0.10772620984534488, gradient norm: 0.05552345174366521\n",
      "Iteration: 4250000, loss: 0.10772562864189744, gradient norm: 0.3212991390359499\n",
      "Iteration: 4251000, loss: 0.10772506565668613, gradient norm: 0.07205371086780045\n",
      "Iteration: 4252000, loss: 0.10772447156983747, gradient norm: 0.24275456614200683\n",
      "Iteration: 4253000, loss: 0.10772373424408974, gradient norm: 0.12429733920068604\n",
      "Iteration: 4254000, loss: 0.10772324700107402, gradient norm: 0.03484181188972559\n",
      "Iteration: 4255000, loss: 0.10772271614983495, gradient norm: 0.09827609688752492\n",
      "Iteration: 4256000, loss: 0.10772197957307182, gradient norm: 0.4959431179347066\n",
      "Iteration: 4257000, loss: 0.10772146376063603, gradient norm: 0.1676139329946366\n",
      "Iteration: 4258000, loss: 0.10772090444763464, gradient norm: 0.41638053957111937\n",
      "Iteration: 4259000, loss: 0.1077204108020586, gradient norm: 0.08606374568071143\n",
      "Iteration: 4260000, loss: 0.107719621804447, gradient norm: 0.02153054181516313\n",
      "Iteration: 4261000, loss: 0.10771915658512864, gradient norm: 0.06440383317220402\n",
      "Iteration: 4262000, loss: 0.1077185976253621, gradient norm: 0.30667010503267605\n",
      "Iteration: 4263000, loss: 0.10771804373858618, gradient norm: 0.27258777143454843\n",
      "Iteration: 4264000, loss: 0.10771727937719482, gradient norm: 0.013175812650138953\n",
      "Iteration: 4265000, loss: 0.10771684996033289, gradient norm: 0.05182561810727806\n",
      "Iteration: 4266000, loss: 0.10771626360449651, gradient norm: 0.48579867516549186\n",
      "Iteration: 4267000, loss: 0.1077155636678754, gradient norm: 0.028177071638638283\n",
      "Iteration: 4268000, loss: 0.10771510421836039, gradient norm: 0.0713621477291628\n",
      "Iteration: 4269000, loss: 0.10771439745341503, gradient norm: 0.23986009779854517\n",
      "Iteration: 4270000, loss: 0.10771395273599771, gradient norm: 0.11399542095101886\n",
      "Iteration: 4271000, loss: 0.1077133187464646, gradient norm: 0.04616744673096211\n",
      "Iteration: 4272000, loss: 0.10771259299015616, gradient norm: 0.06177259253876164\n",
      "Iteration: 4273000, loss: 0.10771219431059778, gradient norm: 0.02206407495311409\n",
      "Iteration: 4274000, loss: 0.10771150944102469, gradient norm: 0.13792954443994615\n",
      "Iteration: 4275000, loss: 0.10771101058704051, gradient norm: 0.07328718333061564\n",
      "Iteration: 4276000, loss: 0.1077102310928229, gradient norm: 0.01416406591097573\n",
      "Iteration: 4277000, loss: 0.10770985431560524, gradient norm: 0.08445538615783521\n",
      "Iteration: 4278000, loss: 0.10770926256242416, gradient norm: 0.07718910641686014\n",
      "Iteration: 4279000, loss: 0.10770845377129387, gradient norm: 0.19464271404123143\n",
      "Iteration: 4280000, loss: 0.10770807807175956, gradient norm: 0.04876291705322662\n",
      "Iteration: 4281000, loss: 0.107707436762354, gradient norm: 0.0848545765840717\n",
      "Iteration: 4282000, loss: 0.10770692298464138, gradient norm: 0.12401646357880786\n",
      "Iteration: 4283000, loss: 0.1077062390009541, gradient norm: 0.11758098178588718\n",
      "Iteration: 4284000, loss: 0.10770569001595501, gradient norm: 0.15351461068797587\n",
      "Iteration: 4285000, loss: 0.10770515139452126, gradient norm: 0.05025341407586589\n",
      "Iteration: 4286000, loss: 0.10770463216932238, gradient norm: 0.18245960547534887\n",
      "Iteration: 4287000, loss: 0.10770388845396203, gradient norm: 0.12262583305072625\n",
      "Iteration: 4288000, loss: 0.10770335558147932, gradient norm: 0.07511293032302854\n",
      "Iteration: 4289000, loss: 0.10770276759997577, gradient norm: 0.33396970307089263\n",
      "Iteration: 4290000, loss: 0.10770226435823331, gradient norm: 0.009856745176699601\n",
      "Iteration: 4291000, loss: 0.10770177359847333, gradient norm: 0.20836958773226857\n",
      "Iteration: 4292000, loss: 0.10770101256302253, gradient norm: 0.3965140423603807\n",
      "Iteration: 4293000, loss: 0.1077004545887731, gradient norm: 0.07774133500807204\n",
      "Iteration: 4294000, loss: 0.1076999509144182, gradient norm: 0.3144473037964196\n",
      "Iteration: 4295000, loss: 0.10769930851499455, gradient norm: 0.07147143651972362\n",
      "Iteration: 4296000, loss: 0.10769875929134849, gradient norm: 0.411390641990989\n",
      "Iteration: 4297000, loss: 0.10769818030732872, gradient norm: 0.5395008767924249\n",
      "Iteration: 4298000, loss: 0.10769757488818318, gradient norm: 0.3160519221777845\n",
      "Iteration: 4299000, loss: 0.10769706063492043, gradient norm: 0.33336632750215744\n",
      "Iteration: 4300000, loss: 0.10769645002641016, gradient norm: 0.17260026662203998\n",
      "Iteration: 4301000, loss: 0.10769584281111125, gradient norm: 0.13799508115601453\n",
      "Iteration: 4302000, loss: 0.10769548251175044, gradient norm: 0.10697748253329013\n",
      "Iteration: 4303000, loss: 0.10769455739008811, gradient norm: 0.07020550605270452\n",
      "Iteration: 4304000, loss: 0.10769411190193734, gradient norm: 0.44813349142885783\n",
      "Iteration: 4305000, loss: 0.1076935897448388, gradient norm: 0.14778799585860755\n",
      "Iteration: 4306000, loss: 0.10769295906429761, gradient norm: 0.514635677569975\n",
      "Iteration: 4307000, loss: 0.10769247045099141, gradient norm: 0.07925096725980822\n",
      "Iteration: 4308000, loss: 0.10769191249017132, gradient norm: 0.21013927646333463\n",
      "Iteration: 4309000, loss: 0.1076911203702483, gradient norm: 0.210364451827074\n",
      "Iteration: 4310000, loss: 0.10769081216913308, gradient norm: 0.33260549730236333\n",
      "Iteration: 4311000, loss: 0.10769013612480102, gradient norm: 0.2889498560591339\n",
      "Iteration: 4312000, loss: 0.10768947116946077, gradient norm: 0.045391705321192945\n",
      "Iteration: 4313000, loss: 0.10768892848412213, gradient norm: 0.04672918693625702\n",
      "Iteration: 4314000, loss: 0.10768841030765246, gradient norm: 0.7888447408136205\n",
      "Iteration: 4315000, loss: 0.1076878943431259, gradient norm: 0.27262525298458173\n",
      "Iteration: 4316000, loss: 0.10768713619988585, gradient norm: 0.15755566942025345\n",
      "Iteration: 4317000, loss: 0.10768671330416936, gradient norm: 0.07731184754659823\n",
      "Iteration: 4318000, loss: 0.1076860426149819, gradient norm: 0.23826953186063937\n",
      "Iteration: 4319000, loss: 0.10768558407766261, gradient norm: 0.07721475257621402\n",
      "Iteration: 4320000, loss: 0.1076849947593695, gradient norm: 0.1147499944685422\n",
      "Iteration: 4321000, loss: 0.1076843653438605, gradient norm: 0.7643452342625197\n",
      "Iteration: 4322000, loss: 0.10768390654183543, gradient norm: 0.11554234122495512\n",
      "Iteration: 4323000, loss: 0.1076833163142294, gradient norm: 0.07769215480135065\n",
      "Iteration: 4324000, loss: 0.10768250379053293, gradient norm: 0.24624564583648242\n",
      "Iteration: 4325000, loss: 0.10768209606132659, gradient norm: 0.060432242097980586\n",
      "Iteration: 4326000, loss: 0.10768159393815588, gradient norm: 0.05331935049997056\n",
      "Iteration: 4327000, loss: 0.10768094605699016, gradient norm: 0.14116286592727967\n",
      "Iteration: 4328000, loss: 0.10768031628392266, gradient norm: 0.21259990400698112\n",
      "Iteration: 4329000, loss: 0.10767993996376811, gradient norm: 0.17153362676612322\n",
      "Iteration: 4330000, loss: 0.10767918322263757, gradient norm: 0.05017682501384125\n",
      "Iteration: 4331000, loss: 0.10767863701840269, gradient norm: 0.1356966538965928\n",
      "Iteration: 4332000, loss: 0.10767820971065512, gradient norm: 0.4721178260461055\n",
      "Iteration: 4333000, loss: 0.10767745118350097, gradient norm: 0.10117412519571291\n",
      "Iteration: 4334000, loss: 0.10767711011401235, gradient norm: 0.2800449626400838\n",
      "Iteration: 4335000, loss: 0.10767633347638118, gradient norm: 0.25483989574954935\n",
      "Iteration: 4336000, loss: 0.10767591383731284, gradient norm: 0.15328206079824686\n",
      "Iteration: 4337000, loss: 0.10767533950584957, gradient norm: 0.11350218212302442\n",
      "Iteration: 4338000, loss: 0.10767459379267541, gradient norm: 0.3153221277446073\n",
      "Iteration: 4339000, loss: 0.10767404272218353, gradient norm: 0.09207046029089228\n",
      "Iteration: 4340000, loss: 0.10767375635224223, gradient norm: 0.2925745795958278\n",
      "Iteration: 4341000, loss: 0.10767293830628287, gradient norm: 0.02554834352947827\n",
      "Iteration: 4342000, loss: 0.10767250791730992, gradient norm: 0.39226403502996665\n",
      "Iteration: 4343000, loss: 0.10767177389817913, gradient norm: 0.08331118681735038\n",
      "Iteration: 4344000, loss: 0.10767141380871863, gradient norm: 0.28424293930131467\n",
      "Iteration: 4345000, loss: 0.10767071714785058, gradient norm: 0.24608587254183997\n",
      "Iteration: 4346000, loss: 0.10767021637280956, gradient norm: 0.1782112169978724\n",
      "Iteration: 4347000, loss: 0.10766953970707656, gradient norm: 0.17424813463807473\n",
      "Iteration: 4348000, loss: 0.10766904221387938, gradient norm: 0.07440602653326858\n",
      "Iteration: 4349000, loss: 0.10766840028005932, gradient norm: 0.035455673277308065\n",
      "Iteration: 4350000, loss: 0.10766807669652076, gradient norm: 0.5989041450827186\n",
      "Iteration: 4351000, loss: 0.10766751624380393, gradient norm: 0.2124421982627141\n",
      "Iteration: 4352000, loss: 0.10766667436522363, gradient norm: 0.03299896570243767\n",
      "Iteration: 4353000, loss: 0.10766624165389972, gradient norm: 0.12967142409213137\n",
      "Iteration: 4354000, loss: 0.10766572449674677, gradient norm: 0.15789023856157544\n",
      "Iteration: 4355000, loss: 0.10766506237545406, gradient norm: 0.12252837838134778\n",
      "Iteration: 4356000, loss: 0.10766456459153662, gradient norm: 0.3228169063804436\n",
      "Iteration: 4357000, loss: 0.1076639735963858, gradient norm: 0.36617732489843036\n",
      "Iteration: 4358000, loss: 0.10766341392883326, gradient norm: 0.3724468304620698\n",
      "Iteration: 4359000, loss: 0.10766294451822991, gradient norm: 0.11329845599341459\n",
      "Iteration: 4360000, loss: 0.10766232449007335, gradient norm: 0.19984213715183305\n",
      "Iteration: 4361000, loss: 0.10766168836004279, gradient norm: 0.17671882406373968\n",
      "Iteration: 4362000, loss: 0.10766129502485317, gradient norm: 0.31669005971381886\n",
      "Iteration: 4363000, loss: 0.1076606785690434, gradient norm: 0.1753260541185393\n",
      "Iteration: 4364000, loss: 0.10765998495078268, gradient norm: 0.10449605584513888\n",
      "Iteration: 4365000, loss: 0.10765955511692608, gradient norm: 0.419095412145674\n",
      "Iteration: 4366000, loss: 0.10765909178271789, gradient norm: 0.03743104733325741\n",
      "Iteration: 4367000, loss: 0.10765823609564665, gradient norm: 0.1222423954313299\n",
      "Iteration: 4368000, loss: 0.10765801038300828, gradient norm: 0.18222987967773605\n",
      "Iteration: 4369000, loss: 0.1076571600550826, gradient norm: 0.029095131527160242\n",
      "Iteration: 4370000, loss: 0.10765678885800276, gradient norm: 0.39492315419481533\n",
      "Iteration: 4371000, loss: 0.10765610125325867, gradient norm: 0.06705497904542237\n",
      "Iteration: 4372000, loss: 0.10765566373437133, gradient norm: 0.08100616356408848\n",
      "Iteration: 4373000, loss: 0.10765507702057492, gradient norm: 0.06485250779981637\n",
      "Iteration: 4374000, loss: 0.1076545266192195, gradient norm: 0.5667357717756116\n",
      "Iteration: 4375000, loss: 0.1076539288212826, gradient norm: 0.05007280501311421\n",
      "Iteration: 4376000, loss: 0.10765339551363698, gradient norm: 0.9201921395232545\n",
      "Iteration: 4377000, loss: 0.10765288936753482, gradient norm: 0.016444022896830522\n",
      "Iteration: 4378000, loss: 0.10765240015944581, gradient norm: 0.21518914970870798\n",
      "Iteration: 4379000, loss: 0.107651504019554, gradient norm: 0.056672757298570936\n",
      "Iteration: 4380000, loss: 0.10765119686914154, gradient norm: 0.14587140272121532\n",
      "Iteration: 4381000, loss: 0.10765063229242663, gradient norm: 0.09827561118855427\n",
      "Iteration: 4382000, loss: 0.10765011454633909, gradient norm: 0.07944756904674913\n",
      "Iteration: 4383000, loss: 0.10764945063565483, gradient norm: 0.07973886077538866\n",
      "Iteration: 4384000, loss: 0.10764891363817096, gradient norm: 0.37574503014139954\n",
      "Iteration: 4385000, loss: 0.10764855488387809, gradient norm: 0.08123551683407097\n",
      "Iteration: 4386000, loss: 0.10764772453856573, gradient norm: 0.06644504528841201\n",
      "Iteration: 4387000, loss: 0.10764742552952339, gradient norm: 0.04201741815934027\n",
      "Iteration: 4388000, loss: 0.10764668138070065, gradient norm: 0.13787798690325606\n",
      "Iteration: 4389000, loss: 0.10764617386158645, gradient norm: 0.721228271101078\n",
      "Iteration: 4390000, loss: 0.10764557590155951, gradient norm: 0.056715732564632804\n",
      "Iteration: 4391000, loss: 0.1076451140673737, gradient norm: 0.017715048809623483\n",
      "Iteration: 4392000, loss: 0.10764446084711864, gradient norm: 0.39441661434269637\n",
      "Iteration: 4393000, loss: 0.10764407293347064, gradient norm: 0.3023152228695925\n",
      "Iteration: 4394000, loss: 0.1076434112572752, gradient norm: 0.22399889767674278\n",
      "Iteration: 4395000, loss: 0.107642950626027, gradient norm: 0.2411041576882708\n",
      "Iteration: 4396000, loss: 0.10764224609812668, gradient norm: 0.574159583313029\n",
      "Iteration: 4397000, loss: 0.10764181321776577, gradient norm: 0.05735934871732518\n",
      "Iteration: 4398000, loss: 0.10764120352026761, gradient norm: 0.13366167060158904\n",
      "Iteration: 4399000, loss: 0.10764088392745379, gradient norm: 0.1382088688294239\n",
      "Iteration: 4400000, loss: 0.10763988562831443, gradient norm: 0.24740721240599303\n",
      "Iteration: 4401000, loss: 0.10763967085183945, gradient norm: 0.04070986471493679\n",
      "Iteration: 4402000, loss: 0.10763904125618483, gradient norm: 0.2471667361289374\n",
      "Iteration: 4403000, loss: 0.10763853286433277, gradient norm: 0.2493267353866192\n",
      "Iteration: 4404000, loss: 0.10763795878504046, gradient norm: 0.06739283330550436\n",
      "Iteration: 4405000, loss: 0.10763727846337212, gradient norm: 0.23892267800817193\n",
      "Iteration: 4406000, loss: 0.10763684249735775, gradient norm: 0.10050824706495091\n",
      "Iteration: 4407000, loss: 0.10763627049497779, gradient norm: 0.019622791242407497\n",
      "Iteration: 4408000, loss: 0.10763580538119716, gradient norm: 0.06896408448659226\n",
      "Iteration: 4409000, loss: 0.10763524473348716, gradient norm: 0.1113468603835576\n",
      "Iteration: 4410000, loss: 0.10763459354335893, gradient norm: 0.0500859700677601\n",
      "Iteration: 4411000, loss: 0.10763428829707618, gradient norm: 0.2689589763021078\n",
      "Iteration: 4412000, loss: 0.10763344207505299, gradient norm: 0.40520549989609167\n",
      "Iteration: 4413000, loss: 0.10763292161466487, gradient norm: 0.1085753413256488\n",
      "Iteration: 4414000, loss: 0.10763256895357073, gradient norm: 0.09952132325514727\n",
      "Iteration: 4415000, loss: 0.1076319000457766, gradient norm: 0.07433752746180053\n",
      "Iteration: 4416000, loss: 0.10763147371503848, gradient norm: 0.13759967561933978\n",
      "Iteration: 4417000, loss: 0.10763087034374809, gradient norm: 0.23849833273049129\n",
      "Iteration: 4418000, loss: 0.10763019439260851, gradient norm: 0.09220965385455748\n",
      "Iteration: 4419000, loss: 0.10762971412166696, gradient norm: 0.03900002153259577\n",
      "Iteration: 4420000, loss: 0.1076292511205645, gradient norm: 0.3229094992286712\n",
      "Iteration: 4421000, loss: 0.10762856301671099, gradient norm: 0.25650948272031165\n",
      "Iteration: 4422000, loss: 0.10762819614734116, gradient norm: 0.09681020720294935\n",
      "Iteration: 4423000, loss: 0.10762762017965176, gradient norm: 0.00984561071090931\n",
      "Iteration: 4424000, loss: 0.10762691312051381, gradient norm: 0.1627932741817976\n",
      "Iteration: 4425000, loss: 0.10762652701157413, gradient norm: 0.0641815580996149\n",
      "Iteration: 4426000, loss: 0.10762600652561187, gradient norm: 0.5900335002892815\n",
      "Iteration: 4427000, loss: 0.10762545097146235, gradient norm: 0.24034257307104348\n",
      "Iteration: 4428000, loss: 0.1076248304866769, gradient norm: 0.5986620379715352\n",
      "Iteration: 4429000, loss: 0.10762433669858296, gradient norm: 0.142529597062886\n",
      "Iteration: 4430000, loss: 0.10762376627477074, gradient norm: 0.028562023043081253\n",
      "Iteration: 4431000, loss: 0.10762328906605334, gradient norm: 0.5501288587086485\n",
      "Iteration: 4432000, loss: 0.10762259176628912, gradient norm: 0.24636969276254922\n",
      "Iteration: 4433000, loss: 0.10762236282044511, gradient norm: 0.17035105870254424\n",
      "Iteration: 4434000, loss: 0.10762152528698372, gradient norm: 0.38814333154714376\n",
      "Iteration: 4435000, loss: 0.1076211099167476, gradient norm: 0.13699189054258795\n",
      "Iteration: 4436000, loss: 0.10762046804051818, gradient norm: 0.030157366099623207\n",
      "Iteration: 4437000, loss: 0.10762006893628802, gradient norm: 0.2916589816758696\n",
      "Iteration: 4438000, loss: 0.10761946650629832, gradient norm: 0.17375185847911329\n",
      "Iteration: 4439000, loss: 0.10761894648446721, gradient norm: 0.1780914627619354\n",
      "Iteration: 4440000, loss: 0.10761834388103203, gradient norm: 0.15929700381486017\n",
      "Iteration: 4441000, loss: 0.10761785637205969, gradient norm: 0.16201107681409663\n",
      "Iteration: 4442000, loss: 0.10761732150062078, gradient norm: 0.09411964703351178\n",
      "Iteration: 4443000, loss: 0.10761675108522871, gradient norm: 0.31632621780084064\n",
      "Iteration: 4444000, loss: 0.1076162551157119, gradient norm: 0.409580722435298\n",
      "Iteration: 4445000, loss: 0.10761570252885608, gradient norm: 0.5757411015970072\n",
      "Iteration: 4446000, loss: 0.10761527620347763, gradient norm: 0.037347467178222724\n",
      "Iteration: 4447000, loss: 0.10761455374553734, gradient norm: 0.2408709273307381\n",
      "Iteration: 4448000, loss: 0.10761412480045564, gradient norm: 0.1948501496494237\n",
      "Iteration: 4449000, loss: 0.10761363140720664, gradient norm: 0.20370211017364745\n",
      "Iteration: 4450000, loss: 0.10761285820884231, gradient norm: 0.15828898286390072\n",
      "Iteration: 4451000, loss: 0.10761249946022183, gradient norm: 0.03742191808389484\n",
      "Iteration: 4452000, loss: 0.1076120697589554, gradient norm: 0.4531290993191322\n",
      "Iteration: 4453000, loss: 0.10761134466837513, gradient norm: 0.07490861632728331\n",
      "Iteration: 4454000, loss: 0.10761103141970685, gradient norm: 0.3615821269508606\n",
      "Iteration: 4455000, loss: 0.10761019768589022, gradient norm: 0.5010031627877052\n",
      "Iteration: 4456000, loss: 0.10760987965262135, gradient norm: 0.1030986687475083\n",
      "Iteration: 4457000, loss: 0.1076092422281665, gradient norm: 0.32971381666570104\n",
      "Iteration: 4458000, loss: 0.10760865815073124, gradient norm: 0.39027383646876707\n",
      "Iteration: 4459000, loss: 0.10760832332378788, gradient norm: 0.10496773250935844\n",
      "Iteration: 4460000, loss: 0.10760764686141441, gradient norm: 0.3030924445782275\n",
      "Iteration: 4461000, loss: 0.10760717612777077, gradient norm: 0.14911763445130216\n",
      "Iteration: 4462000, loss: 0.10760663267145418, gradient norm: 0.39035688551076664\n",
      "Iteration: 4463000, loss: 0.10760601827285836, gradient norm: 0.5549009121386249\n",
      "Iteration: 4464000, loss: 0.10760562819704426, gradient norm: 0.295881144744172\n",
      "Iteration: 4465000, loss: 0.10760492645387411, gradient norm: 0.32785307081453696\n",
      "Iteration: 4466000, loss: 0.10760451641575831, gradient norm: 0.09244769154940088\n",
      "Iteration: 4467000, loss: 0.10760394062113326, gradient norm: 0.013778694528455107\n",
      "Iteration: 4468000, loss: 0.10760339921750382, gradient norm: 0.5826196738856567\n",
      "Iteration: 4469000, loss: 0.1076029176320136, gradient norm: 0.4423051047224358\n",
      "Iteration: 4470000, loss: 0.10760222876925322, gradient norm: 0.20775510603387035\n",
      "Iteration: 4471000, loss: 0.10760189643405679, gradient norm: 0.2018056973255425\n",
      "Iteration: 4472000, loss: 0.10760120646486745, gradient norm: 0.06976994793390474\n",
      "Iteration: 4473000, loss: 0.10760083410478441, gradient norm: 0.30753912405425515\n",
      "Iteration: 4474000, loss: 0.1076001829638289, gradient norm: 0.013100537548199237\n",
      "Iteration: 4475000, loss: 0.10759977706347655, gradient norm: 0.1047557749401384\n",
      "Iteration: 4476000, loss: 0.1075992051193922, gradient norm: 0.4066506908245376\n",
      "Iteration: 4477000, loss: 0.1075986163402379, gradient norm: 0.11947543237969203\n",
      "Iteration: 4478000, loss: 0.10759812131194216, gradient norm: 0.14781586456245271\n",
      "Iteration: 4479000, loss: 0.10759762952632067, gradient norm: 0.08716476040361669\n",
      "Iteration: 4480000, loss: 0.10759703369550712, gradient norm: 0.15765351671859282\n",
      "Iteration: 4481000, loss: 0.10759648347728071, gradient norm: 0.40003131694191296\n",
      "Iteration: 4482000, loss: 0.10759605872392801, gradient norm: 0.2412811614358388\n",
      "Iteration: 4483000, loss: 0.10759544222471104, gradient norm: 0.08340893867678928\n",
      "Iteration: 4484000, loss: 0.10759502167651994, gradient norm: 0.2553014359510511\n",
      "Iteration: 4485000, loss: 0.10759439954567286, gradient norm: 0.41727531203043766\n",
      "Iteration: 4486000, loss: 0.10759392718491376, gradient norm: 0.12720433218608357\n",
      "Iteration: 4487000, loss: 0.10759336698672892, gradient norm: 0.14992863436175946\n",
      "Iteration: 4488000, loss: 0.10759288332153068, gradient norm: 0.10262902947782795\n",
      "Iteration: 4489000, loss: 0.10759229929053785, gradient norm: 0.08359206861921877\n",
      "Iteration: 4490000, loss: 0.10759189940530291, gradient norm: 0.10763033628212997\n",
      "Iteration: 4491000, loss: 0.10759126082712649, gradient norm: 0.1740727384965395\n",
      "Iteration: 4492000, loss: 0.10759085032917534, gradient norm: 0.2794092184498081\n",
      "Iteration: 4493000, loss: 0.10759007154431754, gradient norm: 0.05480144320943033\n",
      "Iteration: 4494000, loss: 0.10758973437769909, gradient norm: 0.15326597357553182\n",
      "Iteration: 4495000, loss: 0.10758907474190675, gradient norm: 0.09623705647898917\n",
      "Iteration: 4496000, loss: 0.10758864454418535, gradient norm: 0.5668681733897379\n",
      "Iteration: 4497000, loss: 0.10758814507932533, gradient norm: 0.37687829290018604\n",
      "Iteration: 4498000, loss: 0.1075875679517145, gradient norm: 0.1941459624517436\n",
      "Iteration: 4499000, loss: 0.10758717735052048, gradient norm: 0.02133864299981516\n",
      "Iteration: 4500000, loss: 0.1075866038110939, gradient norm: 0.12761368719497698\n",
      "Iteration: 4501000, loss: 0.10758603998246333, gradient norm: 0.061025248863817334\n",
      "Iteration: 4502000, loss: 0.10758557751955442, gradient norm: 0.09348742737324336\n",
      "Iteration: 4503000, loss: 0.1075849538379506, gradient norm: 0.25323628843765145\n",
      "Iteration: 4504000, loss: 0.1075844476041168, gradient norm: 0.04832943230139423\n",
      "Iteration: 4505000, loss: 0.107583876839382, gradient norm: 0.4035445293597184\n",
      "Iteration: 4506000, loss: 0.107583502532749, gradient norm: 0.17528313240474072\n",
      "Iteration: 4507000, loss: 0.1075828497535916, gradient norm: 0.09060506717760851\n",
      "Iteration: 4508000, loss: 0.10758251648065413, gradient norm: 0.2884205822046716\n",
      "Iteration: 4509000, loss: 0.10758178453800361, gradient norm: 0.05728081546847893\n",
      "Iteration: 4510000, loss: 0.10758136381987768, gradient norm: 0.4444310017014526\n",
      "Iteration: 4511000, loss: 0.10758085120638347, gradient norm: 0.09184462629353916\n",
      "Iteration: 4512000, loss: 0.10758028055492135, gradient norm: 0.037826713577309164\n",
      "Iteration: 4513000, loss: 0.10757981510098438, gradient norm: 0.14328008532536224\n",
      "Iteration: 4514000, loss: 0.10757930958359566, gradient norm: 0.36446666123839433\n",
      "Iteration: 4515000, loss: 0.1075787467321704, gradient norm: 0.3172337742910825\n",
      "Iteration: 4516000, loss: 0.10757821269250306, gradient norm: 0.10437506979339718\n",
      "Iteration: 4517000, loss: 0.10757771225807605, gradient norm: 0.07000507221726576\n",
      "Iteration: 4518000, loss: 0.10757721067260186, gradient norm: 0.21447731016343113\n",
      "Iteration: 4519000, loss: 0.10757664596396763, gradient norm: 0.0799932573129888\n",
      "Iteration: 4520000, loss: 0.10757613564941658, gradient norm: 0.03866023880292045\n",
      "Iteration: 4521000, loss: 0.10757558864479684, gradient norm: 0.06849914033226479\n",
      "Iteration: 4522000, loss: 0.10757521611225082, gradient norm: 0.13608586241559667\n",
      "Iteration: 4523000, loss: 0.10757454872602477, gradient norm: 0.13760295449625337\n",
      "Iteration: 4524000, loss: 0.10757409637225257, gradient norm: 0.23727791868460507\n",
      "Iteration: 4525000, loss: 0.10757356966311044, gradient norm: 0.1313602593754383\n",
      "Iteration: 4526000, loss: 0.10757305883286986, gradient norm: 0.26612919462355833\n",
      "Iteration: 4527000, loss: 0.10757253244658924, gradient norm: 0.19967785188138756\n",
      "Iteration: 4528000, loss: 0.10757192916370588, gradient norm: 0.09578697801241064\n",
      "Iteration: 4529000, loss: 0.10757158489208382, gradient norm: 0.030172045548854415\n",
      "Iteration: 4530000, loss: 0.10757096712263878, gradient norm: 0.04347404159859378\n",
      "Iteration: 4531000, loss: 0.10757052118284077, gradient norm: 0.2597892118783949\n",
      "Iteration: 4532000, loss: 0.1075698308787355, gradient norm: 0.2120183020161079\n",
      "Iteration: 4533000, loss: 0.1075694659982241, gradient norm: 0.3477729195151763\n",
      "Iteration: 4534000, loss: 0.10756887890018872, gradient norm: 0.31417623334547395\n",
      "Iteration: 4535000, loss: 0.10756838347007826, gradient norm: 0.10413035090384962\n",
      "Iteration: 4536000, loss: 0.10756799637929837, gradient norm: 0.1353888126388492\n",
      "Iteration: 4537000, loss: 0.10756748645567396, gradient norm: 0.5618852123566536\n",
      "Iteration: 4538000, loss: 0.10756677341729497, gradient norm: 0.06465607182485941\n",
      "Iteration: 4539000, loss: 0.10756641147188266, gradient norm: 0.3769215453826117\n",
      "Iteration: 4540000, loss: 0.10756589601134355, gradient norm: 0.07117123322128839\n",
      "Iteration: 4541000, loss: 0.10756531992113756, gradient norm: 0.33149866405061684\n",
      "Iteration: 4542000, loss: 0.10756486543823536, gradient norm: 0.26128646032429487\n",
      "Iteration: 4543000, loss: 0.1075642777476408, gradient norm: 0.07555532737616406\n",
      "Iteration: 4544000, loss: 0.10756382067443541, gradient norm: 0.07066685914095434\n",
      "Iteration: 4545000, loss: 0.10756325962848007, gradient norm: 0.056432046940420776\n",
      "Iteration: 4546000, loss: 0.10756269491744126, gradient norm: 0.1578197730281177\n",
      "Iteration: 4547000, loss: 0.10756231407414171, gradient norm: 0.18776466119547455\n",
      "Iteration: 4548000, loss: 0.10756175951879482, gradient norm: 0.05298248993869893\n",
      "Iteration: 4549000, loss: 0.1075612798769748, gradient norm: 0.18545085767202452\n",
      "Iteration: 4550000, loss: 0.10756075009407093, gradient norm: 0.014785693420700715\n",
      "Iteration: 4551000, loss: 0.10756036759922458, gradient norm: 0.26706711724572724\n",
      "Iteration: 4552000, loss: 0.10755947281195911, gradient norm: 0.29526543514979614\n",
      "Iteration: 4553000, loss: 0.10755929722382623, gradient norm: 0.03691178883563963\n",
      "Iteration: 4554000, loss: 0.10755877612697043, gradient norm: 0.3736688157747624\n",
      "Iteration: 4555000, loss: 0.10755809092105707, gradient norm: 0.3166096887156685\n",
      "Iteration: 4556000, loss: 0.10755772608632472, gradient norm: 0.04935854466819378\n",
      "Iteration: 4557000, loss: 0.1075570880314861, gradient norm: 0.19453108135529412\n",
      "Iteration: 4558000, loss: 0.10755672442053489, gradient norm: 0.09705977735338422\n",
      "Iteration: 4559000, loss: 0.10755618541562117, gradient norm: 0.5571134997956495\n",
      "Iteration: 4560000, loss: 0.10755556867621377, gradient norm: 0.11693356791886438\n",
      "Iteration: 4561000, loss: 0.10755512268014919, gradient norm: 0.061460933988358814\n",
      "Iteration: 4562000, loss: 0.10755464297343152, gradient norm: 0.1700959699409078\n",
      "Iteration: 4563000, loss: 0.10755417974609596, gradient norm: 0.2641201649269327\n",
      "Iteration: 4564000, loss: 0.10755346938487806, gradient norm: 0.17666054530502603\n",
      "Iteration: 4565000, loss: 0.10755317724941506, gradient norm: 0.08256675556026659\n",
      "Iteration: 4566000, loss: 0.10755246007720254, gradient norm: 0.07715446916196621\n",
      "Iteration: 4567000, loss: 0.10755219639519847, gradient norm: 0.122037184329865\n",
      "Iteration: 4568000, loss: 0.10755162674889782, gradient norm: 0.0744419464049473\n",
      "Iteration: 4569000, loss: 0.10755101886911123, gradient norm: 0.030057856520601257\n",
      "Iteration: 4570000, loss: 0.10755059098595232, gradient norm: 0.13465683361812034\n",
      "Iteration: 4571000, loss: 0.10754999615691346, gradient norm: 0.13583854994339847\n",
      "Iteration: 4572000, loss: 0.10754955872143811, gradient norm: 0.17241095991058492\n",
      "Iteration: 4573000, loss: 0.10754899254358202, gradient norm: 0.19390899335589865\n",
      "Iteration: 4574000, loss: 0.10754853051436819, gradient norm: 0.25975671792952615\n",
      "Iteration: 4575000, loss: 0.1075480604724419, gradient norm: 0.10241731275573249\n",
      "Iteration: 4576000, loss: 0.10754754798764889, gradient norm: 0.043900256712653185\n",
      "Iteration: 4577000, loss: 0.10754712956451279, gradient norm: 0.3163384430076124\n",
      "Iteration: 4578000, loss: 0.10754643741184651, gradient norm: 0.06942819738556501\n",
      "Iteration: 4579000, loss: 0.1075460206845832, gradient norm: 0.10784651349975585\n",
      "Iteration: 4580000, loss: 0.10754550415790953, gradient norm: 0.025368176396229385\n",
      "Iteration: 4581000, loss: 0.10754496000264684, gradient norm: 0.31004549668031917\n",
      "Iteration: 4582000, loss: 0.10754455495342154, gradient norm: 0.43130280937975174\n",
      "Iteration: 4583000, loss: 0.1075439463153514, gradient norm: 0.1022503169118807\n",
      "Iteration: 4584000, loss: 0.10754358898829099, gradient norm: 0.08683449940383793\n",
      "Iteration: 4585000, loss: 0.10754294809239895, gradient norm: 0.054186886095272765\n",
      "Iteration: 4586000, loss: 0.107542423505665, gradient norm: 0.2353658456161245\n",
      "Iteration: 4587000, loss: 0.10754207175968804, gradient norm: 0.26353539013669247\n",
      "Iteration: 4588000, loss: 0.10754138766726004, gradient norm: 0.19777290465017314\n",
      "Iteration: 4589000, loss: 0.10754097958108634, gradient norm: 0.04593850687066343\n",
      "Iteration: 4590000, loss: 0.10754046346225808, gradient norm: 0.2741166756014639\n",
      "Iteration: 4591000, loss: 0.10754010217167602, gradient norm: 0.10158417514056975\n",
      "Iteration: 4592000, loss: 0.10753944812451453, gradient norm: 0.1908465194361948\n",
      "Iteration: 4593000, loss: 0.1075389580742074, gradient norm: 0.1842173878988989\n",
      "Iteration: 4594000, loss: 0.10753844416409741, gradient norm: 0.4889036502947713\n",
      "Iteration: 4595000, loss: 0.10753797593232858, gradient norm: 0.0520224288406058\n",
      "Iteration: 4596000, loss: 0.1075375302904018, gradient norm: 0.12223050236221869\n",
      "Iteration: 4597000, loss: 0.10753693707950984, gradient norm: 0.09977955043902835\n",
      "Iteration: 4598000, loss: 0.10753650320638909, gradient norm: 0.3077398009829105\n",
      "Iteration: 4599000, loss: 0.10753595182554844, gradient norm: 0.0815942244635552\n",
      "Iteration: 4600000, loss: 0.10753543834279233, gradient norm: 0.05195487201312612\n",
      "Iteration: 4601000, loss: 0.10753500656483196, gradient norm: 0.32965394005179827\n",
      "Iteration: 4602000, loss: 0.10753451533658308, gradient norm: 0.0662001872276156\n",
      "Iteration: 4603000, loss: 0.10753388453983978, gradient norm: 0.12987156902968944\n",
      "Iteration: 4604000, loss: 0.10753357395454846, gradient norm: 0.12918603794045982\n",
      "Iteration: 4605000, loss: 0.10753291529871656, gradient norm: 0.2154661792410418\n",
      "Iteration: 4606000, loss: 0.10753257108971079, gradient norm: 0.2582951286896475\n",
      "Iteration: 4607000, loss: 0.10753188674417775, gradient norm: 0.3827200359518468\n",
      "Iteration: 4608000, loss: 0.10753160819386733, gradient norm: 0.31596398133519604\n",
      "Iteration: 4609000, loss: 0.10753084753672644, gradient norm: 0.13114382831953952\n",
      "Iteration: 4610000, loss: 0.10753046035201191, gradient norm: 0.009572686335596086\n",
      "Iteration: 4611000, loss: 0.1075300693707308, gradient norm: 0.13257885881074202\n",
      "Iteration: 4612000, loss: 0.10752964703799363, gradient norm: 0.22511563268786597\n",
      "Iteration: 4613000, loss: 0.10752885376922158, gradient norm: 0.04446741532753614\n",
      "Iteration: 4614000, loss: 0.10752865911012152, gradient norm: 0.14247603665477124\n",
      "Iteration: 4615000, loss: 0.10752793677610259, gradient norm: 0.03161513954321998\n",
      "Iteration: 4616000, loss: 0.10752750950486892, gradient norm: 0.05092346884464532\n",
      "Iteration: 4617000, loss: 0.10752719166722817, gradient norm: 0.22274272144097707\n",
      "Iteration: 4618000, loss: 0.10752643740170802, gradient norm: 0.18571226947354613\n",
      "Iteration: 4619000, loss: 0.10752607006187886, gradient norm: 0.4601142781361118\n",
      "Iteration: 4620000, loss: 0.10752558775107224, gradient norm: 0.17884083742585982\n",
      "Iteration: 4621000, loss: 0.1075250454543489, gradient norm: 0.29633067224853266\n",
      "Iteration: 4622000, loss: 0.10752446468112885, gradient norm: 0.24297278952297555\n",
      "Iteration: 4623000, loss: 0.10752405167941237, gradient norm: 0.11442053185269416\n",
      "Iteration: 4624000, loss: 0.10752371694570184, gradient norm: 0.06463489185871484\n",
      "Iteration: 4625000, loss: 0.1075229269674647, gradient norm: 0.06378683401291141\n",
      "Iteration: 4626000, loss: 0.10752263518672012, gradient norm: 0.010313170223514575\n",
      "Iteration: 4627000, loss: 0.10752207606984934, gradient norm: 0.06473502612935186\n",
      "Iteration: 4628000, loss: 0.10752177944666602, gradient norm: 0.32024280415291007\n",
      "Iteration: 4629000, loss: 0.10752105079844547, gradient norm: 0.2449395396183842\n",
      "Iteration: 4630000, loss: 0.10752052211605921, gradient norm: 0.18255480261971213\n",
      "Iteration: 4631000, loss: 0.10752024886977017, gradient norm: 0.5131839590445345\n",
      "Iteration: 4632000, loss: 0.10751958185878073, gradient norm: 0.3603901327605604\n",
      "Iteration: 4633000, loss: 0.10751922148074365, gradient norm: 0.04250401632414727\n",
      "Iteration: 4634000, loss: 0.10751870252749388, gradient norm: 0.4124974942120365\n",
      "Iteration: 4635000, loss: 0.10751811970585198, gradient norm: 0.10225145167633666\n",
      "Iteration: 4636000, loss: 0.1075176621998176, gradient norm: 0.19799435642922084\n",
      "Iteration: 4637000, loss: 0.10751716875932754, gradient norm: 0.054224141963928246\n",
      "Iteration: 4638000, loss: 0.10751678880877262, gradient norm: 0.05560016857429298\n",
      "Iteration: 4639000, loss: 0.10751613937219738, gradient norm: 0.22585336877632325\n",
      "Iteration: 4640000, loss: 0.10751570693159682, gradient norm: 0.40164814551881506\n",
      "Iteration: 4641000, loss: 0.10751517780178392, gradient norm: 0.07228073345830256\n",
      "Iteration: 4642000, loss: 0.10751484837045987, gradient norm: 0.24311953685294124\n",
      "Iteration: 4643000, loss: 0.1075142225557301, gradient norm: 0.01836513353877685\n",
      "Iteration: 4644000, loss: 0.10751366816562173, gradient norm: 0.03676008067843513\n",
      "Iteration: 4645000, loss: 0.10751329091744607, gradient norm: 0.13267209847389613\n",
      "Iteration: 4646000, loss: 0.10751287475833886, gradient norm: 0.060622986024525066\n",
      "Iteration: 4647000, loss: 0.10751220193388628, gradient norm: 0.27748410344142255\n",
      "Iteration: 4648000, loss: 0.1075119359227252, gradient norm: 0.03305401337160446\n",
      "Iteration: 4649000, loss: 0.107511276728717, gradient norm: 0.4400811108488639\n",
      "Iteration: 4650000, loss: 0.10751085357481432, gradient norm: 0.2591668321940413\n",
      "Iteration: 4651000, loss: 0.10751031495163116, gradient norm: 0.6214584419061259\n",
      "Iteration: 4652000, loss: 0.1075098472155988, gradient norm: 0.37696299408345324\n",
      "Iteration: 4653000, loss: 0.10750929347212712, gradient norm: 0.08418808549625945\n",
      "Iteration: 4654000, loss: 0.1075090926495408, gradient norm: 0.1492694393125505\n",
      "Iteration: 4655000, loss: 0.10750829439748957, gradient norm: 0.08547219709233692\n",
      "Iteration: 4656000, loss: 0.10750784634079309, gradient norm: 0.041240923874455875\n",
      "Iteration: 4657000, loss: 0.10750739593280602, gradient norm: 0.12342604004450347\n",
      "Iteration: 4658000, loss: 0.10750708227367312, gradient norm: 0.14735011870994766\n",
      "Iteration: 4659000, loss: 0.10750635813734626, gradient norm: 0.04753263309679051\n",
      "Iteration: 4660000, loss: 0.1075059653968989, gradient norm: 0.5111165141445998\n",
      "Iteration: 4661000, loss: 0.1075054910491879, gradient norm: 0.032981583789745014\n",
      "Iteration: 4662000, loss: 0.10750500570722747, gradient norm: 0.05359636122390651\n",
      "Iteration: 4663000, loss: 0.10750463174186627, gradient norm: 0.15425276399425492\n",
      "Iteration: 4664000, loss: 0.10750394822091378, gradient norm: 0.14873974199694817\n",
      "Iteration: 4665000, loss: 0.10750362062489131, gradient norm: 0.13862898313052327\n",
      "Iteration: 4666000, loss: 0.10750296780779124, gradient norm: 0.12392313339072085\n",
      "Iteration: 4667000, loss: 0.10750266189279978, gradient norm: 0.09156309414745002\n",
      "Iteration: 4668000, loss: 0.10750202429899047, gradient norm: 0.02474923604780289\n",
      "Iteration: 4669000, loss: 0.10750175807550723, gradient norm: 0.0017923965559878015\n",
      "Iteration: 4670000, loss: 0.10750117025765407, gradient norm: 0.25310073434620933\n",
      "Iteration: 4671000, loss: 0.10750056851082099, gradient norm: 0.7257534900603586\n",
      "Iteration: 4672000, loss: 0.10750014201385409, gradient norm: 0.1643442762844542\n",
      "Iteration: 4673000, loss: 0.10749975054394241, gradient norm: 0.08748966957600308\n",
      "Iteration: 4674000, loss: 0.10749917822813089, gradient norm: 0.15771471859575503\n",
      "Iteration: 4675000, loss: 0.10749873315845766, gradient norm: 0.26763951963318794\n",
      "Iteration: 4676000, loss: 0.10749819405693069, gradient norm: 0.09299159668984334\n",
      "Iteration: 4677000, loss: 0.10749786599511373, gradient norm: 0.2915285005116295\n",
      "Iteration: 4678000, loss: 0.10749728150097992, gradient norm: 0.1350049497646758\n",
      "Iteration: 4679000, loss: 0.10749665813593141, gradient norm: 0.1985558127474213\n",
      "Iteration: 4680000, loss: 0.10749641857345799, gradient norm: 0.042778315907669257\n",
      "Iteration: 4681000, loss: 0.10749576365615247, gradient norm: 0.06790932855763993\n",
      "Iteration: 4682000, loss: 0.10749532809327107, gradient norm: 0.15016610984767262\n",
      "Iteration: 4683000, loss: 0.10749486430947112, gradient norm: 0.08479948595444847\n",
      "Iteration: 4684000, loss: 0.10749443866285173, gradient norm: 0.10963402896021107\n",
      "Iteration: 4685000, loss: 0.10749395102182012, gradient norm: 0.12403848300360805\n",
      "Iteration: 4686000, loss: 0.10749332523469189, gradient norm: 0.06330646027512399\n",
      "Iteration: 4687000, loss: 0.10749298789314646, gradient norm: 0.062101689482240666\n",
      "Iteration: 4688000, loss: 0.10749248510017802, gradient norm: 0.10617562715747436\n",
      "Iteration: 4689000, loss: 0.107491980454583, gradient norm: 0.03467184518689164\n",
      "Iteration: 4690000, loss: 0.10749166958538964, gradient norm: 0.15119146745205742\n",
      "Iteration: 4691000, loss: 0.1074909360274801, gradient norm: 0.08510141975654488\n",
      "Iteration: 4692000, loss: 0.10749056480434453, gradient norm: 0.03005904361382012\n",
      "Iteration: 4693000, loss: 0.1074901295123359, gradient norm: 0.01399951284386709\n",
      "Iteration: 4694000, loss: 0.10748958174431286, gradient norm: 0.0635598246198741\n",
      "Iteration: 4695000, loss: 0.10748915951882441, gradient norm: 0.10440049417018608\n",
      "Iteration: 4696000, loss: 0.10748855453018319, gradient norm: 0.10312700842814967\n",
      "Iteration: 4697000, loss: 0.10748826885743953, gradient norm: 0.6093659920126001\n",
      "Iteration: 4698000, loss: 0.1074877449577796, gradient norm: 0.10975529649131535\n",
      "Iteration: 4699000, loss: 0.10748711190424783, gradient norm: 0.49207916116562594\n",
      "Iteration: 4700000, loss: 0.10748669305032768, gradient norm: 0.07430231153793117\n",
      "Iteration: 4701000, loss: 0.10748659922726952, gradient norm: 0.2645936269656376\n",
      "Iteration: 4702000, loss: 0.10748568948756074, gradient norm: 0.17930735303366074\n",
      "Iteration: 4703000, loss: 0.10748525330691612, gradient norm: 0.5454871568807773\n",
      "Iteration: 4704000, loss: 0.10748487285614516, gradient norm: 0.036150336742042766\n",
      "Iteration: 4705000, loss: 0.10748442061955248, gradient norm: 0.4029829548935087\n",
      "Iteration: 4706000, loss: 0.10748383892349431, gradient norm: 0.0800188876044469\n",
      "Iteration: 4707000, loss: 0.1074834620649579, gradient norm: 0.16670309193405108\n",
      "Iteration: 4708000, loss: 0.10748285992624887, gradient norm: 0.18838424053788982\n",
      "Iteration: 4709000, loss: 0.10748249888866072, gradient norm: 0.32057614387956485\n",
      "Iteration: 4710000, loss: 0.10748202997972657, gradient norm: 0.08950013720397082\n",
      "Iteration: 4711000, loss: 0.10748148939490636, gradient norm: 0.5099351262713543\n",
      "Iteration: 4712000, loss: 0.10748111870067036, gradient norm: 0.22846756676746988\n",
      "Iteration: 4713000, loss: 0.10748059135218674, gradient norm: 0.025395553212198756\n",
      "Iteration: 4714000, loss: 0.10748009024300278, gradient norm: 0.12864424736535238\n",
      "Iteration: 4715000, loss: 0.10747969152661474, gradient norm: 0.47923074503777585\n",
      "Iteration: 4716000, loss: 0.10747910573557413, gradient norm: 0.019373301402610177\n",
      "Iteration: 4717000, loss: 0.1074786545549347, gradient norm: 0.043558678434610754\n",
      "Iteration: 4718000, loss: 0.10747825686906062, gradient norm: 0.1841744145918266\n",
      "Iteration: 4719000, loss: 0.10747775582504694, gradient norm: 0.19643980755583787\n",
      "Iteration: 4720000, loss: 0.10747718267238028, gradient norm: 0.161375078077597\n",
      "Iteration: 4721000, loss: 0.10747687653453394, gradient norm: 0.22122968241851715\n",
      "Iteration: 4722000, loss: 0.10747623861876565, gradient norm: 0.14270621453613946\n",
      "Iteration: 4723000, loss: 0.10747593528965632, gradient norm: 0.36752079163372975\n",
      "Iteration: 4724000, loss: 0.10747535971612132, gradient norm: 0.05695174809569438\n",
      "Iteration: 4725000, loss: 0.1074749000292371, gradient norm: 0.10783083820995881\n",
      "Iteration: 4726000, loss: 0.10747454029006982, gradient norm: 0.05594208362920722\n",
      "Iteration: 4727000, loss: 0.10747401070877448, gradient norm: 0.12686312648006381\n",
      "Iteration: 4728000, loss: 0.10747336159018238, gradient norm: 0.522601519070294\n",
      "Iteration: 4729000, loss: 0.10747312353868482, gradient norm: 0.10751717940857532\n",
      "Iteration: 4730000, loss: 0.1074725470370781, gradient norm: 0.11373889403977361\n",
      "Iteration: 4731000, loss: 0.10747206153329651, gradient norm: 0.019606386698620325\n",
      "Iteration: 4732000, loss: 0.10747169307837234, gradient norm: 0.12786098907894197\n",
      "Iteration: 4733000, loss: 0.10747124894207703, gradient norm: 0.3695483625422679\n",
      "Iteration: 4734000, loss: 0.10747075348601058, gradient norm: 0.04228897087535868\n",
      "Iteration: 4735000, loss: 0.1074701454871327, gradient norm: 0.5902978449617186\n",
      "Iteration: 4736000, loss: 0.10746973113565139, gradient norm: 0.08339865497431866\n",
      "Iteration: 4737000, loss: 0.10746935342013976, gradient norm: 0.11381425931605829\n",
      "Iteration: 4738000, loss: 0.10746879338422456, gradient norm: 0.22429317246805008\n",
      "Iteration: 4739000, loss: 0.10746830718603667, gradient norm: 0.14951101575514003\n",
      "Iteration: 4740000, loss: 0.10746790735755835, gradient norm: 0.12055001549254138\n",
      "Iteration: 4741000, loss: 0.10746736901713315, gradient norm: 0.22403286027275102\n",
      "Iteration: 4742000, loss: 0.10746693987540255, gradient norm: 0.08708542734509236\n",
      "Iteration: 4743000, loss: 0.10746654489313687, gradient norm: 0.28614879506535723\n",
      "Iteration: 4744000, loss: 0.10746596453080941, gradient norm: 0.15172082712356535\n",
      "Iteration: 4745000, loss: 0.10746566152330768, gradient norm: 0.19487165999982597\n",
      "Iteration: 4746000, loss: 0.10746500248542423, gradient norm: 0.2052676339697014\n",
      "Iteration: 4747000, loss: 0.10746460800333159, gradient norm: 0.09871147605830939\n",
      "Iteration: 4748000, loss: 0.10746408818242054, gradient norm: 0.7149163419348173\n",
      "Iteration: 4749000, loss: 0.10746374895417947, gradient norm: 0.5095229970561111\n",
      "Iteration: 4750000, loss: 0.10746334057514659, gradient norm: 0.42760744383013527\n",
      "Iteration: 4751000, loss: 0.1074626331600372, gradient norm: 0.08972713454484908\n",
      "Iteration: 4752000, loss: 0.10746229061480178, gradient norm: 0.29513929380281256\n",
      "Iteration: 4753000, loss: 0.1074618358095879, gradient norm: 0.50471381237949\n",
      "Iteration: 4754000, loss: 0.10746140246070347, gradient norm: 0.4831363328032932\n",
      "Iteration: 4755000, loss: 0.10746080891983403, gradient norm: 0.5689239653452173\n",
      "Iteration: 4756000, loss: 0.1074603518089712, gradient norm: 0.061108609664369526\n",
      "Iteration: 4757000, loss: 0.10746003301313835, gradient norm: 0.2134572783163046\n",
      "Iteration: 4758000, loss: 0.10745949107807577, gradient norm: 0.1491459760421278\n",
      "Iteration: 4759000, loss: 0.10745904274497511, gradient norm: 0.10718127987460584\n",
      "Iteration: 4760000, loss: 0.10745860075329469, gradient norm: 0.17623444950294218\n",
      "Iteration: 4761000, loss: 0.10745811133217739, gradient norm: 0.27442783641727786\n",
      "Iteration: 4762000, loss: 0.10745767447206868, gradient norm: 0.061514725839689845\n",
      "Iteration: 4763000, loss: 0.10745707076514008, gradient norm: 0.1367689426708687\n",
      "Iteration: 4764000, loss: 0.10745677298651361, gradient norm: 0.692533934090138\n",
      "Iteration: 4765000, loss: 0.1074561714937877, gradient norm: 0.3280258997736217\n",
      "Iteration: 4766000, loss: 0.10745599796058226, gradient norm: 0.4572472401625313\n",
      "Iteration: 4767000, loss: 0.10745515078452927, gradient norm: 0.0235228532180364\n",
      "Iteration: 4768000, loss: 0.10745496557568851, gradient norm: 0.49319161623270524\n",
      "Iteration: 4769000, loss: 0.10745426943047302, gradient norm: 0.3587287348242246\n",
      "Iteration: 4770000, loss: 0.10745406106256371, gradient norm: 0.14147403392439056\n",
      "Iteration: 4771000, loss: 0.10745352620097279, gradient norm: 0.2539662637757351\n",
      "Iteration: 4772000, loss: 0.10745295781113251, gradient norm: 0.16372545756495122\n",
      "Iteration: 4773000, loss: 0.10745258029779328, gradient norm: 0.03176641026078412\n",
      "Iteration: 4774000, loss: 0.10745217469665917, gradient norm: 0.09934316076606613\n",
      "Iteration: 4775000, loss: 0.10745163270514081, gradient norm: 0.13303939196972264\n",
      "Iteration: 4776000, loss: 0.10745105315206238, gradient norm: 0.1405239450629612\n",
      "Iteration: 4777000, loss: 0.10745076822155956, gradient norm: 0.1512628792647976\n",
      "Iteration: 4778000, loss: 0.10745024464815248, gradient norm: 0.04816725475153166\n",
      "Iteration: 4779000, loss: 0.10744989431226304, gradient norm: 0.1252183330258001\n",
      "Iteration: 4780000, loss: 0.1074492590261658, gradient norm: 0.178068313808799\n",
      "Iteration: 4781000, loss: 0.10744891360692316, gradient norm: 0.5306658580310915\n",
      "Iteration: 4782000, loss: 0.10744849021888615, gradient norm: 0.19564944072198742\n",
      "Iteration: 4783000, loss: 0.10744795466752555, gradient norm: 0.17163772605234648\n",
      "Iteration: 4784000, loss: 0.10744740215636421, gradient norm: 0.11645186135121541\n",
      "Iteration: 4785000, loss: 0.10744712554778911, gradient norm: 0.0856956834190659\n",
      "Iteration: 4786000, loss: 0.10744657342194935, gradient norm: 0.08556248015730913\n",
      "Iteration: 4787000, loss: 0.10744618388436526, gradient norm: 0.24260327295412468\n",
      "Iteration: 4788000, loss: 0.10744555090292982, gradient norm: 0.04284225902954694\n",
      "Iteration: 4789000, loss: 0.1074452409398504, gradient norm: 0.14686808140004942\n",
      "Iteration: 4790000, loss: 0.10744486334408765, gradient norm: 0.43196756251342827\n",
      "Iteration: 4791000, loss: 0.10744419551062549, gradient norm: 0.06144557213507721\n",
      "Iteration: 4792000, loss: 0.10744392744096296, gradient norm: 0.3587246108475813\n",
      "Iteration: 4793000, loss: 0.10744339755808832, gradient norm: 0.04188451071367586\n",
      "Iteration: 4794000, loss: 0.10744294014999076, gradient norm: 0.3136266261279566\n",
      "Iteration: 4795000, loss: 0.10744247426294651, gradient norm: 0.1804247657844059\n",
      "Iteration: 4796000, loss: 0.10744199168177727, gradient norm: 0.3948929698115759\n",
      "Iteration: 4797000, loss: 0.10744148667902768, gradient norm: 0.135405479978542\n",
      "Iteration: 4798000, loss: 0.10744120124824863, gradient norm: 0.08177506842149534\n",
      "Iteration: 4799000, loss: 0.10744069894198996, gradient norm: 0.1493989165380286\n",
      "Iteration: 4800000, loss: 0.107440184397886, gradient norm: 0.14358910766872704\n",
      "Iteration: 4801000, loss: 0.10743991805323831, gradient norm: 0.5141690277563542\n",
      "Iteration: 4802000, loss: 0.10743911956031413, gradient norm: 0.047955035155072184\n",
      "Iteration: 4803000, loss: 0.10743889159617685, gradient norm: 0.14695933456343968\n",
      "Iteration: 4804000, loss: 0.10743830413294955, gradient norm: 0.2855582619875922\n",
      "Iteration: 4805000, loss: 0.10743795052567656, gradient norm: 0.10196303368533657\n",
      "Iteration: 4806000, loss: 0.10743753498234848, gradient norm: 0.6965811235266949\n",
      "Iteration: 4807000, loss: 0.10743715644108975, gradient norm: 0.22682372585801944\n",
      "Iteration: 4808000, loss: 0.10743644936497543, gradient norm: 0.160429647955441\n",
      "Iteration: 4809000, loss: 0.10743604213381608, gradient norm: 0.11370941893638974\n",
      "Iteration: 4810000, loss: 0.10743564862578053, gradient norm: 0.280510049005915\n",
      "Iteration: 4811000, loss: 0.10743528436925964, gradient norm: 0.22119656074850252\n",
      "Iteration: 4812000, loss: 0.10743489584643083, gradient norm: 0.1748928730468344\n",
      "Iteration: 4813000, loss: 0.10743416009648889, gradient norm: 0.1435185705806492\n",
      "Iteration: 4814000, loss: 0.10743374701606666, gradient norm: 0.28267431842516183\n",
      "Iteration: 4815000, loss: 0.10743352725773073, gradient norm: 0.21834566565146996\n",
      "Iteration: 4816000, loss: 0.10743295145373509, gradient norm: 0.2731302422263169\n",
      "Iteration: 4817000, loss: 0.10743239610840823, gradient norm: 0.08646939093560661\n",
      "Iteration: 4818000, loss: 0.10743210573527577, gradient norm: 0.40972595379949905\n",
      "Iteration: 4819000, loss: 0.10743153583072274, gradient norm: 0.1754679777389468\n",
      "Iteration: 4820000, loss: 0.10743111938515396, gradient norm: 0.059738077359235496\n",
      "Iteration: 4821000, loss: 0.10743062320781224, gradient norm: 0.21101216989764868\n",
      "Iteration: 4822000, loss: 0.10743024333241427, gradient norm: 0.08588353028952092\n",
      "Iteration: 4823000, loss: 0.10742989752349595, gradient norm: 0.3738836971688116\n",
      "Iteration: 4824000, loss: 0.10742925594247332, gradient norm: 0.12098154200542308\n",
      "Iteration: 4825000, loss: 0.1074288551893995, gradient norm: 0.10656205281278697\n",
      "Iteration: 4826000, loss: 0.10742842656296218, gradient norm: 0.1901628429259371\n",
      "Iteration: 4827000, loss: 0.10742794641580924, gradient norm: 0.21600904348889546\n",
      "Iteration: 4828000, loss: 0.10742756639948098, gradient norm: 0.030082506053882428\n",
      "Iteration: 4829000, loss: 0.107427132450662, gradient norm: 0.04922054458839522\n",
      "Iteration: 4830000, loss: 0.10742648788591051, gradient norm: 0.21836913279924927\n",
      "Iteration: 4831000, loss: 0.10742627299213962, gradient norm: 0.0807569517835875\n",
      "Iteration: 4832000, loss: 0.10742561183558269, gradient norm: 0.14524218447898157\n",
      "Iteration: 4833000, loss: 0.10742540582582379, gradient norm: 0.2932649552531366\n",
      "Iteration: 4834000, loss: 0.10742482479572796, gradient norm: 0.16872829044257295\n",
      "Iteration: 4835000, loss: 0.10742436019517196, gradient norm: 0.09107559415841952\n",
      "Iteration: 4836000, loss: 0.10742398330477132, gradient norm: 0.24159114297446754\n",
      "Iteration: 4837000, loss: 0.10742337165666932, gradient norm: 0.09415665135071236\n",
      "Iteration: 4838000, loss: 0.10742312925079683, gradient norm: 0.19175570151155777\n",
      "Iteration: 4839000, loss: 0.10742257487425565, gradient norm: 0.4718525119356585\n",
      "Iteration: 4840000, loss: 0.10742217546001609, gradient norm: 0.03179515662391542\n",
      "Iteration: 4841000, loss: 0.10742157213283342, gradient norm: 0.09630897490719305\n",
      "Iteration: 4842000, loss: 0.10742149315556036, gradient norm: 0.07958631723297556\n",
      "Iteration: 4843000, loss: 0.10742065394562945, gradient norm: 0.22413853684889878\n",
      "Iteration: 4844000, loss: 0.10742022904947543, gradient norm: 0.13746305849730459\n",
      "Iteration: 4845000, loss: 0.10741993055189795, gradient norm: 0.04121303818788184\n",
      "Iteration: 4846000, loss: 0.1074195885456991, gradient norm: 0.191326596805339\n",
      "Iteration: 4847000, loss: 0.10741899360700213, gradient norm: 0.25920700474624603\n",
      "Iteration: 4848000, loss: 0.10741857224639158, gradient norm: 0.24163523792939592\n",
      "Iteration: 4849000, loss: 0.1074181196492295, gradient norm: 0.2173570238607218\n",
      "Iteration: 4850000, loss: 0.10741775832365794, gradient norm: 0.09704736902606717\n",
      "Iteration: 4851000, loss: 0.10741708554269988, gradient norm: 0.4734773108445471\n",
      "Iteration: 4852000, loss: 0.1074168043379457, gradient norm: 0.05759281799967028\n",
      "Iteration: 4853000, loss: 0.10741641295191462, gradient norm: 0.04972492299833845\n",
      "Iteration: 4854000, loss: 0.107415839923132, gradient norm: 0.34414274432933234\n",
      "Iteration: 4855000, loss: 0.10741538361740825, gradient norm: 0.1805155978460854\n",
      "Iteration: 4856000, loss: 0.10741523482671354, gradient norm: 0.08759172743089873\n",
      "Iteration: 4857000, loss: 0.10741440390130812, gradient norm: 0.031216565186760914\n",
      "Iteration: 4858000, loss: 0.10741412693908017, gradient norm: 0.03806791463275521\n",
      "Iteration: 4859000, loss: 0.10741375407645161, gradient norm: 0.36988398618242135\n",
      "Iteration: 4860000, loss: 0.10741316186158378, gradient norm: 0.4011320512751121\n",
      "Iteration: 4861000, loss: 0.10741292392705472, gradient norm: 0.04402390132089715\n",
      "Iteration: 4862000, loss: 0.10741228262188014, gradient norm: 0.08408939757513013\n",
      "Iteration: 4863000, loss: 0.10741208191978212, gradient norm: 0.14872817888625126\n",
      "Iteration: 4864000, loss: 0.10741132286061761, gradient norm: 0.30952292588664937\n",
      "Iteration: 4865000, loss: 0.10741117011858634, gradient norm: 0.18746381846243249\n",
      "Iteration: 4866000, loss: 0.10741038546437147, gradient norm: 0.10256663510995155\n",
      "Iteration: 4867000, loss: 0.10741014522226829, gradient norm: 0.12152727070172437\n",
      "Iteration: 4868000, loss: 0.10740969916302649, gradient norm: 0.12218264475634716\n",
      "Iteration: 4869000, loss: 0.10740928629669719, gradient norm: 0.06062964073273035\n",
      "Iteration: 4870000, loss: 0.10740883135613259, gradient norm: 0.2514187561557893\n",
      "Iteration: 4871000, loss: 0.10740844720378058, gradient norm: 0.4765457761417626\n",
      "Iteration: 4872000, loss: 0.10740780556678108, gradient norm: 0.0490276750166893\n",
      "Iteration: 4873000, loss: 0.10740745771744512, gradient norm: 0.13946002363646898\n",
      "Iteration: 4874000, loss: 0.10740713372759227, gradient norm: 0.4074297812490938\n",
      "Iteration: 4875000, loss: 0.10740665343844424, gradient norm: 0.034630570358081286\n",
      "Iteration: 4876000, loss: 0.10740615687854355, gradient norm: 0.07303352109492914\n",
      "Iteration: 4877000, loss: 0.10740578770937062, gradient norm: 0.3717348023338201\n",
      "Iteration: 4878000, loss: 0.10740529368823204, gradient norm: 0.023716144685925138\n",
      "Iteration: 4879000, loss: 0.10740483358051647, gradient norm: 0.5668576093562521\n",
      "Iteration: 4880000, loss: 0.10740438637791548, gradient norm: 0.33064819257126293\n",
      "Iteration: 4881000, loss: 0.1074040196658237, gradient norm: 0.22375734880970008\n",
      "Iteration: 4882000, loss: 0.10740339343398272, gradient norm: 0.10631428957042158\n",
      "Iteration: 4883000, loss: 0.10740314559579353, gradient norm: 0.04710640687462105\n",
      "Iteration: 4884000, loss: 0.10740251184014805, gradient norm: 0.09602697344790334\n",
      "Iteration: 4885000, loss: 0.10740227701301405, gradient norm: 0.1870273158921666\n",
      "Iteration: 4886000, loss: 0.10740171500441494, gradient norm: 0.04957710116126139\n",
      "Iteration: 4887000, loss: 0.10740131709272617, gradient norm: 0.12010099243443233\n",
      "Iteration: 4888000, loss: 0.10740090083620572, gradient norm: 0.5402389740163216\n",
      "Iteration: 4889000, loss: 0.1074004287017348, gradient norm: 0.18073526639025012\n",
      "Iteration: 4890000, loss: 0.10740001997322082, gradient norm: 0.25081642991817577\n",
      "Iteration: 4891000, loss: 0.10739968622154511, gradient norm: 0.04322270679561189\n",
      "Iteration: 4892000, loss: 0.10739908778853528, gradient norm: 0.1829093220569785\n",
      "Iteration: 4893000, loss: 0.10739868875274325, gradient norm: 0.058361301752947056\n",
      "Iteration: 4894000, loss: 0.10739828569084356, gradient norm: 0.26541836233741006\n",
      "Iteration: 4895000, loss: 0.10739774500321078, gradient norm: 0.31219507647506617\n",
      "Iteration: 4896000, loss: 0.10739732830395468, gradient norm: 0.10766539881330522\n",
      "Iteration: 4897000, loss: 0.10739719626029078, gradient norm: 0.532477213864439\n",
      "Iteration: 4898000, loss: 0.10739634782029427, gradient norm: 0.00794054237710466\n",
      "Iteration: 4899000, loss: 0.10739612404454554, gradient norm: 0.19242252038343396\n",
      "Iteration: 4900000, loss: 0.10739552885177879, gradient norm: 0.0620681236570071\n",
      "Iteration: 4901000, loss: 0.10739520962718378, gradient norm: 0.40604641711947254\n",
      "Iteration: 4902000, loss: 0.10739479998282322, gradient norm: 0.40285678651957674\n",
      "Iteration: 4903000, loss: 0.10739431856227477, gradient norm: 0.7246826303588315\n",
      "Iteration: 4904000, loss: 0.10739394799790503, gradient norm: 0.1833905479927299\n",
      "Iteration: 4905000, loss: 0.10739357965255246, gradient norm: 0.4921018785235771\n",
      "Iteration: 4906000, loss: 0.10739279901287668, gradient norm: 0.09736821179161423\n",
      "Iteration: 4907000, loss: 0.10739260190684852, gradient norm: 0.4018024484990895\n",
      "Iteration: 4908000, loss: 0.10739213191763088, gradient norm: 0.22836964557774964\n",
      "Iteration: 4909000, loss: 0.10739178206440468, gradient norm: 0.024253938065636513\n",
      "Iteration: 4910000, loss: 0.10739125282868793, gradient norm: 0.05258125187554422\n",
      "Iteration: 4911000, loss: 0.10739077032962151, gradient norm: 0.09509189920018483\n",
      "Iteration: 4912000, loss: 0.10739049375199036, gradient norm: 0.2076856552391226\n",
      "Iteration: 4913000, loss: 0.10738984566144794, gradient norm: 0.035227570097986306\n",
      "Iteration: 4914000, loss: 0.10738966434309406, gradient norm: 0.03663682831876632\n",
      "Iteration: 4915000, loss: 0.10738910599899884, gradient norm: 0.3945918350168666\n",
      "Iteration: 4916000, loss: 0.1073885914167569, gradient norm: 0.08072383666062215\n",
      "Iteration: 4917000, loss: 0.10738835000189303, gradient norm: 0.07397468305254543\n",
      "Iteration: 4918000, loss: 0.10738772745587265, gradient norm: 0.08425678359180816\n",
      "Iteration: 4919000, loss: 0.10738741050083278, gradient norm: 0.12643763389864027\n",
      "Iteration: 4920000, loss: 0.10738687974746976, gradient norm: 0.33971085806396983\n",
      "Iteration: 4921000, loss: 0.10738653015427947, gradient norm: 0.09544814879099865\n",
      "Iteration: 4922000, loss: 0.10738601926370941, gradient norm: 0.0453663524078598\n",
      "Iteration: 4923000, loss: 0.10738585194238182, gradient norm: 0.04215700001259973\n",
      "Iteration: 4924000, loss: 0.1073851729994453, gradient norm: 0.021727094561461022\n",
      "Iteration: 4925000, loss: 0.10738467796283319, gradient norm: 0.13011860480462953\n",
      "Iteration: 4926000, loss: 0.10738446593609811, gradient norm: 0.04056104674269425\n",
      "Iteration: 4927000, loss: 0.10738375560449791, gradient norm: 0.04839698678231804\n",
      "Iteration: 4928000, loss: 0.10738353965647124, gradient norm: 0.09703436372979288\n",
      "Iteration: 4929000, loss: 0.10738308124609007, gradient norm: 0.09272668554239348\n",
      "Iteration: 4930000, loss: 0.10738261039874343, gradient norm: 0.12694933407231698\n",
      "Iteration: 4931000, loss: 0.10738230053601826, gradient norm: 0.21015541163297577\n",
      "Iteration: 4932000, loss: 0.10738172554015184, gradient norm: 0.3981003859747589\n",
      "Iteration: 4933000, loss: 0.10738119832917203, gradient norm: 0.18105004487202797\n",
      "Iteration: 4934000, loss: 0.10738088661122235, gradient norm: 0.052463747959838145\n",
      "Iteration: 4935000, loss: 0.10738058905395943, gradient norm: 0.17377033182010002\n",
      "Iteration: 4936000, loss: 0.10738000548560209, gradient norm: 0.550124807931052\n",
      "Iteration: 4937000, loss: 0.10737967906870002, gradient norm: 0.4087883140859504\n",
      "Iteration: 4938000, loss: 0.10737910958553203, gradient norm: 0.5217084169861564\n",
      "Iteration: 4939000, loss: 0.1073787899543025, gradient norm: 0.694028452824577\n",
      "Iteration: 4940000, loss: 0.10737841778347286, gradient norm: 0.16002373098608552\n",
      "Iteration: 4941000, loss: 0.1073778088082218, gradient norm: 0.49937890583157657\n",
      "Iteration: 4942000, loss: 0.10737739244303332, gradient norm: 0.27783216238673714\n",
      "Iteration: 4943000, loss: 0.10737719032295008, gradient norm: 0.11310112723983054\n",
      "Iteration: 4944000, loss: 0.10737659384842342, gradient norm: 0.10795971786517046\n",
      "Iteration: 4945000, loss: 0.10737608441291385, gradient norm: 0.13040571547011912\n",
      "Iteration: 4946000, loss: 0.10737578427506717, gradient norm: 0.026424820270245418\n",
      "Iteration: 4947000, loss: 0.10737539791821049, gradient norm: 0.548719341705178\n",
      "Iteration: 4948000, loss: 0.10737478869936373, gradient norm: 0.152084720261938\n",
      "Iteration: 4949000, loss: 0.10737452472754185, gradient norm: 0.10442510224976247\n",
      "Iteration: 4950000, loss: 0.10737408660201117, gradient norm: 0.6253349683902074\n",
      "Iteration: 4951000, loss: 0.10737362577942657, gradient norm: 0.27652592095361694\n",
      "Iteration: 4952000, loss: 0.1073731816990159, gradient norm: 0.09224679445337546\n",
      "Iteration: 4953000, loss: 0.10737274492944007, gradient norm: 0.08268349137988465\n",
      "Iteration: 4954000, loss: 0.10737223743647974, gradient norm: 0.08062139749658585\n",
      "Iteration: 4955000, loss: 0.10737214273931299, gradient norm: 0.30254631527521525\n",
      "Iteration: 4956000, loss: 0.10737142320297244, gradient norm: 0.18525122162773824\n",
      "Iteration: 4957000, loss: 0.10737095638735539, gradient norm: 0.550195600139442\n",
      "Iteration: 4958000, loss: 0.10737056888456632, gradient norm: 0.3942426234116806\n",
      "Iteration: 4959000, loss: 0.10737029164901213, gradient norm: 0.038821778563465324\n",
      "Iteration: 4960000, loss: 0.10736971114504078, gradient norm: 0.42133665013986776\n",
      "Iteration: 4961000, loss: 0.10736946158296541, gradient norm: 0.10956079704842676\n",
      "Iteration: 4962000, loss: 0.10736884593752663, gradient norm: 0.015840485396780322\n",
      "Iteration: 4963000, loss: 0.10736855798737688, gradient norm: 0.6563385772564462\n",
      "Iteration: 4964000, loss: 0.10736811049791059, gradient norm: 0.23224065840930672\n",
      "Iteration: 4965000, loss: 0.10736762106736146, gradient norm: 0.013747491298588342\n",
      "Iteration: 4966000, loss: 0.10736719828191758, gradient norm: 0.20718054492375926\n",
      "Iteration: 4967000, loss: 0.1073668261786706, gradient norm: 0.0906721887294242\n",
      "Iteration: 4968000, loss: 0.10736637255927309, gradient norm: 0.33091896352180195\n",
      "Iteration: 4969000, loss: 0.10736588276979128, gradient norm: 0.6100664901633738\n",
      "Iteration: 4970000, loss: 0.1073655687087251, gradient norm: 0.09966502661413311\n",
      "Iteration: 4971000, loss: 0.10736526821119756, gradient norm: 0.21167428036296768\n",
      "Iteration: 4972000, loss: 0.10736455474475365, gradient norm: 0.6535420516442367\n",
      "Iteration: 4973000, loss: 0.10736425911037813, gradient norm: 0.04319291332726518\n",
      "Iteration: 4974000, loss: 0.10736366197501454, gradient norm: 0.15019035842520556\n",
      "Iteration: 4975000, loss: 0.10736339774379604, gradient norm: 0.41701654976321034\n",
      "Iteration: 4976000, loss: 0.10736310124966132, gradient norm: 0.12850550157569918\n",
      "Iteration: 4977000, loss: 0.10736254153247714, gradient norm: 0.3380154295942322\n",
      "Iteration: 4978000, loss: 0.10736226628222721, gradient norm: 0.06075399574296104\n",
      "Iteration: 4979000, loss: 0.10736171140131391, gradient norm: 0.0857227741071995\n",
      "Iteration: 4980000, loss: 0.107361200166871, gradient norm: 0.2795207823725144\n",
      "Iteration: 4981000, loss: 0.10736079314266132, gradient norm: 0.4168087597743527\n",
      "Iteration: 4982000, loss: 0.10736052317413075, gradient norm: 0.1733058810806263\n",
      "Iteration: 4983000, loss: 0.10736008085717387, gradient norm: 0.11973355847791323\n",
      "Iteration: 4984000, loss: 0.1073596410088324, gradient norm: 0.14051294751994836\n",
      "Iteration: 4985000, loss: 0.10735924939908471, gradient norm: 0.3114000252159007\n",
      "Iteration: 4986000, loss: 0.10735869931949549, gradient norm: 0.17348688359646777\n",
      "Iteration: 4987000, loss: 0.10735832751732081, gradient norm: 0.1776450857421973\n",
      "Iteration: 4988000, loss: 0.10735791744393973, gradient norm: 0.014664052538660724\n",
      "Iteration: 4989000, loss: 0.10735758119550456, gradient norm: 0.33284094003238024\n",
      "Iteration: 4990000, loss: 0.10735698487797035, gradient norm: 0.21417899325609563\n",
      "Iteration: 4991000, loss: 0.10735672843389335, gradient norm: 0.2965544612060434\n",
      "Iteration: 4992000, loss: 0.10735622672111313, gradient norm: 0.14739516668437866\n",
      "Iteration: 4993000, loss: 0.10735571720371899, gradient norm: 0.10979404566388669\n",
      "Iteration: 4994000, loss: 0.107355636519872, gradient norm: 0.2700768098380203\n",
      "Iteration: 4995000, loss: 0.10735483317948966, gradient norm: 0.22552016560328408\n",
      "Iteration: 4996000, loss: 0.10735456644429528, gradient norm: 0.6139552234276046\n",
      "Iteration: 4997000, loss: 0.10735425952544406, gradient norm: 0.14508623486768593\n",
      "Iteration: 4998000, loss: 0.10735370844326494, gradient norm: 0.11988156897626825\n",
      "Iteration: 4999000, loss: 0.10735321991116284, gradient norm: 0.049861469277015784\n",
      "Iteration: 5000000, loss: 0.10735299977842777, gradient norm: 0.9122768268976938\n",
      "Iteration: 5001000, loss: 0.1073523457520804, gradient norm: 0.39720546244158933\n",
      "Iteration: 5002000, loss: 0.10735211827582797, gradient norm: 0.16266139559423382\n",
      "Iteration: 5003000, loss: 0.10735178347442612, gradient norm: 0.34378919709672223\n",
      "Iteration: 5004000, loss: 0.10735099182980025, gradient norm: 0.05201947041660498\n",
      "Iteration: 5005000, loss: 0.10735099335144384, gradient norm: 0.10658281890592215\n",
      "Iteration: 5006000, loss: 0.10735030736019371, gradient norm: 0.07089917280932533\n",
      "Iteration: 5007000, loss: 0.10734998978533641, gradient norm: 0.05081212051679492\n",
      "Iteration: 5008000, loss: 0.1073494973819405, gradient norm: 0.13605538091699837\n",
      "Iteration: 5009000, loss: 0.10734912024697194, gradient norm: 0.06646107371688036\n",
      "Iteration: 5010000, loss: 0.1073487799665803, gradient norm: 0.2518109436494256\n",
      "Iteration: 5011000, loss: 0.107348297334716, gradient norm: 0.4109456974749822\n",
      "Iteration: 5012000, loss: 0.10734791260519605, gradient norm: 0.44263739132327773\n",
      "Iteration: 5013000, loss: 0.10734757332354954, gradient norm: 0.13578545885469293\n",
      "Iteration: 5014000, loss: 0.10734689911135699, gradient norm: 0.3132256512513641\n",
      "Iteration: 5015000, loss: 0.10734661384591129, gradient norm: 0.06499474393689952\n",
      "Iteration: 5016000, loss: 0.10734627895754871, gradient norm: 0.06810454609383221\n",
      "Iteration: 5017000, loss: 0.1073457751232913, gradient norm: 0.394207236542149\n",
      "Iteration: 5018000, loss: 0.1073454398743931, gradient norm: 0.2897347785631038\n",
      "Iteration: 5019000, loss: 0.10734497415513579, gradient norm: 0.1547160606661663\n",
      "Iteration: 5020000, loss: 0.10734445077034163, gradient norm: 0.3801092834980697\n",
      "Iteration: 5021000, loss: 0.10734417405794139, gradient norm: 0.2432793298014824\n",
      "Iteration: 5022000, loss: 0.10734375488048213, gradient norm: 0.1135987140471801\n",
      "Iteration: 5023000, loss: 0.10734319976751211, gradient norm: 0.08182606924259758\n",
      "Iteration: 5024000, loss: 0.10734298784367027, gradient norm: 0.5234782583687514\n",
      "Iteration: 5025000, loss: 0.10734260904065732, gradient norm: 0.2585021973718151\n",
      "Iteration: 5026000, loss: 0.10734191521695732, gradient norm: 0.5462318510952129\n",
      "Iteration: 5027000, loss: 0.10734163761300815, gradient norm: 0.03768260833830122\n",
      "Iteration: 5028000, loss: 0.1073412834059398, gradient norm: 0.3876223807114169\n",
      "Iteration: 5029000, loss: 0.10734073735488549, gradient norm: 0.30022329382587754\n",
      "Iteration: 5030000, loss: 0.10734056495427174, gradient norm: 0.07373914866857688\n",
      "Iteration: 5031000, loss: 0.10733982204079828, gradient norm: 0.10788046002928982\n",
      "Iteration: 5032000, loss: 0.10733963166037568, gradient norm: 0.49779784058734045\n",
      "Iteration: 5033000, loss: 0.10733916747883063, gradient norm: 0.09228916882049909\n",
      "Iteration: 5034000, loss: 0.1073387490659156, gradient norm: 0.04112317582218935\n",
      "Iteration: 5035000, loss: 0.10733840755824091, gradient norm: 0.48632187163316415\n",
      "Iteration: 5036000, loss: 0.1073379016245419, gradient norm: 0.1000914048203726\n",
      "Iteration: 5037000, loss: 0.10733753822384005, gradient norm: 0.2655234141102235\n",
      "Iteration: 5038000, loss: 0.10733709874108235, gradient norm: 0.07923124796049688\n",
      "Iteration: 5039000, loss: 0.10733667544753422, gradient norm: 0.6962529121984712\n",
      "Iteration: 5040000, loss: 0.1073362326713504, gradient norm: 0.1287641335580674\n",
      "Iteration: 5041000, loss: 0.1073358394118685, gradient norm: 0.3406562012924269\n",
      "Iteration: 5042000, loss: 0.10733559587842041, gradient norm: 0.16834847974008066\n",
      "Iteration: 5043000, loss: 0.10733506701256437, gradient norm: 0.29941130187357196\n",
      "Iteration: 5044000, loss: 0.1073347438958904, gradient norm: 0.524671793046123\n",
      "Iteration: 5045000, loss: 0.10733407823895316, gradient norm: 0.616754571739107\n",
      "Iteration: 5046000, loss: 0.10733386662500749, gradient norm: 0.09728803462192857\n",
      "Iteration: 5047000, loss: 0.10733344333996195, gradient norm: 0.003452121009895955\n",
      "Iteration: 5048000, loss: 0.10733291914701752, gradient norm: 0.3325751305034917\n",
      "Iteration: 5049000, loss: 0.10733255103897832, gradient norm: 0.19805302508582961\n",
      "Iteration: 5050000, loss: 0.10733228522846894, gradient norm: 0.10721335493353731\n",
      "Iteration: 5051000, loss: 0.10733162268191543, gradient norm: 0.27372396311799035\n",
      "Iteration: 5052000, loss: 0.10733141058948303, gradient norm: 0.15560069166412452\n",
      "Iteration: 5053000, loss: 0.10733090236384792, gradient norm: 0.5316361686095336\n",
      "Iteration: 5054000, loss: 0.10733060642292538, gradient norm: 0.24448416507688944\n",
      "Iteration: 5055000, loss: 0.10733013975465001, gradient norm: 0.08683687584989164\n",
      "Iteration: 5056000, loss: 0.10732964505597295, gradient norm: 0.07728647761134402\n",
      "Iteration: 5057000, loss: 0.10732943576666756, gradient norm: 0.13900149342977602\n",
      "Iteration: 5058000, loss: 0.10732881296105355, gradient norm: 0.20054050366053397\n",
      "Iteration: 5059000, loss: 0.10732849058559842, gradient norm: 0.06551483433183199\n",
      "Iteration: 5060000, loss: 0.10732809573656041, gradient norm: 0.30414930606412255\n",
      "Iteration: 5061000, loss: 0.1073276031049332, gradient norm: 0.05767307563052688\n",
      "Iteration: 5062000, loss: 0.10732725509249823, gradient norm: 0.437645572678545\n",
      "Iteration: 5063000, loss: 0.10732688747171043, gradient norm: 0.013761815710462269\n",
      "Iteration: 5064000, loss: 0.10732646239426583, gradient norm: 0.1637315686543799\n",
      "Iteration: 5065000, loss: 0.10732595466915065, gradient norm: 0.5623052499412041\n",
      "Iteration: 5066000, loss: 0.10732560353208863, gradient norm: 0.09240564576353015\n",
      "Iteration: 5067000, loss: 0.10732517691934403, gradient norm: 0.2225758666613672\n",
      "Iteration: 5068000, loss: 0.10732484294408462, gradient norm: 0.07202804784555768\n",
      "Iteration: 5069000, loss: 0.107324311984149, gradient norm: 0.07365090806009283\n",
      "Iteration: 5070000, loss: 0.10732411641809196, gradient norm: 0.3408927752361969\n",
      "Iteration: 5071000, loss: 0.107323524977917, gradient norm: 0.14856787362886642\n",
      "Iteration: 5072000, loss: 0.10732314875313265, gradient norm: 0.5088205891795827\n",
      "Iteration: 5073000, loss: 0.1073227998868149, gradient norm: 0.4888171263649955\n",
      "Iteration: 5074000, loss: 0.10732237295239103, gradient norm: 0.0874594235430201\n",
      "Iteration: 5075000, loss: 0.10732186695692109, gradient norm: 0.09839144500064148\n",
      "Iteration: 5076000, loss: 0.10732163330473526, gradient norm: 0.22500094236833396\n",
      "Iteration: 5077000, loss: 0.10732118095701393, gradient norm: 0.0731817089015633\n",
      "Iteration: 5078000, loss: 0.10732069348679799, gradient norm: 0.23415449740503472\n",
      "Iteration: 5079000, loss: 0.10732027522955365, gradient norm: 0.08727180458203339\n",
      "Iteration: 5080000, loss: 0.10731991737039537, gradient norm: 0.04387634811489184\n",
      "Iteration: 5081000, loss: 0.10731951355786347, gradient norm: 0.2826007226843808\n",
      "Iteration: 5082000, loss: 0.10731913389014387, gradient norm: 0.03596768120938257\n",
      "Iteration: 5083000, loss: 0.10731879553476087, gradient norm: 0.0611866966790992\n",
      "Iteration: 5084000, loss: 0.10731828738115874, gradient norm: 0.3433773195717515\n",
      "Iteration: 5085000, loss: 0.10731787523333475, gradient norm: 0.15317528155901977\n",
      "Iteration: 5086000, loss: 0.10731747497921255, gradient norm: 0.2319707079839143\n",
      "Iteration: 5087000, loss: 0.10731707712837202, gradient norm: 0.4872639326097793\n",
      "Iteration: 5088000, loss: 0.10731664453026828, gradient norm: 0.2959654005665463\n",
      "Iteration: 5089000, loss: 0.10731639959663303, gradient norm: 0.25795338961770115\n",
      "Iteration: 5090000, loss: 0.10731578907167172, gradient norm: 0.23018297577664865\n",
      "Iteration: 5091000, loss: 0.10731548149370344, gradient norm: 0.020439595240280845\n",
      "Iteration: 5092000, loss: 0.10731506737881237, gradient norm: 0.07197960265094734\n",
      "Iteration: 5093000, loss: 0.10731469767582905, gradient norm: 0.34107127652660785\n",
      "Iteration: 5094000, loss: 0.1073142192614146, gradient norm: 0.30451564400229386\n",
      "Iteration: 5095000, loss: 0.1073139211507875, gradient norm: 0.2215886834930888\n",
      "Iteration: 5096000, loss: 0.10731335071529167, gradient norm: 0.23224963352065664\n",
      "Iteration: 5097000, loss: 0.10731307467290728, gradient norm: 0.11687617841704676\n",
      "Iteration: 5098000, loss: 0.10731267178918237, gradient norm: 0.014241813361842432\n",
      "Iteration: 5099000, loss: 0.10731225519673863, gradient norm: 0.23125355993236388\n",
      "Iteration: 5100000, loss: 0.10731188350350257, gradient norm: 0.08483694025577608\n",
      "Iteration: 5101000, loss: 0.10731135792323929, gradient norm: 0.11574472542151296\n",
      "Iteration: 5102000, loss: 0.10731105695496748, gradient norm: 0.024791943168432466\n",
      "Iteration: 5103000, loss: 0.10731064082419697, gradient norm: 0.018609728156438732\n",
      "Iteration: 5104000, loss: 0.10731027518062025, gradient norm: 0.09452131421938684\n",
      "Iteration: 5105000, loss: 0.10730983801556683, gradient norm: 0.08985932411977202\n",
      "Iteration: 5106000, loss: 0.10730957956553931, gradient norm: 0.1300900108661003\n",
      "Iteration: 5107000, loss: 0.10730892558607052, gradient norm: 0.10726388848667674\n",
      "Iteration: 5108000, loss: 0.10730866370838958, gradient norm: 0.2825367859269886\n",
      "Iteration: 5109000, loss: 0.10730822598448732, gradient norm: 0.18928555095805374\n",
      "Iteration: 5110000, loss: 0.10730782722173847, gradient norm: 0.33417095824616416\n",
      "Iteration: 5111000, loss: 0.10730733920867676, gradient norm: 0.23963536101343058\n",
      "Iteration: 5112000, loss: 0.1073070791715131, gradient norm: 0.025176745026219343\n",
      "Iteration: 5113000, loss: 0.10730672717940884, gradient norm: 0.3013274058010052\n",
      "Iteration: 5114000, loss: 0.10730630474067704, gradient norm: 0.2155328492180385\n",
      "Iteration: 5115000, loss: 0.10730559475710134, gradient norm: 0.15273937928788806\n",
      "Iteration: 5116000, loss: 0.10730563269763232, gradient norm: 0.5178759634946987\n",
      "Iteration: 5117000, loss: 0.10730494455236184, gradient norm: 0.16684836490215893\n",
      "Iteration: 5118000, loss: 0.10730455947134893, gradient norm: 0.0610586198085794\n",
      "Iteration: 5119000, loss: 0.10730426971027365, gradient norm: 0.13170105484205097\n",
      "Iteration: 5120000, loss: 0.10730385092492212, gradient norm: 0.4802794759048565\n",
      "Iteration: 5121000, loss: 0.10730348577632973, gradient norm: 0.3371852002948467\n",
      "Iteration: 5122000, loss: 0.10730309291683986, gradient norm: 0.2225106449401823\n",
      "Iteration: 5123000, loss: 0.10730260833611716, gradient norm: 0.08467789910966177\n",
      "Iteration: 5124000, loss: 0.10730217890065637, gradient norm: 0.0871291569242762\n",
      "Iteration: 5125000, loss: 0.10730180149003037, gradient norm: 0.05207302885512595\n",
      "Iteration: 5126000, loss: 0.10730144878739538, gradient norm: 0.05269278137811546\n",
      "Iteration: 5127000, loss: 0.10730100318042642, gradient norm: 0.7446107588204632\n",
      "Iteration: 5128000, loss: 0.10730066957105873, gradient norm: 0.5885756747298246\n",
      "Iteration: 5129000, loss: 0.10730023752881673, gradient norm: 0.2621141975709959\n",
      "Iteration: 5130000, loss: 0.10729982654501248, gradient norm: 0.08716429392464303\n",
      "Iteration: 5131000, loss: 0.10729940196272256, gradient norm: 0.1403068779534415\n",
      "Iteration: 5132000, loss: 0.10729912020495418, gradient norm: 0.48694789545191214\n",
      "Iteration: 5133000, loss: 0.10729863727673063, gradient norm: 0.1723578704094324\n",
      "Iteration: 5134000, loss: 0.10729828439287159, gradient norm: 0.07535308489513845\n",
      "Iteration: 5135000, loss: 0.10729776233515591, gradient norm: 0.4611028142425792\n",
      "Iteration: 5136000, loss: 0.10729746793525116, gradient norm: 0.0425242326022641\n",
      "Iteration: 5137000, loss: 0.10729703917456138, gradient norm: 0.3858740377983664\n",
      "Iteration: 5138000, loss: 0.10729673462537645, gradient norm: 0.10936096895442611\n",
      "Iteration: 5139000, loss: 0.10729634721113121, gradient norm: 0.10216243907747177\n",
      "Iteration: 5140000, loss: 0.1072956746821506, gradient norm: 0.4617374000210166\n",
      "Iteration: 5141000, loss: 0.10729564696264396, gradient norm: 0.1505067381169041\n",
      "Iteration: 5142000, loss: 0.10729506626478559, gradient norm: 0.3178269774126365\n",
      "Iteration: 5143000, loss: 0.10729459409875441, gradient norm: 0.11155976476031394\n",
      "Iteration: 5144000, loss: 0.10729427913385862, gradient norm: 0.13751379093270985\n",
      "Iteration: 5145000, loss: 0.10729393529362094, gradient norm: 0.047539411786013286\n",
      "Iteration: 5146000, loss: 0.10729351313440584, gradient norm: 0.14872129602755405\n",
      "Iteration: 5147000, loss: 0.10729305462891327, gradient norm: 0.037060335454018135\n",
      "Iteration: 5148000, loss: 0.10729276879637652, gradient norm: 0.3101876637694597\n",
      "Iteration: 5149000, loss: 0.10729238560660213, gradient norm: 0.562802027514293\n",
      "Iteration: 5150000, loss: 0.10729202771964393, gradient norm: 0.11596184684507423\n",
      "Iteration: 5151000, loss: 0.10729136020006934, gradient norm: 0.05558989995630206\n",
      "Iteration: 5152000, loss: 0.10729103981874619, gradient norm: 0.5551968242820451\n",
      "Iteration: 5153000, loss: 0.10729075288777061, gradient norm: 0.2999427182986082\n",
      "Iteration: 5154000, loss: 0.10729042096765984, gradient norm: 0.3641258384346725\n",
      "Iteration: 5155000, loss: 0.10728989826665158, gradient norm: 0.5517024318683234\n",
      "Iteration: 5156000, loss: 0.10728955266896141, gradient norm: 0.15773058418852454\n",
      "Iteration: 5157000, loss: 0.10728916191049172, gradient norm: 0.02839308772964047\n",
      "Iteration: 5158000, loss: 0.10728867381725193, gradient norm: 0.1633949523793934\n",
      "Iteration: 5159000, loss: 0.10728831300313066, gradient norm: 0.34119747168338743\n",
      "Iteration: 5160000, loss: 0.10728807535899937, gradient norm: 0.025163088938745762\n",
      "Iteration: 5161000, loss: 0.10728755131021402, gradient norm: 0.32112113324706587\n",
      "Iteration: 5162000, loss: 0.10728732504372192, gradient norm: 0.17861592863636078\n",
      "Iteration: 5163000, loss: 0.10728667230592458, gradient norm: 0.043722250992820996\n",
      "Iteration: 5164000, loss: 0.10728645839037779, gradient norm: 0.07459069275719081\n",
      "Iteration: 5165000, loss: 0.10728599250867785, gradient norm: 0.10993443481083637\n",
      "Iteration: 5166000, loss: 0.10728554298908693, gradient norm: 0.08272981885917212\n",
      "Iteration: 5167000, loss: 0.10728528404798439, gradient norm: 0.04678165125335834\n",
      "Iteration: 5168000, loss: 0.1072849343965077, gradient norm: 0.2581689127875909\n",
      "Iteration: 5169000, loss: 0.10728443061057784, gradient norm: 0.07953939190616935\n",
      "Iteration: 5170000, loss: 0.10728398745493654, gradient norm: 0.698702140563984\n",
      "Iteration: 5171000, loss: 0.10728362814644854, gradient norm: 0.09321035839060551\n",
      "Iteration: 5172000, loss: 0.10728323388356094, gradient norm: 0.1632459821110206\n",
      "Iteration: 5173000, loss: 0.10728301724719908, gradient norm: 0.6135643354086039\n",
      "Iteration: 5174000, loss: 0.10728229866943376, gradient norm: 0.0769441134552514\n",
      "Iteration: 5175000, loss: 0.10728210008822808, gradient norm: 0.026750837571280873\n",
      "Iteration: 5176000, loss: 0.10728175116194506, gradient norm: 0.4191700787320315\n",
      "Iteration: 5177000, loss: 0.10728128030532873, gradient norm: 0.11699268120995011\n",
      "Iteration: 5178000, loss: 0.10728091003972712, gradient norm: 0.1402305653317585\n",
      "Iteration: 5179000, loss: 0.10728058182500445, gradient norm: 0.12716326828387525\n",
      "Iteration: 5180000, loss: 0.10728007443465493, gradient norm: 0.5091488331726531\n",
      "Iteration: 5181000, loss: 0.10727985706098504, gradient norm: 0.13762809275403684\n",
      "Iteration: 5182000, loss: 0.10727951327374659, gradient norm: 0.23750696262903473\n",
      "Iteration: 5183000, loss: 0.1072787820256922, gradient norm: 0.017325037731850703\n",
      "Iteration: 5184000, loss: 0.10727861280781534, gradient norm: 0.3973551934145828\n",
      "Iteration: 5185000, loss: 0.10727821174754587, gradient norm: 0.045317911460763124\n",
      "Iteration: 5186000, loss: 0.1072776844509035, gradient norm: 0.1663398688351864\n",
      "Iteration: 5187000, loss: 0.10727747975253793, gradient norm: 0.24768989559504784\n",
      "Iteration: 5188000, loss: 0.10727701719422748, gradient norm: 0.12674325963703717\n",
      "Iteration: 5189000, loss: 0.10727663619166931, gradient norm: 0.13829056595401118\n",
      "Iteration: 5190000, loss: 0.1072761687327628, gradient norm: 0.5496018413465552\n",
      "Iteration: 5191000, loss: 0.10727593991759996, gradient norm: 0.07043267646115794\n",
      "Iteration: 5192000, loss: 0.1072753864614619, gradient norm: 0.06559588727553134\n",
      "Iteration: 5193000, loss: 0.10727509808463055, gradient norm: 0.26065953815885295\n",
      "Iteration: 5194000, loss: 0.10727465874400892, gradient norm: 0.14451453007772294\n",
      "Iteration: 5195000, loss: 0.10727441475144306, gradient norm: 0.015835752904948434\n",
      "Iteration: 5196000, loss: 0.10727391179133522, gradient norm: 0.04893647096888624\n",
      "Iteration: 5197000, loss: 0.10727349582429357, gradient norm: 0.21557829360802022\n",
      "Iteration: 5198000, loss: 0.10727298952405562, gradient norm: 0.40490013922894696\n",
      "Iteration: 5199000, loss: 0.10727286085186015, gradient norm: 0.1743203070254979\n",
      "Iteration: 5200000, loss: 0.10727229601351292, gradient norm: 0.6274414856261195\n",
      "Iteration: 5201000, loss: 0.10727200556475702, gradient norm: 0.13310831916964622\n",
      "Iteration: 5202000, loss: 0.10727152090804952, gradient norm: 0.0835479615977435\n",
      "Iteration: 5203000, loss: 0.10727133155431191, gradient norm: 0.2998482870698058\n",
      "Iteration: 5204000, loss: 0.10727079243623111, gradient norm: 0.4569371002740439\n",
      "Iteration: 5205000, loss: 0.10727036379206961, gradient norm: 0.2556145593284183\n",
      "Iteration: 5206000, loss: 0.10727010460626758, gradient norm: 0.0757158736880661\n",
      "Iteration: 5207000, loss: 0.10726951263502257, gradient norm: 0.17845616166560002\n",
      "Iteration: 5208000, loss: 0.10726934153334268, gradient norm: 0.2839483442907867\n",
      "Iteration: 5209000, loss: 0.10726885561832372, gradient norm: 0.3689012543857225\n",
      "Iteration: 5210000, loss: 0.10726861375208466, gradient norm: 0.1482692252311386\n",
      "Iteration: 5211000, loss: 0.10726801096382059, gradient norm: 0.06557894320508033\n",
      "Iteration: 5212000, loss: 0.1072677926240626, gradient norm: 0.11477306484772032\n",
      "Iteration: 5213000, loss: 0.10726728009870876, gradient norm: 0.5467322185260507\n",
      "Iteration: 5214000, loss: 0.10726681896634224, gradient norm: 0.09187505228062127\n",
      "Iteration: 5215000, loss: 0.10726662748268102, gradient norm: 0.13984519509850202\n",
      "Iteration: 5216000, loss: 0.10726641202534275, gradient norm: 0.16897227189105485\n",
      "Iteration: 5217000, loss: 0.10726552711137932, gradient norm: 0.20962954488875152\n",
      "Iteration: 5218000, loss: 0.1072654213913789, gradient norm: 0.11730248826391872\n",
      "Iteration: 5219000, loss: 0.1072650401607099, gradient norm: 0.1929589426020312\n",
      "Iteration: 5220000, loss: 0.10726460872985233, gradient norm: 0.028285363333764205\n",
      "Iteration: 5221000, loss: 0.10726421492644031, gradient norm: 0.2182746901962314\n",
      "Iteration: 5222000, loss: 0.10726399586814096, gradient norm: 0.40895704312828574\n",
      "Iteration: 5223000, loss: 0.10726332500579264, gradient norm: 0.11479434137538115\n",
      "Iteration: 5224000, loss: 0.10726323481482293, gradient norm: 0.2788596937083494\n",
      "Iteration: 5225000, loss: 0.1072626780013447, gradient norm: 0.20368167059232342\n",
      "Iteration: 5226000, loss: 0.10726237558061266, gradient norm: 0.05924779233952215\n",
      "Iteration: 5227000, loss: 0.10726189564552123, gradient norm: 0.022270918195004305\n",
      "Iteration: 5228000, loss: 0.10726163193640727, gradient norm: 0.04674571224049548\n",
      "Iteration: 5229000, loss: 0.10726115019795221, gradient norm: 0.17646551361025886\n",
      "Iteration: 5230000, loss: 0.10726073397597216, gradient norm: 0.035968059607883296\n",
      "Iteration: 5231000, loss: 0.10726052638548705, gradient norm: 0.17615224864358195\n",
      "Iteration: 5232000, loss: 0.10726018131266823, gradient norm: 0.30962361584842785\n",
      "Iteration: 5233000, loss: 0.1072595259098309, gradient norm: 0.5172613211333189\n",
      "Iteration: 5234000, loss: 0.10725922842515186, gradient norm: 0.3367113303307444\n",
      "Iteration: 5235000, loss: 0.10725894646487419, gradient norm: 0.15890460765036582\n",
      "Iteration: 5236000, loss: 0.10725852559406425, gradient norm: 0.3135603877276907\n",
      "Iteration: 5237000, loss: 0.1072579712049701, gradient norm: 0.19094382107495225\n",
      "Iteration: 5238000, loss: 0.1072576868786199, gradient norm: 0.08805225368187876\n",
      "Iteration: 5239000, loss: 0.10725744652598582, gradient norm: 0.03418939834648206\n",
      "Iteration: 5240000, loss: 0.10725695419235694, gradient norm: 0.12005932276028024\n",
      "Iteration: 5241000, loss: 0.10725661485109583, gradient norm: 0.5946157698360415\n",
      "Iteration: 5242000, loss: 0.1072562766783067, gradient norm: 0.11879154586213957\n",
      "Iteration: 5243000, loss: 0.10725576566648053, gradient norm: 0.21182834771438075\n",
      "Iteration: 5244000, loss: 0.10725542791722176, gradient norm: 0.11792358999183908\n",
      "Iteration: 5245000, loss: 0.10725512876602869, gradient norm: 0.08976823694171547\n",
      "Iteration: 5246000, loss: 0.10725460864918537, gradient norm: 0.5235869260558789\n",
      "Iteration: 5247000, loss: 0.10725427917371073, gradient norm: 0.09386568879980821\n",
      "Iteration: 5248000, loss: 0.10725408652422368, gradient norm: 0.34780502404870545\n",
      "Iteration: 5249000, loss: 0.10725343564860347, gradient norm: 0.8165104944477322\n",
      "Iteration: 5250000, loss: 0.10725315987965363, gradient norm: 0.3146780512140795\n",
      "Iteration: 5251000, loss: 0.10725280584723881, gradient norm: 0.25955770431886266\n",
      "Iteration: 5252000, loss: 0.10725235272210855, gradient norm: 0.1346705284379337\n",
      "Iteration: 5253000, loss: 0.10725199499195455, gradient norm: 0.023481317975755873\n",
      "Iteration: 5254000, loss: 0.10725165941873058, gradient norm: 0.07194825975380621\n",
      "Iteration: 5255000, loss: 0.10725129932915917, gradient norm: 0.15712713597531164\n",
      "Iteration: 5256000, loss: 0.1072508847060061, gradient norm: 0.14888539310414386\n",
      "Iteration: 5257000, loss: 0.10725041254333918, gradient norm: 0.03323285779000471\n",
      "Iteration: 5258000, loss: 0.10725019347108368, gradient norm: 0.47950611291163614\n",
      "Iteration: 5259000, loss: 0.10724960878568908, gradient norm: 0.16464209451388373\n",
      "Iteration: 5260000, loss: 0.10724941516517862, gradient norm: 0.35650754376725036\n",
      "Iteration: 5261000, loss: 0.1072491151819626, gradient norm: 0.11857983040138048\n",
      "Iteration: 5262000, loss: 0.10724848809827014, gradient norm: 0.09235057545750458\n",
      "Iteration: 5263000, loss: 0.10724834556872417, gradient norm: 0.09403812424905825\n",
      "Iteration: 5264000, loss: 0.10724772983456321, gradient norm: 0.309466873283058\n",
      "Iteration: 5265000, loss: 0.10724761534547121, gradient norm: 0.17046165002117147\n",
      "Iteration: 5266000, loss: 0.10724702975126618, gradient norm: 0.6421101631482994\n",
      "Iteration: 5267000, loss: 0.10724663398954286, gradient norm: 0.3277808509672303\n",
      "Iteration: 5268000, loss: 0.10724640501022248, gradient norm: 0.42936418700351686\n",
      "Iteration: 5269000, loss: 0.1072459137976553, gradient norm: 0.14226323367348723\n",
      "Iteration: 5270000, loss: 0.10724561908770214, gradient norm: 0.1408058898937129\n",
      "Iteration: 5271000, loss: 0.10724512862940888, gradient norm: 0.06827432241019063\n",
      "Iteration: 5272000, loss: 0.107244863927783, gradient norm: 0.06585945279660785\n",
      "Iteration: 5273000, loss: 0.10724458908431346, gradient norm: 0.16240802575614943\n",
      "Iteration: 5274000, loss: 0.10724391462885807, gradient norm: 0.0837150167994068\n",
      "Iteration: 5275000, loss: 0.10724367967094248, gradient norm: 0.023279345350587708\n",
      "Iteration: 5276000, loss: 0.10724335623980966, gradient norm: 0.4669514826279717\n",
      "Iteration: 5277000, loss: 0.1072430599316534, gradient norm: 0.18699876123474496\n",
      "Iteration: 5278000, loss: 0.10724264521446156, gradient norm: 0.11507256867427262\n",
      "Iteration: 5279000, loss: 0.10724207322068255, gradient norm: 0.3852496963520577\n",
      "Iteration: 5280000, loss: 0.10724181570250477, gradient norm: 0.13975254160038225\n",
      "Iteration: 5281000, loss: 0.10724140283110885, gradient norm: 0.13526366692409145\n",
      "Iteration: 5282000, loss: 0.10724095771925221, gradient norm: 0.17032989998904424\n",
      "Iteration: 5283000, loss: 0.10724077730604888, gradient norm: 0.19833150193306912\n",
      "Iteration: 5284000, loss: 0.1072404429755578, gradient norm: 0.33231951356511485\n",
      "Iteration: 5285000, loss: 0.10723981724830667, gradient norm: 0.16157531386778812\n",
      "Iteration: 5286000, loss: 0.10723953793717643, gradient norm: 0.18204219177439745\n",
      "Iteration: 5287000, loss: 0.10723923924739442, gradient norm: 0.05054400910627262\n",
      "Iteration: 5288000, loss: 0.10723872892421249, gradient norm: 0.24933846456753986\n",
      "Iteration: 5289000, loss: 0.1072384823003816, gradient norm: 0.4484987981837018\n",
      "Iteration: 5290000, loss: 0.10723804242329557, gradient norm: 0.025675569533912555\n",
      "Iteration: 5291000, loss: 0.10723773468048393, gradient norm: 0.11570886583676758\n",
      "Iteration: 5292000, loss: 0.10723722008991339, gradient norm: 0.12177898211066772\n",
      "Iteration: 5293000, loss: 0.10723694707023626, gradient norm: 0.056239242377182176\n",
      "Iteration: 5294000, loss: 0.10723654613432732, gradient norm: 0.09634450788462973\n",
      "Iteration: 5295000, loss: 0.10723615476267734, gradient norm: 0.5167819817806462\n",
      "Iteration: 5296000, loss: 0.10723580949269629, gradient norm: 0.06872511294646338\n",
      "Iteration: 5297000, loss: 0.10723542046199395, gradient norm: 0.13076191484318955\n",
      "Iteration: 5298000, loss: 0.10723510422247705, gradient norm: 0.4971425160334142\n",
      "Iteration: 5299000, loss: 0.10723462462776313, gradient norm: 0.038457485229610996\n",
      "Iteration: 5300000, loss: 0.10723438834291434, gradient norm: 0.6329138475868554\n",
      "Iteration: 5301000, loss: 0.10723400552287356, gradient norm: 0.5349075939085146\n",
      "Iteration: 5302000, loss: 0.10723337334423365, gradient norm: 0.11528657026696953\n",
      "Iteration: 5303000, loss: 0.10723325833188269, gradient norm: 0.12776378165178717\n",
      "Iteration: 5304000, loss: 0.10723277830467635, gradient norm: 0.1625734284728045\n",
      "Iteration: 5305000, loss: 0.10723242956798698, gradient norm: 0.04962313713790097\n",
      "Iteration: 5306000, loss: 0.10723216252091293, gradient norm: 0.1518583270165723\n",
      "Iteration: 5307000, loss: 0.10723165518577593, gradient norm: 0.09246343107163636\n",
      "Iteration: 5308000, loss: 0.10723128979618682, gradient norm: 0.129830460195446\n",
      "Iteration: 5309000, loss: 0.10723089893488214, gradient norm: 0.2530311278379852\n",
      "Iteration: 5310000, loss: 0.10723070178425025, gradient norm: 0.034396389736532994\n",
      "Iteration: 5311000, loss: 0.10723022146035244, gradient norm: 0.011866820921101322\n",
      "Iteration: 5312000, loss: 0.107229772425615, gradient norm: 0.17780978091772381\n",
      "Iteration: 5313000, loss: 0.10722946650338828, gradient norm: 0.28810412191792756\n",
      "Iteration: 5314000, loss: 0.10722895479333017, gradient norm: 0.2972963108063087\n",
      "Iteration: 5315000, loss: 0.10722887666431051, gradient norm: 0.07968083214045359\n",
      "Iteration: 5316000, loss: 0.1072282562250979, gradient norm: 0.11446265910682385\n",
      "Iteration: 5317000, loss: 0.10722808002579275, gradient norm: 0.06733592220127536\n",
      "Iteration: 5318000, loss: 0.10722763040302427, gradient norm: 0.1052482761556863\n",
      "Iteration: 5319000, loss: 0.10722735521377362, gradient norm: 0.023088948167931143\n",
      "Iteration: 5320000, loss: 0.10722675090364563, gradient norm: 0.07440910833044923\n",
      "Iteration: 5321000, loss: 0.10722645221510109, gradient norm: 0.12481734027313145\n",
      "Iteration: 5322000, loss: 0.1072261315724071, gradient norm: 0.5229739708428816\n",
      "Iteration: 5323000, loss: 0.10722579054380482, gradient norm: 0.10177467424039939\n",
      "Iteration: 5324000, loss: 0.10722548854064823, gradient norm: 0.37886300807715234\n",
      "Iteration: 5325000, loss: 0.10722500041927231, gradient norm: 0.4256879982830432\n",
      "Iteration: 5326000, loss: 0.10722449830735395, gradient norm: 0.6049112950051858\n",
      "Iteration: 5327000, loss: 0.10722436700972161, gradient norm: 0.33736325158734576\n",
      "Iteration: 5328000, loss: 0.10722384126521532, gradient norm: 0.009032872140300453\n",
      "Iteration: 5329000, loss: 0.1072235264659444, gradient norm: 0.1538486349409669\n",
      "Iteration: 5330000, loss: 0.10722327969031122, gradient norm: 0.2916533062743857\n",
      "Iteration: 5331000, loss: 0.10722272298588845, gradient norm: 0.03115420680934498\n",
      "Iteration: 5332000, loss: 0.10722242614933541, gradient norm: 0.07338231047347592\n",
      "Iteration: 5333000, loss: 0.10722217053858549, gradient norm: 0.331345114411467\n",
      "Iteration: 5334000, loss: 0.10722164660519928, gradient norm: 0.13139253478133261\n",
      "Iteration: 5335000, loss: 0.1072212129660268, gradient norm: 0.023532079630640603\n",
      "Iteration: 5336000, loss: 0.10722107023018206, gradient norm: 0.14352131095460371\n",
      "Iteration: 5337000, loss: 0.10722055821566276, gradient norm: 0.595342859522058\n",
      "Iteration: 5338000, loss: 0.1072201664761637, gradient norm: 0.11383168194062039\n",
      "Iteration: 5339000, loss: 0.1072199716535087, gradient norm: 0.4183554989384146\n",
      "Iteration: 5340000, loss: 0.10721927877978561, gradient norm: 0.025965355512283038\n",
      "Iteration: 5341000, loss: 0.10721924031183187, gradient norm: 0.11114994277169744\n",
      "Iteration: 5342000, loss: 0.10721872796227018, gradient norm: 0.0788863019478055\n",
      "Iteration: 5343000, loss: 0.10721847787482215, gradient norm: 0.12184677595122391\n",
      "Iteration: 5344000, loss: 0.10721790551803215, gradient norm: 0.07743239698959642\n",
      "Iteration: 5345000, loss: 0.10721760977653863, gradient norm: 0.17797384946063777\n",
      "Iteration: 5346000, loss: 0.10721727836548461, gradient norm: 0.25590556745275744\n",
      "Iteration: 5347000, loss: 0.10721683950933951, gradient norm: 0.3218133853027356\n",
      "Iteration: 5348000, loss: 0.10721662518363852, gradient norm: 0.17362516856603058\n",
      "Iteration: 5349000, loss: 0.10721609128372489, gradient norm: 0.08373496675561258\n",
      "Iteration: 5350000, loss: 0.10721582250275469, gradient norm: 0.056351406200554344\n",
      "Iteration: 5351000, loss: 0.1072155568723942, gradient norm: 0.1972939235167999\n",
      "Iteration: 5352000, loss: 0.10721499281966289, gradient norm: 0.04005542116259547\n",
      "Iteration: 5353000, loss: 0.10721471659553226, gradient norm: 0.5487816917865066\n",
      "Iteration: 5354000, loss: 0.10721422505025358, gradient norm: 0.13863765228472177\n",
      "Iteration: 5355000, loss: 0.10721406144226744, gradient norm: 0.11511207946696823\n",
      "Iteration: 5356000, loss: 0.10721362487364941, gradient norm: 0.30717904152190134\n",
      "Iteration: 5357000, loss: 0.10721319373879003, gradient norm: 0.23617926369226847\n",
      "Iteration: 5358000, loss: 0.10721282669071527, gradient norm: 0.7129916135420131\n",
      "Iteration: 5359000, loss: 0.10721253400206805, gradient norm: 0.046139881309272164\n",
      "Iteration: 5360000, loss: 0.1072121890512062, gradient norm: 0.05566284491924266\n",
      "Iteration: 5361000, loss: 0.10721178486591117, gradient norm: 0.19697189674886192\n",
      "Iteration: 5362000, loss: 0.10721140417280224, gradient norm: 0.12995495505898824\n",
      "Iteration: 5363000, loss: 0.107211022953832, gradient norm: 0.08949253687201751\n",
      "Iteration: 5364000, loss: 0.10721066260217577, gradient norm: 0.14204925177255948\n",
      "Iteration: 5365000, loss: 0.1072102846986994, gradient norm: 0.023306739918294517\n",
      "Iteration: 5366000, loss: 0.10721000041363861, gradient norm: 0.4337661070980834\n",
      "Iteration: 5367000, loss: 0.10720960604615673, gradient norm: 0.0843512416661241\n",
      "Iteration: 5368000, loss: 0.1072091751213166, gradient norm: 0.01889800552323963\n",
      "Iteration: 5369000, loss: 0.10720895712564192, gradient norm: 0.5500566428561412\n",
      "Iteration: 5370000, loss: 0.10720850018242203, gradient norm: 0.18336315473671666\n",
      "Iteration: 5371000, loss: 0.10720791276302365, gradient norm: 0.2848203784569965\n",
      "Iteration: 5372000, loss: 0.10720793840587325, gradient norm: 0.6139245424737524\n",
      "Iteration: 5373000, loss: 0.10720732142783493, gradient norm: 0.30984910310973807\n",
      "Iteration: 5374000, loss: 0.10720695719125495, gradient norm: 0.294747421105231\n",
      "Iteration: 5375000, loss: 0.10720669925191002, gradient norm: 0.008475499014241272\n",
      "Iteration: 5376000, loss: 0.10720636338319721, gradient norm: 0.08591905133593988\n",
      "Iteration: 5377000, loss: 0.10720580657562323, gradient norm: 0.3230840964008276\n",
      "Iteration: 5378000, loss: 0.1072056849778514, gradient norm: 0.07367921774209336\n",
      "Iteration: 5379000, loss: 0.10720517524446956, gradient norm: 0.13914015758436232\n",
      "Iteration: 5380000, loss: 0.1072048360553125, gradient norm: 0.12588110302362582\n",
      "Iteration: 5381000, loss: 0.10720445247327211, gradient norm: 0.1818591552299588\n",
      "Iteration: 5382000, loss: 0.10720410221490315, gradient norm: 0.3642566744822923\n",
      "Iteration: 5383000, loss: 0.10720381537046064, gradient norm: 0.2583984293331509\n",
      "Iteration: 5384000, loss: 0.10720342041776675, gradient norm: 0.2844603692884471\n",
      "Iteration: 5385000, loss: 0.10720301289483487, gradient norm: 0.3738383796620222\n",
      "Iteration: 5386000, loss: 0.10720264152461809, gradient norm: 0.10165964255680107\n",
      "Iteration: 5387000, loss: 0.10720230549629818, gradient norm: 0.1470343223402681\n",
      "Iteration: 5388000, loss: 0.10720196978988072, gradient norm: 0.08039537911418419\n",
      "Iteration: 5389000, loss: 0.10720170054698235, gradient norm: 0.01612023657534724\n",
      "Iteration: 5390000, loss: 0.1072011612578216, gradient norm: 0.16000976062224412\n",
      "Iteration: 5391000, loss: 0.10720099920754055, gradient norm: 0.5719568992143973\n",
      "Iteration: 5392000, loss: 0.10720041649763039, gradient norm: 0.06617588406932715\n",
      "Iteration: 5393000, loss: 0.10720005563909407, gradient norm: 0.38360866321900894\n",
      "Iteration: 5394000, loss: 0.107199763745056, gradient norm: 0.06504446715777186\n",
      "Iteration: 5395000, loss: 0.10719942998427079, gradient norm: 0.27751434540460446\n",
      "Iteration: 5396000, loss: 0.10719908988655655, gradient norm: 0.07569872601108082\n",
      "Iteration: 5397000, loss: 0.10719869822568953, gradient norm: 0.12546723863178763\n",
      "Iteration: 5398000, loss: 0.10719822200602858, gradient norm: 0.6663149744876625\n",
      "Iteration: 5399000, loss: 0.10719802441940461, gradient norm: 0.5999890052915987\n",
      "Iteration: 5400000, loss: 0.10719771276365202, gradient norm: 0.19136257128181186\n",
      "Iteration: 5401000, loss: 0.10719705028342835, gradient norm: 0.5949279131964526\n",
      "Iteration: 5402000, loss: 0.10719697772125676, gradient norm: 0.009991584039391217\n",
      "Iteration: 5403000, loss: 0.10719649321612994, gradient norm: 0.6249213864650343\n",
      "Iteration: 5404000, loss: 0.10719622362283332, gradient norm: 0.05068489281134702\n",
      "Iteration: 5405000, loss: 0.10719583375025267, gradient norm: 0.45397273301813573\n",
      "Iteration: 5406000, loss: 0.1071954999393546, gradient norm: 0.5169228824932128\n",
      "Iteration: 5407000, loss: 0.10719513557420561, gradient norm: 0.254819378247754\n",
      "Iteration: 5408000, loss: 0.10719464617431482, gradient norm: 0.03684913035867162\n",
      "Iteration: 5409000, loss: 0.10719423995110386, gradient norm: 0.3629366805309641\n",
      "Iteration: 5410000, loss: 0.10719414407087516, gradient norm: 0.380477484054084\n",
      "Iteration: 5411000, loss: 0.10719370960066732, gradient norm: 0.3724687266805338\n",
      "Iteration: 5412000, loss: 0.10719314430093484, gradient norm: 0.513981738519429\n",
      "Iteration: 5413000, loss: 0.10719298491588374, gradient norm: 0.18941178849146345\n",
      "Iteration: 5414000, loss: 0.10719247259201713, gradient norm: 0.2736764559301045\n",
      "Iteration: 5415000, loss: 0.10719241441184976, gradient norm: 0.10012016663239084\n",
      "Iteration: 5416000, loss: 0.10719180716796955, gradient norm: 0.088031296448763\n",
      "Iteration: 5417000, loss: 0.10719149939790727, gradient norm: 0.32237392494785455\n",
      "Iteration: 5418000, loss: 0.1071911726910727, gradient norm: 0.8434361478547444\n",
      "Iteration: 5419000, loss: 0.10719061420755856, gradient norm: 0.07299444137655825\n",
      "Iteration: 5420000, loss: 0.10719050983810935, gradient norm: 0.1146586374293996\n",
      "Iteration: 5421000, loss: 0.10719008846855714, gradient norm: 0.30671965170483356\n",
      "Iteration: 5422000, loss: 0.10718964583351025, gradient norm: 0.2640829096825672\n",
      "Iteration: 5423000, loss: 0.10718933752672131, gradient norm: 0.08744793634197799\n",
      "Iteration: 5424000, loss: 0.10718906329978449, gradient norm: 0.029243153621804706\n",
      "Iteration: 5425000, loss: 0.10718855454224133, gradient norm: 0.7074111143201065\n",
      "Iteration: 5426000, loss: 0.10718834427341255, gradient norm: 0.1486241845711492\n",
      "Iteration: 5427000, loss: 0.10718794570017755, gradient norm: 0.123836742665644\n",
      "Iteration: 5428000, loss: 0.10718748509868045, gradient norm: 0.08652688619958736\n",
      "Iteration: 5429000, loss: 0.10718732076112587, gradient norm: 0.4313863978640438\n",
      "Iteration: 5430000, loss: 0.10718686222683002, gradient norm: 0.4102528787948246\n",
      "Iteration: 5431000, loss: 0.10718634813594355, gradient norm: 0.10095903557473922\n",
      "Iteration: 5432000, loss: 0.10718611514942876, gradient norm: 0.1082111502587865\n",
      "Iteration: 5433000, loss: 0.10718590871885741, gradient norm: 0.37125013834647547\n",
      "Iteration: 5434000, loss: 0.10718547765109417, gradient norm: 0.3562896069853397\n",
      "Iteration: 5435000, loss: 0.10718494751300514, gradient norm: 0.7262315402355382\n",
      "Iteration: 5436000, loss: 0.1071846867805682, gradient norm: 0.05733745800400379\n",
      "Iteration: 5437000, loss: 0.10718437811547077, gradient norm: 0.20934048745254644\n",
      "Iteration: 5438000, loss: 0.10718400699526989, gradient norm: 0.5726989498779355\n",
      "Iteration: 5439000, loss: 0.10718359484972083, gradient norm: 0.2076944851201473\n",
      "Iteration: 5440000, loss: 0.10718324485082466, gradient norm: 0.4976336613449958\n",
      "Iteration: 5441000, loss: 0.10718303215349792, gradient norm: 0.04685079195128106\n",
      "Iteration: 5442000, loss: 0.10718255078174789, gradient norm: 0.17283220173019662\n",
      "Iteration: 5443000, loss: 0.10718221260468934, gradient norm: 0.6861343233857151\n",
      "Iteration: 5444000, loss: 0.10718187518702771, gradient norm: 0.022184135785150313\n",
      "Iteration: 5445000, loss: 0.10718143865951316, gradient norm: 0.40329714744929\n",
      "Iteration: 5446000, loss: 0.10718135416743266, gradient norm: 0.5097702377089695\n",
      "Iteration: 5447000, loss: 0.10718086950607035, gradient norm: 0.24846796899870432\n",
      "Iteration: 5448000, loss: 0.10718034308125565, gradient norm: 0.7190801539423413\n",
      "Iteration: 5449000, loss: 0.10718007408984713, gradient norm: 0.7127101042388336\n",
      "Iteration: 5450000, loss: 0.10717970307940457, gradient norm: 0.040681388975127714\n",
      "Iteration: 5451000, loss: 0.10717940741844284, gradient norm: 0.03018223618605279\n",
      "Iteration: 5452000, loss: 0.10717898235872766, gradient norm: 0.10327590080698697\n",
      "Iteration: 5453000, loss: 0.10717874756553314, gradient norm: 0.24067478617576105\n",
      "Iteration: 5454000, loss: 0.1071782555550258, gradient norm: 0.5046867412828544\n",
      "Iteration: 5455000, loss: 0.10717790709647886, gradient norm: 0.3881240886752509\n",
      "Iteration: 5456000, loss: 0.10717779381019855, gradient norm: 0.6563088098006398\n",
      "Iteration: 5457000, loss: 0.10717718838186806, gradient norm: 0.025185824688981437\n",
      "Iteration: 5458000, loss: 0.10717703798963236, gradient norm: 0.09470513838230736\n",
      "Iteration: 5459000, loss: 0.10717649596317556, gradient norm: 0.15243418729128913\n",
      "Iteration: 5460000, loss: 0.10717612054302532, gradient norm: 0.007993965023847329\n",
      "Iteration: 5461000, loss: 0.10717599965123116, gradient norm: 0.4442848532396978\n",
      "Iteration: 5462000, loss: 0.10717537352783071, gradient norm: 0.06477650599676303\n",
      "Iteration: 5463000, loss: 0.10717525288402328, gradient norm: 0.051086352046310325\n",
      "Iteration: 5464000, loss: 0.10717476479823845, gradient norm: 0.02368960031309032\n",
      "Iteration: 5465000, loss: 0.10717459873219405, gradient norm: 0.07865796637223782\n",
      "Iteration: 5466000, loss: 0.10717389108399698, gradient norm: 0.04831338681403207\n",
      "Iteration: 5467000, loss: 0.10717378288221464, gradient norm: 0.31023574214906524\n",
      "Iteration: 5468000, loss: 0.10717344137915363, gradient norm: 0.02861306112572283\n",
      "Iteration: 5469000, loss: 0.10717300406697776, gradient norm: 0.2136738737813095\n",
      "Iteration: 5470000, loss: 0.10717265680065123, gradient norm: 0.1374576345574061\n",
      "Iteration: 5471000, loss: 0.10717222061102649, gradient norm: 0.09395459417321937\n",
      "Iteration: 5472000, loss: 0.10717211106669355, gradient norm: 0.06722640996704783\n",
      "Iteration: 5473000, loss: 0.10717155744340147, gradient norm: 0.3219493309765079\n",
      "Iteration: 5474000, loss: 0.107171257754658, gradient norm: 0.5862607153017895\n",
      "Iteration: 5475000, loss: 0.10717090045345615, gradient norm: 0.06887613828316394\n",
      "Iteration: 5476000, loss: 0.10717056966281728, gradient norm: 0.0845310962174244\n",
      "Iteration: 5477000, loss: 0.10717009851601099, gradient norm: 0.2898122447066279\n",
      "Iteration: 5478000, loss: 0.10716987531598517, gradient norm: 0.37645731364290963\n",
      "Iteration: 5479000, loss: 0.1071695883477212, gradient norm: 0.48154124478010113\n",
      "Iteration: 5480000, loss: 0.10716913470200273, gradient norm: 0.4283728126236226\n",
      "Iteration: 5481000, loss: 0.10716882810221094, gradient norm: 0.08119051871315165\n",
      "Iteration: 5482000, loss: 0.10716845305148072, gradient norm: 0.6239432740314261\n",
      "Iteration: 5483000, loss: 0.107168242545644, gradient norm: 0.0126848581883538\n",
      "Iteration: 5484000, loss: 0.10716765263495459, gradient norm: 0.32096793166387283\n",
      "Iteration: 5485000, loss: 0.10716746126309502, gradient norm: 0.11398920401935181\n",
      "Iteration: 5486000, loss: 0.10716700094472993, gradient norm: 0.3458983114274958\n",
      "Iteration: 5487000, loss: 0.10716679955065095, gradient norm: 0.07830833412136314\n",
      "Iteration: 5488000, loss: 0.10716643454940467, gradient norm: 0.08850563710382515\n",
      "Iteration: 5489000, loss: 0.10716595165952605, gradient norm: 0.1078129106710365\n",
      "Iteration: 5490000, loss: 0.10716566596348813, gradient norm: 0.2518025711252743\n",
      "Iteration: 5491000, loss: 0.1071653061999458, gradient norm: 0.5467834119070957\n",
      "Iteration: 5492000, loss: 0.10716493371409883, gradient norm: 0.05077647960286386\n",
      "Iteration: 5493000, loss: 0.10716459196441276, gradient norm: 0.20043597045142159\n",
      "Iteration: 5494000, loss: 0.10716434243376373, gradient norm: 0.2351938917247004\n",
      "Iteration: 5495000, loss: 0.10716396203678934, gradient norm: 0.12570988084398688\n",
      "Iteration: 5496000, loss: 0.10716342843792324, gradient norm: 0.11010602264145407\n",
      "Iteration: 5497000, loss: 0.10716321102111127, gradient norm: 0.10082901693634376\n",
      "Iteration: 5498000, loss: 0.10716286472433782, gradient norm: 0.200905860425674\n",
      "Iteration: 5499000, loss: 0.1071624749008138, gradient norm: 0.06612164462688125\n",
      "Iteration: 5500000, loss: 0.107162110286992, gradient norm: 0.17598580916680495\n",
      "Iteration: 5501000, loss: 0.10716183437167555, gradient norm: 0.19704530469583834\n",
      "Iteration: 5502000, loss: 0.10716148731903136, gradient norm: 0.18709585360419467\n",
      "Iteration: 5503000, loss: 0.10716102493518301, gradient norm: 0.09480397188434277\n",
      "Iteration: 5504000, loss: 0.10716089426035824, gradient norm: 0.11310884279842345\n",
      "Iteration: 5505000, loss: 0.10716050584071007, gradient norm: 0.30048296431401067\n",
      "Iteration: 5506000, loss: 0.10715993069242137, gradient norm: 0.1651450712809342\n",
      "Iteration: 5507000, loss: 0.10715970044778496, gradient norm: 0.22747218982517153\n",
      "Iteration: 5508000, loss: 0.10715936334230114, gradient norm: 0.06890466108113345\n",
      "Iteration: 5509000, loss: 0.10715907693700095, gradient norm: 0.09972103785797383\n",
      "Iteration: 5510000, loss: 0.10715873352729713, gradient norm: 0.18779390304495047\n",
      "Iteration: 5511000, loss: 0.10715826354856393, gradient norm: 0.2847189311022718\n",
      "Iteration: 5512000, loss: 0.10715816886423976, gradient norm: 0.06951301632534727\n",
      "Iteration: 5513000, loss: 0.10715756710284494, gradient norm: 0.06517777102945498\n",
      "Iteration: 5514000, loss: 0.1071572878340465, gradient norm: 0.13223736417348914\n",
      "Iteration: 5515000, loss: 0.10715690729974361, gradient norm: 0.1602327884432666\n",
      "Iteration: 5516000, loss: 0.10715658789816525, gradient norm: 0.21417696784143184\n",
      "Iteration: 5517000, loss: 0.1071564603202747, gradient norm: 0.7514985238236562\n",
      "Iteration: 5518000, loss: 0.1071557799019003, gradient norm: 0.06441855252353508\n",
      "Iteration: 5519000, loss: 0.10715560698277599, gradient norm: 0.255254572969382\n",
      "Iteration: 5520000, loss: 0.1071552731890211, gradient norm: 0.29887560661144014\n",
      "Iteration: 5521000, loss: 0.10715479926639773, gradient norm: 0.7713638859068539\n",
      "Iteration: 5522000, loss: 0.10715442952645009, gradient norm: 0.17510967593269675\n",
      "Iteration: 5523000, loss: 0.10715422566025445, gradient norm: 0.09018512166652434\n",
      "Iteration: 5524000, loss: 0.10715395779931809, gradient norm: 0.11695433562662921\n",
      "Iteration: 5525000, loss: 0.10715345191272828, gradient norm: 0.12627424921561156\n",
      "Iteration: 5526000, loss: 0.10715312741163055, gradient norm: 0.042622091985223334\n",
      "Iteration: 5527000, loss: 0.10715280697294408, gradient norm: 0.15002863247799392\n",
      "Iteration: 5528000, loss: 0.1071525851410333, gradient norm: 0.27630021890477613\n",
      "Iteration: 5529000, loss: 0.1071520866652662, gradient norm: 0.08726202092571123\n",
      "Iteration: 5530000, loss: 0.10715167626056664, gradient norm: 0.13341023908835487\n",
      "Iteration: 5531000, loss: 0.10715139363830864, gradient norm: 0.08890916054598134\n",
      "Iteration: 5532000, loss: 0.1071511489360536, gradient norm: 0.11820536619048641\n",
      "Iteration: 5533000, loss: 0.10715075439245024, gradient norm: 0.12574699973893302\n",
      "Iteration: 5534000, loss: 0.107150323435726, gradient norm: 0.15034811905394552\n",
      "Iteration: 5535000, loss: 0.10714996137097255, gradient norm: 0.7354205606932412\n",
      "Iteration: 5536000, loss: 0.10714981422314873, gradient norm: 0.30559560653155093\n",
      "Iteration: 5537000, loss: 0.1071494479236199, gradient norm: 0.45314316579719144\n",
      "Iteration: 5538000, loss: 0.10714883870753168, gradient norm: 0.06720658812740551\n",
      "Iteration: 5539000, loss: 0.10714867806073246, gradient norm: 0.021037682323868385\n",
      "Iteration: 5540000, loss: 0.10714832493840203, gradient norm: 0.19285261096275455\n",
      "Iteration: 5541000, loss: 0.10714804955510342, gradient norm: 0.21415505142979266\n",
      "Iteration: 5542000, loss: 0.10714760563747487, gradient norm: 0.11253893057901387\n",
      "Iteration: 5543000, loss: 0.10714727838736186, gradient norm: 0.012530215494738175\n",
      "Iteration: 5544000, loss: 0.10714699438469358, gradient norm: 0.715193212432177\n",
      "Iteration: 5545000, loss: 0.10714658876942161, gradient norm: 0.4354532516326942\n",
      "Iteration: 5546000, loss: 0.10714632660231284, gradient norm: 0.5163844096694772\n",
      "Iteration: 5547000, loss: 0.10714582300794996, gradient norm: 0.1997061251096198\n",
      "Iteration: 5548000, loss: 0.10714568777158218, gradient norm: 0.15245629731991037\n",
      "Iteration: 5549000, loss: 0.10714519761019826, gradient norm: 0.17389819333444642\n",
      "Iteration: 5550000, loss: 0.10714484132955941, gradient norm: 0.2908185750124957\n",
      "Iteration: 5551000, loss: 0.10714452257076398, gradient norm: 0.6003727076079772\n",
      "Iteration: 5552000, loss: 0.10714421000457057, gradient norm: 0.026369571386666752\n",
      "Iteration: 5553000, loss: 0.10714394352775841, gradient norm: 0.007065986521972637\n",
      "Iteration: 5554000, loss: 0.10714353780167177, gradient norm: 0.1445396852977743\n",
      "Iteration: 5555000, loss: 0.10714319929855279, gradient norm: 0.43838682615979957\n",
      "Iteration: 5556000, loss: 0.10714285583421793, gradient norm: 0.2504566910829018\n",
      "Iteration: 5557000, loss: 0.10714249371467839, gradient norm: 0.2009211082441819\n",
      "Iteration: 5558000, loss: 0.10714209167467878, gradient norm: 0.6247613190746217\n",
      "Iteration: 5559000, loss: 0.10714181407573341, gradient norm: 0.19652733375097287\n",
      "Iteration: 5560000, loss: 0.10714148928654277, gradient norm: 0.195126071933021\n",
      "Iteration: 5561000, loss: 0.10714114574695213, gradient norm: 0.47083336354432137\n",
      "Iteration: 5562000, loss: 0.10714075882804909, gradient norm: 0.1492840624872368\n",
      "Iteration: 5563000, loss: 0.10714046927785453, gradient norm: 0.04594513954716195\n",
      "Iteration: 5564000, loss: 0.10714014168743809, gradient norm: 0.05343686632132686\n",
      "Iteration: 5565000, loss: 0.10713968400068495, gradient norm: 0.07343568000144816\n",
      "Iteration: 5566000, loss: 0.10713929121473416, gradient norm: 0.3179764546631807\n",
      "Iteration: 5567000, loss: 0.10713916853957475, gradient norm: 0.27753144942438923\n",
      "Iteration: 5568000, loss: 0.1071387500989239, gradient norm: 0.22301417471343918\n",
      "Iteration: 5569000, loss: 0.10713845762762739, gradient norm: 0.2776686016910776\n",
      "Iteration: 5570000, loss: 0.10713810175306822, gradient norm: 0.13233432264414527\n",
      "Iteration: 5571000, loss: 0.10713753237751314, gradient norm: 0.06571066147201526\n",
      "Iteration: 5572000, loss: 0.10713748888345254, gradient norm: 0.08219038018144707\n",
      "Iteration: 5573000, loss: 0.1071369462487524, gradient norm: 0.27442846601039195\n",
      "Iteration: 5574000, loss: 0.10713680780582445, gradient norm: 0.07575128086767897\n",
      "Iteration: 5575000, loss: 0.10713631731991737, gradient norm: 0.10160989643262\n",
      "Iteration: 5576000, loss: 0.10713591636212404, gradient norm: 0.03429911133952444\n",
      "Iteration: 5577000, loss: 0.10713569082700118, gradient norm: 0.18918353456202472\n",
      "Iteration: 5578000, loss: 0.10713550315491392, gradient norm: 0.2040129349198006\n",
      "Iteration: 5579000, loss: 0.1071349411876661, gradient norm: 0.5632814440197886\n",
      "Iteration: 5580000, loss: 0.10713464451596232, gradient norm: 0.10165308818990101\n",
      "Iteration: 5581000, loss: 0.10713427486080246, gradient norm: 0.11389009369842985\n",
      "Iteration: 5582000, loss: 0.10713403745876376, gradient norm: 0.28291095312316306\n",
      "Iteration: 5583000, loss: 0.10713354564567455, gradient norm: 0.16556053304577667\n",
      "Iteration: 5584000, loss: 0.10713334426985086, gradient norm: 0.10740697319723998\n",
      "Iteration: 5585000, loss: 0.10713296418040522, gradient norm: 0.15199471994963584\n",
      "Iteration: 5586000, loss: 0.10713272818801117, gradient norm: 0.165500514902161\n",
      "Iteration: 5587000, loss: 0.10713218996904811, gradient norm: 0.16565796184926557\n",
      "Iteration: 5588000, loss: 0.10713202189697822, gradient norm: 0.32062811960277005\n",
      "Iteration: 5589000, loss: 0.10713155948399551, gradient norm: 0.31876483271267303\n",
      "Iteration: 5590000, loss: 0.10713130019757607, gradient norm: 0.09966202789548105\n",
      "Iteration: 5591000, loss: 0.10713091113028361, gradient norm: 0.02875954865704322\n",
      "Iteration: 5592000, loss: 0.10713060704734806, gradient norm: 0.20225752269637026\n",
      "Iteration: 5593000, loss: 0.10713028938108737, gradient norm: 0.11795846454301585\n",
      "Iteration: 5594000, loss: 0.10712998563641861, gradient norm: 0.31707981294492316\n",
      "Iteration: 5595000, loss: 0.10712964449033961, gradient norm: 0.9197658931154614\n",
      "Iteration: 5596000, loss: 0.10712915660986096, gradient norm: 0.16144324808049876\n",
      "Iteration: 5597000, loss: 0.10712890399033934, gradient norm: 0.17071868483829264\n",
      "Iteration: 5598000, loss: 0.10712868848033534, gradient norm: 0.4180686757292961\n",
      "Iteration: 5599000, loss: 0.10712820176001253, gradient norm: 0.3013461612753971\n",
      "Iteration: 5600000, loss: 0.10712779038711068, gradient norm: 0.19943509924796188\n",
      "Iteration: 5601000, loss: 0.10712763912088588, gradient norm: 0.3724523195428324\n",
      "Iteration: 5602000, loss: 0.1071271871197841, gradient norm: 0.5761664806936719\n",
      "Iteration: 5603000, loss: 0.10712689943802303, gradient norm: 0.19497271996676674\n",
      "Iteration: 5604000, loss: 0.10712651763441058, gradient norm: 0.1981606033659017\n",
      "Iteration: 5605000, loss: 0.10712633586821468, gradient norm: 0.08934723170393716\n",
      "Iteration: 5606000, loss: 0.10712577607886252, gradient norm: 0.05874386577893529\n",
      "Iteration: 5607000, loss: 0.10712554853823485, gradient norm: 0.18448816404597143\n",
      "Iteration: 5608000, loss: 0.10712525807784318, gradient norm: 0.1865130602004859\n",
      "Iteration: 5609000, loss: 0.10712489749595491, gradient norm: 0.35946622409898826\n",
      "Iteration: 5610000, loss: 0.10712458926153165, gradient norm: 0.8247481073854952\n",
      "Iteration: 5611000, loss: 0.10712415709315948, gradient norm: 0.6429564587430101\n",
      "Iteration: 5612000, loss: 0.10712385570924467, gradient norm: 0.4951700120981508\n",
      "Iteration: 5613000, loss: 0.10712361196379629, gradient norm: 0.8800581988839214\n",
      "Iteration: 5614000, loss: 0.10712312881593689, gradient norm: 0.3955125491624774\n",
      "Iteration: 5615000, loss: 0.10712282524383823, gradient norm: 0.09294558827456614\n",
      "Iteration: 5616000, loss: 0.10712250477860281, gradient norm: 0.40026749056878846\n",
      "Iteration: 5617000, loss: 0.10712225651864662, gradient norm: 0.271148900042762\n",
      "Iteration: 5618000, loss: 0.10712183519783317, gradient norm: 0.3940698071319619\n",
      "Iteration: 5619000, loss: 0.10712141407563401, gradient norm: 0.018730895287076575\n",
      "Iteration: 5620000, loss: 0.10712130081984397, gradient norm: 0.14533113843478834\n",
      "Iteration: 5621000, loss: 0.10712078402920501, gradient norm: 0.06912406601094037\n",
      "Iteration: 5622000, loss: 0.10712072394589199, gradient norm: 0.15963872501431933\n",
      "Iteration: 5623000, loss: 0.1071199837072851, gradient norm: 0.26878145569534834\n",
      "Iteration: 5624000, loss: 0.1071198900698335, gradient norm: 0.1967468691193962\n",
      "Iteration: 5625000, loss: 0.10711951638173764, gradient norm: 0.11874147103639442\n",
      "Iteration: 5626000, loss: 0.10711916267061326, gradient norm: 0.0845335654328357\n",
      "Iteration: 5627000, loss: 0.10711878118237103, gradient norm: 0.24411749822300471\n",
      "Iteration: 5628000, loss: 0.10711855004449151, gradient norm: 0.10981999800276646\n",
      "Iteration: 5629000, loss: 0.1071181862888038, gradient norm: 0.5401685402599425\n",
      "Iteration: 5630000, loss: 0.10711783235239175, gradient norm: 0.2284836244961574\n",
      "Iteration: 5631000, loss: 0.10711740041944315, gradient norm: 0.1852550812472957\n",
      "Iteration: 5632000, loss: 0.10711727334729551, gradient norm: 0.22509922881376074\n",
      "Iteration: 5633000, loss: 0.10711687696772253, gradient norm: 0.3210366904913777\n",
      "Iteration: 5634000, loss: 0.10711648867050985, gradient norm: 0.11618919483477973\n",
      "Iteration: 5635000, loss: 0.10711616991975018, gradient norm: 0.02881296538806008\n",
      "Iteration: 5636000, loss: 0.10711579728770448, gradient norm: 0.09091568855322346\n",
      "Iteration: 5637000, loss: 0.1071156046208763, gradient norm: 0.12401832049104294\n",
      "Iteration: 5638000, loss: 0.10711516881512935, gradient norm: 0.12172614837788018\n",
      "Iteration: 5639000, loss: 0.10711476360475214, gradient norm: 0.5642800371195521\n",
      "Iteration: 5640000, loss: 0.10711451156628582, gradient norm: 0.3003347012324114\n",
      "Iteration: 5641000, loss: 0.10711425647420715, gradient norm: 0.27034739488426746\n",
      "Iteration: 5642000, loss: 0.10711369848814703, gradient norm: 0.2101420921676137\n",
      "Iteration: 5643000, loss: 0.10711374246204963, gradient norm: 0.5628048649297277\n",
      "Iteration: 5644000, loss: 0.10711310923152108, gradient norm: 0.33783668805575856\n",
      "Iteration: 5645000, loss: 0.10711272287529544, gradient norm: 0.3722763105641153\n",
      "Iteration: 5646000, loss: 0.10711268339354592, gradient norm: 0.013798571766986878\n",
      "Iteration: 5647000, loss: 0.10711200529915064, gradient norm: 0.0946638740308497\n",
      "Iteration: 5648000, loss: 0.10711193170262147, gradient norm: 0.11924967629120395\n",
      "Iteration: 5649000, loss: 0.10711142628537941, gradient norm: 0.15491795764708818\n",
      "Iteration: 5650000, loss: 0.1071113963458594, gradient norm: 0.1992320519044615\n",
      "Iteration: 5651000, loss: 0.10711072960157896, gradient norm: 0.09900097282582315\n",
      "Iteration: 5652000, loss: 0.10711063543773215, gradient norm: 0.34734240984065706\n",
      "Iteration: 5653000, loss: 0.10711020988116553, gradient norm: 0.8060084032956186\n",
      "Iteration: 5654000, loss: 0.10710979370178779, gradient norm: 0.05010855260108296\n",
      "Iteration: 5655000, loss: 0.10710962945828832, gradient norm: 0.6303860575354675\n",
      "Iteration: 5656000, loss: 0.1071090886209137, gradient norm: 0.13460112724595577\n",
      "Iteration: 5657000, loss: 0.10710889653070142, gradient norm: 0.2606099984050056\n",
      "Iteration: 5658000, loss: 0.10710860089837523, gradient norm: 0.48243728065133107\n",
      "Iteration: 5659000, loss: 0.1071081287589255, gradient norm: 0.14933699270703388\n",
      "Iteration: 5660000, loss: 0.10710795782846241, gradient norm: 0.2070438040561164\n",
      "Iteration: 5661000, loss: 0.10710742472228048, gradient norm: 0.5194309567575137\n",
      "Iteration: 5662000, loss: 0.10710722722368637, gradient norm: 0.6116409556500624\n",
      "Iteration: 5663000, loss: 0.10710693231667957, gradient norm: 0.2595764820424784\n",
      "Iteration: 5664000, loss: 0.10710656783369922, gradient norm: 0.3035834115568847\n",
      "Iteration: 5665000, loss: 0.1071061763317741, gradient norm: 0.4583349562923894\n",
      "Iteration: 5666000, loss: 0.10710594982904863, gradient norm: 0.13118987860933973\n",
      "Iteration: 5667000, loss: 0.10710564758743064, gradient norm: 0.3630740581092736\n",
      "Iteration: 5668000, loss: 0.10710526689488144, gradient norm: 0.3820354212107612\n",
      "Iteration: 5669000, loss: 0.10710480228753692, gradient norm: 0.09284683437720465\n",
      "Iteration: 5670000, loss: 0.10710455185434466, gradient norm: 0.24093888748651773\n",
      "Iteration: 5671000, loss: 0.10710421922720323, gradient norm: 0.689385312707142\n",
      "Iteration: 5672000, loss: 0.10710412669995834, gradient norm: 0.048205807087212026\n",
      "Iteration: 5673000, loss: 0.10710346741883425, gradient norm: 0.6782709449600087\n",
      "Iteration: 5674000, loss: 0.10710322932655134, gradient norm: 0.1468848341621423\n",
      "Iteration: 5675000, loss: 0.10710301527622902, gradient norm: 0.2841671962572848\n",
      "Iteration: 5676000, loss: 0.10710241201938817, gradient norm: 0.305487985331322\n",
      "Iteration: 5677000, loss: 0.10710226477096009, gradient norm: 0.08235519895932633\n",
      "Iteration: 5678000, loss: 0.10710205675414472, gradient norm: 0.7882116872389047\n",
      "Iteration: 5679000, loss: 0.10710171199926562, gradient norm: 0.4106010178945253\n",
      "Iteration: 5680000, loss: 0.10710127937104243, gradient norm: 0.37107788949747894\n",
      "Iteration: 5681000, loss: 0.10710090275053849, gradient norm: 0.10306245844832446\n",
      "Iteration: 5682000, loss: 0.10710056759738576, gradient norm: 0.27650680757832397\n",
      "Iteration: 5683000, loss: 0.10710045560206838, gradient norm: 0.15179391822278035\n",
      "Iteration: 5684000, loss: 0.1070999577894648, gradient norm: 0.07010836609486452\n",
      "Iteration: 5685000, loss: 0.10709959839617768, gradient norm: 0.04257949357174662\n",
      "Iteration: 5686000, loss: 0.1070994684110879, gradient norm: 0.04219353624068565\n",
      "Iteration: 5687000, loss: 0.10709889622372537, gradient norm: 0.21716491768462967\n",
      "Iteration: 5688000, loss: 0.10709873095755046, gradient norm: 0.24393587933218241\n",
      "Iteration: 5689000, loss: 0.10709830786015476, gradient norm: 0.20475664044283376\n",
      "Iteration: 5690000, loss: 0.10709810348053532, gradient norm: 0.393079594042645\n",
      "Iteration: 5691000, loss: 0.10709762909336364, gradient norm: 0.0936616578967516\n",
      "Iteration: 5692000, loss: 0.10709734300624583, gradient norm: 0.08595742217581279\n",
      "Iteration: 5693000, loss: 0.10709698076678842, gradient norm: 0.08682894664921084\n",
      "Iteration: 5694000, loss: 0.10709682816711881, gradient norm: 0.08660426454432883\n",
      "Iteration: 5695000, loss: 0.10709628358872092, gradient norm: 0.05973851949199264\n",
      "Iteration: 5696000, loss: 0.10709607175235349, gradient norm: 0.8964007320476369\n",
      "Iteration: 5697000, loss: 0.10709577771344415, gradient norm: 0.04634850016267221\n",
      "Iteration: 5698000, loss: 0.10709535660631711, gradient norm: 0.09090060690184754\n",
      "Iteration: 5699000, loss: 0.10709498597451354, gradient norm: 0.5325214760016476\n",
      "Iteration: 5700000, loss: 0.10709470418531898, gradient norm: 0.047977148413067855\n",
      "Iteration: 5701000, loss: 0.10709463994861956, gradient norm: 0.11111482958023941\n",
      "Iteration: 5702000, loss: 0.10709387305008031, gradient norm: 0.1732495507696292\n",
      "Iteration: 5703000, loss: 0.10709376147942663, gradient norm: 0.2498438545423254\n",
      "Iteration: 5704000, loss: 0.10709351772988607, gradient norm: 0.5789935669055163\n",
      "Iteration: 5705000, loss: 0.10709310055189097, gradient norm: 0.13439925043985687\n",
      "Iteration: 5706000, loss: 0.10709287027802905, gradient norm: 0.6323853093899706\n",
      "Iteration: 5707000, loss: 0.10709238714980722, gradient norm: 0.8768518206267909\n",
      "Iteration: 5708000, loss: 0.10709208877654766, gradient norm: 0.26270262947840944\n",
      "Iteration: 5709000, loss: 0.1070917188469086, gradient norm: 0.05812273143755064\n",
      "Iteration: 5710000, loss: 0.10709152284584197, gradient norm: 0.5461009072917903\n",
      "Iteration: 5711000, loss: 0.10709127918860892, gradient norm: 0.25744972477978034\n",
      "Iteration: 5712000, loss: 0.1070907243124585, gradient norm: 0.2892199962638684\n",
      "Iteration: 5713000, loss: 0.10709064939816883, gradient norm: 0.09534437953430555\n",
      "Iteration: 5714000, loss: 0.10709011212714689, gradient norm: 0.2327189568467918\n",
      "Iteration: 5715000, loss: 0.10708983440278436, gradient norm: 0.11247936926678072\n",
      "Iteration: 5716000, loss: 0.10708961900554025, gradient norm: 0.12803548621287042\n",
      "Iteration: 5717000, loss: 0.10708932806013008, gradient norm: 0.28577301501077157\n",
      "Iteration: 5718000, loss: 0.10708880357275918, gradient norm: 0.4274028933435793\n",
      "Iteration: 5719000, loss: 0.10708843074365737, gradient norm: 0.3149779774597636\n",
      "Iteration: 5720000, loss: 0.10708836072369056, gradient norm: 0.4522358541917653\n",
      "Iteration: 5721000, loss: 0.10708779156358543, gradient norm: 0.4748655385438236\n",
      "Iteration: 5722000, loss: 0.10708763369350217, gradient norm: 0.22236528746223486\n",
      "Iteration: 5723000, loss: 0.10708721939768878, gradient norm: 0.17541365791852706\n",
      "Iteration: 5724000, loss: 0.10708694018946131, gradient norm: 0.17301241752948138\n",
      "Iteration: 5725000, loss: 0.10708660323796633, gradient norm: 0.09422116644637625\n",
      "Iteration: 5726000, loss: 0.10708627630804626, gradient norm: 0.021321822778007635\n",
      "Iteration: 5727000, loss: 0.10708591371727938, gradient norm: 0.36441841732867364\n",
      "Iteration: 5728000, loss: 0.10708563688373836, gradient norm: 0.04579727599873488\n",
      "Iteration: 5729000, loss: 0.10708542570034879, gradient norm: 0.2439418202104938\n",
      "Iteration: 5730000, loss: 0.10708490856036455, gradient norm: 0.18282260951176765\n",
      "Iteration: 5731000, loss: 0.10708462672251962, gradient norm: 0.7635636254992206\n",
      "Iteration: 5732000, loss: 0.10708437536241598, gradient norm: 0.6135358429224199\n",
      "Iteration: 5733000, loss: 0.10708402940423184, gradient norm: 0.20792191951922062\n",
      "Iteration: 5734000, loss: 0.10708360705930732, gradient norm: 0.1757561171520079\n",
      "Iteration: 5735000, loss: 0.10708341919974283, gradient norm: 0.1323250017908595\n",
      "Iteration: 5736000, loss: 0.10708308529561322, gradient norm: 0.14230568371257948\n",
      "Iteration: 5737000, loss: 0.10708266779783597, gradient norm: 0.07684457749030064\n",
      "Iteration: 5738000, loss: 0.10708245404295413, gradient norm: 0.20719329767575875\n",
      "Iteration: 5739000, loss: 0.10708214188150979, gradient norm: 0.24700339693655926\n",
      "Iteration: 5740000, loss: 0.10708167846376221, gradient norm: 0.030836727839484417\n",
      "Iteration: 5741000, loss: 0.10708138545774838, gradient norm: 0.03883149653101586\n",
      "Iteration: 5742000, loss: 0.1070810953246122, gradient norm: 0.1560618335135966\n",
      "Iteration: 5743000, loss: 0.10708093410613014, gradient norm: 0.43167123369883986\n",
      "Iteration: 5744000, loss: 0.10708043456052833, gradient norm: 0.1969613351381491\n",
      "Iteration: 5745000, loss: 0.10708013067294471, gradient norm: 0.07089321672034161\n",
      "Iteration: 5746000, loss: 0.10707981439909423, gradient norm: 0.15850981534681916\n",
      "Iteration: 5747000, loss: 0.1070795802189956, gradient norm: 0.048453365533528074\n",
      "Iteration: 5748000, loss: 0.10707920677136722, gradient norm: 0.3489398366795157\n",
      "Iteration: 5749000, loss: 0.10707888242830955, gradient norm: 0.07857063719511836\n",
      "Iteration: 5750000, loss: 0.10707853355939685, gradient norm: 0.19829754381385567\n",
      "Iteration: 5751000, loss: 0.10707813727522572, gradient norm: 0.08644580149107023\n",
      "Iteration: 5752000, loss: 0.10707786075327234, gradient norm: 0.1470733670897645\n",
      "Iteration: 5753000, loss: 0.1070775613144001, gradient norm: 0.08497015954065253\n",
      "Iteration: 5754000, loss: 0.1070772176470642, gradient norm: 0.35226197745248367\n",
      "Iteration: 5755000, loss: 0.10707701598097852, gradient norm: 0.21498579049757952\n",
      "Iteration: 5756000, loss: 0.1070766281792759, gradient norm: 0.1273633591447768\n",
      "Iteration: 5757000, loss: 0.10707621394376331, gradient norm: 0.02287781498616084\n",
      "Iteration: 5758000, loss: 0.10707602714778021, gradient norm: 0.09460124265741955\n",
      "Iteration: 5759000, loss: 0.1070755673545427, gradient norm: 0.20359241153936797\n",
      "Iteration: 5760000, loss: 0.10707543206753734, gradient norm: 0.269278969225025\n",
      "Iteration: 5761000, loss: 0.10707490672672688, gradient norm: 0.5641072232849\n",
      "Iteration: 5762000, loss: 0.10707474463013852, gradient norm: 0.4321796111335564\n",
      "Iteration: 5763000, loss: 0.1070743757931829, gradient norm: 0.3587911496805909\n",
      "Iteration: 5764000, loss: 0.1070739470170756, gradient norm: 0.35016919064485513\n",
      "Iteration: 5765000, loss: 0.10707386242815593, gradient norm: 0.18300438804903277\n",
      "Iteration: 5766000, loss: 0.1070733887877058, gradient norm: 0.1123256388044294\n",
      "Iteration: 5767000, loss: 0.10707302276581364, gradient norm: 0.07973896910266345\n",
      "Iteration: 5768000, loss: 0.10707275977801031, gradient norm: 0.36260429928909443\n",
      "Iteration: 5769000, loss: 0.10707248786310032, gradient norm: 0.2885092716322696\n",
      "Iteration: 5770000, loss: 0.10707224765719275, gradient norm: 0.2714239835899609\n",
      "Iteration: 5771000, loss: 0.1070717187460295, gradient norm: 0.08768862917106227\n",
      "Iteration: 5772000, loss: 0.10707136736247992, gradient norm: 0.31113458733833027\n",
      "Iteration: 5773000, loss: 0.1070713619223994, gradient norm: 0.23611787271783022\n",
      "Iteration: 5774000, loss: 0.10707078047808144, gradient norm: 0.07445396755053875\n",
      "Iteration: 5775000, loss: 0.10707049119723389, gradient norm: 0.05622332659330577\n",
      "Iteration: 5776000, loss: 0.10707030897387187, gradient norm: 0.08679403314551817\n",
      "Iteration: 5777000, loss: 0.10706975234852834, gradient norm: 0.05569884463844306\n",
      "Iteration: 5778000, loss: 0.1070697147692102, gradient norm: 0.22790105872189942\n",
      "Iteration: 5779000, loss: 0.10706918232516724, gradient norm: 0.1140417331801107\n",
      "Iteration: 5780000, loss: 0.10706891450918403, gradient norm: 0.1285339959893767\n",
      "Iteration: 5781000, loss: 0.10706873903454689, gradient norm: 0.25137654537127363\n",
      "Iteration: 5782000, loss: 0.10706818069369754, gradient norm: 0.042549389590811795\n",
      "Iteration: 5783000, loss: 0.10706799013354444, gradient norm: 0.29688591167688955\n",
      "Iteration: 5784000, loss: 0.10706767871766505, gradient norm: 0.49336111734754795\n",
      "Iteration: 5785000, loss: 0.10706730021798033, gradient norm: 0.1003353751884911\n",
      "Iteration: 5786000, loss: 0.1070671119443746, gradient norm: 0.15861459923014434\n",
      "Iteration: 5787000, loss: 0.10706658460822885, gradient norm: 0.24476148891339994\n",
      "Iteration: 5788000, loss: 0.10706646529085302, gradient norm: 0.2744510736878472\n",
      "Iteration: 5789000, loss: 0.10706605644133682, gradient norm: 0.024089016912808785\n",
      "Iteration: 5790000, loss: 0.10706577264747363, gradient norm: 0.19338527504014866\n",
      "Iteration: 5791000, loss: 0.10706542867698664, gradient norm: 0.5162771878724977\n",
      "Iteration: 5792000, loss: 0.10706515275501846, gradient norm: 0.2681403826719916\n",
      "Iteration: 5793000, loss: 0.10706475466613061, gradient norm: 0.0631924464984103\n",
      "Iteration: 5794000, loss: 0.10706444265727438, gradient norm: 0.3130848783757063\n",
      "Iteration: 5795000, loss: 0.10706423901234517, gradient norm: 0.06985224933713957\n",
      "Iteration: 5796000, loss: 0.10706382787318822, gradient norm: 0.07887935098661056\n",
      "Iteration: 5797000, loss: 0.10706355506537366, gradient norm: 0.15264558453201163\n",
      "Iteration: 5798000, loss: 0.10706328283326166, gradient norm: 0.29178564832558124\n",
      "Iteration: 5799000, loss: 0.10706295624036995, gradient norm: 0.20748594052476987\n",
      "Iteration: 5800000, loss: 0.10706256263212444, gradient norm: 0.08781885023060833\n",
      "Iteration: 5801000, loss: 0.10706221099116811, gradient norm: 0.23768697220946491\n",
      "Iteration: 5802000, loss: 0.10706189980385512, gradient norm: 0.17560136979604654\n",
      "Iteration: 5803000, loss: 0.10706161993394706, gradient norm: 0.04639565573980497\n",
      "Iteration: 5804000, loss: 0.1070613698213364, gradient norm: 0.5030620734950175\n",
      "Iteration: 5805000, loss: 0.10706100541857894, gradient norm: 0.02783268586919161\n",
      "Iteration: 5806000, loss: 0.10706064675758299, gradient norm: 0.17711386881776517\n",
      "Iteration: 5807000, loss: 0.10706031766199436, gradient norm: 0.4691963555322776\n",
      "Iteration: 5808000, loss: 0.10706007280470609, gradient norm: 0.050931679304951266\n",
      "Iteration: 5809000, loss: 0.10705978455217896, gradient norm: 0.13315094726335747\n",
      "Iteration: 5810000, loss: 0.10705941705892423, gradient norm: 0.2246391330189887\n",
      "Iteration: 5811000, loss: 0.10705905247461368, gradient norm: 0.11547096608992649\n",
      "Iteration: 5812000, loss: 0.10705879121317452, gradient norm: 0.4518657323819176\n",
      "Iteration: 5813000, loss: 0.10705851048687728, gradient norm: 0.602727337542115\n",
      "Iteration: 5814000, loss: 0.10705817080304499, gradient norm: 0.3505808563636552\n",
      "Iteration: 5815000, loss: 0.10705788834311107, gradient norm: 0.0848858146973996\n",
      "Iteration: 5816000, loss: 0.10705743525494257, gradient norm: 0.1478560459007602\n",
      "Iteration: 5817000, loss: 0.10705726675836098, gradient norm: 0.06831447442017144\n",
      "Iteration: 5818000, loss: 0.10705689833336529, gradient norm: 0.7004459365207508\n",
      "Iteration: 5819000, loss: 0.10705656346294946, gradient norm: 0.513286225148172\n",
      "Iteration: 5820000, loss: 0.10705621129194853, gradient norm: 0.24073137716456955\n",
      "Iteration: 5821000, loss: 0.10705604167580608, gradient norm: 0.19041820078417535\n",
      "Iteration: 5822000, loss: 0.10705554670848266, gradient norm: 0.40212991911299967\n",
      "Iteration: 5823000, loss: 0.10705537402520747, gradient norm: 0.799854789976346\n",
      "Iteration: 5824000, loss: 0.10705517207590645, gradient norm: 0.4702774298333161\n",
      "Iteration: 5825000, loss: 0.10705469335329736, gradient norm: 0.9059459000621218\n",
      "Iteration: 5826000, loss: 0.10705429887126224, gradient norm: 0.1419010995423733\n",
      "Iteration: 5827000, loss: 0.10705405433088964, gradient norm: 0.07136687234211443\n",
      "Iteration: 5828000, loss: 0.10705379712246119, gradient norm: 0.6687949833086411\n",
      "Iteration: 5829000, loss: 0.10705338228718825, gradient norm: 0.04148040058585333\n",
      "Iteration: 5830000, loss: 0.10705322640688512, gradient norm: 0.2381530566012751\n",
      "Iteration: 5831000, loss: 0.10705264242531164, gradient norm: 0.1425676208121397\n",
      "Iteration: 5832000, loss: 0.10705262416985743, gradient norm: 0.5623458065242165\n",
      "Iteration: 5833000, loss: 0.10705217183378411, gradient norm: 0.10532868328817449\n",
      "Iteration: 5834000, loss: 0.10705201115563832, gradient norm: 0.18172713646949842\n",
      "Iteration: 5835000, loss: 0.10705150045724109, gradient norm: 0.08904276138444929\n",
      "Iteration: 5836000, loss: 0.10705130705854105, gradient norm: 0.31798933857810635\n",
      "Iteration: 5837000, loss: 0.10705091011563116, gradient norm: 0.2859464745985373\n",
      "Iteration: 5838000, loss: 0.10705061266684031, gradient norm: 0.27335719803488456\n",
      "Iteration: 5839000, loss: 0.10705044344229447, gradient norm: 0.06906079180969421\n",
      "Iteration: 5840000, loss: 0.10704989460654378, gradient norm: 0.6675256253300248\n",
      "Iteration: 5841000, loss: 0.10704964540990161, gradient norm: 0.15865579401903906\n",
      "Iteration: 5842000, loss: 0.1070493651098352, gradient norm: 0.08472217369816078\n",
      "Iteration: 5843000, loss: 0.1070490935324333, gradient norm: 0.6643015878946371\n",
      "Iteration: 5844000, loss: 0.10704871428746804, gradient norm: 0.54622608580873\n",
      "Iteration: 5845000, loss: 0.10704845157752284, gradient norm: 0.15328277664766143\n",
      "Iteration: 5846000, loss: 0.10704819861035113, gradient norm: 0.06698112611979036\n",
      "Iteration: 5847000, loss: 0.10704782700819317, gradient norm: 0.2782060518772822\n",
      "Iteration: 5848000, loss: 0.10704765961226423, gradient norm: 0.4921981941749064\n",
      "Iteration: 5849000, loss: 0.10704698197532975, gradient norm: 0.6898713316897686\n",
      "Iteration: 5850000, loss: 0.10704699691467319, gradient norm: 0.1814896660946799\n",
      "Iteration: 5851000, loss: 0.10704652990943143, gradient norm: 0.4648091729528305\n",
      "Iteration: 5852000, loss: 0.10704624001714384, gradient norm: 0.21935147478384406\n",
      "Iteration: 5853000, loss: 0.10704589360133332, gradient norm: 0.22690365912182836\n",
      "Iteration: 5854000, loss: 0.10704568698036357, gradient norm: 0.8437115677413455\n",
      "Iteration: 5855000, loss: 0.10704534303437359, gradient norm: 0.07108313686407636\n",
      "Iteration: 5856000, loss: 0.10704500302756456, gradient norm: 0.07606093047106167\n",
      "Iteration: 5857000, loss: 0.10704478721119631, gradient norm: 0.16414613454345103\n",
      "Iteration: 5858000, loss: 0.10704438613215667, gradient norm: 0.3911601513696139\n",
      "Iteration: 5859000, loss: 0.10704397788460929, gradient norm: 0.16036699283677036\n",
      "Iteration: 5860000, loss: 0.10704383485252719, gradient norm: 0.7212162050932655\n",
      "Iteration: 5861000, loss: 0.10704343863657781, gradient norm: 0.10911560705670621\n",
      "Iteration: 5862000, loss: 0.10704313659092882, gradient norm: 0.07123499473055887\n",
      "Iteration: 5863000, loss: 0.10704284358080497, gradient norm: 0.07588255910598635\n",
      "Iteration: 5864000, loss: 0.10704257642813136, gradient norm: 0.12285394127694077\n",
      "Iteration: 5865000, loss: 0.10704222203810924, gradient norm: 0.27216164815235044\n",
      "Iteration: 5866000, loss: 0.10704190144288389, gradient norm: 0.23709190597870655\n",
      "Iteration: 5867000, loss: 0.10704158468061131, gradient norm: 0.24056306308910508\n",
      "Iteration: 5868000, loss: 0.10704122321606716, gradient norm: 0.058894121076381874\n",
      "Iteration: 5869000, loss: 0.107040930171445, gradient norm: 0.4025330470459734\n",
      "Iteration: 5870000, loss: 0.10704072723376205, gradient norm: 0.10459984003789524\n",
      "Iteration: 5871000, loss: 0.10704039051097483, gradient norm: 0.22860253627879706\n",
      "Iteration: 5872000, loss: 0.10703991273929056, gradient norm: 0.37490068922219993\n",
      "Iteration: 5873000, loss: 0.10703986591561546, gradient norm: 0.2903457124430985\n",
      "Iteration: 5874000, loss: 0.10703938793521707, gradient norm: 0.13158405147477584\n",
      "Iteration: 5875000, loss: 0.10703914526329766, gradient norm: 0.14885015732585247\n",
      "Iteration: 5876000, loss: 0.10703873645211959, gradient norm: 0.5466077608392297\n",
      "Iteration: 5877000, loss: 0.10703853473079308, gradient norm: 0.29952563894288636\n",
      "Iteration: 5878000, loss: 0.10703826124241839, gradient norm: 0.19297486147543544\n",
      "Iteration: 5879000, loss: 0.10703791348471658, gradient norm: 0.3927384708363927\n",
      "Iteration: 5880000, loss: 0.10703749829289587, gradient norm: 0.07472649381010263\n",
      "Iteration: 5881000, loss: 0.10703734698952104, gradient norm: 0.11680846411675413\n",
      "Iteration: 5882000, loss: 0.107036905887136, gradient norm: 0.31419316160792715\n",
      "Iteration: 5883000, loss: 0.10703659562982243, gradient norm: 0.09500207390642892\n",
      "Iteration: 5884000, loss: 0.10703636983355996, gradient norm: 0.10067139366442597\n",
      "Iteration: 5885000, loss: 0.10703592682529058, gradient norm: 0.39527209342381175\n",
      "Iteration: 5886000, loss: 0.10703573198914675, gradient norm: 0.0657868580282323\n",
      "Iteration: 5887000, loss: 0.10703543092355002, gradient norm: 0.14836850370901866\n",
      "Iteration: 5888000, loss: 0.10703521126736679, gradient norm: 0.428639356130087\n",
      "Iteration: 5889000, loss: 0.10703484469358758, gradient norm: 0.562470917450104\n",
      "Iteration: 5890000, loss: 0.10703435182337742, gradient norm: 0.1424362462621596\n",
      "Iteration: 5891000, loss: 0.10703414362222491, gradient norm: 0.11510901309179031\n",
      "Iteration: 5892000, loss: 0.10703398546615928, gradient norm: 0.25382440809641577\n",
      "Iteration: 5893000, loss: 0.10703350721317315, gradient norm: 0.6513334427157761\n",
      "Iteration: 5894000, loss: 0.10703332171392858, gradient norm: 0.21225449811808694\n",
      "Iteration: 5895000, loss: 0.1070328207147748, gradient norm: 0.09205049606265225\n",
      "Iteration: 5896000, loss: 0.10703272536807061, gradient norm: 0.1939548049237655\n",
      "Iteration: 5897000, loss: 0.10703229138561085, gradient norm: 0.08090524441209762\n",
      "Iteration: 5898000, loss: 0.10703215644295853, gradient norm: 0.5047142527707912\n",
      "Iteration: 5899000, loss: 0.10703160529801957, gradient norm: 0.582289082262522\n",
      "Iteration: 5900000, loss: 0.10703148568757632, gradient norm: 0.1057925190480017\n",
      "Iteration: 5901000, loss: 0.10703106830376971, gradient norm: 0.08653319539947828\n",
      "Iteration: 5902000, loss: 0.10703085151063632, gradient norm: 0.20197032836746395\n",
      "Iteration: 5903000, loss: 0.10703043758494435, gradient norm: 0.6600722705324299\n",
      "Iteration: 5904000, loss: 0.10703022306910376, gradient norm: 0.17989477458544526\n",
      "Iteration: 5905000, loss: 0.10702976097147061, gradient norm: 0.28862519069572645\n",
      "Iteration: 5906000, loss: 0.10702969397700869, gradient norm: 0.03932175182118385\n",
      "Iteration: 5907000, loss: 0.10702928731422735, gradient norm: 0.23671567900238424\n",
      "Iteration: 5908000, loss: 0.1070288443270048, gradient norm: 0.148081272763591\n",
      "Iteration: 5909000, loss: 0.10702865833369812, gradient norm: 0.28992530788632953\n",
      "Iteration: 5910000, loss: 0.10702857436845584, gradient norm: 0.07427163641238513\n",
      "Iteration: 5911000, loss: 0.10702799557691914, gradient norm: 0.12630846332897616\n",
      "Iteration: 5912000, loss: 0.10702761220057858, gradient norm: 0.3613447573734542\n",
      "Iteration: 5913000, loss: 0.10702744278279147, gradient norm: 0.30546500905812085\n",
      "Iteration: 5914000, loss: 0.10702720116479704, gradient norm: 0.5756783554355945\n",
      "Iteration: 5915000, loss: 0.10702686477687111, gradient norm: 0.11027597095149429\n",
      "Iteration: 5916000, loss: 0.1070263368638395, gradient norm: 0.38405834507591163\n",
      "Iteration: 5917000, loss: 0.10702639795315795, gradient norm: 0.20324512221319688\n",
      "Iteration: 5918000, loss: 0.10702584566535757, gradient norm: 0.6384485930272278\n",
      "Iteration: 5919000, loss: 0.10702559450831521, gradient norm: 0.21986909059202958\n",
      "Iteration: 5920000, loss: 0.10702527110476714, gradient norm: 0.22200421943682885\n",
      "Iteration: 5921000, loss: 0.10702510538531798, gradient norm: 0.2474289157816738\n",
      "Iteration: 5922000, loss: 0.10702461044914627, gradient norm: 0.26258486866591596\n",
      "Iteration: 5923000, loss: 0.1070243643642449, gradient norm: 0.043276190934809465\n",
      "Iteration: 5924000, loss: 0.10702416268248187, gradient norm: 0.3656596533414599\n",
      "Iteration: 5925000, loss: 0.10702370865542978, gradient norm: 0.15793176366797868\n",
      "Iteration: 5926000, loss: 0.1070234457088056, gradient norm: 0.16478523697473715\n",
      "Iteration: 5927000, loss: 0.10702316355412712, gradient norm: 0.17781350421341025\n",
      "Iteration: 5928000, loss: 0.10702284708859965, gradient norm: 0.052486242903994056\n",
      "Iteration: 5929000, loss: 0.10702255299975713, gradient norm: 0.7197870580950252\n",
      "Iteration: 5930000, loss: 0.10702223881100781, gradient norm: 0.35850112261077677\n",
      "Iteration: 5931000, loss: 0.10702205243118146, gradient norm: 0.05083422152179223\n",
      "Iteration: 5932000, loss: 0.10702148361687368, gradient norm: 0.6203042424884975\n",
      "Iteration: 5933000, loss: 0.10702131426994504, gradient norm: 0.09484145834575866\n",
      "Iteration: 5934000, loss: 0.10702107857826071, gradient norm: 0.3031872934351095\n",
      "Iteration: 5935000, loss: 0.1070207681778417, gradient norm: 0.05439917634407219\n",
      "Iteration: 5936000, loss: 0.10702041694251548, gradient norm: 0.31491193335696255\n",
      "Iteration: 5937000, loss: 0.10702023499096458, gradient norm: 0.2508048555750532\n",
      "Iteration: 5938000, loss: 0.10701962469402392, gradient norm: 0.008738132598687132\n",
      "Iteration: 5939000, loss: 0.10701952998723999, gradient norm: 0.1848538518804386\n",
      "Iteration: 5940000, loss: 0.10701925364500188, gradient norm: 0.5950850890460857\n",
      "Iteration: 5941000, loss: 0.10701899201922468, gradient norm: 0.1748813924828158\n",
      "Iteration: 5942000, loss: 0.10701856054611404, gradient norm: 0.24094125056869206\n",
      "Iteration: 5943000, loss: 0.10701825121933493, gradient norm: 0.2594528830816885\n",
      "Iteration: 5944000, loss: 0.10701801641409871, gradient norm: 0.2964195748150022\n",
      "Iteration: 5945000, loss: 0.10701762836925227, gradient norm: 0.11044211828800117\n",
      "Iteration: 5946000, loss: 0.10701735930104969, gradient norm: 0.1420425531000314\n",
      "Iteration: 5947000, loss: 0.1070170562083588, gradient norm: 0.3046765630551307\n",
      "Iteration: 5948000, loss: 0.10701669604855568, gradient norm: 0.5932260525683392\n",
      "Iteration: 5949000, loss: 0.10701656100846921, gradient norm: 0.08832207308673898\n",
      "Iteration: 5950000, loss: 0.10701611348965316, gradient norm: 0.06257891653368124\n",
      "Iteration: 5951000, loss: 0.1070159208988313, gradient norm: 0.08647287781515282\n",
      "Iteration: 5952000, loss: 0.10701554175371052, gradient norm: 0.20911012471793128\n",
      "Iteration: 5953000, loss: 0.10701530030951363, gradient norm: 0.12931426328706286\n",
      "Iteration: 5954000, loss: 0.10701488542480093, gradient norm: 0.27642370732367977\n",
      "Iteration: 5955000, loss: 0.10701474086052241, gradient norm: 0.11497246143103551\n",
      "Iteration: 5956000, loss: 0.10701424367063167, gradient norm: 0.06998323422265215\n",
      "Iteration: 5957000, loss: 0.10701414174187042, gradient norm: 0.762984510591286\n",
      "Iteration: 5958000, loss: 0.10701390981186856, gradient norm: 0.48874796006556714\n",
      "Iteration: 5959000, loss: 0.10701326611924418, gradient norm: 0.49324683288100896\n",
      "Iteration: 5960000, loss: 0.1070132424732405, gradient norm: 0.33849274737543605\n",
      "Iteration: 5961000, loss: 0.10701270109657776, gradient norm: 0.29238852584122105\n",
      "Iteration: 5962000, loss: 0.10701269976245147, gradient norm: 0.10764497825945162\n",
      "Iteration: 5963000, loss: 0.10701226862839106, gradient norm: 0.04639267415732178\n",
      "Iteration: 5964000, loss: 0.10701183188165289, gradient norm: 0.07075642520049319\n",
      "Iteration: 5965000, loss: 0.10701169710191999, gradient norm: 0.059548477759511526\n",
      "Iteration: 5966000, loss: 0.1070112824745367, gradient norm: 0.03689687774363192\n",
      "Iteration: 5967000, loss: 0.1070111419210619, gradient norm: 0.6774144923137747\n",
      "Iteration: 5968000, loss: 0.10701068587424103, gradient norm: 0.09208315846010312\n",
      "Iteration: 5969000, loss: 0.10701041568865723, gradient norm: 0.16289229197256533\n",
      "Iteration: 5970000, loss: 0.10701018333427177, gradient norm: 0.29875588307710205\n",
      "Iteration: 5971000, loss: 0.10700973632999686, gradient norm: 0.10414389720288889\n",
      "Iteration: 5972000, loss: 0.10700960499750367, gradient norm: 0.09894762255446263\n",
      "Iteration: 5973000, loss: 0.10700917348560782, gradient norm: 0.10656992738318777\n",
      "Iteration: 5974000, loss: 0.10700894149589554, gradient norm: 0.4609787319702276\n",
      "Iteration: 5975000, loss: 0.10700865669368691, gradient norm: 0.5044039081591943\n",
      "Iteration: 5976000, loss: 0.1070084619536452, gradient norm: 0.3720272632270519\n",
      "Iteration: 5977000, loss: 0.10700793671908762, gradient norm: 0.1342247799687051\n",
      "Iteration: 5978000, loss: 0.10700768763302589, gradient norm: 0.4229122305004153\n",
      "Iteration: 5979000, loss: 0.10700740456965639, gradient norm: 0.1485563924900992\n",
      "Iteration: 5980000, loss: 0.10700713448663055, gradient norm: 0.22989347447808273\n",
      "Iteration: 5981000, loss: 0.10700688701916226, gradient norm: 0.244638783708143\n",
      "Iteration: 5982000, loss: 0.10700644823266334, gradient norm: 0.09151678562685465\n",
      "Iteration: 5983000, loss: 0.10700626659860676, gradient norm: 0.3067333518785503\n",
      "Iteration: 5984000, loss: 0.10700587107979075, gradient norm: 0.19468265571208798\n",
      "Iteration: 5985000, loss: 0.10700558264598642, gradient norm: 0.055548367503650796\n",
      "Iteration: 5986000, loss: 0.10700527346081969, gradient norm: 0.20416601270455165\n",
      "Iteration: 5987000, loss: 0.10700505876715999, gradient norm: 0.02606067615273065\n",
      "Iteration: 5988000, loss: 0.1070047273310754, gradient norm: 0.2103080157881339\n",
      "Iteration: 5989000, loss: 0.10700443801570417, gradient norm: 0.006634806511370624\n",
      "Iteration: 5990000, loss: 0.10700414139403745, gradient norm: 0.06728597533937618\n",
      "Iteration: 5991000, loss: 0.1070038639180567, gradient norm: 0.6661109150236533\n",
      "Iteration: 5992000, loss: 0.10700348598263529, gradient norm: 0.29658950121790767\n",
      "Iteration: 5993000, loss: 0.10700320555454214, gradient norm: 0.1739009203798979\n",
      "Iteration: 5994000, loss: 0.10700296464604611, gradient norm: 0.24403397860647302\n",
      "Iteration: 5995000, loss: 0.1070026256752144, gradient norm: 0.1751435613295275\n",
      "Iteration: 5996000, loss: 0.10700222294679114, gradient norm: 0.2428559724154358\n",
      "Iteration: 5997000, loss: 0.10700219291708662, gradient norm: 0.11344589419461788\n",
      "Iteration: 5998000, loss: 0.10700176816173479, gradient norm: 0.5958097952276111\n",
      "Iteration: 5999000, loss: 0.10700133889632285, gradient norm: 0.09565740394243283\n",
      "Iteration: 6000000, loss: 0.10700112176419722, gradient norm: 0.13486585520579264\n",
      "Iteration: 6001000, loss: 0.10700087836419374, gradient norm: 0.1475722774556125\n",
      "Iteration: 6002000, loss: 0.10700051912481723, gradient norm: 0.3566296026667711\n",
      "Iteration: 6003000, loss: 0.10700025305554273, gradient norm: 0.05674357842363955\n",
      "Iteration: 6004000, loss: 0.10700005500841701, gradient norm: 0.4376390541913035\n",
      "Iteration: 6005000, loss: 0.1069995553215824, gradient norm: 0.05305832696120007\n",
      "Iteration: 6006000, loss: 0.10699932740615374, gradient norm: 0.1312029987112915\n",
      "Iteration: 6007000, loss: 0.1069992103488697, gradient norm: 0.20935513943175701\n",
      "Iteration: 6008000, loss: 0.10699873469636645, gradient norm: 0.17753747680029955\n",
      "Iteration: 6009000, loss: 0.10699835122744338, gradient norm: 0.1332555014423242\n",
      "Iteration: 6010000, loss: 0.1069982701445704, gradient norm: 0.3645697007599145\n",
      "Iteration: 6011000, loss: 0.1069978224998255, gradient norm: 0.2254142675682716\n",
      "Iteration: 6012000, loss: 0.106997641732683, gradient norm: 0.5865793253788179\n",
      "Iteration: 6013000, loss: 0.10699723251927458, gradient norm: 0.16214311548269095\n",
      "Iteration: 6014000, loss: 0.10699691860519059, gradient norm: 0.11378428102504484\n",
      "Iteration: 6015000, loss: 0.10699664942825254, gradient norm: 0.055347761772779\n",
      "Iteration: 6016000, loss: 0.10699638904684863, gradient norm: 0.023795861446839536\n",
      "Iteration: 6017000, loss: 0.10699601040812473, gradient norm: 0.1389040642876859\n",
      "Iteration: 6018000, loss: 0.10699584251131303, gradient norm: 0.11500134960767598\n",
      "Iteration: 6019000, loss: 0.1069955243436557, gradient norm: 0.05228665110971464\n",
      "Iteration: 6020000, loss: 0.10699514967072674, gradient norm: 0.1036776303879489\n",
      "Iteration: 6021000, loss: 0.10699511736783399, gradient norm: 0.10185921334086596\n",
      "Iteration: 6022000, loss: 0.10699447497941568, gradient norm: 0.017201531720601175\n",
      "Iteration: 6023000, loss: 0.10699425311139646, gradient norm: 0.45692124932981915\n",
      "Iteration: 6024000, loss: 0.10699400266817335, gradient norm: 0.08387446718695278\n",
      "Iteration: 6025000, loss: 0.10699376840540813, gradient norm: 0.3979680510453785\n",
      "Iteration: 6026000, loss: 0.10699336915763413, gradient norm: 0.12118739738936533\n",
      "Iteration: 6027000, loss: 0.10699317747360047, gradient norm: 0.17685629205960265\n",
      "Iteration: 6028000, loss: 0.10699269239346637, gradient norm: 0.1820728883912934\n",
      "Iteration: 6029000, loss: 0.10699258169897942, gradient norm: 0.15305507870625684\n",
      "Iteration: 6030000, loss: 0.10699233736749511, gradient norm: 0.012814435985815808\n",
      "Iteration: 6031000, loss: 0.10699181134463771, gradient norm: 0.2250828885870517\n",
      "Iteration: 6032000, loss: 0.1069917229152523, gradient norm: 0.06766428459884483\n",
      "Iteration: 6033000, loss: 0.10699137603400342, gradient norm: 0.4323021017184754\n",
      "Iteration: 6034000, loss: 0.1069911841941746, gradient norm: 0.19565668867615255\n",
      "Iteration: 6035000, loss: 0.10699069853126453, gradient norm: 0.12146160067017107\n",
      "Iteration: 6036000, loss: 0.1069904245273571, gradient norm: 0.7303746936188628\n",
      "Iteration: 6037000, loss: 0.10699003386819643, gradient norm: 0.16786205441239138\n",
      "Iteration: 6038000, loss: 0.10698989820687342, gradient norm: 0.17780924463464737\n",
      "Iteration: 6039000, loss: 0.10698960754992115, gradient norm: 0.15472145888614539\n",
      "Iteration: 6040000, loss: 0.10698928367698682, gradient norm: 0.02990990375801404\n",
      "Iteration: 6041000, loss: 0.10698905998124343, gradient norm: 0.07207109675720216\n",
      "Iteration: 6042000, loss: 0.10698860967473751, gradient norm: 0.3272802841855401\n",
      "Iteration: 6043000, loss: 0.10698842155208041, gradient norm: 0.16783433813382848\n",
      "Iteration: 6044000, loss: 0.1069881501751488, gradient norm: 0.057258333903275345\n",
      "Iteration: 6045000, loss: 0.1069878207689799, gradient norm: 0.8747263461610774\n",
      "Iteration: 6046000, loss: 0.10698736839608905, gradient norm: 0.40220103232733495\n",
      "Iteration: 6047000, loss: 0.10698731264187217, gradient norm: 0.8140512084649237\n",
      "Iteration: 6048000, loss: 0.10698696967164238, gradient norm: 0.1812435155166491\n",
      "Iteration: 6049000, loss: 0.10698652969115965, gradient norm: 0.06439167231317582\n",
      "Iteration: 6050000, loss: 0.10698633574461938, gradient norm: 0.36446002494410357\n",
      "Iteration: 6051000, loss: 0.10698606312023114, gradient norm: 0.17165122259956217\n",
      "Iteration: 6052000, loss: 0.1069856688840496, gradient norm: 0.390171020770441\n",
      "Iteration: 6053000, loss: 0.10698560158215131, gradient norm: 0.14516233211920693\n",
      "Iteration: 6054000, loss: 0.10698503459665785, gradient norm: 0.3792262470552772\n",
      "Iteration: 6055000, loss: 0.10698500295966828, gradient norm: 0.7846452352627692\n",
      "Iteration: 6056000, loss: 0.10698446431344628, gradient norm: 0.5499039319629476\n",
      "Iteration: 6057000, loss: 0.10698435674525936, gradient norm: 0.24479307641866022\n",
      "Iteration: 6058000, loss: 0.10698394359878756, gradient norm: 0.21614356748728336\n",
      "Iteration: 6059000, loss: 0.10698386659314574, gradient norm: 0.06983841942107995\n",
      "Iteration: 6060000, loss: 0.1069832997965955, gradient norm: 0.20035610063127382\n",
      "Iteration: 6061000, loss: 0.106982992977046, gradient norm: 0.5898526744302003\n",
      "Iteration: 6062000, loss: 0.10698282690543715, gradient norm: 0.4967589408473206\n",
      "Iteration: 6063000, loss: 0.10698249394000187, gradient norm: 0.10579370576951147\n",
      "Iteration: 6064000, loss: 0.1069821423886727, gradient norm: 0.06606546744228536\n",
      "Iteration: 6065000, loss: 0.10698202284595276, gradient norm: 0.10222294018626367\n",
      "Iteration: 6066000, loss: 0.10698167547012184, gradient norm: 0.1434036818278418\n",
      "Iteration: 6067000, loss: 0.1069813763765561, gradient norm: 0.4386248469215555\n",
      "Iteration: 6068000, loss: 0.10698103850126688, gradient norm: 0.2402526129378723\n",
      "Iteration: 6069000, loss: 0.10698074098288302, gradient norm: 0.2929530582792448\n",
      "Iteration: 6070000, loss: 0.10698045283562396, gradient norm: 0.08715860104159602\n",
      "Iteration: 6071000, loss: 0.10698021978593872, gradient norm: 0.025406282824309106\n",
      "Iteration: 6072000, loss: 0.1069799546347151, gradient norm: 0.12627091689008796\n",
      "Iteration: 6073000, loss: 0.10697946875896738, gradient norm: 0.3941989627990757\n",
      "Iteration: 6074000, loss: 0.10697945433949688, gradient norm: 0.05277613391119588\n",
      "Iteration: 6075000, loss: 0.10697890398810884, gradient norm: 0.1848693166465908\n",
      "Iteration: 6076000, loss: 0.10697876028654771, gradient norm: 0.5001969159349857\n",
      "Iteration: 6077000, loss: 0.10697835560327322, gradient norm: 0.05881401944874006\n",
      "Iteration: 6078000, loss: 0.10697816202829873, gradient norm: 0.20627504366447527\n",
      "Iteration: 6079000, loss: 0.10697792137145223, gradient norm: 0.11908292204567106\n",
      "Iteration: 6080000, loss: 0.10697757750537121, gradient norm: 0.2477992239851277\n",
      "Iteration: 6081000, loss: 0.10697730049842692, gradient norm: 0.19234463189816808\n",
      "Iteration: 6082000, loss: 0.10697684737236307, gradient norm: 0.22256722062064116\n",
      "Iteration: 6083000, loss: 0.10697672943494041, gradient norm: 0.08707068446929847\n",
      "Iteration: 6084000, loss: 0.10697628828941451, gradient norm: 0.0379644943042783\n",
      "Iteration: 6085000, loss: 0.10697618482349154, gradient norm: 0.1889603479790061\n",
      "Iteration: 6086000, loss: 0.10697580648464677, gradient norm: 0.03599552452687886\n",
      "Iteration: 6087000, loss: 0.10697561846270073, gradient norm: 0.16038911148023027\n",
      "Iteration: 6088000, loss: 0.10697525231359986, gradient norm: 0.5093075516593456\n",
      "Iteration: 6089000, loss: 0.10697487545051332, gradient norm: 0.05215957789811246\n",
      "Iteration: 6090000, loss: 0.10697467346423813, gradient norm: 0.028696945704914736\n",
      "Iteration: 6091000, loss: 0.10697424366852788, gradient norm: 0.37438405209492076\n",
      "Iteration: 6092000, loss: 0.10697410064877343, gradient norm: 0.376941711146534\n",
      "Iteration: 6093000, loss: 0.10697384620133567, gradient norm: 0.29778750299719614\n",
      "Iteration: 6094000, loss: 0.10697340739995, gradient norm: 0.7501024151976727\n",
      "Iteration: 6095000, loss: 0.10697334670128085, gradient norm: 0.3325350245335788\n",
      "Iteration: 6096000, loss: 0.1069728042326528, gradient norm: 0.17881912979498585\n",
      "Iteration: 6097000, loss: 0.10697257295042797, gradient norm: 0.060855700630548466\n",
      "Iteration: 6098000, loss: 0.10697225647131241, gradient norm: 0.11174631888735696\n",
      "Iteration: 6099000, loss: 0.10697215346714112, gradient norm: 0.24810543387549536\n",
      "Iteration: 6100000, loss: 0.1069716603620929, gradient norm: 0.04499445488269662\n",
      "Iteration: 6101000, loss: 0.10697158403346463, gradient norm: 0.40770331889554806\n",
      "Iteration: 6102000, loss: 0.10697101561393234, gradient norm: 0.008593566967789651\n",
      "Iteration: 6103000, loss: 0.10697098206545357, gradient norm: 0.14336231437539518\n",
      "Iteration: 6104000, loss: 0.1069705267893437, gradient norm: 0.3372910106442923\n",
      "Iteration: 6105000, loss: 0.1069703580124199, gradient norm: 0.22406874260747336\n",
      "Iteration: 6106000, loss: 0.10696988832108623, gradient norm: 0.12353244752849814\n",
      "Iteration: 6107000, loss: 0.10696988338288509, gradient norm: 0.20901763047007654\n",
      "Iteration: 6108000, loss: 0.10696940633438899, gradient norm: 0.34588767764669737\n",
      "Iteration: 6109000, loss: 0.10696909975065952, gradient norm: 0.13694603944313852\n",
      "Iteration: 6110000, loss: 0.10696884696591197, gradient norm: 0.044624872440739656\n",
      "Iteration: 6111000, loss: 0.10696846657331052, gradient norm: 0.25521339114330027\n",
      "Iteration: 6112000, loss: 0.10696834404879782, gradient norm: 0.19177279467908628\n",
      "Iteration: 6113000, loss: 0.10696798260002122, gradient norm: 0.6679234202240708\n",
      "Iteration: 6114000, loss: 0.10696769346319797, gradient norm: 0.33552409683427087\n",
      "Iteration: 6115000, loss: 0.10696745280098988, gradient norm: 0.062074186905192036\n",
      "Iteration: 6116000, loss: 0.10696714815764251, gradient norm: 0.18653670014150361\n",
      "Iteration: 6117000, loss: 0.106966676626147, gradient norm: 0.19785064698653174\n",
      "Iteration: 6118000, loss: 0.10696663571763187, gradient norm: 0.16019391095896324\n",
      "Iteration: 6119000, loss: 0.10696621131943734, gradient norm: 0.9113647250736567\n",
      "Iteration: 6120000, loss: 0.1069659884112147, gradient norm: 0.15298922559532302\n",
      "Iteration: 6121000, loss: 0.10696555670019366, gradient norm: 0.3498375559736701\n",
      "Iteration: 6122000, loss: 0.10696555173616301, gradient norm: 0.724774239747554\n",
      "Iteration: 6123000, loss: 0.1069649329094287, gradient norm: 0.10268265393748158\n",
      "Iteration: 6124000, loss: 0.10696485309341543, gradient norm: 0.3424509976691027\n",
      "Iteration: 6125000, loss: 0.10696448855563957, gradient norm: 0.13765591449590944\n",
      "Iteration: 6126000, loss: 0.10696419679486967, gradient norm: 0.05779820893963877\n",
      "Iteration: 6127000, loss: 0.10696398986020056, gradient norm: 0.11802496107402731\n",
      "Iteration: 6128000, loss: 0.10696369138812761, gradient norm: 0.06938438952656462\n",
      "Iteration: 6129000, loss: 0.10696338631319907, gradient norm: 0.4861007700999663\n",
      "Iteration: 6130000, loss: 0.10696307681806994, gradient norm: 0.43075267715747195\n",
      "Iteration: 6131000, loss: 0.10696282432520476, gradient norm: 0.12172648937989437\n",
      "Iteration: 6132000, loss: 0.10696242916558875, gradient norm: 0.16753312276004206\n",
      "Iteration: 6133000, loss: 0.10696212798028239, gradient norm: 0.3205633861368074\n",
      "Iteration: 6134000, loss: 0.10696193000912675, gradient norm: 0.404173225273027\n",
      "Iteration: 6135000, loss: 0.10696173309762225, gradient norm: 0.16377027399634234\n",
      "Iteration: 6136000, loss: 0.10696140184014732, gradient norm: 0.33927328706523696\n",
      "Iteration: 6137000, loss: 0.10696127050299842, gradient norm: 0.6568257468366276\n",
      "Iteration: 6138000, loss: 0.10696052462667345, gradient norm: 0.38967230949338594\n",
      "Iteration: 6139000, loss: 0.10696053455379934, gradient norm: 0.47579110328883895\n",
      "Iteration: 6140000, loss: 0.10696011174298425, gradient norm: 0.082637288919577\n",
      "Iteration: 6141000, loss: 0.10696000192595537, gradient norm: 0.5317561680853937\n",
      "Iteration: 6142000, loss: 0.10695963152354875, gradient norm: 0.44019690546072404\n",
      "Iteration: 6143000, loss: 0.10695928388835949, gradient norm: 0.09789129770747118\n",
      "Iteration: 6144000, loss: 0.10695906871609336, gradient norm: 0.2285454689190418\n",
      "Iteration: 6145000, loss: 0.106958832799547, gradient norm: 0.3374225599279068\n",
      "Iteration: 6146000, loss: 0.10695838721447481, gradient norm: 0.10922124701135226\n",
      "Iteration: 6147000, loss: 0.10695819323233641, gradient norm: 0.06555419169061538\n",
      "Iteration: 6148000, loss: 0.10695810206289759, gradient norm: 0.3832184254469329\n",
      "Iteration: 6149000, loss: 0.10695738257837474, gradient norm: 0.05960903925109226\n",
      "Iteration: 6150000, loss: 0.10695754900307577, gradient norm: 0.16308999739497473\n",
      "Iteration: 6151000, loss: 0.1069569441079054, gradient norm: 0.04079526780805889\n",
      "Iteration: 6152000, loss: 0.10695679898159216, gradient norm: 0.16071327015775408\n",
      "Iteration: 6153000, loss: 0.10695646291886414, gradient norm: 0.07284163659914186\n",
      "Iteration: 6154000, loss: 0.10695613389151816, gradient norm: 0.12095211071785733\n",
      "Iteration: 6155000, loss: 0.10695613773916395, gradient norm: 0.1747544147132522\n",
      "Iteration: 6156000, loss: 0.10695559084349318, gradient norm: 0.2911776212867122\n",
      "Iteration: 6157000, loss: 0.10695529750199077, gradient norm: 0.04896548246896035\n",
      "Iteration: 6158000, loss: 0.10695510172294559, gradient norm: 0.8446340822248465\n",
      "Iteration: 6159000, loss: 0.10695471487140164, gradient norm: 0.13723522135619884\n",
      "Iteration: 6160000, loss: 0.1069544801524056, gradient norm: 0.19779048613920483\n",
      "Iteration: 6161000, loss: 0.10695411398631452, gradient norm: 0.27923539220485805\n",
      "Iteration: 6162000, loss: 0.10695397952453076, gradient norm: 0.05108365064627666\n",
      "Iteration: 6163000, loss: 0.10695373613646293, gradient norm: 0.2702306521721258\n",
      "Iteration: 6164000, loss: 0.10695321604528937, gradient norm: 0.5536222369167622\n",
      "Iteration: 6165000, loss: 0.10695301025752195, gradient norm: 0.28877131141964796\n",
      "Iteration: 6166000, loss: 0.1069529863228503, gradient norm: 0.4558552224590804\n",
      "Iteration: 6167000, loss: 0.10695249812595249, gradient norm: 0.07147184850087368\n",
      "Iteration: 6168000, loss: 0.10695209839383034, gradient norm: 0.2031739663675136\n",
      "Iteration: 6169000, loss: 0.10695202567322014, gradient norm: 0.05842075046732766\n",
      "Iteration: 6170000, loss: 0.10695146865845026, gradient norm: 0.15173243706877707\n",
      "Iteration: 6171000, loss: 0.1069513764943907, gradient norm: 0.07074473053956237\n",
      "Iteration: 6172000, loss: 0.10695128732193272, gradient norm: 0.4740021371532663\n",
      "Iteration: 6173000, loss: 0.1069506152767584, gradient norm: 0.07359037364140564\n",
      "Iteration: 6174000, loss: 0.10695044758580176, gradient norm: 0.029562406023230663\n",
      "Iteration: 6175000, loss: 0.10695026501552093, gradient norm: 0.08479892563830164\n",
      "Iteration: 6176000, loss: 0.10694997361556438, gradient norm: 0.15203877953879713\n",
      "Iteration: 6177000, loss: 0.10694963381383796, gradient norm: 0.14412121772485015\n",
      "Iteration: 6178000, loss: 0.10694933998416445, gradient norm: 0.17419300350498657\n",
      "Iteration: 6179000, loss: 0.10694903054792639, gradient norm: 0.1457054161363406\n",
      "Iteration: 6180000, loss: 0.10694873320560061, gradient norm: 0.08195157072309354\n",
      "Iteration: 6181000, loss: 0.10694846809950817, gradient norm: 0.31824489079802715\n",
      "Iteration: 6182000, loss: 0.10694835997258459, gradient norm: 0.24951733121290093\n",
      "Iteration: 6183000, loss: 0.10694784757312353, gradient norm: 0.06969955674214907\n",
      "Iteration: 6184000, loss: 0.1069476387136032, gradient norm: 0.16951899268348322\n",
      "Iteration: 6185000, loss: 0.10694734849766728, gradient norm: 0.30110273267335713\n",
      "Iteration: 6186000, loss: 0.10694721558998456, gradient norm: 0.38132886043006076\n",
      "Iteration: 6187000, loss: 0.1069465958602497, gradient norm: 0.23459283144998366\n",
      "Iteration: 6188000, loss: 0.10694666999020276, gradient norm: 0.03411690421191131\n",
      "Iteration: 6189000, loss: 0.10694616444146948, gradient norm: 0.1956397788599735\n",
      "Iteration: 6190000, loss: 0.1069460812910766, gradient norm: 0.057924683760042346\n",
      "Iteration: 6191000, loss: 0.10694560571793645, gradient norm: 0.6566294523621405\n",
      "Iteration: 6192000, loss: 0.10694541241620241, gradient norm: 0.34911004258640577\n",
      "Iteration: 6193000, loss: 0.10694517737588496, gradient norm: 0.08874160266653362\n",
      "Iteration: 6194000, loss: 0.106944782711747, gradient norm: 0.08868450378186536\n",
      "Iteration: 6195000, loss: 0.10694456403971804, gradient norm: 0.14749375548623686\n",
      "Iteration: 6196000, loss: 0.10694434401444942, gradient norm: 0.35678165142712776\n",
      "Iteration: 6197000, loss: 0.10694391674565827, gradient norm: 0.21075573881571447\n",
      "Iteration: 6198000, loss: 0.10694366002197357, gradient norm: 0.10162865736880898\n",
      "Iteration: 6199000, loss: 0.10694342430111908, gradient norm: 0.4146976521660477\n",
      "Iteration: 6200000, loss: 0.1069432373549308, gradient norm: 0.6912239889068361\n",
      "Iteration: 6201000, loss: 0.10694276339044571, gradient norm: 0.16435992229786284\n",
      "Iteration: 6202000, loss: 0.10694255602353928, gradient norm: 0.2288001949436233\n",
      "Iteration: 6203000, loss: 0.1069423746374893, gradient norm: 0.04181769266860775\n",
      "Iteration: 6204000, loss: 0.10694205871360585, gradient norm: 0.5145192518136208\n",
      "Iteration: 6205000, loss: 0.10694165039440952, gradient norm: 0.46271339868366396\n",
      "Iteration: 6206000, loss: 0.10694142476335139, gradient norm: 0.21110103426724525\n",
      "Iteration: 6207000, loss: 0.10694115881855319, gradient norm: 0.41795590321071135\n",
      "Iteration: 6208000, loss: 0.10694087015879594, gradient norm: 0.08585519296736253\n",
      "Iteration: 6209000, loss: 0.10694061842627037, gradient norm: 0.6054126981810852\n",
      "Iteration: 6210000, loss: 0.10694024051949484, gradient norm: 0.44347310722692634\n",
      "Iteration: 6211000, loss: 0.10694018245642808, gradient norm: 0.03258265248917542\n",
      "Iteration: 6212000, loss: 0.10693959661625882, gradient norm: 0.4778117255346427\n",
      "Iteration: 6213000, loss: 0.10693950771852676, gradient norm: 0.024766900119221127\n",
      "Iteration: 6214000, loss: 0.1069393432785289, gradient norm: 0.0453409024136879\n",
      "Iteration: 6215000, loss: 0.10693876912751327, gradient norm: 0.45564624666898795\n",
      "Iteration: 6216000, loss: 0.10693870134771666, gradient norm: 0.936150323037574\n",
      "Iteration: 6217000, loss: 0.10693847923066015, gradient norm: 0.1374680805447295\n",
      "Iteration: 6218000, loss: 0.10693793106581177, gradient norm: 0.18145105095543937\n",
      "Iteration: 6219000, loss: 0.10693785940310319, gradient norm: 0.583549292307582\n",
      "Iteration: 6220000, loss: 0.10693750621830793, gradient norm: 0.3808472849570824\n",
      "Iteration: 6221000, loss: 0.106937167583206, gradient norm: 0.3121010686011638\n",
      "Iteration: 6222000, loss: 0.1069369437270668, gradient norm: 0.37691232392440827\n",
      "Iteration: 6223000, loss: 0.10693652663356136, gradient norm: 0.4237202594520078\n",
      "Iteration: 6224000, loss: 0.10693655334687031, gradient norm: 0.06473730169983609\n",
      "Iteration: 6225000, loss: 0.10693609606190344, gradient norm: 0.3431177544915338\n",
      "Iteration: 6226000, loss: 0.10693584544361727, gradient norm: 0.36601962840481067\n",
      "Iteration: 6227000, loss: 0.10693546410032842, gradient norm: 0.06110155307701172\n",
      "Iteration: 6228000, loss: 0.10693537053607764, gradient norm: 0.1642284983872105\n",
      "Iteration: 6229000, loss: 0.10693494165392872, gradient norm: 0.050901476042927704\n",
      "Iteration: 6230000, loss: 0.10693469367847555, gradient norm: 0.04741678080170846\n",
      "Iteration: 6231000, loss: 0.10693433393472375, gradient norm: 0.5980423456151832\n",
      "Iteration: 6232000, loss: 0.10693424086990788, gradient norm: 0.5083404936016841\n",
      "Iteration: 6233000, loss: 0.10693373389340408, gradient norm: 0.26028531574940283\n",
      "Iteration: 6234000, loss: 0.1069336155121339, gradient norm: 0.14736245105249107\n",
      "Iteration: 6235000, loss: 0.10693347045368425, gradient norm: 0.4341604880381667\n",
      "Iteration: 6236000, loss: 0.10693300799987335, gradient norm: 0.07162431819603589\n",
      "Iteration: 6237000, loss: 0.10693275919378235, gradient norm: 0.3251112302638205\n",
      "Iteration: 6238000, loss: 0.10693243164249137, gradient norm: 0.2294257108501987\n",
      "Iteration: 6239000, loss: 0.1069322517044057, gradient norm: 0.19566831067711982\n",
      "Iteration: 6240000, loss: 0.10693175414236643, gradient norm: 0.1858214619918111\n",
      "Iteration: 6241000, loss: 0.10693170146806583, gradient norm: 0.5114966063268312\n",
      "Iteration: 6242000, loss: 0.10693140606154274, gradient norm: 0.2250475864705484\n",
      "Iteration: 6243000, loss: 0.10693096833866479, gradient norm: 0.11292884360022858\n",
      "Iteration: 6244000, loss: 0.10693090811808344, gradient norm: 0.13990695529608035\n",
      "Iteration: 6245000, loss: 0.10693046953999875, gradient norm: 0.6060500391825325\n",
      "Iteration: 6246000, loss: 0.10693022158834174, gradient norm: 0.22219082122051684\n",
      "Iteration: 6247000, loss: 0.10693007153226546, gradient norm: 0.06409236747651129\n",
      "Iteration: 6248000, loss: 0.10692955792499349, gradient norm: 0.07986719155491696\n",
      "Iteration: 6249000, loss: 0.106929577904251, gradient norm: 0.3504939686474224\n",
      "Iteration: 6250000, loss: 0.10692908638235753, gradient norm: 0.600293032855128\n",
      "Iteration: 6251000, loss: 0.10692883078563921, gradient norm: 0.36115795054123184\n",
      "Iteration: 6252000, loss: 0.10692849237258621, gradient norm: 0.5421066728182085\n",
      "Iteration: 6253000, loss: 0.10692829801783416, gradient norm: 0.08344974717850322\n",
      "Iteration: 6254000, loss: 0.10692801460051922, gradient norm: 0.1612066854735318\n",
      "Iteration: 6255000, loss: 0.10692776780069674, gradient norm: 0.2153198279069447\n",
      "Iteration: 6256000, loss: 0.10692741408584497, gradient norm: 0.7745867278743062\n",
      "Iteration: 6257000, loss: 0.1069271781905658, gradient norm: 0.15769811629177247\n",
      "Iteration: 6258000, loss: 0.10692687034136361, gradient norm: 0.4273752926817265\n",
      "Iteration: 6259000, loss: 0.10692662898570306, gradient norm: 0.5300576934299936\n",
      "Iteration: 6260000, loss: 0.10692650121859035, gradient norm: 0.22037577953013812\n",
      "Iteration: 6261000, loss: 0.10692593422568265, gradient norm: 0.061207698983993475\n",
      "Iteration: 6262000, loss: 0.10692602731232012, gradient norm: 0.48336263781926797\n",
      "Iteration: 6263000, loss: 0.10692531597153103, gradient norm: 0.02660842778815078\n",
      "Iteration: 6264000, loss: 0.106925339702559, gradient norm: 0.24987161712531206\n",
      "Iteration: 6265000, loss: 0.10692502951146736, gradient norm: 0.7059752827432751\n",
      "Iteration: 6266000, loss: 0.10692463245659427, gradient norm: 0.07784520229206367\n",
      "Iteration: 6267000, loss: 0.10692438136772844, gradient norm: 0.2709077456612672\n",
      "Iteration: 6268000, loss: 0.10692407526616933, gradient norm: 0.5080910139806605\n",
      "Iteration: 6269000, loss: 0.10692378988553118, gradient norm: 0.23980782475504236\n",
      "Iteration: 6270000, loss: 0.10692374872412826, gradient norm: 0.3370465454586601\n",
      "Iteration: 6271000, loss: 0.10692316830247296, gradient norm: 0.36253718437956484\n",
      "Iteration: 6272000, loss: 0.10692306353049635, gradient norm: 0.17037537777271472\n",
      "Iteration: 6273000, loss: 0.10692272326588237, gradient norm: 0.0223723731682297\n",
      "Iteration: 6274000, loss: 0.10692243911064044, gradient norm: 0.28739444083368987\n",
      "Iteration: 6275000, loss: 0.10692220323682607, gradient norm: 0.5282764123221421\n",
      "Iteration: 6276000, loss: 0.10692208394982537, gradient norm: 0.21742760356149385\n",
      "Iteration: 6277000, loss: 0.10692148192501906, gradient norm: 0.16842549507483104\n",
      "Iteration: 6278000, loss: 0.10692140196440805, gradient norm: 0.22013624143745028\n",
      "Iteration: 6279000, loss: 0.106921097600126, gradient norm: 0.2569129294555489\n",
      "Iteration: 6280000, loss: 0.106920788696529, gradient norm: 0.5303637567169367\n",
      "Iteration: 6281000, loss: 0.10692051394027374, gradient norm: 0.05699765806027699\n",
      "Iteration: 6282000, loss: 0.10692023653896146, gradient norm: 0.09139343230731588\n",
      "Iteration: 6283000, loss: 0.10691999145818766, gradient norm: 0.10841036578550028\n",
      "Iteration: 6284000, loss: 0.1069196407125567, gradient norm: 0.05076197565753268\n",
      "Iteration: 6285000, loss: 0.10691945825386238, gradient norm: 0.07988873840684213\n",
      "Iteration: 6286000, loss: 0.10691913988831851, gradient norm: 0.04882492808996412\n",
      "Iteration: 6287000, loss: 0.10691887604828033, gradient norm: 0.2815810339381532\n",
      "Iteration: 6288000, loss: 0.10691851170449601, gradient norm: 0.20994709386520244\n",
      "Iteration: 6289000, loss: 0.10691835333674331, gradient norm: 0.18823004448209044\n",
      "Iteration: 6290000, loss: 0.1069180359057167, gradient norm: 0.34591531285128146\n",
      "Iteration: 6291000, loss: 0.10691787918563256, gradient norm: 0.029270072593227715\n",
      "Iteration: 6292000, loss: 0.10691744436405132, gradient norm: 0.19062791881874608\n",
      "Iteration: 6293000, loss: 0.10691728503701314, gradient norm: 0.01945811873283328\n",
      "Iteration: 6294000, loss: 0.10691696923116119, gradient norm: 0.08808797647028989\n",
      "Iteration: 6295000, loss: 0.10691679953897387, gradient norm: 0.26213691682504964\n",
      "Iteration: 6296000, loss: 0.10691629983781603, gradient norm: 0.20141404980561275\n",
      "Iteration: 6297000, loss: 0.10691614968543721, gradient norm: 0.49944699451741503\n",
      "Iteration: 6298000, loss: 0.1069158679124663, gradient norm: 0.15352905681498516\n",
      "Iteration: 6299000, loss: 0.10691567717485032, gradient norm: 0.09702695010678018\n",
      "Iteration: 6300000, loss: 0.10691533072240088, gradient norm: 0.19321012757439282\n",
      "Iteration: 6301000, loss: 0.1069149574323912, gradient norm: 0.20851286364055105\n",
      "Iteration: 6302000, loss: 0.10691468985063424, gradient norm: 0.1517137566199679\n",
      "Iteration: 6303000, loss: 0.10691450510818737, gradient norm: 0.02511126003816392\n",
      "Iteration: 6304000, loss: 0.10691425025812262, gradient norm: 0.06182430262196555\n",
      "Iteration: 6305000, loss: 0.10691385539291764, gradient norm: 0.1895983439405783\n",
      "Iteration: 6306000, loss: 0.10691366236058346, gradient norm: 0.21774343106917313\n",
      "Iteration: 6307000, loss: 0.10691341413391635, gradient norm: 0.20083311050076255\n",
      "Iteration: 6308000, loss: 0.1069131107406104, gradient norm: 0.6462771992306947\n",
      "Iteration: 6309000, loss: 0.10691277944483506, gradient norm: 0.11748050490627955\n",
      "Iteration: 6310000, loss: 0.10691244228243994, gradient norm: 0.2949107269924143\n",
      "Iteration: 6311000, loss: 0.10691229900034949, gradient norm: 0.09543014087249407\n",
      "Iteration: 6312000, loss: 0.10691192617768645, gradient norm: 0.24419231532877056\n",
      "Iteration: 6313000, loss: 0.10691195779225228, gradient norm: 0.261669922936031\n",
      "Iteration: 6314000, loss: 0.10691141440600728, gradient norm: 0.03430531630281073\n",
      "Iteration: 6315000, loss: 0.10691123121637183, gradient norm: 0.5097644697172693\n",
      "Iteration: 6316000, loss: 0.10691080796302185, gradient norm: 0.30913276054802413\n",
      "Iteration: 6317000, loss: 0.10691056772675159, gradient norm: 0.08847293055032034\n",
      "Iteration: 6318000, loss: 0.10691041217947536, gradient norm: 0.11860873885708562\n",
      "Iteration: 6319000, loss: 0.10691011836569891, gradient norm: 0.047935246323019494\n",
      "Iteration: 6320000, loss: 0.10690997779928497, gradient norm: 0.5068636498103438\n",
      "Iteration: 6321000, loss: 0.10690942088565049, gradient norm: 0.35613592976444725\n",
      "Iteration: 6322000, loss: 0.1069092888218248, gradient norm: 0.19150292208394187\n",
      "Iteration: 6323000, loss: 0.10690912493146046, gradient norm: 0.6408020852982416\n",
      "Iteration: 6324000, loss: 0.10690863021905823, gradient norm: 0.12928186862777746\n",
      "Iteration: 6325000, loss: 0.10690851166384484, gradient norm: 0.5297967329919683\n",
      "Iteration: 6326000, loss: 0.10690811965119923, gradient norm: 0.6120082216428789\n",
      "Iteration: 6327000, loss: 0.10690817505163955, gradient norm: 0.2813921448387582\n",
      "Iteration: 6328000, loss: 0.10690744930044453, gradient norm: 0.15650453104761308\n",
      "Iteration: 6329000, loss: 0.10690738551474928, gradient norm: 0.1676603057981606\n",
      "Iteration: 6330000, loss: 0.1069071687746485, gradient norm: 0.1300946099791286\n",
      "Iteration: 6331000, loss: 0.10690673027995284, gradient norm: 0.06974871947009463\n",
      "Iteration: 6332000, loss: 0.1069066177096727, gradient norm: 0.22436132457935154\n",
      "Iteration: 6333000, loss: 0.10690635851917689, gradient norm: 0.15017596622796892\n",
      "Iteration: 6334000, loss: 0.10690582212492843, gradient norm: 0.08422716967290159\n",
      "Iteration: 6335000, loss: 0.10690586601057608, gradient norm: 0.31344649263310354\n",
      "Iteration: 6336000, loss: 0.10690542355108405, gradient norm: 0.7031612469869027\n",
      "Iteration: 6337000, loss: 0.10690518086807378, gradient norm: 0.09145535950116891\n",
      "Iteration: 6338000, loss: 0.10690482131118682, gradient norm: 0.5344216261018485\n",
      "Iteration: 6339000, loss: 0.10690474176324932, gradient norm: 0.5674286457398986\n",
      "Iteration: 6340000, loss: 0.10690439513418085, gradient norm: 0.14541450336381193\n",
      "Iteration: 6341000, loss: 0.10690418023884461, gradient norm: 0.48814435512678345\n",
      "Iteration: 6342000, loss: 0.10690364315747769, gradient norm: 0.5301490473753009\n",
      "Iteration: 6343000, loss: 0.10690356480739982, gradient norm: 0.06364125636054523\n",
      "Iteration: 6344000, loss: 0.1069033540078539, gradient norm: 0.6198076547636668\n",
      "Iteration: 6345000, loss: 0.10690307555407548, gradient norm: 0.4609495390220062\n",
      "Iteration: 6346000, loss: 0.10690272083339358, gradient norm: 0.18524762978725162\n",
      "Iteration: 6347000, loss: 0.10690250179782768, gradient norm: 0.09994197078031088\n",
      "Iteration: 6348000, loss: 0.10690217275628808, gradient norm: 0.2230172869689232\n",
      "Iteration: 6349000, loss: 0.10690196829720798, gradient norm: 0.08421317782098488\n",
      "Iteration: 6350000, loss: 0.10690155759803051, gradient norm: 0.03714876548368326\n",
      "Iteration: 6351000, loss: 0.10690141106312062, gradient norm: 0.4626658881392853\n",
      "Iteration: 6352000, loss: 0.10690102188539115, gradient norm: 0.16394432176779378\n",
      "Iteration: 6353000, loss: 0.10690089134802597, gradient norm: 0.11205424965860503\n",
      "Iteration: 6354000, loss: 0.10690052131530962, gradient norm: 0.4200368710143753\n",
      "Iteration: 6355000, loss: 0.10690034261892878, gradient norm: 0.422029246933455\n",
      "Iteration: 6356000, loss: 0.10690015592722499, gradient norm: 0.23667216653303783\n",
      "Iteration: 6357000, loss: 0.10689967254991664, gradient norm: 0.36216120702331767\n",
      "Iteration: 6358000, loss: 0.10689941172111651, gradient norm: 0.16557559422038967\n",
      "Iteration: 6359000, loss: 0.10689928226328835, gradient norm: 0.14606240250042343\n",
      "Iteration: 6360000, loss: 0.1068988523036036, gradient norm: 0.1826634089608032\n",
      "Iteration: 6361000, loss: 0.1068987936595201, gradient norm: 0.2881805971639212\n",
      "Iteration: 6362000, loss: 0.10689841727529274, gradient norm: 0.02157853981825093\n",
      "Iteration: 6363000, loss: 0.10689811761336038, gradient norm: 0.12603512039482273\n",
      "Iteration: 6364000, loss: 0.10689783080384464, gradient norm: 0.42822986669992313\n",
      "Iteration: 6365000, loss: 0.10689777615393001, gradient norm: 0.31905837333105364\n",
      "Iteration: 6366000, loss: 0.10689727296950803, gradient norm: 0.22097988650553038\n",
      "Iteration: 6367000, loss: 0.10689710495230353, gradient norm: 0.24716186220803582\n",
      "Iteration: 6368000, loss: 0.10689672050049963, gradient norm: 0.1382894172571671\n",
      "Iteration: 6369000, loss: 0.10689653416902659, gradient norm: 0.06397996149096828\n",
      "Iteration: 6370000, loss: 0.10689628898616946, gradient norm: 0.14860810019896278\n",
      "Iteration: 6371000, loss: 0.10689606233130543, gradient norm: 0.08488640991778391\n",
      "Iteration: 6372000, loss: 0.106895639883107, gradient norm: 0.1426257122145282\n",
      "Iteration: 6373000, loss: 0.10689539964553174, gradient norm: 0.5508074677678504\n",
      "Iteration: 6374000, loss: 0.10689532168591447, gradient norm: 0.21125853876340106\n",
      "Iteration: 6375000, loss: 0.10689484756937238, gradient norm: 0.2000030502328223\n",
      "Iteration: 6376000, loss: 0.10689459156759028, gradient norm: 0.17608885122955314\n",
      "Iteration: 6377000, loss: 0.10689441441003969, gradient norm: 0.5729120018124134\n",
      "Iteration: 6378000, loss: 0.10689416078283523, gradient norm: 0.190695422802358\n",
      "Iteration: 6379000, loss: 0.10689372262531367, gradient norm: 0.49751442229549153\n",
      "Iteration: 6380000, loss: 0.10689353987717908, gradient norm: 0.11068940275159643\n",
      "Iteration: 6381000, loss: 0.10689343213318829, gradient norm: 0.29602243546420287\n",
      "Iteration: 6382000, loss: 0.10689295526661757, gradient norm: 0.14534089799489525\n",
      "Iteration: 6383000, loss: 0.1068928317310071, gradient norm: 0.2373724218605708\n",
      "Iteration: 6384000, loss: 0.10689236195130826, gradient norm: 0.322741757335058\n",
      "Iteration: 6385000, loss: 0.1068923307300175, gradient norm: 0.6323182046489596\n",
      "Iteration: 6386000, loss: 0.10689200916050937, gradient norm: 0.15546480815518982\n",
      "Iteration: 6387000, loss: 0.10689161174030622, gradient norm: 0.09500015921064173\n",
      "Iteration: 6388000, loss: 0.10689144089160865, gradient norm: 0.23061537195957924\n",
      "Iteration: 6389000, loss: 0.10689116115986386, gradient norm: 0.08663979202552624\n",
      "Iteration: 6390000, loss: 0.10689079085878221, gradient norm: 0.12204288740839878\n",
      "Iteration: 6391000, loss: 0.10689074206529964, gradient norm: 0.7685178287648183\n",
      "Iteration: 6392000, loss: 0.10689020325507044, gradient norm: 0.09207076921576453\n",
      "Iteration: 6393000, loss: 0.10689015863614276, gradient norm: 0.22511139417272388\n",
      "Iteration: 6394000, loss: 0.10688982913526363, gradient norm: 0.1332844224261554\n",
      "Iteration: 6395000, loss: 0.10688952154825293, gradient norm: 0.21563376685719376\n",
      "Iteration: 6396000, loss: 0.1068892086893108, gradient norm: 0.19579251941198167\n",
      "Iteration: 6397000, loss: 0.10688894557636495, gradient norm: 0.3255428152566603\n",
      "Iteration: 6398000, loss: 0.10688883962370208, gradient norm: 0.741647318200721\n",
      "Iteration: 6399000, loss: 0.10688844396711641, gradient norm: 0.01949652545137736\n",
      "Iteration: 6400000, loss: 0.1068882302391579, gradient norm: 0.040604523751649316\n",
      "Iteration: 6401000, loss: 0.10688803097110826, gradient norm: 0.3896712410012184\n",
      "Iteration: 6402000, loss: 0.10688762584187009, gradient norm: 0.4326846588171894\n",
      "Iteration: 6403000, loss: 0.10688725570835653, gradient norm: 0.05941997131561778\n",
      "Iteration: 6404000, loss: 0.1068872572401146, gradient norm: 0.23230387521551435\n",
      "Iteration: 6405000, loss: 0.10688673733976889, gradient norm: 0.1105520142382764\n",
      "Iteration: 6406000, loss: 0.10688666728142429, gradient norm: 0.21066490455816325\n",
      "Iteration: 6407000, loss: 0.10688635293713908, gradient norm: 0.44693395762078864\n",
      "Iteration: 6408000, loss: 0.10688603680250507, gradient norm: 0.27681648883548143\n",
      "Iteration: 6409000, loss: 0.10688600869466579, gradient norm: 0.20092059143119295\n",
      "Iteration: 6410000, loss: 0.10688530619964376, gradient norm: 0.09214308985051782\n",
      "Iteration: 6411000, loss: 0.10688519878561668, gradient norm: 0.09587101228081565\n",
      "Iteration: 6412000, loss: 0.1068850589306318, gradient norm: 0.036853673226102675\n",
      "Iteration: 6413000, loss: 0.10688477771701078, gradient norm: 0.11671271675340181\n",
      "Iteration: 6414000, loss: 0.10688434620003648, gradient norm: 0.13852603391390184\n",
      "Iteration: 6415000, loss: 0.10688430511375184, gradient norm: 0.7717348642548612\n",
      "Iteration: 6416000, loss: 0.10688392124480954, gradient norm: 0.17375342904802957\n",
      "Iteration: 6417000, loss: 0.10688364219111698, gradient norm: 0.20221951696550336\n",
      "Iteration: 6418000, loss: 0.10688348728453695, gradient norm: 0.2731752232812527\n",
      "Iteration: 6419000, loss: 0.10688305502838094, gradient norm: 0.2445353983639034\n",
      "Iteration: 6420000, loss: 0.10688288043528141, gradient norm: 0.041000434908082264\n",
      "Iteration: 6421000, loss: 0.10688263543205369, gradient norm: 0.30101994585071423\n",
      "Iteration: 6422000, loss: 0.10688231859912826, gradient norm: 0.24803888471678115\n",
      "Iteration: 6423000, loss: 0.10688198107431134, gradient norm: 0.4933366659610485\n",
      "Iteration: 6424000, loss: 0.10688180955034943, gradient norm: 0.645979728463414\n",
      "Iteration: 6425000, loss: 0.10688163007972586, gradient norm: 0.12819077801777687\n",
      "Iteration: 6426000, loss: 0.10688136586435208, gradient norm: 0.21439818613275663\n",
      "Iteration: 6427000, loss: 0.10688088071517626, gradient norm: 0.035300742725917905\n",
      "Iteration: 6428000, loss: 0.10688078320544142, gradient norm: 0.05378519193588021\n",
      "Iteration: 6429000, loss: 0.10688056941522363, gradient norm: 0.12359590115455396\n",
      "Iteration: 6430000, loss: 0.10688005924623672, gradient norm: 0.5258736941188391\n",
      "Iteration: 6431000, loss: 0.10687991040879344, gradient norm: 0.1080003516803052\n",
      "Iteration: 6432000, loss: 0.10687972114000119, gradient norm: 0.07481363864300872\n",
      "Iteration: 6433000, loss: 0.10687943960402663, gradient norm: 0.20874091563512293\n",
      "Iteration: 6434000, loss: 0.10687910978432369, gradient norm: 0.18623150468050692\n",
      "Iteration: 6435000, loss: 0.10687893836011267, gradient norm: 0.023734664661140988\n",
      "Iteration: 6436000, loss: 0.10687849828348128, gradient norm: 0.3414717723110252\n",
      "Iteration: 6437000, loss: 0.1068783947090751, gradient norm: 0.07209073218916295\n",
      "Iteration: 6438000, loss: 0.10687809179016444, gradient norm: 0.07755760779534365\n",
      "Iteration: 6439000, loss: 0.10687787416234033, gradient norm: 0.0273506783637665\n",
      "Iteration: 6440000, loss: 0.1068775922373392, gradient norm: 0.028238075836599884\n",
      "Iteration: 6441000, loss: 0.10687743564992504, gradient norm: 0.3021987839960555\n",
      "Iteration: 6442000, loss: 0.10687709544373693, gradient norm: 0.02903501298065977\n",
      "Iteration: 6443000, loss: 0.1068766082168344, gradient norm: 0.05082182498999978\n",
      "Iteration: 6444000, loss: 0.10687654543204143, gradient norm: 0.02495481788976214\n",
      "Iteration: 6445000, loss: 0.10687615790542052, gradient norm: 0.21669644126747048\n",
      "Iteration: 6446000, loss: 0.10687596034198489, gradient norm: 0.1454070443212988\n",
      "Iteration: 6447000, loss: 0.10687569667576188, gradient norm: 0.2095999363792197\n",
      "Iteration: 6448000, loss: 0.10687555493897659, gradient norm: 0.0951473661448651\n",
      "Iteration: 6449000, loss: 0.10687506298140036, gradient norm: 0.274895692458777\n",
      "Iteration: 6450000, loss: 0.1068749757539303, gradient norm: 0.690881727716514\n",
      "Iteration: 6451000, loss: 0.10687455828892382, gradient norm: 0.6479595456387349\n",
      "Iteration: 6452000, loss: 0.10687446558521674, gradient norm: 0.4966526979673639\n",
      "Iteration: 6453000, loss: 0.10687417557018057, gradient norm: 0.1682222326128297\n",
      "Iteration: 6454000, loss: 0.10687391098396981, gradient norm: 0.3117693907850007\n",
      "Iteration: 6455000, loss: 0.10687356627979713, gradient norm: 0.4302075951862152\n",
      "Iteration: 6456000, loss: 0.10687332864939184, gradient norm: 0.22650669449343921\n",
      "Iteration: 6457000, loss: 0.10687311201597441, gradient norm: 0.3029191945347163\n",
      "Iteration: 6458000, loss: 0.10687285547653719, gradient norm: 0.07979705282459412\n",
      "Iteration: 6459000, loss: 0.10687245254446584, gradient norm: 0.13815014754341076\n",
      "Iteration: 6460000, loss: 0.10687224639881447, gradient norm: 0.32997188090027546\n",
      "Iteration: 6461000, loss: 0.10687206833865762, gradient norm: 0.2550727071292824\n",
      "Iteration: 6462000, loss: 0.1068717198258358, gradient norm: 0.20261840584316326\n",
      "Iteration: 6463000, loss: 0.10687153221330303, gradient norm: 0.21093131747022914\n",
      "Iteration: 6464000, loss: 0.10687131578047064, gradient norm: 0.7792512916054171\n",
      "Iteration: 6465000, loss: 0.10687093834622573, gradient norm: 0.5596457075193042\n",
      "Iteration: 6466000, loss: 0.10687069205692248, gradient norm: 0.005432699673163966\n",
      "Iteration: 6467000, loss: 0.10687036270268228, gradient norm: 0.10077012084304732\n",
      "Iteration: 6468000, loss: 0.10687021265470384, gradient norm: 0.04725077742119459\n",
      "Iteration: 6469000, loss: 0.10687022338768452, gradient norm: 0.2067568572370452\n",
      "Iteration: 6470000, loss: 0.10686938354861329, gradient norm: 0.07981701251372818\n",
      "Iteration: 6471000, loss: 0.10686938751611373, gradient norm: 0.17415896022735558\n",
      "Iteration: 6472000, loss: 0.10686916922730953, gradient norm: 0.12800922108773558\n",
      "Iteration: 6473000, loss: 0.10686881044980097, gradient norm: 0.3078152527703392\n",
      "Iteration: 6474000, loss: 0.10686861635897471, gradient norm: 0.0793593731696018\n",
      "Iteration: 6475000, loss: 0.10686831954429218, gradient norm: 0.3077806903839382\n",
      "Iteration: 6476000, loss: 0.10686817972701622, gradient norm: 0.16164948879102597\n",
      "Iteration: 6477000, loss: 0.10686784082160775, gradient norm: 0.2762633000426156\n",
      "Iteration: 6478000, loss: 0.10686765493615703, gradient norm: 0.5562739101208624\n",
      "Iteration: 6479000, loss: 0.10686709745135964, gradient norm: 0.048925887539548435\n",
      "Iteration: 6480000, loss: 0.10686710522554428, gradient norm: 0.4857191282211182\n",
      "Iteration: 6481000, loss: 0.10686668470338637, gradient norm: 0.17864632024209873\n",
      "Iteration: 6482000, loss: 0.10686647706335166, gradient norm: 0.13237194824349535\n",
      "Iteration: 6483000, loss: 0.10686627564092936, gradient norm: 0.07362946078278818\n",
      "Iteration: 6484000, loss: 0.10686598522079103, gradient norm: 0.10686906931908217\n",
      "Iteration: 6485000, loss: 0.10686570148251925, gradient norm: 0.061311765539294544\n",
      "Iteration: 6486000, loss: 0.10686544147236988, gradient norm: 0.535521087882253\n",
      "Iteration: 6487000, loss: 0.1068651420843702, gradient norm: 0.2912891047987264\n",
      "Iteration: 6488000, loss: 0.10686498392615072, gradient norm: 0.29260829893695\n",
      "Iteration: 6489000, loss: 0.10686480558241712, gradient norm: 0.34237460550061133\n",
      "Iteration: 6490000, loss: 0.10686426691035837, gradient norm: 0.18358632489516116\n",
      "Iteration: 6491000, loss: 0.10686407102978186, gradient norm: 0.1855251918797378\n",
      "Iteration: 6492000, loss: 0.10686397720110972, gradient norm: 0.30127802257560754\n",
      "Iteration: 6493000, loss: 0.10686353512405906, gradient norm: 0.6219076579597723\n",
      "Iteration: 6494000, loss: 0.10686336611081648, gradient norm: 0.18517471291790358\n",
      "Iteration: 6495000, loss: 0.10686319631588444, gradient norm: 0.17862133969938254\n",
      "Iteration: 6496000, loss: 0.10686268261861834, gradient norm: 0.6317023300247601\n",
      "Iteration: 6497000, loss: 0.10686275294795744, gradient norm: 0.522631244264092\n",
      "Iteration: 6498000, loss: 0.10686228277385422, gradient norm: 0.10491649410347889\n",
      "Iteration: 6499000, loss: 0.10686195160988388, gradient norm: 0.06946358180632407\n",
      "Iteration: 6500000, loss: 0.10686192799303537, gradient norm: 0.08081180045982878\n",
      "Iteration: 6501000, loss: 0.1068613839881998, gradient norm: 0.18982104902250574\n",
      "Iteration: 6502000, loss: 0.10686143713653343, gradient norm: 0.36409225609863405\n",
      "Iteration: 6503000, loss: 0.10686105512836733, gradient norm: 0.2711920659964998\n",
      "Iteration: 6504000, loss: 0.10686071702942516, gradient norm: 0.580733685944514\n",
      "Iteration: 6505000, loss: 0.10686056760430282, gradient norm: 0.21260795836977675\n",
      "Iteration: 6506000, loss: 0.1068600634401294, gradient norm: 0.16942120129688001\n",
      "Iteration: 6507000, loss: 0.10686016823936151, gradient norm: 0.09347267101588064\n",
      "Iteration: 6508000, loss: 0.10685960984753057, gradient norm: 0.10077722797389439\n",
      "Iteration: 6509000, loss: 0.10685942065452604, gradient norm: 0.07126566756279484\n",
      "Iteration: 6510000, loss: 0.10685917202302378, gradient norm: 0.8087181576287917\n",
      "Iteration: 6511000, loss: 0.10685892333638779, gradient norm: 0.4990305718281591\n",
      "Iteration: 6512000, loss: 0.1068587651018044, gradient norm: 0.380451436112353\n",
      "Iteration: 6513000, loss: 0.10685834736885243, gradient norm: 0.10893003142628233\n",
      "Iteration: 6514000, loss: 0.10685816452593755, gradient norm: 0.23169332758036643\n",
      "Iteration: 6515000, loss: 0.10685795064485902, gradient norm: 0.2340995257519508\n",
      "Iteration: 6516000, loss: 0.10685749206469522, gradient norm: 0.6757704615269977\n",
      "Iteration: 6517000, loss: 0.10685762659553333, gradient norm: 0.5525264069157153\n",
      "Iteration: 6518000, loss: 0.1068569437630276, gradient norm: 0.6566006357985515\n",
      "Iteration: 6519000, loss: 0.10685682154976141, gradient norm: 0.08269260106527274\n",
      "Iteration: 6520000, loss: 0.10685654762787865, gradient norm: 0.08230191994169316\n",
      "Iteration: 6521000, loss: 0.10685637935711594, gradient norm: 0.11592410246358839\n",
      "Iteration: 6522000, loss: 0.10685611775391765, gradient norm: 0.6627224315847687\n",
      "Iteration: 6523000, loss: 0.10685585208631154, gradient norm: 0.07434860163394158\n",
      "Iteration: 6524000, loss: 0.1068554867702163, gradient norm: 0.14156114101157175\n",
      "Iteration: 6525000, loss: 0.10685529837156306, gradient norm: 0.5864498773725103\n",
      "Iteration: 6526000, loss: 0.10685507986502234, gradient norm: 0.14201675197718935\n",
      "Iteration: 6527000, loss: 0.10685465151139349, gradient norm: 0.08656191995900271\n",
      "Iteration: 6528000, loss: 0.10685473514991715, gradient norm: 0.2726155630220489\n",
      "Iteration: 6529000, loss: 0.10685416379674904, gradient norm: 0.4352491228710262\n",
      "Iteration: 6530000, loss: 0.10685389207441626, gradient norm: 0.27546509173525735\n",
      "Iteration: 6531000, loss: 0.10685386420880147, gradient norm: 0.23267929289710976\n",
      "Iteration: 6532000, loss: 0.10685347250700186, gradient norm: 0.07813913360267469\n",
      "Iteration: 6533000, loss: 0.10685310586685895, gradient norm: 0.14372995692935608\n",
      "Iteration: 6534000, loss: 0.10685308182380059, gradient norm: 0.09774378553147588\n",
      "Iteration: 6535000, loss: 0.10685272593108419, gradient norm: 0.19993260686362418\n",
      "Iteration: 6536000, loss: 0.1068524340070783, gradient norm: 0.03759009126920202\n",
      "Iteration: 6537000, loss: 0.10685216528250199, gradient norm: 0.3974440592279607\n",
      "Iteration: 6538000, loss: 0.10685186105451361, gradient norm: 0.4863870282106147\n",
      "Iteration: 6539000, loss: 0.10685175940554105, gradient norm: 0.05787754084805845\n",
      "Iteration: 6540000, loss: 0.1068513703482307, gradient norm: 0.24084639451246612\n",
      "Iteration: 6541000, loss: 0.10685115724828881, gradient norm: 0.23860845059847038\n",
      "Iteration: 6542000, loss: 0.10685089272939495, gradient norm: 0.36260545276664374\n",
      "Iteration: 6543000, loss: 0.10685067671532954, gradient norm: 0.7512622759986699\n",
      "Iteration: 6544000, loss: 0.10685041099591483, gradient norm: 0.20708079056544287\n",
      "Iteration: 6545000, loss: 0.10685010473722997, gradient norm: 0.6585546651224218\n",
      "Iteration: 6546000, loss: 0.10684983700157126, gradient norm: 0.5698445298820594\n",
      "Iteration: 6547000, loss: 0.10684959710968334, gradient norm: 0.07179100365133517\n",
      "Iteration: 6548000, loss: 0.10684926497522923, gradient norm: 0.27960737426789267\n",
      "Iteration: 6549000, loss: 0.10684912401687614, gradient norm: 0.1551281721225764\n",
      "Iteration: 6550000, loss: 0.10684889841286625, gradient norm: 0.5567733147954562\n",
      "Iteration: 6551000, loss: 0.1068485590682121, gradient norm: 0.5124053562508447\n",
      "Iteration: 6552000, loss: 0.10684818056519484, gradient norm: 0.07164741925411564\n",
      "Iteration: 6553000, loss: 0.10684811057383857, gradient norm: 0.46533508405357066\n",
      "Iteration: 6554000, loss: 0.10684786889636749, gradient norm: 0.34717547582528974\n",
      "Iteration: 6555000, loss: 0.1068475042683603, gradient norm: 0.20897958636580644\n",
      "Iteration: 6556000, loss: 0.1068473381868377, gradient norm: 0.1940158386424172\n",
      "Iteration: 6557000, loss: 0.10684697760250136, gradient norm: 0.15796161243389586\n",
      "Iteration: 6558000, loss: 0.10684674524864433, gradient norm: 0.17606436765257422\n",
      "Iteration: 6559000, loss: 0.10684656942595586, gradient norm: 0.18191671878940194\n",
      "Iteration: 6560000, loss: 0.10684625645089972, gradient norm: 0.07296986095885956\n",
      "Iteration: 6561000, loss: 0.10684591551940097, gradient norm: 0.13768001955946801\n",
      "Iteration: 6562000, loss: 0.10684582274662495, gradient norm: 0.1696941198217297\n",
      "Iteration: 6563000, loss: 0.10684546166112292, gradient norm: 0.11610660065077603\n",
      "Iteration: 6564000, loss: 0.10684524716247705, gradient norm: 0.12688016060619312\n",
      "Iteration: 6565000, loss: 0.10684501027253035, gradient norm: 0.234686754024044\n",
      "Iteration: 6566000, loss: 0.1068447103946516, gradient norm: 0.6557485995563317\n",
      "Iteration: 6567000, loss: 0.1068445540613608, gradient norm: 0.05583914197733397\n",
      "Iteration: 6568000, loss: 0.10684421449425525, gradient norm: 0.2475441573344973\n",
      "Iteration: 6569000, loss: 0.10684385379471312, gradient norm: 0.10489993441768378\n",
      "Iteration: 6570000, loss: 0.10684387667781511, gradient norm: 0.5259823943177588\n",
      "Iteration: 6571000, loss: 0.10684338141259234, gradient norm: 0.11389568006072368\n",
      "Iteration: 6572000, loss: 0.10684316172134682, gradient norm: 0.1894368478173354\n",
      "Iteration: 6573000, loss: 0.10684294356059712, gradient norm: 0.1302616005167061\n",
      "Iteration: 6574000, loss: 0.106842823325007, gradient norm: 0.15339071875918353\n",
      "Iteration: 6575000, loss: 0.10684223333495885, gradient norm: 0.044462609511831404\n",
      "Iteration: 6576000, loss: 0.10684205377764013, gradient norm: 0.22758071191818982\n",
      "Iteration: 6577000, loss: 0.10684204906933477, gradient norm: 0.1376736466897792\n",
      "Iteration: 6578000, loss: 0.10684154023693104, gradient norm: 0.4345810589797355\n",
      "Iteration: 6579000, loss: 0.10684135036321195, gradient norm: 0.392555453797336\n",
      "Iteration: 6580000, loss: 0.10684127419351043, gradient norm: 0.09401544332662436\n",
      "Iteration: 6581000, loss: 0.10684102924279794, gradient norm: 0.03946893436137237\n",
      "Iteration: 6582000, loss: 0.10684044737896778, gradient norm: 0.25664183041390104\n",
      "Iteration: 6583000, loss: 0.10684040646314583, gradient norm: 0.19262359635451912\n",
      "Iteration: 6584000, loss: 0.10684012468285578, gradient norm: 0.5685937862713257\n",
      "Iteration: 6585000, loss: 0.1068398900661797, gradient norm: 0.40790857252449736\n",
      "Iteration: 6586000, loss: 0.10683950217184533, gradient norm: 0.24999292496357053\n",
      "Iteration: 6587000, loss: 0.10683934975692969, gradient norm: 0.1478663481946588\n",
      "Iteration: 6588000, loss: 0.10683915687643201, gradient norm: 0.2700387373584292\n",
      "Iteration: 6589000, loss: 0.10683873061641315, gradient norm: 0.14618964568157408\n",
      "Iteration: 6590000, loss: 0.10683858889733042, gradient norm: 0.2190085462862613\n",
      "Iteration: 6591000, loss: 0.10683831843200384, gradient norm: 0.0770638727834322\n",
      "Iteration: 6592000, loss: 0.10683818845702613, gradient norm: 0.11069425339740092\n",
      "Iteration: 6593000, loss: 0.10683776526946767, gradient norm: 0.41064634519176874\n",
      "Iteration: 6594000, loss: 0.10683752549183172, gradient norm: 0.37108717968196886\n",
      "Iteration: 6595000, loss: 0.10683726843501, gradient norm: 0.5892434947088674\n",
      "Iteration: 6596000, loss: 0.10683717459180511, gradient norm: 0.8536517429207097\n",
      "Iteration: 6597000, loss: 0.10683674977598423, gradient norm: 0.6231865740729452\n",
      "Iteration: 6598000, loss: 0.10683655761373007, gradient norm: 0.449532396409333\n",
      "Iteration: 6599000, loss: 0.10683631292696379, gradient norm: 0.15456328029552593\n",
      "Iteration: 6600000, loss: 0.10683609123986432, gradient norm: 0.18304357015571796\n",
      "Iteration: 6601000, loss: 0.10683565683252202, gradient norm: 0.3359992146572965\n",
      "Iteration: 6602000, loss: 0.10683561988496289, gradient norm: 0.26419228591453214\n",
      "Iteration: 6603000, loss: 0.10683527270120419, gradient norm: 0.1902590428608954\n",
      "Iteration: 6604000, loss: 0.10683505956477739, gradient norm: 0.155996773452445\n",
      "Iteration: 6605000, loss: 0.10683469545327143, gradient norm: 0.4142122889914531\n",
      "Iteration: 6606000, loss: 0.10683452377864033, gradient norm: 0.03130765451299311\n",
      "Iteration: 6607000, loss: 0.1068342047041045, gradient norm: 0.10912124880069292\n",
      "Iteration: 6608000, loss: 0.10683399388592547, gradient norm: 0.0411392448756755\n",
      "Iteration: 6609000, loss: 0.10683384127989391, gradient norm: 0.22267413153110055\n",
      "Iteration: 6610000, loss: 0.10683350818686989, gradient norm: 0.4471102316407836\n",
      "Iteration: 6611000, loss: 0.10683318001349269, gradient norm: 0.4290034516272518\n",
      "Iteration: 6612000, loss: 0.10683305145204239, gradient norm: 0.18832741561926006\n",
      "Iteration: 6613000, loss: 0.10683278770582086, gradient norm: 0.31294485781668824\n",
      "Iteration: 6614000, loss: 0.10683238389134286, gradient norm: 0.10946946771503417\n",
      "Iteration: 6615000, loss: 0.10683221631083123, gradient norm: 0.13145452772935368\n",
      "Iteration: 6616000, loss: 0.10683192607255743, gradient norm: 0.04490871360506959\n",
      "Iteration: 6617000, loss: 0.10683179039644745, gradient norm: 0.48489380747846333\n",
      "Iteration: 6618000, loss: 0.10683131471143512, gradient norm: 0.24328165847611627\n",
      "Iteration: 6619000, loss: 0.10683137334207471, gradient norm: 0.2898881462282227\n",
      "Iteration: 6620000, loss: 0.10683102202117244, gradient norm: 0.3334619571273765\n",
      "Iteration: 6621000, loss: 0.10683058918337493, gradient norm: 0.14259847394775285\n",
      "Iteration: 6622000, loss: 0.10683045823513049, gradient norm: 0.06757293226514066\n",
      "Iteration: 6623000, loss: 0.10683019294131205, gradient norm: 0.6678870832120765\n",
      "Iteration: 6624000, loss: 0.1068298872547934, gradient norm: 0.11527889407754686\n",
      "Iteration: 6625000, loss: 0.10682971311902136, gradient norm: 0.6096469986798433\n",
      "Iteration: 6626000, loss: 0.10682953986802732, gradient norm: 0.011930505540193573\n",
      "Iteration: 6627000, loss: 0.10682913593525324, gradient norm: 0.06652999563218122\n",
      "Iteration: 6628000, loss: 0.10682905508591836, gradient norm: 0.6779076110098945\n",
      "Iteration: 6629000, loss: 0.10682857478906245, gradient norm: 0.05381666715184038\n",
      "Iteration: 6630000, loss: 0.10682837588633319, gradient norm: 0.6637582424406385\n",
      "Iteration: 6631000, loss: 0.10682811570654074, gradient norm: 0.7351152768222249\n",
      "Iteration: 6632000, loss: 0.10682802348481495, gradient norm: 0.043728901909565056\n",
      "Iteration: 6633000, loss: 0.10682760547224518, gradient norm: 0.20065559068733482\n",
      "Iteration: 6634000, loss: 0.10682752481213525, gradient norm: 0.21572354690728182\n",
      "Iteration: 6635000, loss: 0.10682707838935493, gradient norm: 0.04748121209686111\n",
      "Iteration: 6636000, loss: 0.10682694175852293, gradient norm: 0.16657923644940154\n",
      "Iteration: 6637000, loss: 0.10682666229981348, gradient norm: 0.14338187312045858\n",
      "Iteration: 6638000, loss: 0.10682648428290066, gradient norm: 0.4088564653934586\n",
      "Iteration: 6639000, loss: 0.10682611618068362, gradient norm: 0.4788182870641424\n",
      "Iteration: 6640000, loss: 0.10682591201303523, gradient norm: 0.030091319761040708\n",
      "Iteration: 6641000, loss: 0.10682562330656976, gradient norm: 0.851791015523416\n",
      "Iteration: 6642000, loss: 0.1068252877693689, gradient norm: 0.3610089820850794\n",
      "Iteration: 6643000, loss: 0.10682531240215641, gradient norm: 0.08644561564126868\n",
      "Iteration: 6644000, loss: 0.10682480003960801, gradient norm: 0.26429542816696666\n",
      "Iteration: 6645000, loss: 0.10682463510797297, gradient norm: 0.2135534604146016\n",
      "Iteration: 6646000, loss: 0.10682440623717818, gradient norm: 0.27690798506809994\n",
      "Iteration: 6647000, loss: 0.10682409511129967, gradient norm: 0.5923616477615099\n",
      "Iteration: 6648000, loss: 0.10682390908592362, gradient norm: 0.16355413161627008\n",
      "Iteration: 6649000, loss: 0.1068236518833675, gradient norm: 0.2945379637908754\n",
      "Iteration: 6650000, loss: 0.10682337729377774, gradient norm: 0.16318229791746217\n",
      "Iteration: 6651000, loss: 0.10682318020315643, gradient norm: 0.23850124183826266\n",
      "Iteration: 6652000, loss: 0.10682302198227467, gradient norm: 0.05592448215813967\n",
      "Iteration: 6653000, loss: 0.10682247131406433, gradient norm: 0.06363746326702799\n",
      "Iteration: 6654000, loss: 0.1068223633977442, gradient norm: 0.448320918514089\n",
      "Iteration: 6655000, loss: 0.10682211358037512, gradient norm: 0.1205367828884361\n",
      "Iteration: 6656000, loss: 0.10682188868834344, gradient norm: 0.36344770707446467\n",
      "Iteration: 6657000, loss: 0.10682162143701222, gradient norm: 0.16921174824578394\n",
      "Iteration: 6658000, loss: 0.10682130968734169, gradient norm: 0.20847111071538713\n",
      "Iteration: 6659000, loss: 0.10682116797074051, gradient norm: 0.08137736972896743\n",
      "Iteration: 6660000, loss: 0.10682107054869457, gradient norm: 0.3190153327396864\n",
      "Iteration: 6661000, loss: 0.10682054687979428, gradient norm: 0.043231689827693424\n",
      "Iteration: 6662000, loss: 0.10682023928806751, gradient norm: 0.05038517740866835\n",
      "Iteration: 6663000, loss: 0.10682030396496033, gradient norm: 0.20134648669445138\n",
      "Iteration: 6664000, loss: 0.10681984195652951, gradient norm: 0.2894798539371939\n",
      "Iteration: 6665000, loss: 0.10681954058033961, gradient norm: 0.04414117701398629\n",
      "Iteration: 6666000, loss: 0.10681932099690421, gradient norm: 0.07614562350808254\n",
      "Iteration: 6667000, loss: 0.10681913144374383, gradient norm: 0.666153498168464\n",
      "Iteration: 6668000, loss: 0.10681889135271413, gradient norm: 0.21508647698833525\n",
      "Iteration: 6669000, loss: 0.1068186919280094, gradient norm: 0.7073427505187746\n",
      "Iteration: 6670000, loss: 0.10681823509898761, gradient norm: 0.13293419067896287\n",
      "Iteration: 6671000, loss: 0.10681805669548582, gradient norm: 0.3525388562556275\n",
      "Iteration: 6672000, loss: 0.10681802400793981, gradient norm: 0.4713833548804435\n",
      "Iteration: 6673000, loss: 0.1068175194620851, gradient norm: 0.05700321555564804\n",
      "Iteration: 6674000, loss: 0.10681740432787774, gradient norm: 0.409053798996189\n",
      "Iteration: 6675000, loss: 0.10681713476234107, gradient norm: 0.19143154979854982\n",
      "Iteration: 6676000, loss: 0.1068169085211452, gradient norm: 0.10140661297121163\n",
      "Iteration: 6677000, loss: 0.10681652081619106, gradient norm: 0.1353727185411043\n",
      "Iteration: 6678000, loss: 0.10681646055434514, gradient norm: 0.25394950077540457\n",
      "Iteration: 6679000, loss: 0.10681610173554851, gradient norm: 0.06644495638455666\n",
      "Iteration: 6680000, loss: 0.10681589099729752, gradient norm: 0.147481780564698\n",
      "Iteration: 6681000, loss: 0.10681559203217234, gradient norm: 0.05094707211998514\n",
      "Iteration: 6682000, loss: 0.10681536068027839, gradient norm: 0.6647503041278975\n",
      "Iteration: 6683000, loss: 0.10681517129683396, gradient norm: 0.1005069582396783\n",
      "Iteration: 6684000, loss: 0.10681486921409553, gradient norm: 0.01894512083419877\n",
      "Iteration: 6685000, loss: 0.10681474267844479, gradient norm: 0.0700980532193026\n",
      "Iteration: 6686000, loss: 0.10681422024193106, gradient norm: 0.5155643914228758\n",
      "Iteration: 6687000, loss: 0.10681415337029807, gradient norm: 0.10683996991199143\n",
      "Iteration: 6688000, loss: 0.10681399594075287, gradient norm: 0.3224754752345533\n",
      "Iteration: 6689000, loss: 0.10681346861635022, gradient norm: 0.5057901460553953\n",
      "Iteration: 6690000, loss: 0.10681339299545571, gradient norm: 0.12174463107756288\n",
      "Iteration: 6691000, loss: 0.10681311209532716, gradient norm: 0.13620075273703058\n",
      "Iteration: 6692000, loss: 0.10681300664021193, gradient norm: 0.18630137087661106\n",
      "Iteration: 6693000, loss: 0.10681263841899172, gradient norm: 0.11190329310667357\n",
      "Iteration: 6694000, loss: 0.10681243161674504, gradient norm: 0.12831627604243315\n",
      "Iteration: 6695000, loss: 0.10681212157962473, gradient norm: 0.09664082219003581\n",
      "Iteration: 6696000, loss: 0.10681169172355302, gradient norm: 0.21385283488067344\n",
      "Iteration: 6697000, loss: 0.10681174398595455, gradient norm: 0.05037859565157416\n",
      "Iteration: 6698000, loss: 0.1068113734619512, gradient norm: 0.2070604445384721\n",
      "Iteration: 6699000, loss: 0.10681115538745285, gradient norm: 0.09639981940261234\n",
      "Iteration: 6700000, loss: 0.10681080889281369, gradient norm: 0.2936602703849108\n",
      "Iteration: 6701000, loss: 0.10681072998079051, gradient norm: 0.12073847369285509\n",
      "Iteration: 6702000, loss: 0.10681047087358018, gradient norm: 0.700709529909403\n",
      "Iteration: 6703000, loss: 0.10681015954161173, gradient norm: 0.8535052260978873\n",
      "Iteration: 6704000, loss: 0.10680981327228133, gradient norm: 0.027803956958416146\n",
      "Iteration: 6705000, loss: 0.10680965570723629, gradient norm: 0.4618021988246787\n",
      "Iteration: 6706000, loss: 0.10680936285257442, gradient norm: 0.20843358111668203\n",
      "Iteration: 6707000, loss: 0.10680918283578808, gradient norm: 0.22790557778402729\n",
      "Iteration: 6708000, loss: 0.10680887964165063, gradient norm: 0.34529286741366666\n",
      "Iteration: 6709000, loss: 0.10680869565991483, gradient norm: 0.25722185129090586\n",
      "Iteration: 6710000, loss: 0.10680849563844089, gradient norm: 0.19392554506456613\n",
      "Iteration: 6711000, loss: 0.1068080541928822, gradient norm: 0.040095927412194816\n",
      "Iteration: 6712000, loss: 0.10680794133160876, gradient norm: 0.9534516953017019\n",
      "Iteration: 6713000, loss: 0.10680760171707582, gradient norm: 0.6863808858754727\n",
      "Iteration: 6714000, loss: 0.10680761994983314, gradient norm: 0.682320404083013\n",
      "Iteration: 6715000, loss: 0.10680716525375912, gradient norm: 0.09250383324255552\n",
      "Iteration: 6716000, loss: 0.10680680949302492, gradient norm: 0.09501513574992186\n",
      "Iteration: 6717000, loss: 0.10680680357081988, gradient norm: 0.14671479797859044\n",
      "Iteration: 6718000, loss: 0.10680631279656103, gradient norm: 0.04627284768012161\n",
      "Iteration: 6719000, loss: 0.10680641591239207, gradient norm: 0.02725259503585389\n",
      "Iteration: 6720000, loss: 0.10680568049268312, gradient norm: 0.3741753076719868\n",
      "Iteration: 6721000, loss: 0.10680577737276892, gradient norm: 0.09150895011206893\n",
      "Iteration: 6722000, loss: 0.10680549137745238, gradient norm: 0.07846465922819602\n",
      "Iteration: 6723000, loss: 0.10680512240913072, gradient norm: 0.19746521570578554\n",
      "Iteration: 6724000, loss: 0.10680496931313363, gradient norm: 0.9016933622184388\n",
      "Iteration: 6725000, loss: 0.10680463444381212, gradient norm: 0.2256005234352733\n",
      "Iteration: 6726000, loss: 0.10680450208535698, gradient norm: 0.23704234012181888\n",
      "Iteration: 6727000, loss: 0.1068040888234492, gradient norm: 0.017826659119378505\n",
      "Iteration: 6728000, loss: 0.10680414011101855, gradient norm: 0.13870736281027432\n",
      "Iteration: 6729000, loss: 0.10680373215050985, gradient norm: 0.08307188355157043\n",
      "Iteration: 6730000, loss: 0.10680349016911421, gradient norm: 0.4369922659727943\n",
      "Iteration: 6731000, loss: 0.10680306399432221, gradient norm: 0.21007370596196928\n",
      "Iteration: 6732000, loss: 0.10680309646289349, gradient norm: 0.3698106573710799\n",
      "Iteration: 6733000, loss: 0.10680264579229552, gradient norm: 0.05104447738068148\n",
      "Iteration: 6734000, loss: 0.10680259717962028, gradient norm: 0.28300760534434055\n",
      "Iteration: 6735000, loss: 0.10680215550721178, gradient norm: 0.1548488009745671\n",
      "Iteration: 6736000, loss: 0.10680187789668373, gradient norm: 0.09003437834058096\n",
      "Iteration: 6737000, loss: 0.10680198228820457, gradient norm: 0.5486334627972127\n",
      "Iteration: 6738000, loss: 0.10680141835959878, gradient norm: 0.14397247757673848\n",
      "Iteration: 6739000, loss: 0.10680116209092072, gradient norm: 0.3153652301548745\n",
      "Iteration: 6740000, loss: 0.10680098827307008, gradient norm: 0.10318510076357262\n",
      "Iteration: 6741000, loss: 0.10680080684613176, gradient norm: 0.1660618937376697\n",
      "Iteration: 6742000, loss: 0.10680053437321947, gradient norm: 0.08346475524520018\n",
      "Iteration: 6743000, loss: 0.10680019997633763, gradient norm: 0.2305978820207295\n",
      "Iteration: 6744000, loss: 0.10679996456158619, gradient norm: 0.24851718927408775\n",
      "Iteration: 6745000, loss: 0.10679977452084442, gradient norm: 0.09731554732536489\n",
      "Iteration: 6746000, loss: 0.10679958882258374, gradient norm: 0.7400816254300756\n",
      "Iteration: 6747000, loss: 0.10679931451831069, gradient norm: 0.3398133019096769\n",
      "Iteration: 6748000, loss: 0.10679897674225973, gradient norm: 0.11643801593582799\n",
      "Iteration: 6749000, loss: 0.1067988805142183, gradient norm: 0.20707425055928874\n",
      "Iteration: 6750000, loss: 0.10679849671340662, gradient norm: 0.4571589254889248\n",
      "Iteration: 6751000, loss: 0.10679833206740277, gradient norm: 0.29050037226082853\n",
      "Iteration: 6752000, loss: 0.10679807929694744, gradient norm: 0.06313726885661673\n",
      "Iteration: 6753000, loss: 0.10679781729436148, gradient norm: 0.5377412555750608\n",
      "Iteration: 6754000, loss: 0.10679746329008168, gradient norm: 0.38089522646627183\n",
      "Iteration: 6755000, loss: 0.10679740460173827, gradient norm: 0.1364342414418818\n",
      "Iteration: 6756000, loss: 0.10679701275779019, gradient norm: 0.030067994785244283\n",
      "Iteration: 6757000, loss: 0.10679670279130347, gradient norm: 0.0579543213860333\n",
      "Iteration: 6758000, loss: 0.10679670395388403, gradient norm: 0.5718825396784203\n",
      "Iteration: 6759000, loss: 0.1067963435599732, gradient norm: 0.09721020592719333\n",
      "Iteration: 6760000, loss: 0.10679609221768982, gradient norm: 0.6940873789798391\n",
      "Iteration: 6761000, loss: 0.10679588096239068, gradient norm: 0.05372466981725081\n",
      "Iteration: 6762000, loss: 0.10679564991713685, gradient norm: 0.20484672362964318\n",
      "Iteration: 6763000, loss: 0.10679538633731091, gradient norm: 0.5972483752806019\n",
      "Iteration: 6764000, loss: 0.10679517873407697, gradient norm: 0.27700077641364307\n",
      "Iteration: 6765000, loss: 0.10679468154775924, gradient norm: 0.17252630841855054\n",
      "Iteration: 6766000, loss: 0.10679478666022629, gradient norm: 0.033263107784515356\n",
      "Iteration: 6767000, loss: 0.10679436322937857, gradient norm: 0.06869717104463031\n",
      "Iteration: 6768000, loss: 0.10679425750679257, gradient norm: 0.8525235562617521\n",
      "Iteration: 6769000, loss: 0.10679364730685867, gradient norm: 0.5084224699798817\n",
      "Iteration: 6770000, loss: 0.10679371865304838, gradient norm: 0.10756727808735311\n",
      "Iteration: 6771000, loss: 0.10679338423497174, gradient norm: 0.1240660090965778\n",
      "Iteration: 6772000, loss: 0.1067931156959288, gradient norm: 0.6141487941270084\n",
      "Iteration: 6773000, loss: 0.10679307617653208, gradient norm: 0.14717643539690312\n",
      "Iteration: 6774000, loss: 0.10679263217469312, gradient norm: 0.16599039862649276\n",
      "Iteration: 6775000, loss: 0.10679241448693835, gradient norm: 0.16671476665977417\n",
      "Iteration: 6776000, loss: 0.10679215313384226, gradient norm: 0.07452627432608447\n",
      "Iteration: 6777000, loss: 0.1067919151342837, gradient norm: 0.2033258221392664\n",
      "Iteration: 6778000, loss: 0.1067916091939044, gradient norm: 0.5516771529976795\n",
      "Iteration: 6779000, loss: 0.10679151276146757, gradient norm: 0.6494913186120963\n",
      "Iteration: 6780000, loss: 0.10679130700765348, gradient norm: 0.3504603122304113\n",
      "Iteration: 6781000, loss: 0.10679090863719833, gradient norm: 0.04392416452793842\n",
      "Iteration: 6782000, loss: 0.10679072246035173, gradient norm: 0.6617945501090542\n",
      "Iteration: 6783000, loss: 0.10679049357045085, gradient norm: 0.1902509154506343\n",
      "Iteration: 6784000, loss: 0.10679016835155761, gradient norm: 0.3029147145339003\n",
      "Iteration: 6785000, loss: 0.10679000059925763, gradient norm: 0.028106777272548743\n",
      "Iteration: 6786000, loss: 0.10678976711620027, gradient norm: 0.19575700955944017\n",
      "Iteration: 6787000, loss: 0.10678948570186576, gradient norm: 0.1277087982893537\n",
      "Iteration: 6788000, loss: 0.10678925649781863, gradient norm: 0.1647935206588683\n",
      "Iteration: 6789000, loss: 0.10678898877145371, gradient norm: 0.02261360643457827\n",
      "Iteration: 6790000, loss: 0.10678886376898178, gradient norm: 0.5428698039868833\n",
      "Iteration: 6791000, loss: 0.10678853570040048, gradient norm: 0.5355147813728619\n",
      "Iteration: 6792000, loss: 0.10678818353094545, gradient norm: 0.02221926463320154\n",
      "Iteration: 6793000, loss: 0.1067881295320733, gradient norm: 0.0790563886765172\n",
      "Iteration: 6794000, loss: 0.10678764127453638, gradient norm: 0.15274936076855442\n",
      "Iteration: 6795000, loss: 0.10678761117304263, gradient norm: 0.22032612867537713\n",
      "Iteration: 6796000, loss: 0.10678732742278126, gradient norm: 0.6914322125170578\n",
      "Iteration: 6797000, loss: 0.10678712109637695, gradient norm: 0.44243121813052266\n",
      "Iteration: 6798000, loss: 0.10678677753388421, gradient norm: 0.1900074364604492\n",
      "Iteration: 6799000, loss: 0.10678649765115394, gradient norm: 0.05425906718332203\n",
      "Iteration: 6800000, loss: 0.10678645564201644, gradient norm: 0.3878441461185552\n",
      "Iteration: 6801000, loss: 0.10678603142868827, gradient norm: 0.11738757807474659\n",
      "Iteration: 6802000, loss: 0.10678585177547864, gradient norm: 0.0616115381101446\n",
      "Iteration: 6803000, loss: 0.10678555250462189, gradient norm: 0.2695933318548695\n",
      "Iteration: 6804000, loss: 0.10678534431725728, gradient norm: 0.22347236564250236\n",
      "Iteration: 6805000, loss: 0.10678510910805926, gradient norm: 0.34563583665040243\n",
      "Iteration: 6806000, loss: 0.10678490587631524, gradient norm: 0.3010222577267569\n",
      "Iteration: 6807000, loss: 0.10678468195467708, gradient norm: 0.17633459467862236\n",
      "Iteration: 6808000, loss: 0.10678444517205153, gradient norm: 0.13363477915631625\n",
      "Iteration: 6809000, loss: 0.10678419415602752, gradient norm: 0.32439792849025545\n",
      "Iteration: 6810000, loss: 0.1067838042707146, gradient norm: 0.26774627499872494\n",
      "Iteration: 6811000, loss: 0.10678365078892603, gradient norm: 0.0664684582201151\n",
      "Iteration: 6812000, loss: 0.10678339621844256, gradient norm: 0.18992291163164624\n",
      "Iteration: 6813000, loss: 0.10678318207252398, gradient norm: 0.09651978583025897\n",
      "Iteration: 6814000, loss: 0.10678295179954009, gradient norm: 0.03193159634240314\n",
      "Iteration: 6815000, loss: 0.10678266665724795, gradient norm: 0.2899096515349134\n",
      "Iteration: 6816000, loss: 0.10678259537736952, gradient norm: 0.32454868124419706\n",
      "Iteration: 6817000, loss: 0.10678228673258068, gradient norm: 0.6377376426188501\n",
      "Iteration: 6818000, loss: 0.10678180945715512, gradient norm: 0.15797163378459825\n",
      "Iteration: 6819000, loss: 0.10678180114290851, gradient norm: 0.9925196953450945\n",
      "Iteration: 6820000, loss: 0.10678139175397049, gradient norm: 0.015478353753629992\n",
      "Iteration: 6821000, loss: 0.10678118227641951, gradient norm: 0.06505562338340426\n",
      "Iteration: 6822000, loss: 0.10678111246911048, gradient norm: 0.08389874847979935\n",
      "Iteration: 6823000, loss: 0.10678077466442126, gradient norm: 0.2220994754545379\n",
      "Iteration: 6824000, loss: 0.10678044710789528, gradient norm: 0.21425568715684423\n",
      "Iteration: 6825000, loss: 0.10678033364946199, gradient norm: 0.09038973823098422\n",
      "Iteration: 6826000, loss: 0.10678002211821391, gradient norm: 0.6908825164661616\n",
      "Iteration: 6827000, loss: 0.10677996734109231, gradient norm: 0.0808777450516669\n",
      "Iteration: 6828000, loss: 0.10677953118063431, gradient norm: 0.29197013794385057\n",
      "Iteration: 6829000, loss: 0.10677924385474574, gradient norm: 0.14594988705882592\n",
      "Iteration: 6830000, loss: 0.10677918900425232, gradient norm: 0.1976354174388553\n",
      "Iteration: 6831000, loss: 0.10677876512380291, gradient norm: 0.16348876173899782\n",
      "Iteration: 6832000, loss: 0.10677860987011431, gradient norm: 0.3181725044227374\n",
      "Iteration: 6833000, loss: 0.10677827289819193, gradient norm: 0.19667680885615874\n",
      "Iteration: 6834000, loss: 0.10677824123781861, gradient norm: 0.10115445174012314\n",
      "Iteration: 6835000, loss: 0.10677779070052834, gradient norm: 0.4261818187070485\n",
      "Iteration: 6836000, loss: 0.10677750795369406, gradient norm: 0.14904833107967844\n",
      "Iteration: 6837000, loss: 0.10677756669337435, gradient norm: 0.2761953787984523\n",
      "Iteration: 6838000, loss: 0.10677708286793162, gradient norm: 0.09896826739055582\n",
      "Iteration: 6839000, loss: 0.10677681243951764, gradient norm: 0.15463735062633838\n",
      "Iteration: 6840000, loss: 0.10677669088679295, gradient norm: 0.10481139081686222\n",
      "Iteration: 6841000, loss: 0.1067764648588816, gradient norm: 0.09395681128509595\n",
      "Iteration: 6842000, loss: 0.10677616227048708, gradient norm: 0.4350751694391861\n",
      "Iteration: 6843000, loss: 0.1067760452016162, gradient norm: 0.18247505867083008\n",
      "Iteration: 6844000, loss: 0.10677560497414945, gradient norm: 0.5721953654330484\n",
      "Iteration: 6845000, loss: 0.10677551684729852, gradient norm: 0.30610847760516047\n",
      "Iteration: 6846000, loss: 0.1067752071833405, gradient norm: 0.07935505329938596\n",
      "Iteration: 6847000, loss: 0.10677500908900638, gradient norm: 0.13244941878376854\n",
      "Iteration: 6848000, loss: 0.10677496306469919, gradient norm: 0.20933781467655418\n",
      "Iteration: 6849000, loss: 0.10677434764788453, gradient norm: 0.13452691365761973\n",
      "Iteration: 6850000, loss: 0.10677418655173032, gradient norm: 0.5665007691745676\n",
      "Iteration: 6851000, loss: 0.1067740817669664, gradient norm: 0.06867161999089505\n",
      "Iteration: 6852000, loss: 0.10677377719840672, gradient norm: 0.14385161078443276\n",
      "Iteration: 6853000, loss: 0.10677349230578545, gradient norm: 0.19714746332493738\n",
      "Iteration: 6854000, loss: 0.10677331080572852, gradient norm: 0.0820175165616582\n",
      "Iteration: 6855000, loss: 0.10677312167165644, gradient norm: 0.4145962428615724\n",
      "Iteration: 6856000, loss: 0.10677267590700987, gradient norm: 0.32639376144495386\n",
      "Iteration: 6857000, loss: 0.10677266622905793, gradient norm: 0.10232119124941781\n",
      "Iteration: 6858000, loss: 0.10677225529616595, gradient norm: 0.07529542172606034\n",
      "Iteration: 6859000, loss: 0.10677212388169621, gradient norm: 0.08671310170581992\n",
      "Iteration: 6860000, loss: 0.10677181524628526, gradient norm: 0.7745850208498627\n",
      "Iteration: 6861000, loss: 0.10677183238877419, gradient norm: 0.5193800797905911\n",
      "Iteration: 6862000, loss: 0.10677116352340979, gradient norm: 0.6916148524895221\n",
      "Iteration: 6863000, loss: 0.10677127509746645, gradient norm: 0.008263993473215651\n",
      "Iteration: 6864000, loss: 0.10677091575901071, gradient norm: 0.08362148957223488\n",
      "Iteration: 6865000, loss: 0.10677058403229953, gradient norm: 0.023694845307982465\n",
      "Iteration: 6866000, loss: 0.10677054302946501, gradient norm: 0.4558711068168032\n",
      "Iteration: 6867000, loss: 0.10677031210837903, gradient norm: 0.24746217976408377\n",
      "Iteration: 6868000, loss: 0.10676978080465986, gradient norm: 0.3026448276169434\n",
      "Iteration: 6869000, loss: 0.10676963186189867, gradient norm: 0.1693233159373388\n",
      "Iteration: 6870000, loss: 0.10676951485278525, gradient norm: 0.7543478227756372\n",
      "Iteration: 6871000, loss: 0.10676929842760349, gradient norm: 0.6046929599548124\n",
      "Iteration: 6872000, loss: 0.10676896548569008, gradient norm: 0.9309758423511365\n",
      "Iteration: 6873000, loss: 0.10676869429611771, gradient norm: 0.5286318010241068\n",
      "Iteration: 6874000, loss: 0.1067684390064186, gradient norm: 0.22520057312950487\n",
      "Iteration: 6875000, loss: 0.10676826481279278, gradient norm: 0.6509054407090376\n",
      "Iteration: 6876000, loss: 0.10676809461391497, gradient norm: 0.2617611209152945\n",
      "Iteration: 6877000, loss: 0.10676783676450873, gradient norm: 0.23536408345753917\n",
      "Iteration: 6878000, loss: 0.10676761138810889, gradient norm: 0.1974323198411198\n",
      "Iteration: 6879000, loss: 0.10676724531627992, gradient norm: 0.1897187262202166\n",
      "Iteration: 6880000, loss: 0.10676708449508233, gradient norm: 0.035362787252987984\n",
      "Iteration: 6881000, loss: 0.10676681408168358, gradient norm: 0.050229087155096476\n",
      "Iteration: 6882000, loss: 0.10676670724836887, gradient norm: 0.04243095431064038\n",
      "Iteration: 6883000, loss: 0.10676631190789503, gradient norm: 0.22069962866432416\n",
      "Iteration: 6884000, loss: 0.1067659861926886, gradient norm: 0.16860879702497483\n",
      "Iteration: 6885000, loss: 0.10676594699250228, gradient norm: 0.04739717165596992\n",
      "Iteration: 6886000, loss: 0.10676549072537805, gradient norm: 0.10060572534379696\n",
      "Iteration: 6887000, loss: 0.10676555854151121, gradient norm: 0.06820099760781667\n",
      "Iteration: 6888000, loss: 0.10676523383639464, gradient norm: 0.14746634756056054\n",
      "Iteration: 6889000, loss: 0.10676496548287798, gradient norm: 0.023145361757829986\n",
      "Iteration: 6890000, loss: 0.10676465762602932, gradient norm: 0.3084971907271447\n",
      "Iteration: 6891000, loss: 0.10676439628272867, gradient norm: 0.1140788347398117\n",
      "Iteration: 6892000, loss: 0.10676431282774002, gradient norm: 0.01961525997735203\n",
      "Iteration: 6893000, loss: 0.10676388580139726, gradient norm: 0.4517174687873538\n",
      "Iteration: 6894000, loss: 0.10676378900720922, gradient norm: 0.14087423298808263\n",
      "Iteration: 6895000, loss: 0.10676358198917771, gradient norm: 0.17989940679040933\n",
      "Iteration: 6896000, loss: 0.10676309317558724, gradient norm: 0.3058591449804968\n",
      "Iteration: 6897000, loss: 0.10676296935898441, gradient norm: 0.7948064252248557\n",
      "Iteration: 6898000, loss: 0.10676300958097498, gradient norm: 0.6456313265956414\n",
      "Iteration: 6899000, loss: 0.10676247213713475, gradient norm: 0.2112684200390124\n",
      "Iteration: 6900000, loss: 0.10676223155282864, gradient norm: 0.42972795938947833\n",
      "Iteration: 6901000, loss: 0.10676207601365582, gradient norm: 0.49551994237539765\n",
      "Iteration: 6902000, loss: 0.1067619364150154, gradient norm: 0.12397712731478258\n",
      "Iteration: 6903000, loss: 0.1067613826490463, gradient norm: 0.26652643671713205\n",
      "Iteration: 6904000, loss: 0.10676139350621738, gradient norm: 0.4228178761280266\n",
      "Iteration: 6905000, loss: 0.10676103242389641, gradient norm: 0.0560283929211551\n",
      "Iteration: 6906000, loss: 0.10676094135662666, gradient norm: 0.06763121151676335\n",
      "Iteration: 6907000, loss: 0.10676055967240211, gradient norm: 0.20223169619767783\n",
      "Iteration: 6908000, loss: 0.10676023862134983, gradient norm: 0.2214611705897856\n",
      "Iteration: 6909000, loss: 0.10675994505915028, gradient norm: 0.12508634739899566\n",
      "Iteration: 6910000, loss: 0.10675985215758275, gradient norm: 0.08193288980567624\n",
      "Iteration: 6911000, loss: 0.10675935465571133, gradient norm: 0.2587949207535456\n",
      "Iteration: 6912000, loss: 0.10675912182553876, gradient norm: 0.5652440317712709\n",
      "Iteration: 6913000, loss: 0.10675875905517071, gradient norm: 0.09622365226762303\n",
      "Iteration: 6914000, loss: 0.10675835838187829, gradient norm: 0.7115918596877363\n",
      "Iteration: 6915000, loss: 0.1067577325481829, gradient norm: 0.12492923989988806\n",
      "Iteration: 6916000, loss: 0.10675741355992488, gradient norm: 0.23246463091933717\n",
      "Iteration: 6917000, loss: 0.10675684039878762, gradient norm: 0.033794437322686124\n",
      "Iteration: 6918000, loss: 0.1067565224851643, gradient norm: 0.09232531949795336\n",
      "Iteration: 6919000, loss: 0.10675585651810716, gradient norm: 0.16548223564515685\n",
      "Iteration: 6920000, loss: 0.10675548622888885, gradient norm: 0.07223249330945485\n",
      "Iteration: 6921000, loss: 0.10675508685324797, gradient norm: 0.6913417611335444\n",
      "Iteration: 6922000, loss: 0.10675455178120527, gradient norm: 0.5609106875122302\n",
      "Iteration: 6923000, loss: 0.1067541445192695, gradient norm: 0.31316157484925927\n",
      "Iteration: 6924000, loss: 0.10675383058893748, gradient norm: 0.14527178588325515\n",
      "Iteration: 6925000, loss: 0.10675317204375818, gradient norm: 0.21844215744978096\n",
      "Iteration: 6926000, loss: 0.1067529271558122, gradient norm: 0.13858271163744468\n",
      "Iteration: 6927000, loss: 0.10675258745925319, gradient norm: 0.32898381069490956\n",
      "Iteration: 6928000, loss: 0.10675219959538296, gradient norm: 0.19571968751182584\n",
      "Iteration: 6929000, loss: 0.10675173236296316, gradient norm: 0.12517698198089383\n",
      "Iteration: 6930000, loss: 0.10675142492869674, gradient norm: 0.6880770795680814\n",
      "Iteration: 6931000, loss: 0.10675101883299441, gradient norm: 0.7449109202510812\n",
      "Iteration: 6932000, loss: 0.10675054372794052, gradient norm: 0.10962516292181743\n",
      "Iteration: 6933000, loss: 0.10675038290427266, gradient norm: 0.4794907188208536\n",
      "Iteration: 6934000, loss: 0.10674993498730419, gradient norm: 0.10880729924881487\n",
      "Iteration: 6935000, loss: 0.10674958497994382, gradient norm: 0.20894890570350072\n",
      "Iteration: 6936000, loss: 0.10674924413520351, gradient norm: 0.22730002377511904\n",
      "Iteration: 6937000, loss: 0.10674881602811245, gradient norm: 0.5215731740860312\n",
      "Iteration: 6938000, loss: 0.10674845298408049, gradient norm: 0.2959470055241379\n",
      "Iteration: 6939000, loss: 0.10674838177049048, gradient norm: 0.4373037665325032\n",
      "Iteration: 6940000, loss: 0.10674783653487478, gradient norm: 0.3285651805806363\n",
      "Iteration: 6941000, loss: 0.10674741676988833, gradient norm: 0.08767085460438677\n",
      "Iteration: 6942000, loss: 0.10674727680166084, gradient norm: 0.43742444824457305\n",
      "Iteration: 6943000, loss: 0.10674684324988958, gradient norm: 0.04114948607941399\n",
      "Iteration: 6944000, loss: 0.10674651803901807, gradient norm: 0.42902588811901327\n",
      "Iteration: 6945000, loss: 0.10674618615048871, gradient norm: 0.016480730328377217\n",
      "Iteration: 6946000, loss: 0.10674598976646524, gradient norm: 0.06452417704377747\n",
      "Iteration: 6947000, loss: 0.1067455799676423, gradient norm: 0.5302200324428122\n",
      "Iteration: 6948000, loss: 0.10674534533679944, gradient norm: 0.4439406272075366\n",
      "Iteration: 6949000, loss: 0.10674495562314834, gradient norm: 0.023480528920881153\n",
      "Iteration: 6950000, loss: 0.10674471641282852, gradient norm: 0.19377874158767638\n",
      "Iteration: 6951000, loss: 0.10674416338233393, gradient norm: 0.14391965697848283\n",
      "Iteration: 6952000, loss: 0.10674410951497469, gradient norm: 0.21929534761012523\n",
      "Iteration: 6953000, loss: 0.10674368049442946, gradient norm: 0.656801410107104\n",
      "Iteration: 6954000, loss: 0.10674340677458921, gradient norm: 0.6015787531462498\n",
      "Iteration: 6955000, loss: 0.10674306181056764, gradient norm: 0.741935170834224\n",
      "Iteration: 6956000, loss: 0.10674301788700935, gradient norm: 0.18346469866229473\n",
      "Iteration: 6957000, loss: 0.10674232447272032, gradient norm: 0.04072807666302173\n",
      "Iteration: 6958000, loss: 0.10674203029409936, gradient norm: 0.2260512888352422\n",
      "Iteration: 6959000, loss: 0.10674179026527757, gradient norm: 0.7863911062792069\n",
      "Iteration: 6960000, loss: 0.10674158632344104, gradient norm: 0.39410792452226545\n",
      "Iteration: 6961000, loss: 0.10674132172911097, gradient norm: 0.17107712393952237\n",
      "Iteration: 6962000, loss: 0.10674089723201322, gradient norm: 0.2521431073650052\n",
      "Iteration: 6963000, loss: 0.10674070327569829, gradient norm: 0.7023641702841658\n",
      "Iteration: 6964000, loss: 0.10674027937334839, gradient norm: 0.18008990350738918\n",
      "Iteration: 6965000, loss: 0.10674002628071937, gradient norm: 0.3757386077253172\n",
      "Iteration: 6966000, loss: 0.10673975141820395, gradient norm: 0.11224593480455801\n",
      "Iteration: 6967000, loss: 0.10673942055430788, gradient norm: 0.20194938195350354\n",
      "Iteration: 6968000, loss: 0.10673916562949037, gradient norm: 0.6901805338207883\n",
      "Iteration: 6969000, loss: 0.10673894581032405, gradient norm: 0.08250722381414739\n",
      "Iteration: 6970000, loss: 0.10673844228190514, gradient norm: 0.14177582570540256\n",
      "Iteration: 6971000, loss: 0.10673831851638713, gradient norm: 0.1302698732976442\n",
      "Iteration: 6972000, loss: 0.1067378368232165, gradient norm: 0.45536307297323797\n",
      "Iteration: 6973000, loss: 0.10673776440509318, gradient norm: 0.11841935277142415\n",
      "Iteration: 6974000, loss: 0.10673743656510415, gradient norm: 0.25524923309811576\n",
      "Iteration: 6975000, loss: 0.1067371155642294, gradient norm: 0.1779565556949901\n",
      "Iteration: 6976000, loss: 0.10673672376747638, gradient norm: 0.2944586928406058\n",
      "Iteration: 6977000, loss: 0.10673650980538056, gradient norm: 0.24442721070164286\n",
      "Iteration: 6978000, loss: 0.1067362044730766, gradient norm: 0.032980089486721556\n",
      "Iteration: 6979000, loss: 0.10673581571776695, gradient norm: 0.41034057389899364\n",
      "Iteration: 6980000, loss: 0.1067358281778689, gradient norm: 0.07568919071572328\n",
      "Iteration: 6981000, loss: 0.10673527687545216, gradient norm: 0.5055221131800232\n",
      "Iteration: 6982000, loss: 0.10673505894720367, gradient norm: 0.5506644901827494\n",
      "Iteration: 6983000, loss: 0.10673463082562086, gradient norm: 0.20721091345051051\n",
      "Iteration: 6984000, loss: 0.1067345731231229, gradient norm: 0.4924437073488429\n",
      "Iteration: 6985000, loss: 0.10673425231892955, gradient norm: 0.28424104749370355\n",
      "Iteration: 6986000, loss: 0.10673373657219258, gradient norm: 0.11178800276870175\n",
      "Iteration: 6987000, loss: 0.106733666058066, gradient norm: 0.20651051364764292\n",
      "Iteration: 6988000, loss: 0.10673328210486732, gradient norm: 0.09912224085339447\n",
      "Iteration: 6989000, loss: 0.10673308184789673, gradient norm: 0.12268478064455231\n",
      "Iteration: 6990000, loss: 0.10673275806397126, gradient norm: 0.18115148706036086\n",
      "Iteration: 6991000, loss: 0.10673253278724386, gradient norm: 0.18120960764656452\n",
      "Iteration: 6992000, loss: 0.1067322375334742, gradient norm: 0.334968816472522\n",
      "Iteration: 6993000, loss: 0.10673181014780021, gradient norm: 0.1773336432092976\n",
      "Iteration: 6994000, loss: 0.10673165992703261, gradient norm: 0.07086489590311529\n",
      "Iteration: 6995000, loss: 0.10673147914686175, gradient norm: 0.064063728575403\n",
      "Iteration: 6996000, loss: 0.1067310433685803, gradient norm: 0.14059839176643707\n",
      "Iteration: 6997000, loss: 0.10673087333407748, gradient norm: 0.2855642774731642\n",
      "Iteration: 6998000, loss: 0.10673043815133701, gradient norm: 0.11990185596240044\n",
      "Iteration: 6999000, loss: 0.10673036466960667, gradient norm: 0.6245162962054269\n",
      "Iteration: 7000000, loss: 0.10673000233567072, gradient norm: 0.18703744700435476\n",
      "Iteration: 7001000, loss: 0.10672962010246485, gradient norm: 0.268406044265355\n",
      "Iteration: 7002000, loss: 0.10672959696466257, gradient norm: 0.8694046602298509\n",
      "Iteration: 7003000, loss: 0.10672906591690645, gradient norm: 0.12207141620215753\n",
      "Iteration: 7004000, loss: 0.10672891417299948, gradient norm: 0.18778193638767465\n",
      "Iteration: 7005000, loss: 0.1067285668796059, gradient norm: 0.26988687797985433\n",
      "Iteration: 7006000, loss: 0.10672829618231126, gradient norm: 0.6491898403307579\n",
      "Iteration: 7007000, loss: 0.10672803837442324, gradient norm: 0.4826411385148868\n",
      "Iteration: 7008000, loss: 0.10672777165218408, gradient norm: 0.4153837769367414\n",
      "Iteration: 7009000, loss: 0.10672752720876792, gradient norm: 0.3086626885934608\n",
      "Iteration: 7010000, loss: 0.1067272876899274, gradient norm: 0.08134044258962823\n",
      "Iteration: 7011000, loss: 0.10672685396584024, gradient norm: 0.7261306602470057\n",
      "Iteration: 7012000, loss: 0.10672683495773465, gradient norm: 0.5567522968180885\n",
      "Iteration: 7013000, loss: 0.10672629832372431, gradient norm: 0.4391112848678677\n",
      "Iteration: 7014000, loss: 0.10672616810629684, gradient norm: 0.24931155645850453\n",
      "Iteration: 7015000, loss: 0.10672587571028111, gradient norm: 0.1296320119522028\n",
      "Iteration: 7016000, loss: 0.10672565839590774, gradient norm: 0.13734247074255443\n",
      "Iteration: 7017000, loss: 0.10672528404474531, gradient norm: 0.05136697929090019\n",
      "Iteration: 7018000, loss: 0.10672523843187248, gradient norm: 0.4282795191598654\n",
      "Iteration: 7019000, loss: 0.10672463464727483, gradient norm: 0.029129162455634756\n",
      "Iteration: 7020000, loss: 0.10672454531322548, gradient norm: 0.08902712617410535\n",
      "Iteration: 7021000, loss: 0.10672421516598746, gradient norm: 0.43040370796588207\n",
      "Iteration: 7022000, loss: 0.10672407624743946, gradient norm: 0.5984805703568997\n",
      "Iteration: 7023000, loss: 0.10672371707039424, gradient norm: 0.12331321998697471\n",
      "Iteration: 7024000, loss: 0.10672357353958888, gradient norm: 0.3270052236565563\n",
      "Iteration: 7025000, loss: 0.10672318197621536, gradient norm: 0.10798295587528438\n",
      "Iteration: 7026000, loss: 0.10672301702016822, gradient norm: 0.17661454056978096\n",
      "Iteration: 7027000, loss: 0.10672263784594613, gradient norm: 0.1089666101719961\n",
      "Iteration: 7028000, loss: 0.10672234642249614, gradient norm: 0.17745598158030698\n",
      "Iteration: 7029000, loss: 0.10672208442201606, gradient norm: 0.1282481061915978\n",
      "Iteration: 7030000, loss: 0.10672186361267925, gradient norm: 0.03822972575226505\n",
      "Iteration: 7031000, loss: 0.10672165817244869, gradient norm: 0.5188267595888226\n",
      "Iteration: 7032000, loss: 0.1067212856568489, gradient norm: 0.18223700173620117\n",
      "Iteration: 7033000, loss: 0.1067211013231835, gradient norm: 0.6743355170921818\n",
      "Iteration: 7034000, loss: 0.1067206833921584, gradient norm: 0.2820431249832538\n",
      "Iteration: 7035000, loss: 0.10672068610496868, gradient norm: 0.2559428365671713\n",
      "Iteration: 7036000, loss: 0.10672012096029568, gradient norm: 0.6950273315719339\n",
      "Iteration: 7037000, loss: 0.10672017991689006, gradient norm: 0.28670379069357455\n",
      "Iteration: 7038000, loss: 0.10671958878428527, gradient norm: 0.17165241747130047\n",
      "Iteration: 7039000, loss: 0.10671951929662299, gradient norm: 0.3960497706370841\n",
      "Iteration: 7040000, loss: 0.10671929041950762, gradient norm: 0.3760712275259527\n",
      "Iteration: 7041000, loss: 0.10671884104924026, gradient norm: 0.005374888782281623\n",
      "Iteration: 7042000, loss: 0.1067187134877688, gradient norm: 0.13647520004566754\n",
      "Iteration: 7043000, loss: 0.10671841856383714, gradient norm: 0.037338357295599735\n",
      "Iteration: 7044000, loss: 0.10671825788148896, gradient norm: 0.34947442060670714\n",
      "Iteration: 7045000, loss: 0.10671791395133345, gradient norm: 0.2750061178736501\n",
      "Iteration: 7046000, loss: 0.10671754995281504, gradient norm: 0.34472005791761234\n",
      "Iteration: 7047000, loss: 0.1067174364532773, gradient norm: 0.18941239383371625\n",
      "Iteration: 7048000, loss: 0.1067170907928768, gradient norm: 0.08451858933812605\n",
      "Iteration: 7049000, loss: 0.10671700449018502, gradient norm: 0.4400367470476464\n",
      "Iteration: 7050000, loss: 0.10671639736536835, gradient norm: 0.23200758891903628\n",
      "Iteration: 7051000, loss: 0.10671647341420491, gradient norm: 0.26429011942182645\n",
      "Iteration: 7052000, loss: 0.10671596324017449, gradient norm: 0.11660328295098993\n",
      "Iteration: 7053000, loss: 0.10671580481727155, gradient norm: 0.28637098501442587\n",
      "Iteration: 7054000, loss: 0.10671559248279729, gradient norm: 0.19547619399306507\n",
      "Iteration: 7055000, loss: 0.10671531570565519, gradient norm: 0.1440311228813523\n",
      "Iteration: 7056000, loss: 0.10671506397342194, gradient norm: 0.375482856426019\n",
      "Iteration: 7057000, loss: 0.10671472499986334, gradient norm: 0.0634674949902249\n",
      "Iteration: 7058000, loss: 0.10671462750704155, gradient norm: 0.032175206140419986\n",
      "Iteration: 7059000, loss: 0.10671420987886548, gradient norm: 0.7335696175737038\n",
      "Iteration: 7060000, loss: 0.10671399437624923, gradient norm: 0.12278256769082065\n",
      "Iteration: 7061000, loss: 0.10671367032550398, gradient norm: 0.13891529295198835\n",
      "Iteration: 7062000, loss: 0.10671362758407822, gradient norm: 0.1286815595284474\n",
      "Iteration: 7063000, loss: 0.10671317179529931, gradient norm: 0.6281947703428793\n",
      "Iteration: 7064000, loss: 0.10671296498790489, gradient norm: 0.05450530319982413\n",
      "Iteration: 7065000, loss: 0.10671266238916133, gradient norm: 0.20750577200069836\n",
      "Iteration: 7066000, loss: 0.10671250116486505, gradient norm: 0.6123432138547282\n",
      "Iteration: 7067000, loss: 0.1067122966140297, gradient norm: 0.1664350412359437\n",
      "Iteration: 7068000, loss: 0.10671191342987174, gradient norm: 0.3002220960373988\n",
      "Iteration: 7069000, loss: 0.10671161498967688, gradient norm: 0.16016696862442834\n",
      "Iteration: 7070000, loss: 0.10671151150969316, gradient norm: 0.12103960982830607\n",
      "Iteration: 7071000, loss: 0.10671105752829887, gradient norm: 0.7307207050439012\n",
      "Iteration: 7072000, loss: 0.10671097274097531, gradient norm: 0.21144398993713823\n",
      "Iteration: 7073000, loss: 0.10671057637948973, gradient norm: 0.4763994460739586\n",
      "Iteration: 7074000, loss: 0.10671054230891275, gradient norm: 0.44430904155579354\n",
      "Iteration: 7075000, loss: 0.10671005776251877, gradient norm: 0.48645251531040845\n",
      "Iteration: 7076000, loss: 0.10670980335548373, gradient norm: 0.40607961935261266\n",
      "Iteration: 7077000, loss: 0.10670976392765491, gradient norm: 0.06959518719837224\n",
      "Iteration: 7078000, loss: 0.1067093162674238, gradient norm: 0.10818182754709263\n",
      "Iteration: 7079000, loss: 0.10670910994931579, gradient norm: 0.28503999417635645\n",
      "Iteration: 7080000, loss: 0.1067090158987734, gradient norm: 0.3266766187177045\n",
      "Iteration: 7081000, loss: 0.10670846686593402, gradient norm: 0.15936333492798513\n",
      "Iteration: 7082000, loss: 0.10670847716730555, gradient norm: 0.09724616184812732\n",
      "Iteration: 7083000, loss: 0.10670798762805253, gradient norm: 0.4131770406181767\n",
      "Iteration: 7084000, loss: 0.10670787771567662, gradient norm: 0.08137393728776371\n",
      "Iteration: 7085000, loss: 0.1067076393746468, gradient norm: 0.2459841983050631\n",
      "Iteration: 7086000, loss: 0.10670738670309783, gradient norm: 0.2148515085174653\n",
      "Iteration: 7087000, loss: 0.10670709473367157, gradient norm: 0.1735034748755829\n",
      "Iteration: 7088000, loss: 0.10670670782728729, gradient norm: 0.21996605398958213\n",
      "Iteration: 7089000, loss: 0.10670657187719797, gradient norm: 0.39163063746071686\n",
      "Iteration: 7090000, loss: 0.10670631582334289, gradient norm: 0.635245666740061\n",
      "Iteration: 7091000, loss: 0.10670613884780292, gradient norm: 0.45017878291056607\n",
      "Iteration: 7092000, loss: 0.10670574558802007, gradient norm: 0.0999728341605\n",
      "Iteration: 7093000, loss: 0.10670562457834598, gradient norm: 0.276760473656356\n",
      "Iteration: 7094000, loss: 0.10670536311134998, gradient norm: 0.041674669881033286\n",
      "Iteration: 7095000, loss: 0.10670496979666448, gradient norm: 0.5852244790087632\n",
      "Iteration: 7096000, loss: 0.10670478730879845, gradient norm: 0.037449924618886014\n",
      "Iteration: 7097000, loss: 0.10670456930572547, gradient norm: 0.5756652080171895\n",
      "Iteration: 7098000, loss: 0.10670429679931062, gradient norm: 0.429686529582836\n",
      "Iteration: 7099000, loss: 0.10670402371675772, gradient norm: 0.9048132249822066\n",
      "Iteration: 7100000, loss: 0.1067037678322429, gradient norm: 0.18801021215595098\n",
      "Iteration: 7101000, loss: 0.10670356904853222, gradient norm: 0.15436203905235243\n",
      "Iteration: 7102000, loss: 0.10670333451389093, gradient norm: 0.3013531358040737\n",
      "Iteration: 7103000, loss: 0.10670304595463255, gradient norm: 0.17149361837829163\n",
      "Iteration: 7104000, loss: 0.10670270030548752, gradient norm: 0.26796289946986995\n",
      "Iteration: 7105000, loss: 0.10670256051687002, gradient norm: 0.2136919829736846\n",
      "Iteration: 7106000, loss: 0.10670237470719261, gradient norm: 0.2353108869606059\n",
      "Iteration: 7107000, loss: 0.106701996555934, gradient norm: 0.041855578837917816\n",
      "Iteration: 7108000, loss: 0.10670174357829718, gradient norm: 0.24457915635260918\n",
      "Iteration: 7109000, loss: 0.1067016342392402, gradient norm: 0.11848185325541445\n",
      "Iteration: 7110000, loss: 0.10670139519552543, gradient norm: 0.19582287010476054\n",
      "Iteration: 7111000, loss: 0.10670086456260261, gradient norm: 0.34335482821403035\n",
      "Iteration: 7112000, loss: 0.10670082293525332, gradient norm: 0.5159142403902892\n",
      "Iteration: 7113000, loss: 0.1067005027194738, gradient norm: 0.2947452055909575\n",
      "Iteration: 7114000, loss: 0.10670029156919841, gradient norm: 0.5712581888052214\n",
      "Iteration: 7115000, loss: 0.1067000390450563, gradient norm: 0.46259038044872297\n",
      "Iteration: 7116000, loss: 0.10669973692909793, gradient norm: 0.06789875831112947\n",
      "Iteration: 7117000, loss: 0.10669956692212781, gradient norm: 0.10035370523423652\n",
      "Iteration: 7118000, loss: 0.10669925272639985, gradient norm: 0.15338526831283741\n",
      "Iteration: 7119000, loss: 0.10669910268724433, gradient norm: 0.5727052047168804\n",
      "Iteration: 7120000, loss: 0.10669872208411675, gradient norm: 0.46615699424316737\n",
      "Iteration: 7121000, loss: 0.10669852631074449, gradient norm: 0.05897880507704519\n",
      "Iteration: 7122000, loss: 0.10669832309999905, gradient norm: 0.7822385094105331\n",
      "Iteration: 7123000, loss: 0.1066980096560333, gradient norm: 0.20095895979277195\n",
      "Iteration: 7124000, loss: 0.10669774718035341, gradient norm: 0.030133227036673784\n",
      "Iteration: 7125000, loss: 0.10669749603261994, gradient norm: 0.20904307523573484\n",
      "Iteration: 7126000, loss: 0.1066973467186423, gradient norm: 0.01596657884242793\n",
      "Iteration: 7127000, loss: 0.10669710102695851, gradient norm: 0.14416208627752172\n",
      "Iteration: 7128000, loss: 0.10669672131971214, gradient norm: 0.2175272508335598\n",
      "Iteration: 7129000, loss: 0.1066965631331212, gradient norm: 0.5729773393097313\n",
      "Iteration: 7130000, loss: 0.10669632780205131, gradient norm: 0.3973131832043113\n",
      "Iteration: 7131000, loss: 0.10669607278452978, gradient norm: 0.31104439304262715\n",
      "Iteration: 7132000, loss: 0.10669582332374455, gradient norm: 0.15966346487983485\n",
      "Iteration: 7133000, loss: 0.1066955947976786, gradient norm: 0.05640557774225309\n",
      "Iteration: 7134000, loss: 0.10669522932345822, gradient norm: 0.22461938578105686\n",
      "Iteration: 7135000, loss: 0.10669512585590576, gradient norm: 0.14194742130473065\n",
      "Iteration: 7136000, loss: 0.1066946842543308, gradient norm: 0.058849380195493385\n",
      "Iteration: 7137000, loss: 0.10669456933351201, gradient norm: 0.053222439456254794\n",
      "Iteration: 7138000, loss: 0.10669430244641867, gradient norm: 0.5453542199708405\n",
      "Iteration: 7139000, loss: 0.10669413115653015, gradient norm: 0.13354275542742955\n",
      "Iteration: 7140000, loss: 0.10669372214655527, gradient norm: 0.2203603190223048\n",
      "Iteration: 7141000, loss: 0.10669373294369994, gradient norm: 0.054440312515233506\n",
      "Iteration: 7142000, loss: 0.1066931178310224, gradient norm: 0.13002113515871755\n",
      "Iteration: 7143000, loss: 0.10669308842516276, gradient norm: 0.061471889250566285\n",
      "Iteration: 7144000, loss: 0.10669281141809378, gradient norm: 0.12367105659848822\n",
      "Iteration: 7145000, loss: 0.10669280284977979, gradient norm: 0.080230699129508\n",
      "Iteration: 7146000, loss: 0.10669216084760119, gradient norm: 0.47467489792689715\n",
      "Iteration: 7147000, loss: 0.1066921254994393, gradient norm: 0.26616873268236174\n",
      "Iteration: 7148000, loss: 0.10669174810027561, gradient norm: 0.22383097638501134\n",
      "Iteration: 7149000, loss: 0.10669152810943922, gradient norm: 0.5157376523403888\n",
      "Iteration: 7150000, loss: 0.1066914191928112, gradient norm: 0.0969825368033281\n",
      "Iteration: 7151000, loss: 0.10669104724821962, gradient norm: 0.11195584583266996\n",
      "Iteration: 7152000, loss: 0.10669091652581654, gradient norm: 0.06569794456293443\n",
      "Iteration: 7153000, loss: 0.10669056643730739, gradient norm: 0.5085364741872028\n",
      "Iteration: 7154000, loss: 0.10669037081369825, gradient norm: 0.5196122136519218\n",
      "Iteration: 7155000, loss: 0.1066901306920848, gradient norm: 0.023452892055170198\n",
      "Iteration: 7156000, loss: 0.10668992625265783, gradient norm: 0.2174812702198393\n",
      "Iteration: 7157000, loss: 0.10668957101534865, gradient norm: 0.06217996027433614\n",
      "Iteration: 7158000, loss: 0.10668954609168514, gradient norm: 0.4385761613036265\n",
      "Iteration: 7159000, loss: 0.10668903635835005, gradient norm: 0.4198436524996172\n",
      "Iteration: 7160000, loss: 0.10668884851479574, gradient norm: 0.5231460502500034\n",
      "Iteration: 7161000, loss: 0.10668859972109773, gradient norm: 0.06859616916459048\n",
      "Iteration: 7162000, loss: 0.1066884400134064, gradient norm: 0.2003977393402647\n",
      "Iteration: 7163000, loss: 0.106688276311962, gradient norm: 0.26453742516531153\n",
      "Iteration: 7164000, loss: 0.10668769524190234, gradient norm: 0.06112786720453275\n",
      "Iteration: 7165000, loss: 0.10668770507712919, gradient norm: 0.1857958491572954\n",
      "Iteration: 7166000, loss: 0.10668747317129254, gradient norm: 0.19934548437298202\n",
      "Iteration: 7167000, loss: 0.10668725942047516, gradient norm: 0.10633135723993006\n",
      "Iteration: 7168000, loss: 0.10668673681692187, gradient norm: 0.10858914634802558\n",
      "Iteration: 7169000, loss: 0.10668671274262706, gradient norm: 0.2897487888018714\n",
      "Iteration: 7170000, loss: 0.10668642007690204, gradient norm: 0.2595366433501419\n",
      "Iteration: 7171000, loss: 0.10668621951325567, gradient norm: 0.4898014402179347\n",
      "Iteration: 7172000, loss: 0.10668593710439626, gradient norm: 0.13153027126896275\n",
      "Iteration: 7173000, loss: 0.10668576054960574, gradient norm: 0.14107806090437647\n",
      "Iteration: 7174000, loss: 0.10668548000713, gradient norm: 0.7725755988817072\n",
      "Iteration: 7175000, loss: 0.10668513349668719, gradient norm: 0.28775018489770887\n",
      "Iteration: 7176000, loss: 0.10668504159750494, gradient norm: 0.11011988561034933\n",
      "Iteration: 7177000, loss: 0.10668476106846714, gradient norm: 0.13772293260739304\n",
      "Iteration: 7178000, loss: 0.10668436192642117, gradient norm: 0.3637137249555009\n",
      "Iteration: 7179000, loss: 0.10668431426351765, gradient norm: 0.2393818538157519\n",
      "Iteration: 7180000, loss: 0.10668408766822166, gradient norm: 0.19888883681286648\n",
      "Iteration: 7181000, loss: 0.10668360937971011, gradient norm: 0.24414874155130176\n",
      "Iteration: 7182000, loss: 0.10668364975787528, gradient norm: 0.19836504997522847\n",
      "Iteration: 7183000, loss: 0.10668325864338073, gradient norm: 0.10571366366608521\n",
      "Iteration: 7184000, loss: 0.10668308217300179, gradient norm: 0.17327136930802256\n",
      "Iteration: 7185000, loss: 0.1066826601836178, gradient norm: 0.4750150745075142\n",
      "Iteration: 7186000, loss: 0.10668261500199869, gradient norm: 0.2065584040239683\n",
      "Iteration: 7187000, loss: 0.10668239280860195, gradient norm: 0.2670378444030574\n",
      "Iteration: 7188000, loss: 0.10668200204564077, gradient norm: 0.05139712495386092\n",
      "Iteration: 7189000, loss: 0.10668164926252638, gradient norm: 0.3158448955089532\n",
      "Iteration: 7190000, loss: 0.10668164666246045, gradient norm: 0.1456152225741697\n",
      "Iteration: 7191000, loss: 0.10668142951490418, gradient norm: 0.2657258883757388\n",
      "Iteration: 7192000, loss: 0.10668114837239664, gradient norm: 0.40644718050250483\n",
      "Iteration: 7193000, loss: 0.10668065137195483, gradient norm: 0.12666940703421137\n",
      "Iteration: 7194000, loss: 0.10668068430558003, gradient norm: 0.08680807910142299\n",
      "Iteration: 7195000, loss: 0.10668037272107415, gradient norm: 0.3440664284883957\n",
      "Iteration: 7196000, loss: 0.10668011882097284, gradient norm: 0.17756578239359508\n",
      "Iteration: 7197000, loss: 0.10667995880719754, gradient norm: 0.06596835537691685\n",
      "Iteration: 7198000, loss: 0.10667960372103358, gradient norm: 0.23229535555478908\n",
      "Iteration: 7199000, loss: 0.10667933939423846, gradient norm: 0.24705949087680237\n",
      "Iteration: 7200000, loss: 0.10667910278459326, gradient norm: 0.7393801811319336\n",
      "Iteration: 7201000, loss: 0.10667905467597194, gradient norm: 0.07299340956267923\n",
      "Iteration: 7202000, loss: 0.10667860398431063, gradient norm: 0.026462912880345114\n",
      "Iteration: 7203000, loss: 0.10667841170803717, gradient norm: 0.03963387472288091\n",
      "Iteration: 7204000, loss: 0.1066783408942668, gradient norm: 0.2798603030788647\n",
      "Iteration: 7205000, loss: 0.10667783099627502, gradient norm: 0.1808564822759077\n",
      "Iteration: 7206000, loss: 0.10667759403525119, gradient norm: 0.18067192572249763\n",
      "Iteration: 7207000, loss: 0.1066775710270795, gradient norm: 0.2576934404106375\n",
      "Iteration: 7208000, loss: 0.10667750957571326, gradient norm: 0.3419408063904689\n",
      "Iteration: 7209000, loss: 0.10667673427498696, gradient norm: 0.15946509685988178\n",
      "Iteration: 7210000, loss: 0.1066766480276685, gradient norm: 0.09352229322373437\n",
      "Iteration: 7211000, loss: 0.10667660672798503, gradient norm: 0.4117116421271684\n",
      "Iteration: 7212000, loss: 0.10667621354772547, gradient norm: 0.09192893050954405\n",
      "Iteration: 7213000, loss: 0.10667610711843872, gradient norm: 0.23809713629590884\n",
      "Iteration: 7214000, loss: 0.1066756547952212, gradient norm: 0.2946054164135821\n",
      "Iteration: 7215000, loss: 0.10667568844090326, gradient norm: 0.1937297509876905\n",
      "Iteration: 7216000, loss: 0.10667521435890981, gradient norm: 0.042077912526607954\n",
      "Iteration: 7217000, loss: 0.10667501638501374, gradient norm: 0.1539266596899929\n",
      "Iteration: 7218000, loss: 0.10667487097327749, gradient norm: 0.12907206252296122\n",
      "Iteration: 7219000, loss: 0.10667456217433552, gradient norm: 0.21281902316955068\n",
      "Iteration: 7220000, loss: 0.1066743214962546, gradient norm: 0.24082806060014994\n",
      "Iteration: 7221000, loss: 0.10667410032229874, gradient norm: 0.394911889838816\n",
      "Iteration: 7222000, loss: 0.10667386544279069, gradient norm: 0.4583820211037144\n",
      "Iteration: 7223000, loss: 0.10667358728128175, gradient norm: 0.07385910405922459\n",
      "Iteration: 7224000, loss: 0.10667336704540167, gradient norm: 0.08811314226830208\n",
      "Iteration: 7225000, loss: 0.10667305129048149, gradient norm: 0.3344369568938478\n",
      "Iteration: 7226000, loss: 0.1066729188625876, gradient norm: 0.3670174989655433\n",
      "Iteration: 7227000, loss: 0.10667272365731853, gradient norm: 0.8529831797769972\n",
      "Iteration: 7228000, loss: 0.1066723626076346, gradient norm: 0.6546691582927567\n",
      "Iteration: 7229000, loss: 0.10667210441239393, gradient norm: 0.3398601323944618\n",
      "Iteration: 7230000, loss: 0.10667203060805522, gradient norm: 0.09623483188259267\n",
      "Iteration: 7231000, loss: 0.10667175249483926, gradient norm: 0.2563567627830637\n",
      "Iteration: 7232000, loss: 0.10667145318613536, gradient norm: 0.11642864006379691\n",
      "Iteration: 7233000, loss: 0.10667131751243511, gradient norm: 0.16494602454749785\n",
      "Iteration: 7234000, loss: 0.10667093306179112, gradient norm: 0.4995036827155144\n",
      "Iteration: 7235000, loss: 0.10667072580636644, gradient norm: 0.2660718318582047\n",
      "Iteration: 7236000, loss: 0.10667052490123874, gradient norm: 0.23348797122481157\n",
      "Iteration: 7237000, loss: 0.10667028933345492, gradient norm: 0.14737640062822377\n",
      "Iteration: 7238000, loss: 0.10666991792276571, gradient norm: 0.15545632387169198\n",
      "Iteration: 7239000, loss: 0.10666979650254806, gradient norm: 0.20234458943913017\n",
      "Iteration: 7240000, loss: 0.10666976760983314, gradient norm: 0.5063407565505931\n",
      "Iteration: 7241000, loss: 0.10666931708418075, gradient norm: 0.5921380621604784\n",
      "Iteration: 7242000, loss: 0.1066689582348058, gradient norm: 0.17219548893597775\n",
      "Iteration: 7243000, loss: 0.10666883268576063, gradient norm: 0.14242355305049254\n",
      "Iteration: 7244000, loss: 0.10666863940522761, gradient norm: 0.07209863471302926\n",
      "Iteration: 7245000, loss: 0.10666836539121027, gradient norm: 0.11712715306676609\n",
      "Iteration: 7246000, loss: 0.10666800262920044, gradient norm: 0.2363481050700211\n",
      "Iteration: 7247000, loss: 0.1066678917608266, gradient norm: 0.2879048790875567\n",
      "Iteration: 7248000, loss: 0.1066677776067005, gradient norm: 0.11317487187055164\n",
      "Iteration: 7249000, loss: 0.10666745101961995, gradient norm: 0.47716400665946135\n",
      "Iteration: 7250000, loss: 0.10666706572417407, gradient norm: 0.0912492673903163\n",
      "Iteration: 7251000, loss: 0.10666699637095593, gradient norm: 0.6858584313370859\n",
      "Iteration: 7252000, loss: 0.10666657842739397, gradient norm: 0.5124214621767424\n",
      "Iteration: 7253000, loss: 0.10666658111125349, gradient norm: 0.3388984901134263\n",
      "Iteration: 7254000, loss: 0.10666611017197625, gradient norm: 0.2802614571892184\n",
      "Iteration: 7255000, loss: 0.1066661694079287, gradient norm: 0.20795368440054865\n",
      "Iteration: 7256000, loss: 0.1066657022347628, gradient norm: 0.23794611895910794\n",
      "Iteration: 7257000, loss: 0.10666547728490665, gradient norm: 0.2993340040242288\n",
      "Iteration: 7258000, loss: 0.10666530510016517, gradient norm: 0.2268330407208071\n",
      "Iteration: 7259000, loss: 0.10666505753090623, gradient norm: 0.39978884804445874\n",
      "Iteration: 7260000, loss: 0.10666467844596988, gradient norm: 0.07473527514714456\n",
      "Iteration: 7261000, loss: 0.10666462978356839, gradient norm: 0.7011307027306031\n",
      "Iteration: 7262000, loss: 0.10666425594877185, gradient norm: 0.14001789843518941\n",
      "Iteration: 7263000, loss: 0.10666426972081186, gradient norm: 0.47354732764856666\n",
      "Iteration: 7264000, loss: 0.10666374488098769, gradient norm: 0.2884130455219227\n",
      "Iteration: 7265000, loss: 0.1066637584596523, gradient norm: 0.04732384958776303\n",
      "Iteration: 7266000, loss: 0.10666340469760181, gradient norm: 0.5709559135817434\n",
      "Iteration: 7267000, loss: 0.10666305215501476, gradient norm: 0.22297249530919663\n",
      "Iteration: 7268000, loss: 0.106662966834347, gradient norm: 0.6781175322706633\n",
      "Iteration: 7269000, loss: 0.10666267646076098, gradient norm: 0.10828699956007716\n",
      "Iteration: 7270000, loss: 0.10666236513356492, gradient norm: 0.21536119505053883\n",
      "Iteration: 7271000, loss: 0.10666217448711453, gradient norm: 0.48269864849222976\n",
      "Iteration: 7272000, loss: 0.10666209083932285, gradient norm: 0.5377020283226221\n",
      "Iteration: 7273000, loss: 0.10666178934094349, gradient norm: 0.25719690443751714\n",
      "Iteration: 7274000, loss: 0.10666136651986506, gradient norm: 0.13825931133226796\n",
      "Iteration: 7275000, loss: 0.10666133260760685, gradient norm: 0.19176479704782612\n",
      "Iteration: 7276000, loss: 0.10666102287604394, gradient norm: 0.2068882090683686\n",
      "Iteration: 7277000, loss: 0.10666067001932274, gradient norm: 0.5707287396141033\n",
      "Iteration: 7278000, loss: 0.10666060018711654, gradient norm: 0.25078338724062227\n",
      "Iteration: 7279000, loss: 0.10666045067097774, gradient norm: 0.17580109470815425\n",
      "Iteration: 7280000, loss: 0.10665997957825323, gradient norm: 0.31909852344515194\n",
      "Iteration: 7281000, loss: 0.10665984016048542, gradient norm: 0.5204610780860549\n",
      "Iteration: 7282000, loss: 0.10665954505852933, gradient norm: 0.07401437328297225\n",
      "Iteration: 7283000, loss: 0.10665939698367005, gradient norm: 0.29215576789631936\n",
      "Iteration: 7284000, loss: 0.10665922151344978, gradient norm: 0.7215198071365702\n",
      "Iteration: 7285000, loss: 0.10665891504793563, gradient norm: 0.09776338918091326\n",
      "Iteration: 7286000, loss: 0.10665873712370434, gradient norm: 0.08794755304974351\n",
      "Iteration: 7287000, loss: 0.10665836054636843, gradient norm: 0.015731240732177347\n",
      "Iteration: 7288000, loss: 0.10665817196047392, gradient norm: 0.34551324228292163\n",
      "Iteration: 7289000, loss: 0.10665794713301528, gradient norm: 0.8371608406396991\n",
      "Iteration: 7290000, loss: 0.10665766479807348, gradient norm: 0.0678346809904086\n",
      "Iteration: 7291000, loss: 0.10665749458958095, gradient norm: 0.1603013408411957\n",
      "Iteration: 7292000, loss: 0.10665735590342358, gradient norm: 0.04532293437923986\n",
      "Iteration: 7293000, loss: 0.10665707007264812, gradient norm: 0.2041426498608238\n",
      "Iteration: 7294000, loss: 0.10665673581205687, gradient norm: 0.7786688936671557\n",
      "Iteration: 7295000, loss: 0.10665653413117449, gradient norm: 0.4981643821192181\n",
      "Iteration: 7296000, loss: 0.10665635893579598, gradient norm: 1.108848274664985\n",
      "Iteration: 7297000, loss: 0.10665597416123507, gradient norm: 0.12533340885298638\n",
      "Iteration: 7298000, loss: 0.10665593668810973, gradient norm: 0.08922736013859411\n",
      "Iteration: 7299000, loss: 0.10665563271767187, gradient norm: 0.14436321034247346\n",
      "Iteration: 7300000, loss: 0.10665540140771872, gradient norm: 0.021988581908892264\n",
      "Iteration: 7301000, loss: 0.10665520652335363, gradient norm: 0.01751699531700838\n",
      "Iteration: 7302000, loss: 0.10665492671874875, gradient norm: 0.09412031850820408\n",
      "Iteration: 7303000, loss: 0.10665456726779052, gradient norm: 0.16138732664369995\n",
      "Iteration: 7304000, loss: 0.10665441351181774, gradient norm: 0.26118704073528554\n",
      "Iteration: 7305000, loss: 0.10665428339718394, gradient norm: 0.13000398019100037\n",
      "Iteration: 7306000, loss: 0.10665415090413945, gradient norm: 0.4359463800502907\n",
      "Iteration: 7307000, loss: 0.10665360086294501, gradient norm: 0.4791717271440277\n",
      "Iteration: 7308000, loss: 0.10665351938460836, gradient norm: 0.4506202905834355\n",
      "Iteration: 7309000, loss: 0.10665321152146989, gradient norm: 0.26675480642356575\n",
      "Iteration: 7310000, loss: 0.1066530599015472, gradient norm: 0.6351853069480726\n",
      "Iteration: 7311000, loss: 0.10665277692811045, gradient norm: 0.2880225273983751\n",
      "Iteration: 7312000, loss: 0.10665273171096468, gradient norm: 0.10669014461152279\n",
      "Iteration: 7313000, loss: 0.10665226541250379, gradient norm: 0.23512747218068436\n",
      "Iteration: 7314000, loss: 0.10665223358762667, gradient norm: 0.07682932631427405\n",
      "Iteration: 7315000, loss: 0.10665176069483405, gradient norm: 0.18905197297028556\n",
      "Iteration: 7316000, loss: 0.10665186249552781, gradient norm: 0.19487942726180932\n",
      "Iteration: 7317000, loss: 0.10665123308617704, gradient norm: 0.10754234841626818\n",
      "Iteration: 7318000, loss: 0.10665114141893592, gradient norm: 0.02320439542225409\n",
      "Iteration: 7319000, loss: 0.10665100429324684, gradient norm: 0.01890410369023462\n",
      "Iteration: 7320000, loss: 0.1066505395408575, gradient norm: 0.47928336625123164\n",
      "Iteration: 7321000, loss: 0.10665061177959383, gradient norm: 0.46499865655865513\n",
      "Iteration: 7322000, loss: 0.10665025697911404, gradient norm: 0.10616919441730036\n",
      "Iteration: 7323000, loss: 0.10664992846373486, gradient norm: 0.44872447252588943\n",
      "Iteration: 7324000, loss: 0.10664989809714354, gradient norm: 0.16795030530512878\n",
      "Iteration: 7325000, loss: 0.1066494043340717, gradient norm: 0.13663969526849576\n",
      "Iteration: 7326000, loss: 0.10664944002805632, gradient norm: 0.03204643059411262\n",
      "Iteration: 7327000, loss: 0.10664905221488413, gradient norm: 0.2603970671408322\n",
      "Iteration: 7328000, loss: 0.10664891334223822, gradient norm: 0.05483490366940404\n",
      "Iteration: 7329000, loss: 0.10664874610798813, gradient norm: 0.11302210409343304\n",
      "Iteration: 7330000, loss: 0.10664829855514145, gradient norm: 0.25993279474864406\n",
      "Iteration: 7331000, loss: 0.10664811818110678, gradient norm: 0.5478598136836352\n",
      "Iteration: 7332000, loss: 0.10664788895742004, gradient norm: 0.10440720230182426\n",
      "Iteration: 7333000, loss: 0.10664774994929092, gradient norm: 0.059255654588718724\n",
      "Iteration: 7334000, loss: 0.1066474496377571, gradient norm: 0.10103264489774919\n",
      "Iteration: 7335000, loss: 0.10664716350558154, gradient norm: 0.06992793427014658\n",
      "Iteration: 7336000, loss: 0.10664706216672457, gradient norm: 0.20177759091469769\n",
      "Iteration: 7337000, loss: 0.10664671064952284, gradient norm: 0.4072948253845817\n",
      "Iteration: 7338000, loss: 0.10664656781424352, gradient norm: 0.03520810527538396\n",
      "Iteration: 7339000, loss: 0.10664633536460892, gradient norm: 0.1013034267776389\n",
      "Iteration: 7340000, loss: 0.10664599071362729, gradient norm: 0.05854229586606091\n",
      "Iteration: 7341000, loss: 0.10664601627242451, gradient norm: 0.05817041715443272\n",
      "Iteration: 7342000, loss: 0.1066455885269067, gradient norm: 0.353735756793319\n",
      "Iteration: 7343000, loss: 0.1066453005391657, gradient norm: 0.06293156435033237\n",
      "Iteration: 7344000, loss: 0.10664517295363334, gradient norm: 0.12577650128940648\n",
      "Iteration: 7345000, loss: 0.10664492430727882, gradient norm: 0.020980080625520956\n",
      "Iteration: 7346000, loss: 0.10664489686516164, gradient norm: 0.2350633017017018\n",
      "Iteration: 7347000, loss: 0.10664424146399556, gradient norm: 0.11696417490958336\n",
      "Iteration: 7348000, loss: 0.10664432316890435, gradient norm: 0.1841383376046515\n",
      "Iteration: 7349000, loss: 0.10664385057338946, gradient norm: 0.6390537464399579\n",
      "Iteration: 7350000, loss: 0.10664380927884544, gradient norm: 0.19705249711549858\n",
      "Iteration: 7351000, loss: 0.1066435309973651, gradient norm: 0.09367329352511024\n",
      "Iteration: 7352000, loss: 0.10664331579471176, gradient norm: 0.15770206153006458\n",
      "Iteration: 7353000, loss: 0.10664305750323792, gradient norm: 0.16050132257525154\n",
      "Iteration: 7354000, loss: 0.106642738896159, gradient norm: 0.720106335793688\n",
      "Iteration: 7355000, loss: 0.10664264301180178, gradient norm: 0.4148987065958011\n",
      "Iteration: 7356000, loss: 0.10664235798821549, gradient norm: 0.16898856839557275\n",
      "Iteration: 7357000, loss: 0.10664208298792878, gradient norm: 0.25236561707681565\n",
      "Iteration: 7358000, loss: 0.10664205110132209, gradient norm: 0.13496938609649528\n",
      "Iteration: 7359000, loss: 0.10664149633377008, gradient norm: 0.10431988000458106\n",
      "Iteration: 7360000, loss: 0.10664141400198253, gradient norm: 0.6187007493789948\n",
      "Iteration: 7361000, loss: 0.10664131187758068, gradient norm: 0.25632294216205076\n",
      "Iteration: 7362000, loss: 0.10664094636018591, gradient norm: 0.0918166012218803\n",
      "Iteration: 7363000, loss: 0.106640765044589, gradient norm: 0.10012732329175263\n",
      "Iteration: 7364000, loss: 0.10664038740997762, gradient norm: 0.1477199924833436\n",
      "Iteration: 7365000, loss: 0.10664036043930201, gradient norm: 0.19551312107602176\n",
      "Iteration: 7366000, loss: 0.10663999614976158, gradient norm: 0.21265161094412577\n",
      "Iteration: 7367000, loss: 0.10663994564297241, gradient norm: 0.06057462799664421\n",
      "Iteration: 7368000, loss: 0.1066395505558369, gradient norm: 0.17502309717332135\n",
      "Iteration: 7369000, loss: 0.10663939928837698, gradient norm: 0.5729466925844717\n",
      "Iteration: 7370000, loss: 0.10663907462987629, gradient norm: 0.11830044044537201\n",
      "Iteration: 7371000, loss: 0.10663893391278959, gradient norm: 0.3722503620360404\n",
      "Iteration: 7372000, loss: 0.10663850531544668, gradient norm: 0.33061591246414457\n",
      "Iteration: 7373000, loss: 0.10663854580448236, gradient norm: 0.16025272072912458\n",
      "Iteration: 7374000, loss: 0.106638204414555, gradient norm: 0.07417687139165012\n",
      "Iteration: 7375000, loss: 0.10663808265400572, gradient norm: 0.09954615800815751\n",
      "Iteration: 7376000, loss: 0.10663754593682702, gradient norm: 0.21371067911017685\n",
      "Iteration: 7377000, loss: 0.10663750273976, gradient norm: 0.9242428485248546\n",
      "Iteration: 7378000, loss: 0.10663738153993715, gradient norm: 0.5789596869785376\n",
      "Iteration: 7379000, loss: 0.10663700509387827, gradient norm: 0.08048109008335291\n",
      "Iteration: 7380000, loss: 0.10663668902537222, gradient norm: 0.26901688208984276\n",
      "Iteration: 7381000, loss: 0.10663665331140652, gradient norm: 0.49938990467332844\n",
      "Iteration: 7382000, loss: 0.1066362693618717, gradient norm: 0.169588043858797\n",
      "Iteration: 7383000, loss: 0.10663630584745755, gradient norm: 0.7504203128422925\n",
      "Iteration: 7384000, loss: 0.10663578092080823, gradient norm: 0.2827533604698142\n",
      "Iteration: 7385000, loss: 0.10663554374136026, gradient norm: 0.14149521907084256\n",
      "Iteration: 7386000, loss: 0.10663535178898238, gradient norm: 0.11151654561402007\n",
      "Iteration: 7387000, loss: 0.10663519551356336, gradient norm: 0.2549794756959483\n",
      "Iteration: 7388000, loss: 0.1066348847117784, gradient norm: 0.17371673229572684\n",
      "Iteration: 7389000, loss: 0.10663456222616223, gradient norm: 0.436754940156988\n",
      "Iteration: 7390000, loss: 0.10663419409327465, gradient norm: 0.5731622896084257\n",
      "Iteration: 7391000, loss: 0.10663357739500938, gradient norm: 0.30726602371609496\n",
      "Iteration: 7392000, loss: 0.10663281405186475, gradient norm: 0.2803797211875276\n",
      "Iteration: 7393000, loss: 0.10663207909626155, gradient norm: 0.10047066413930186\n",
      "Iteration: 7394000, loss: 0.10663157218971195, gradient norm: 0.1907496782837801\n",
      "Iteration: 7395000, loss: 0.10663108457852996, gradient norm: 0.8366746457360469\n",
      "Iteration: 7396000, loss: 0.10663061583877413, gradient norm: 0.14649705372534227\n",
      "Iteration: 7397000, loss: 0.10663024455242247, gradient norm: 0.097135403466042\n",
      "Iteration: 7398000, loss: 0.1066299619459025, gradient norm: 0.19839718403638695\n",
      "Iteration: 7399000, loss: 0.10662955810542835, gradient norm: 0.6242149059380964\n",
      "Iteration: 7400000, loss: 0.10662919379554495, gradient norm: 0.22889987779191848\n",
      "Iteration: 7401000, loss: 0.10662881930933207, gradient norm: 0.2661500535006436\n",
      "Iteration: 7402000, loss: 0.10662861744231072, gradient norm: 0.20458673523795767\n",
      "Iteration: 7403000, loss: 0.10662821950138818, gradient norm: 0.5659478019635118\n",
      "Iteration: 7404000, loss: 0.10662781428107085, gradient norm: 0.2777169035934836\n",
      "Iteration: 7405000, loss: 0.1066277294425617, gradient norm: 0.6323985217973793\n",
      "Iteration: 7406000, loss: 0.10662713038413443, gradient norm: 0.2391418873401613\n",
      "Iteration: 7407000, loss: 0.10662715790145409, gradient norm: 0.17161044641460702\n",
      "Iteration: 7408000, loss: 0.1066267040979464, gradient norm: 0.6770911865295254\n",
      "Iteration: 7409000, loss: 0.10662652378686466, gradient norm: 0.3182806268156074\n",
      "Iteration: 7410000, loss: 0.10662610145920141, gradient norm: 0.012361696375619815\n",
      "Iteration: 7411000, loss: 0.10662592769317518, gradient norm: 0.04093345705990965\n",
      "Iteration: 7412000, loss: 0.10662560133780259, gradient norm: 0.7033273931164816\n",
      "Iteration: 7413000, loss: 0.10662531892190205, gradient norm: 0.11437042346916171\n",
      "Iteration: 7414000, loss: 0.10662522864262812, gradient norm: 0.20239080944872115\n",
      "Iteration: 7415000, loss: 0.10662474665552112, gradient norm: 0.3988485440356904\n",
      "Iteration: 7416000, loss: 0.10662458899536792, gradient norm: 0.5295525656476816\n",
      "Iteration: 7417000, loss: 0.10662425092611763, gradient norm: 0.4760004426336129\n",
      "Iteration: 7418000, loss: 0.10662411527810242, gradient norm: 0.32621743343416487\n",
      "Iteration: 7419000, loss: 0.10662383547583781, gradient norm: 0.14482190985243112\n",
      "Iteration: 7420000, loss: 0.10662344531483998, gradient norm: 0.07323621377062542\n",
      "Iteration: 7421000, loss: 0.10662339692800103, gradient norm: 0.7489861589009532\n",
      "Iteration: 7422000, loss: 0.10662273566110561, gradient norm: 0.35210437058298477\n",
      "Iteration: 7423000, loss: 0.10662289395679576, gradient norm: 0.17215002206804025\n",
      "Iteration: 7424000, loss: 0.10662243345340955, gradient norm: 0.1458133988987334\n",
      "Iteration: 7425000, loss: 0.1066222206187157, gradient norm: 0.10643674419168458\n",
      "Iteration: 7426000, loss: 0.10662197952513182, gradient norm: 0.44222255343157635\n",
      "Iteration: 7427000, loss: 0.10662176475811842, gradient norm: 0.19007281581288196\n",
      "Iteration: 7428000, loss: 0.10662139792193791, gradient norm: 0.14496104885691688\n",
      "Iteration: 7429000, loss: 0.1066214109492937, gradient norm: 0.040296733288370384\n",
      "Iteration: 7430000, loss: 0.10662097969576155, gradient norm: 0.04663814505817434\n",
      "Iteration: 7431000, loss: 0.10662073681531481, gradient norm: 0.18462971316414054\n",
      "Iteration: 7432000, loss: 0.10662036061635273, gradient norm: 0.23025715656257456\n",
      "Iteration: 7433000, loss: 0.10662029856015072, gradient norm: 0.29878555140189245\n",
      "Iteration: 7434000, loss: 0.10661989526222829, gradient norm: 0.14907397592279142\n",
      "Iteration: 7435000, loss: 0.1066198787759576, gradient norm: 0.4270111820155894\n",
      "Iteration: 7436000, loss: 0.10661969037237322, gradient norm: 0.22237906909233457\n",
      "Iteration: 7437000, loss: 0.1066190892382034, gradient norm: 0.2884391177368903\n",
      "Iteration: 7438000, loss: 0.10661911510267581, gradient norm: 0.20716506374575167\n",
      "Iteration: 7439000, loss: 0.1066187949648158, gradient norm: 0.26975030951147105\n",
      "Iteration: 7440000, loss: 0.10661838814363978, gradient norm: 0.39541375815863583\n",
      "Iteration: 7441000, loss: 0.1066183201434007, gradient norm: 0.5938663760063204\n",
      "Iteration: 7442000, loss: 0.10661815242241195, gradient norm: 0.3899878726410788\n",
      "Iteration: 7443000, loss: 0.1066177432252707, gradient norm: 0.6227088931949172\n",
      "Iteration: 7444000, loss: 0.10661769461255015, gradient norm: 0.10427050437359922\n",
      "Iteration: 7445000, loss: 0.10661733057626964, gradient norm: 0.3502540864199742\n",
      "Iteration: 7446000, loss: 0.10661706974657889, gradient norm: 0.2361953746435648\n",
      "Iteration: 7447000, loss: 0.10661698865037471, gradient norm: 0.20407249523649243\n",
      "Iteration: 7448000, loss: 0.10661651277812356, gradient norm: 0.09326963373216203\n",
      "Iteration: 7449000, loss: 0.10661654051953678, gradient norm: 0.5916072341688706\n",
      "Iteration: 7450000, loss: 0.10661598634341264, gradient norm: 0.15717013576799518\n",
      "Iteration: 7451000, loss: 0.10661590261723677, gradient norm: 0.09445567899815913\n",
      "Iteration: 7452000, loss: 0.10661573481806753, gradient norm: 0.4567961905802801\n",
      "Iteration: 7453000, loss: 0.10661539589661337, gradient norm: 0.16159585932948975\n",
      "Iteration: 7454000, loss: 0.10661520476586639, gradient norm: 0.36761408075978763\n",
      "Iteration: 7455000, loss: 0.10661492853362486, gradient norm: 0.18638727436437597\n",
      "Iteration: 7456000, loss: 0.106614816834944, gradient norm: 0.2681491890938877\n",
      "Iteration: 7457000, loss: 0.10661445727786471, gradient norm: 0.5998999973968181\n",
      "Iteration: 7458000, loss: 0.10661432751099943, gradient norm: 0.08910245262937334\n",
      "Iteration: 7459000, loss: 0.10661391958570283, gradient norm: 0.2542102982226327\n",
      "Iteration: 7460000, loss: 0.10661387946246019, gradient norm: 0.05360429331953894\n",
      "Iteration: 7461000, loss: 0.10661350662583899, gradient norm: 0.25438194961947086\n",
      "Iteration: 7462000, loss: 0.10661329894565713, gradient norm: 0.6218718174966078\n",
      "Iteration: 7463000, loss: 0.10661316195819973, gradient norm: 0.3237522103816716\n",
      "Iteration: 7464000, loss: 0.10661292500363985, gradient norm: 0.34406100970044967\n",
      "Iteration: 7465000, loss: 0.10661241974497715, gradient norm: 0.1345886854179039\n",
      "Iteration: 7466000, loss: 0.10661252120843502, gradient norm: 0.32301610016230115\n",
      "Iteration: 7467000, loss: 0.10661207090940814, gradient norm: 0.1376018731738202\n",
      "Iteration: 7468000, loss: 0.10661193062554024, gradient norm: 0.12805189667472644\n",
      "Iteration: 7469000, loss: 0.10661167706341594, gradient norm: 0.11316340435090062\n",
      "Iteration: 7470000, loss: 0.10661152403454162, gradient norm: 0.21311574788197407\n",
      "Iteration: 7471000, loss: 0.10661118442233779, gradient norm: 0.08108806067250726\n",
      "Iteration: 7472000, loss: 0.1066110192539166, gradient norm: 0.022257110069604787\n",
      "Iteration: 7473000, loss: 0.10661087388661294, gradient norm: 0.27589941141985547\n",
      "Iteration: 7474000, loss: 0.10661049093442028, gradient norm: 0.22787261825229815\n",
      "Iteration: 7475000, loss: 0.10661036615586487, gradient norm: 0.1262313755537846\n",
      "Iteration: 7476000, loss: 0.10661015777234323, gradient norm: 0.25677518169404046\n",
      "Iteration: 7477000, loss: 0.10660986035426025, gradient norm: 0.014547248133459933\n",
      "Iteration: 7478000, loss: 0.10660941773880207, gradient norm: 0.7893261170231562\n",
      "Iteration: 7479000, loss: 0.10660938131629857, gradient norm: 0.471456002935716\n",
      "Iteration: 7480000, loss: 0.10660926577348559, gradient norm: 0.3847596906378801\n",
      "Iteration: 7481000, loss: 0.10660878781772982, gradient norm: 0.33941403460825464\n",
      "Iteration: 7482000, loss: 0.10660872538973251, gradient norm: 0.15061545006836224\n",
      "Iteration: 7483000, loss: 0.10660845170964481, gradient norm: 0.16820614131607764\n",
      "Iteration: 7484000, loss: 0.10660831144070734, gradient norm: 0.15940921145136863\n",
      "Iteration: 7485000, loss: 0.106608060995389, gradient norm: 0.8123998255773964\n",
      "Iteration: 7486000, loss: 0.10660772071687705, gradient norm: 1.2228929976901186\n",
      "Iteration: 7487000, loss: 0.1066076093310098, gradient norm: 0.21719275957171064\n",
      "Iteration: 7488000, loss: 0.10660726718698382, gradient norm: 0.4966685533212361\n",
      "Iteration: 7489000, loss: 0.10660715215849428, gradient norm: 0.2550115349114106\n",
      "Iteration: 7490000, loss: 0.10660676257654716, gradient norm: 0.1523484215896849\n",
      "Iteration: 7491000, loss: 0.10660660403418254, gradient norm: 0.5987048878075235\n",
      "Iteration: 7492000, loss: 0.10660632973876119, gradient norm: 0.16764401239082016\n",
      "Iteration: 7493000, loss: 0.10660620608453195, gradient norm: 0.739746622310306\n",
      "Iteration: 7494000, loss: 0.10660598827334354, gradient norm: 0.30416139729964903\n",
      "Iteration: 7495000, loss: 0.1066056362352841, gradient norm: 0.13987973166845072\n",
      "Iteration: 7496000, loss: 0.1066054902548031, gradient norm: 0.4641260256335398\n",
      "Iteration: 7497000, loss: 0.10660531719196556, gradient norm: 0.21246059577604712\n",
      "Iteration: 7498000, loss: 0.10660498400418345, gradient norm: 0.6506619807480394\n",
      "Iteration: 7499000, loss: 0.10660475977358636, gradient norm: 0.22618844308954053\n",
      "Iteration: 7500000, loss: 0.10660467862988963, gradient norm: 0.5222239368081202\n",
      "Iteration: 7501000, loss: 0.10660434394757402, gradient norm: 0.24679738153453776\n",
      "Iteration: 7502000, loss: 0.1066040058106618, gradient norm: 0.5203706658414904\n",
      "Iteration: 7503000, loss: 0.10660398284890676, gradient norm: 0.09296441476198702\n",
      "Iteration: 7504000, loss: 0.10660352402879582, gradient norm: 0.19092245184996834\n",
      "Iteration: 7505000, loss: 0.10660349085047136, gradient norm: 0.30262015213155175\n",
      "Iteration: 7506000, loss: 0.10660315126781422, gradient norm: 0.22592031714598523\n",
      "Iteration: 7507000, loss: 0.10660303748241334, gradient norm: 0.12646289346878534\n",
      "Iteration: 7508000, loss: 0.10660283892446203, gradient norm: 0.15221675854516015\n",
      "Iteration: 7509000, loss: 0.1066023855868972, gradient norm: 0.39745218583564157\n",
      "Iteration: 7510000, loss: 0.10660244989990988, gradient norm: 0.9206734434502589\n",
      "Iteration: 7511000, loss: 0.10660199817714455, gradient norm: 0.04503778329021069\n",
      "Iteration: 7512000, loss: 0.10660188864537898, gradient norm: 0.4461510511476544\n",
      "Iteration: 7513000, loss: 0.10660174544814291, gradient norm: 0.5357324942816843\n",
      "Iteration: 7514000, loss: 0.10660123398311651, gradient norm: 0.23458496525209113\n",
      "Iteration: 7515000, loss: 0.10660117023080226, gradient norm: 0.21448717277904222\n",
      "Iteration: 7516000, loss: 0.10660099317618073, gradient norm: 0.6224810547174207\n",
      "Iteration: 7517000, loss: 0.10660071587542826, gradient norm: 0.11967599920575382\n",
      "Iteration: 7518000, loss: 0.10660039557984245, gradient norm: 0.04489346971020995\n",
      "Iteration: 7519000, loss: 0.10660032604751742, gradient norm: 0.17962865826145738\n",
      "Iteration: 7520000, loss: 0.1066001532157683, gradient norm: 0.22544419013075004\n",
      "Iteration: 7521000, loss: 0.10659982370488119, gradient norm: 0.11805363001837825\n",
      "Iteration: 7522000, loss: 0.1065994525061175, gradient norm: 0.2386430194554172\n",
      "Iteration: 7523000, loss: 0.10659959590868809, gradient norm: 0.3723073503626024\n",
      "Iteration: 7524000, loss: 0.10659893639795825, gradient norm: 0.5101309159366385\n",
      "Iteration: 7525000, loss: 0.10659900570649543, gradient norm: 0.022793291317889316\n",
      "Iteration: 7526000, loss: 0.1065987517285839, gradient norm: 0.11157207742487998\n",
      "Iteration: 7527000, loss: 0.10659840576271465, gradient norm: 0.030282787825697183\n",
      "Iteration: 7528000, loss: 0.10659832593566194, gradient norm: 0.4407605842074063\n",
      "Iteration: 7529000, loss: 0.10659789827849254, gradient norm: 0.3275438929478399\n",
      "Iteration: 7530000, loss: 0.1065979264242392, gradient norm: 0.05781190021845952\n",
      "Iteration: 7531000, loss: 0.10659745505553804, gradient norm: 0.7895927443885626\n",
      "Iteration: 7532000, loss: 0.10659743747434935, gradient norm: 0.13584417958153736\n",
      "Iteration: 7533000, loss: 0.1065971109774739, gradient norm: 0.046341698274027926\n",
      "Iteration: 7534000, loss: 0.10659686268159178, gradient norm: 0.25143284577890135\n",
      "Iteration: 7535000, loss: 0.10659659636679293, gradient norm: 0.012896620570262526\n",
      "Iteration: 7536000, loss: 0.10659657080202725, gradient norm: 0.6350002269476995\n",
      "Iteration: 7537000, loss: 0.10659620518955368, gradient norm: 0.336752892914058\n",
      "Iteration: 7538000, loss: 0.10659609519236725, gradient norm: 0.392233271039582\n",
      "Iteration: 7539000, loss: 0.10659567901283967, gradient norm: 0.6839186737418765\n",
      "Iteration: 7540000, loss: 0.10659560667165398, gradient norm: 0.44742079364474496\n",
      "Iteration: 7541000, loss: 0.10659541985313507, gradient norm: 0.09806579885981886\n",
      "Iteration: 7542000, loss: 0.10659509741648386, gradient norm: 0.079702818877992\n",
      "Iteration: 7543000, loss: 0.10659476079415901, gradient norm: 0.2446240209992767\n",
      "Iteration: 7544000, loss: 0.10659473876871366, gradient norm: 0.13324381895675824\n",
      "Iteration: 7545000, loss: 0.10659444089949988, gradient norm: 0.18999209881178028\n",
      "Iteration: 7546000, loss: 0.10659432622166128, gradient norm: 0.22378081948619014\n",
      "Iteration: 7547000, loss: 0.10659395034467041, gradient norm: 0.7779621776938939\n",
      "Iteration: 7548000, loss: 0.10659384371919596, gradient norm: 0.15821842835090855\n",
      "Iteration: 7549000, loss: 0.10659354817899394, gradient norm: 0.03686355681412211\n",
      "Iteration: 7550000, loss: 0.10659316167763748, gradient norm: 0.4698519566145103\n",
      "Iteration: 7551000, loss: 0.10659325685634513, gradient norm: 0.5120063990701371\n",
      "Iteration: 7552000, loss: 0.10659290350298965, gradient norm: 0.2579304937735573\n",
      "Iteration: 7553000, loss: 0.10659244811240888, gradient norm: 0.42605100127648887\n",
      "Iteration: 7554000, loss: 0.10659271940854953, gradient norm: 0.12212836197202522\n",
      "Iteration: 7555000, loss: 0.10659206865024858, gradient norm: 0.108939296260244\n",
      "Iteration: 7556000, loss: 0.10659195316754021, gradient norm: 0.6531496882897053\n",
      "Iteration: 7557000, loss: 0.10659183017739639, gradient norm: 0.1917885988047657\n",
      "Iteration: 7558000, loss: 0.10659150239492045, gradient norm: 0.13414393305754072\n",
      "Iteration: 7559000, loss: 0.10659131364508234, gradient norm: 0.14619079610769853\n",
      "Iteration: 7560000, loss: 0.10659109127439677, gradient norm: 0.37671031756876433\n",
      "Iteration: 7561000, loss: 0.10659081849249238, gradient norm: 0.45280856423170746\n",
      "Iteration: 7562000, loss: 0.10659077064668684, gradient norm: 0.16238831802864756\n",
      "Iteration: 7563000, loss: 0.10659033580027144, gradient norm: 0.19883467949466643\n",
      "Iteration: 7564000, loss: 0.10659023188564785, gradient norm: 0.32722164217980076\n",
      "Iteration: 7565000, loss: 0.10659005220578015, gradient norm: 0.37356081720964174\n",
      "Iteration: 7566000, loss: 0.10658975449319576, gradient norm: 0.5898929861740235\n",
      "Iteration: 7567000, loss: 0.1065895319589932, gradient norm: 0.21539490195590077\n",
      "Iteration: 7568000, loss: 0.10658944799203156, gradient norm: 0.1512448144212141\n",
      "Iteration: 7569000, loss: 0.1065891682778998, gradient norm: 0.3081667849356755\n",
      "Iteration: 7570000, loss: 0.1065888124322672, gradient norm: 0.1366517187515688\n",
      "Iteration: 7571000, loss: 0.10658857249383524, gradient norm: 0.20460299569352314\n",
      "Iteration: 7572000, loss: 0.10658851512310767, gradient norm: 0.10899480295714872\n",
      "Iteration: 7573000, loss: 0.10658815541618671, gradient norm: 0.11554646355300927\n",
      "Iteration: 7574000, loss: 0.10658795747329029, gradient norm: 0.2965105449766838\n",
      "Iteration: 7575000, loss: 0.10658774647148775, gradient norm: 0.5052342099513194\n",
      "Iteration: 7576000, loss: 0.10658762750649532, gradient norm: 0.1871090319672216\n",
      "Iteration: 7577000, loss: 0.10658738519202085, gradient norm: 0.5013533341887866\n",
      "Iteration: 7578000, loss: 0.10658701182044948, gradient norm: 0.5681852841441475\n",
      "Iteration: 7579000, loss: 0.10658711029303111, gradient norm: 0.12055090164740713\n",
      "Iteration: 7580000, loss: 0.10658654684098955, gradient norm: 0.09167024003599245\n",
      "Iteration: 7581000, loss: 0.10658665676107312, gradient norm: 0.6031709846874318\n",
      "Iteration: 7582000, loss: 0.10658614071271384, gradient norm: 0.18295335253672265\n",
      "Iteration: 7583000, loss: 0.10658587072083413, gradient norm: 0.42857120892750594\n",
      "Iteration: 7584000, loss: 0.10658581796383261, gradient norm: 0.10887144746725179\n",
      "Iteration: 7585000, loss: 0.10658551382069313, gradient norm: 0.1344253714735617\n",
      "Iteration: 7586000, loss: 0.10658549643597252, gradient norm: 0.06486613315767945\n",
      "Iteration: 7587000, loss: 0.10658509808916408, gradient norm: 0.15069095688756617\n",
      "Iteration: 7588000, loss: 0.10658497473950274, gradient norm: 0.6228929514528333\n",
      "Iteration: 7589000, loss: 0.1065845701920557, gradient norm: 0.06365026526236554\n",
      "Iteration: 7590000, loss: 0.1065846696580406, gradient norm: 0.6930297582879474\n",
      "Iteration: 7591000, loss: 0.10658415679905081, gradient norm: 0.06402155324784563\n",
      "Iteration: 7592000, loss: 0.1065840452600193, gradient norm: 0.14207743266251577\n",
      "Iteration: 7593000, loss: 0.1065837292127232, gradient norm: 0.18913416613159298\n",
      "Iteration: 7594000, loss: 0.1065836442160331, gradient norm: 0.1473348057169904\n",
      "Iteration: 7595000, loss: 0.10658338076370649, gradient norm: 0.4267303896879096\n",
      "Iteration: 7596000, loss: 0.10658310327778882, gradient norm: 0.1524545571977066\n",
      "Iteration: 7597000, loss: 0.10658291663339116, gradient norm: 0.35321708504073684\n",
      "Iteration: 7598000, loss: 0.106582709061212, gradient norm: 0.2622548062103062\n",
      "Iteration: 7599000, loss: 0.10658243881628293, gradient norm: 0.13765879169533582\n",
      "Iteration: 7600000, loss: 0.10658236480571037, gradient norm: 0.1480474933082124\n",
      "Iteration: 7601000, loss: 0.10658202666911479, gradient norm: 0.12704055566016195\n",
      "Iteration: 7602000, loss: 0.10658189562981164, gradient norm: 0.3803352710296922\n",
      "Iteration: 7603000, loss: 0.10658162573141458, gradient norm: 0.20757024373144345\n",
      "Iteration: 7604000, loss: 0.1065813440044661, gradient norm: 0.06667303360912055\n",
      "Iteration: 7605000, loss: 0.10658126687379152, gradient norm: 0.2527972807837867\n",
      "Iteration: 7606000, loss: 0.1065809371548384, gradient norm: 0.18142023961260656\n",
      "Iteration: 7607000, loss: 0.10658063829995007, gradient norm: 0.6916179480116131\n",
      "Iteration: 7608000, loss: 0.10658052182762658, gradient norm: 0.0883791413208232\n",
      "Iteration: 7609000, loss: 0.10658028594607984, gradient norm: 0.13094235486378458\n",
      "Iteration: 7610000, loss: 0.10658015050678024, gradient norm: 0.07899125336428112\n",
      "Iteration: 7611000, loss: 0.10657990375449779, gradient norm: 0.37318508935112177\n",
      "Iteration: 7612000, loss: 0.10657977784058277, gradient norm: 0.12558856753548114\n",
      "Iteration: 7613000, loss: 0.10657929483783515, gradient norm: 0.14619940759605762\n",
      "Iteration: 7614000, loss: 0.10657934217295605, gradient norm: 0.22288922154539595\n",
      "Iteration: 7615000, loss: 0.10657887992341003, gradient norm: 0.07580595758581626\n",
      "Iteration: 7616000, loss: 0.1065787892255451, gradient norm: 0.19681278932222349\n",
      "Iteration: 7617000, loss: 0.10657853202289186, gradient norm: 0.2769184996685072\n",
      "Iteration: 7618000, loss: 0.10657836619330643, gradient norm: 0.13196366363098644\n",
      "Iteration: 7619000, loss: 0.10657814994150495, gradient norm: 0.1491482819011967\n",
      "Iteration: 7620000, loss: 0.1065779641820204, gradient norm: 0.4674137814115384\n",
      "Iteration: 7621000, loss: 0.10657769644650116, gradient norm: 0.5932092453929224\n",
      "Iteration: 7622000, loss: 0.10657735132343844, gradient norm: 0.272160246015408\n",
      "Iteration: 7623000, loss: 0.10657721526082448, gradient norm: 0.2049885550883977\n",
      "Iteration: 7624000, loss: 0.10657712093192555, gradient norm: 0.18530639206852495\n",
      "Iteration: 7625000, loss: 0.10657687440189981, gradient norm: 0.12441469473587918\n",
      "Iteration: 7626000, loss: 0.10657642572619197, gradient norm: 0.06762956247754104\n",
      "Iteration: 7627000, loss: 0.1065764474479025, gradient norm: 0.24777810660286892\n",
      "Iteration: 7628000, loss: 0.10657623178488171, gradient norm: 0.1657051491934378\n",
      "Iteration: 7629000, loss: 0.10657593592574457, gradient norm: 0.07893219217462451\n",
      "Iteration: 7630000, loss: 0.1065757795024156, gradient norm: 0.06633811170370058\n",
      "Iteration: 7631000, loss: 0.10657551988090853, gradient norm: 0.1302853072984795\n",
      "Iteration: 7632000, loss: 0.1065753497295512, gradient norm: 0.16591269108755577\n",
      "Iteration: 7633000, loss: 0.10657496822105443, gradient norm: 0.36139456853928287\n",
      "Iteration: 7634000, loss: 0.10657497712103509, gradient norm: 0.10220738826185832\n",
      "Iteration: 7635000, loss: 0.10657454927613301, gradient norm: 0.04915256242243418\n",
      "Iteration: 7636000, loss: 0.10657445579437526, gradient norm: 0.05956342715465617\n",
      "Iteration: 7637000, loss: 0.10657419907902838, gradient norm: 0.4036967240366807\n",
      "Iteration: 7638000, loss: 0.10657408330187605, gradient norm: 0.12662471891090565\n",
      "Iteration: 7639000, loss: 0.10657385412774992, gradient norm: 0.4461830189925485\n",
      "Iteration: 7640000, loss: 0.10657340417630311, gradient norm: 0.19967968756239068\n",
      "Iteration: 7641000, loss: 0.10657335381579022, gradient norm: 0.7636091632473219\n",
      "Iteration: 7642000, loss: 0.10657318372230708, gradient norm: 0.2880190433605653\n",
      "Iteration: 7643000, loss: 0.10657286460288586, gradient norm: 0.36013718332444467\n",
      "Iteration: 7644000, loss: 0.10657281238227384, gradient norm: 0.229316931172365\n",
      "Iteration: 7645000, loss: 0.10657235590174846, gradient norm: 0.581281895792567\n",
      "Iteration: 7646000, loss: 0.10657223143193129, gradient norm: 0.18765784870058805\n",
      "Iteration: 7647000, loss: 0.10657215347559526, gradient norm: 0.3408288923999634\n",
      "Iteration: 7648000, loss: 0.10657173511308903, gradient norm: 0.07073758493577839\n",
      "Iteration: 7649000, loss: 0.10657163814623613, gradient norm: 0.7164272137170861\n",
      "Iteration: 7650000, loss: 0.10657143590181005, gradient norm: 0.36144358186999037\n",
      "Iteration: 7651000, loss: 0.10657118009715758, gradient norm: 0.06196794704117598\n",
      "Iteration: 7652000, loss: 0.10657096872353393, gradient norm: 0.028917349243328835\n",
      "Iteration: 7653000, loss: 0.10657073718201007, gradient norm: 0.19864122059192166\n",
      "Iteration: 7654000, loss: 0.10657050374822877, gradient norm: 0.423923868679926\n",
      "Iteration: 7655000, loss: 0.10657033796903441, gradient norm: 0.5957067056601515\n",
      "Iteration: 7656000, loss: 0.10657008276124973, gradient norm: 0.11743497253139415\n",
      "Iteration: 7657000, loss: 0.10656991825257923, gradient norm: 0.6487748802971254\n",
      "Iteration: 7658000, loss: 0.10656972334002705, gradient norm: 0.29451960213102407\n",
      "Iteration: 7659000, loss: 0.10656946904828513, gradient norm: 0.6142707639753661\n",
      "Iteration: 7660000, loss: 0.10656919150298176, gradient norm: 0.1970711062783411\n",
      "Iteration: 7661000, loss: 0.10656902919333729, gradient norm: 0.16123848331584284\n",
      "Iteration: 7662000, loss: 0.10656879412132389, gradient norm: 0.5888940120092496\n",
      "Iteration: 7663000, loss: 0.10656855232216464, gradient norm: 0.2857393847492526\n",
      "Iteration: 7664000, loss: 0.1065684138328426, gradient norm: 0.3379596494780801\n",
      "Iteration: 7665000, loss: 0.10656812103131623, gradient norm: 0.286515345550598\n",
      "Iteration: 7666000, loss: 0.10656800471941887, gradient norm: 0.106579476960724\n",
      "Iteration: 7667000, loss: 0.10656773825444077, gradient norm: 0.07911094488961443\n",
      "Iteration: 7668000, loss: 0.10656750160818512, gradient norm: 0.12041802270505786\n",
      "Iteration: 7669000, loss: 0.10656732337432771, gradient norm: 0.3822555589673183\n",
      "Iteration: 7670000, loss: 0.1065670703450481, gradient norm: 0.6325310505549092\n",
      "Iteration: 7671000, loss: 0.1065669556583955, gradient norm: 0.4228284237382057\n",
      "Iteration: 7672000, loss: 0.10656655279428841, gradient norm: 0.39690943586594635\n",
      "Iteration: 7673000, loss: 0.1065663917368688, gradient norm: 0.46742524781052247\n",
      "Iteration: 7674000, loss: 0.10656624152763688, gradient norm: 0.3184359055069205\n",
      "Iteration: 7675000, loss: 0.1065659942636241, gradient norm: 0.2132698445736163\n",
      "Iteration: 7676000, loss: 0.10656576613786889, gradient norm: 0.23812783521632802\n",
      "Iteration: 7677000, loss: 0.10656569412613295, gradient norm: 0.2993179048818676\n",
      "Iteration: 7678000, loss: 0.10656526927617406, gradient norm: 0.09563844524789543\n",
      "Iteration: 7679000, loss: 0.10656517641359924, gradient norm: 0.5885264611880971\n",
      "Iteration: 7680000, loss: 0.10656488150316247, gradient norm: 0.19913107019858056\n",
      "Iteration: 7681000, loss: 0.10656482601768948, gradient norm: 0.2969636038370156\n",
      "Iteration: 7682000, loss: 0.10656449602820794, gradient norm: 0.2671781969476575\n",
      "Iteration: 7683000, loss: 0.10656421794564594, gradient norm: 0.12356010391115903\n",
      "Iteration: 7684000, loss: 0.10656403052935133, gradient norm: 0.5918344598225015\n",
      "Iteration: 7685000, loss: 0.10656390183381284, gradient norm: 0.16235555833928736\n",
      "Iteration: 7686000, loss: 0.10656377938482388, gradient norm: 0.4434574776478284\n",
      "Iteration: 7687000, loss: 0.10656328175239743, gradient norm: 0.3441297831485782\n",
      "Iteration: 7688000, loss: 0.10656335537152087, gradient norm: 0.2887435523384602\n",
      "Iteration: 7689000, loss: 0.10656283148951627, gradient norm: 0.17307997438362555\n",
      "Iteration: 7690000, loss: 0.1065629172379118, gradient norm: 0.2787016634542048\n",
      "Iteration: 7691000, loss: 0.10656255019217349, gradient norm: 0.10268367821881194\n",
      "Iteration: 7692000, loss: 0.10656228902455428, gradient norm: 0.6549939112422011\n",
      "Iteration: 7693000, loss: 0.1065622378200668, gradient norm: 0.013788905389177073\n",
      "Iteration: 7694000, loss: 0.10656189921677223, gradient norm: 0.20655507867366535\n",
      "Iteration: 7695000, loss: 0.10656171897494095, gradient norm: 0.1635183737444158\n",
      "Iteration: 7696000, loss: 0.10656179959714066, gradient norm: 0.24879215870770563\n",
      "Iteration: 7697000, loss: 0.1065610085036411, gradient norm: 0.16632007582042974\n",
      "Iteration: 7698000, loss: 0.10656116900391913, gradient norm: 0.08378472839527039\n",
      "Iteration: 7699000, loss: 0.10656085064933214, gradient norm: 0.2344040827500337\n",
      "Iteration: 7700000, loss: 0.10656068970724865, gradient norm: 0.19810861057133877\n",
      "Iteration: 7701000, loss: 0.10656040353278959, gradient norm: 0.7052721843088317\n",
      "Iteration: 7702000, loss: 0.10656023229548774, gradient norm: 0.18752736171999845\n",
      "Iteration: 7703000, loss: 0.10656009665307713, gradient norm: 0.26851888320699674\n",
      "Iteration: 7704000, loss: 0.1065596947873871, gradient norm: 0.6936140441474197\n",
      "Iteration: 7705000, loss: 0.10655956655707484, gradient norm: 0.4299872224556009\n",
      "Iteration: 7706000, loss: 0.10655949810220781, gradient norm: 0.5151708689398423\n",
      "Iteration: 7707000, loss: 0.10655912737572046, gradient norm: 0.2124587583509545\n",
      "Iteration: 7708000, loss: 0.10655892422805806, gradient norm: 0.08636744608084322\n",
      "Iteration: 7709000, loss: 0.10655881438506194, gradient norm: 0.2944730956910083\n",
      "Iteration: 7710000, loss: 0.10655837158620143, gradient norm: 0.5767324794884389\n",
      "Iteration: 7711000, loss: 0.10655829692310824, gradient norm: 0.7047266143311458\n",
      "Iteration: 7712000, loss: 0.10655821838678926, gradient norm: 0.3882448987421222\n",
      "Iteration: 7713000, loss: 0.10655772230787207, gradient norm: 0.6758638138244217\n",
      "Iteration: 7714000, loss: 0.10655773868689974, gradient norm: 0.4401062550074786\n",
      "Iteration: 7715000, loss: 0.1065574183825259, gradient norm: 0.0727928444930087\n",
      "Iteration: 7716000, loss: 0.10655729618825709, gradient norm: 0.08273972774712798\n",
      "Iteration: 7717000, loss: 0.10655701484688213, gradient norm: 0.32380035934378587\n",
      "Iteration: 7718000, loss: 0.1065568518679884, gradient norm: 0.11096180284141359\n",
      "Iteration: 7719000, loss: 0.10655655761405398, gradient norm: 0.09074608764064375\n",
      "Iteration: 7720000, loss: 0.10655641488806947, gradient norm: 0.12655243010057818\n",
      "Iteration: 7721000, loss: 0.1065562246160652, gradient norm: 0.2036032732841781\n",
      "Iteration: 7722000, loss: 0.10655608566007803, gradient norm: 0.3557109518972665\n",
      "Iteration: 7723000, loss: 0.10655574785670513, gradient norm: 0.3508661554121468\n",
      "Iteration: 7724000, loss: 0.10655537879203578, gradient norm: 0.13305652349206307\n",
      "Iteration: 7725000, loss: 0.10655548195652062, gradient norm: 0.12888933772522668\n",
      "Iteration: 7726000, loss: 0.10655507484781072, gradient norm: 0.14907757476780661\n",
      "Iteration: 7727000, loss: 0.10655491963048339, gradient norm: 0.11557042830803917\n",
      "Iteration: 7728000, loss: 0.10655459104645335, gradient norm: 0.18637834080463606\n",
      "Iteration: 7729000, loss: 0.10655448445377905, gradient norm: 0.18497416522149043\n",
      "Iteration: 7730000, loss: 0.10655446135012955, gradient norm: 0.6419779050146622\n",
      "Iteration: 7731000, loss: 0.10655412687390448, gradient norm: 0.11316408246338662\n",
      "Iteration: 7732000, loss: 0.10655375254397148, gradient norm: 0.6214969902201484\n",
      "Iteration: 7733000, loss: 0.10655362434478968, gradient norm: 0.022694688609259397\n",
      "Iteration: 7734000, loss: 0.10655341478194078, gradient norm: 0.13423216814285516\n",
      "Iteration: 7735000, loss: 0.10655336364854857, gradient norm: 0.2544921616892741\n",
      "Iteration: 7736000, loss: 0.10655290926394613, gradient norm: 0.28641203578237745\n",
      "Iteration: 7737000, loss: 0.10655274717911911, gradient norm: 0.12728037707816325\n",
      "Iteration: 7738000, loss: 0.10655256270218624, gradient norm: 0.11543490805771647\n",
      "Iteration: 7739000, loss: 0.10655236841094191, gradient norm: 0.10521593291603187\n",
      "Iteration: 7740000, loss: 0.10655218732701698, gradient norm: 0.34083154015577466\n",
      "Iteration: 7741000, loss: 0.10655200619353519, gradient norm: 0.3909313207834353\n",
      "Iteration: 7742000, loss: 0.10655163456013857, gradient norm: 0.15254595772654334\n",
      "Iteration: 7743000, loss: 0.10655155640174402, gradient norm: 1.0189801349965675\n",
      "Iteration: 7744000, loss: 0.10655142635482952, gradient norm: 0.1390600297759277\n",
      "Iteration: 7745000, loss: 0.10655097342026669, gradient norm: 0.3080428632002785\n",
      "Iteration: 7746000, loss: 0.10655097792135639, gradient norm: 0.8874455990205595\n",
      "Iteration: 7747000, loss: 0.10655065998973487, gradient norm: 0.2493604679886842\n",
      "Iteration: 7748000, loss: 0.10655044254950619, gradient norm: 0.27661509061029643\n",
      "Iteration: 7749000, loss: 0.1065501839391568, gradient norm: 0.36722605580531026\n",
      "Iteration: 7750000, loss: 0.10655021546372571, gradient norm: 0.9853862374417814\n",
      "Iteration: 7751000, loss: 0.1065496112050721, gradient norm: 0.5770602719747279\n",
      "Iteration: 7752000, loss: 0.10654975428938095, gradient norm: 0.29289640025100216\n",
      "Iteration: 7753000, loss: 0.10654949859207875, gradient norm: 0.4621195985891522\n",
      "Iteration: 7754000, loss: 0.10654919312460816, gradient norm: 0.15755642805636375\n",
      "Iteration: 7755000, loss: 0.10654887470405998, gradient norm: 0.4317860704128999\n",
      "Iteration: 7756000, loss: 0.10654873477634913, gradient norm: 0.43449616096040194\n",
      "Iteration: 7757000, loss: 0.10654875336934282, gradient norm: 0.18019373911926667\n",
      "Iteration: 7758000, loss: 0.10654816952120483, gradient norm: 0.17409559470564284\n",
      "Iteration: 7759000, loss: 0.10654809045631922, gradient norm: 0.19015679571688623\n",
      "Iteration: 7760000, loss: 0.10654804772290126, gradient norm: 0.02592850878653497\n",
      "Iteration: 7761000, loss: 0.10654774423679401, gradient norm: 0.7558817486849309\n",
      "Iteration: 7762000, loss: 0.10654750151678524, gradient norm: 0.8701862757918031\n",
      "Iteration: 7763000, loss: 0.1065473620674948, gradient norm: 0.07451319480849444\n",
      "Iteration: 7764000, loss: 0.10654702856830753, gradient norm: 0.7392676864011902\n",
      "Iteration: 7765000, loss: 0.1065470173263763, gradient norm: 0.07689118917887965\n",
      "Iteration: 7766000, loss: 0.10654662711876912, gradient norm: 0.17853034233624376\n",
      "Iteration: 7767000, loss: 0.10654642780490606, gradient norm: 0.23611893942139603\n",
      "Iteration: 7768000, loss: 0.10654628614113673, gradient norm: 0.648253118913596\n",
      "Iteration: 7769000, loss: 0.10654600229140841, gradient norm: 0.1602981677615047\n",
      "Iteration: 7770000, loss: 0.10654578511197627, gradient norm: 0.07818165305993065\n",
      "Iteration: 7771000, loss: 0.1065457060270432, gradient norm: 0.22288873250497895\n",
      "Iteration: 7772000, loss: 0.10654537351543941, gradient norm: 0.20336882106684215\n",
      "Iteration: 7773000, loss: 0.10654518740511607, gradient norm: 0.29140718086056605\n",
      "Iteration: 7774000, loss: 0.1065449761762585, gradient norm: 0.10865037094823168\n",
      "Iteration: 7775000, loss: 0.10654480027242332, gradient norm: 0.2760337946817724\n",
      "Iteration: 7776000, loss: 0.10654449369884272, gradient norm: 0.15722331440682333\n",
      "Iteration: 7777000, loss: 0.1065444153091228, gradient norm: 0.5618444079005166\n",
      "Iteration: 7778000, loss: 0.10654413115214925, gradient norm: 0.6961599584579357\n",
      "Iteration: 7779000, loss: 0.10654399892931195, gradient norm: 0.3097031194102554\n",
      "Iteration: 7780000, loss: 0.10654381939023756, gradient norm: 0.12695279532851236\n",
      "Iteration: 7781000, loss: 0.10654345750216106, gradient norm: 0.32156976614727506\n",
      "Iteration: 7782000, loss: 0.10654331230503651, gradient norm: 0.12147043744765822\n",
      "Iteration: 7783000, loss: 0.10654301907035033, gradient norm: 0.16941125982844554\n",
      "Iteration: 7784000, loss: 0.10654299881566448, gradient norm: 0.2245053051600318\n",
      "Iteration: 7785000, loss: 0.10654259425852028, gradient norm: 0.6101919838013377\n",
      "Iteration: 7786000, loss: 0.10654251461670393, gradient norm: 0.6387201187402054\n",
      "Iteration: 7787000, loss: 0.10654226178691402, gradient norm: 0.750656261464836\n",
      "Iteration: 7788000, loss: 0.1065422442210555, gradient norm: 0.7482237896286813\n",
      "Iteration: 7789000, loss: 0.10654167162952669, gradient norm: 0.2886610955625555\n",
      "Iteration: 7790000, loss: 0.10654161927531743, gradient norm: 0.23131290291480017\n",
      "Iteration: 7791000, loss: 0.1065414861836093, gradient norm: 0.7133830466040063\n",
      "Iteration: 7792000, loss: 0.10654111913340156, gradient norm: 0.03457709808794601\n",
      "Iteration: 7793000, loss: 0.10654112591579556, gradient norm: 0.14529320070926421\n",
      "Iteration: 7794000, loss: 0.10654080617601792, gradient norm: 0.19836370463922845\n",
      "Iteration: 7795000, loss: 0.10654068288953959, gradient norm: 0.23882689550570924\n",
      "Iteration: 7796000, loss: 0.10654030866582982, gradient norm: 0.5907578508689569\n",
      "Iteration: 7797000, loss: 0.10654007870069993, gradient norm: 0.2765044493332856\n",
      "Iteration: 7798000, loss: 0.10654006954302227, gradient norm: 0.6252046799328824\n",
      "Iteration: 7799000, loss: 0.106539748347442, gradient norm: 0.784549552749062\n",
      "Iteration: 7800000, loss: 0.10653955282921575, gradient norm: 0.6203839640734127\n",
      "Iteration: 7801000, loss: 0.10653937361052103, gradient norm: 0.14989102700836765\n",
      "Iteration: 7802000, loss: 0.10653907117908622, gradient norm: 0.32836025628543675\n",
      "Iteration: 7803000, loss: 0.10653893119230398, gradient norm: 0.35642227264535664\n",
      "Iteration: 7804000, loss: 0.10653869612760912, gradient norm: 0.7258849532935024\n",
      "Iteration: 7805000, loss: 0.10653857382401202, gradient norm: 0.1869589160183257\n",
      "Iteration: 7806000, loss: 0.10653836790738706, gradient norm: 0.2835688204199682\n",
      "Iteration: 7807000, loss: 0.10653797855560868, gradient norm: 0.22499729447974479\n",
      "Iteration: 7808000, loss: 0.10653789830132061, gradient norm: 0.38492378924713466\n",
      "Iteration: 7809000, loss: 0.10653778772567304, gradient norm: 0.5055127016404597\n",
      "Iteration: 7810000, loss: 0.1065374682360451, gradient norm: 0.11999623470888038\n",
      "Iteration: 7811000, loss: 0.10653721797722397, gradient norm: 0.24118919433635047\n",
      "Iteration: 7812000, loss: 0.10653698009133032, gradient norm: 0.14568136453261474\n",
      "Iteration: 7813000, loss: 0.10653692252225447, gradient norm: 0.3373459823408235\n",
      "Iteration: 7814000, loss: 0.10653654076792077, gradient norm: 0.01382161631230758\n",
      "Iteration: 7815000, loss: 0.10653659074740403, gradient norm: 0.00905595311153576\n",
      "Iteration: 7816000, loss: 0.10653618468531997, gradient norm: 0.16661697887355903\n",
      "Iteration: 7817000, loss: 0.10653597081639828, gradient norm: 0.17904375266603315\n",
      "Iteration: 7818000, loss: 0.10653576801924018, gradient norm: 0.0700164101862923\n",
      "Iteration: 7819000, loss: 0.10653560107925017, gradient norm: 0.3859357240532606\n",
      "Iteration: 7820000, loss: 0.10653538947426412, gradient norm: 0.10168761606666389\n",
      "Iteration: 7821000, loss: 0.10653516496427648, gradient norm: 0.1494655761827891\n",
      "Iteration: 7822000, loss: 0.10653489279953117, gradient norm: 0.1031137986952655\n",
      "Iteration: 7823000, loss: 0.1065348707232701, gradient norm: 0.45927145506198835\n",
      "Iteration: 7824000, loss: 0.10653455716353456, gradient norm: 0.2465108051987889\n",
      "Iteration: 7825000, loss: 0.10653430935081104, gradient norm: 0.1521315400408195\n",
      "Iteration: 7826000, loss: 0.10653426348859925, gradient norm: 0.24151133821165519\n",
      "Iteration: 7827000, loss: 0.10653390367375086, gradient norm: 0.17821509752947937\n",
      "Iteration: 7828000, loss: 0.10653369006781313, gradient norm: 0.40476191465738987\n",
      "Iteration: 7829000, loss: 0.10653365575785932, gradient norm: 0.6693713946684269\n",
      "Iteration: 7830000, loss: 0.10653319599423, gradient norm: 0.21719319342458562\n",
      "Iteration: 7831000, loss: 0.10653298100002229, gradient norm: 0.15001680427436195\n",
      "Iteration: 7832000, loss: 0.10653295828767269, gradient norm: 0.16291526435968487\n",
      "Iteration: 7833000, loss: 0.10653266794688954, gradient norm: 0.1872969493670672\n",
      "Iteration: 7834000, loss: 0.10653251677159344, gradient norm: 0.2455068742177656\n",
      "Iteration: 7835000, loss: 0.10653222280848428, gradient norm: 0.08018185511744795\n",
      "Iteration: 7836000, loss: 0.10653212527958962, gradient norm: 0.20993116560392272\n",
      "Iteration: 7837000, loss: 0.10653192278684753, gradient norm: 0.12955149313284153\n",
      "Iteration: 7838000, loss: 0.10653161412855443, gradient norm: 0.3668910072574464\n",
      "Iteration: 7839000, loss: 0.10653151982579953, gradient norm: 0.048073985151081446\n",
      "Iteration: 7840000, loss: 0.10653106558207208, gradient norm: 0.10469239966359783\n",
      "Iteration: 7841000, loss: 0.10653116400020618, gradient norm: 0.7067689034330752\n",
      "Iteration: 7842000, loss: 0.10653076820207645, gradient norm: 0.056810126223458414\n",
      "Iteration: 7843000, loss: 0.10653062132531864, gradient norm: 0.08152714599022796\n",
      "Iteration: 7844000, loss: 0.10653057332354998, gradient norm: 0.7243678816259329\n",
      "Iteration: 7845000, loss: 0.10653016336624566, gradient norm: 0.21077205133938434\n",
      "Iteration: 7846000, loss: 0.10652996348826764, gradient norm: 0.25926675863466897\n",
      "Iteration: 7847000, loss: 0.1065298650551315, gradient norm: 0.4314932361424567\n",
      "Iteration: 7848000, loss: 0.10652974813545003, gradient norm: 0.12522338845344205\n",
      "Iteration: 7849000, loss: 0.10652920688480405, gradient norm: 0.13134703559445754\n",
      "Iteration: 7850000, loss: 0.10652926991176465, gradient norm: 0.22572377472867994\n",
      "Iteration: 7851000, loss: 0.10652880492962215, gradient norm: 0.3508010267008244\n",
      "Iteration: 7852000, loss: 0.1065289380976086, gradient norm: 0.9110242076356485\n",
      "Iteration: 7853000, loss: 0.1065285441032923, gradient norm: 0.5115270238099636\n",
      "Iteration: 7854000, loss: 0.10652838296866546, gradient norm: 0.1079151312667651\n",
      "Iteration: 7855000, loss: 0.10652823270106458, gradient norm: 0.07825478250601645\n",
      "Iteration: 7856000, loss: 0.10652785806594384, gradient norm: 0.04351486217069287\n",
      "Iteration: 7857000, loss: 0.1065276867774946, gradient norm: 0.6163629411819286\n",
      "Iteration: 7858000, loss: 0.10652761000530313, gradient norm: 0.14182847985755231\n",
      "Iteration: 7859000, loss: 0.10652728811212134, gradient norm: 0.03327849454554448\n",
      "Iteration: 7860000, loss: 0.10652711571295763, gradient norm: 0.7156026337012722\n",
      "Iteration: 7861000, loss: 0.10652711724134742, gradient norm: 0.03452302568548513\n",
      "Iteration: 7862000, loss: 0.10652658495740143, gradient norm: 0.05883042803409696\n",
      "Iteration: 7863000, loss: 0.10652651246197778, gradient norm: 0.2500017354752309\n",
      "Iteration: 7864000, loss: 0.10652625459162884, gradient norm: 0.09533092228698781\n",
      "Iteration: 7865000, loss: 0.10652608214976346, gradient norm: 0.1261504722960342\n",
      "Iteration: 7866000, loss: 0.10652600960069451, gradient norm: 0.26371043510974584\n",
      "Iteration: 7867000, loss: 0.10652555461735634, gradient norm: 0.054266886250010686\n",
      "Iteration: 7868000, loss: 0.10652539509301602, gradient norm: 0.19985324207137645\n",
      "Iteration: 7869000, loss: 0.10652531742114946, gradient norm: 0.1554176844152977\n",
      "Iteration: 7870000, loss: 0.10652524942869292, gradient norm: 0.8381756437104316\n",
      "Iteration: 7871000, loss: 0.10652477225418883, gradient norm: 0.2639018906801905\n",
      "Iteration: 7872000, loss: 0.1065245777814608, gradient norm: 0.5075067390678806\n",
      "Iteration: 7873000, loss: 0.10652458380175601, gradient norm: 0.3020987678649733\n",
      "Iteration: 7874000, loss: 0.10652406434290643, gradient norm: 0.23770149618440356\n",
      "Iteration: 7875000, loss: 0.10652413444649886, gradient norm: 0.3847257623623244\n",
      "Iteration: 7876000, loss: 0.1065237996554866, gradient norm: 0.766189043168066\n",
      "Iteration: 7877000, loss: 0.10652374750999331, gradient norm: 1.07963110483449\n",
      "Iteration: 7878000, loss: 0.1065233761484749, gradient norm: 0.2694193626895918\n",
      "Iteration: 7879000, loss: 0.10652312326844735, gradient norm: 0.004926683552873689\n",
      "Iteration: 7880000, loss: 0.10652319603090496, gradient norm: 0.08899673520773524\n",
      "Iteration: 7881000, loss: 0.10652262802853561, gradient norm: 0.2692385824079725\n",
      "Iteration: 7882000, loss: 0.10652264470836653, gradient norm: 0.21340150588518952\n",
      "Iteration: 7883000, loss: 0.10652249940907522, gradient norm: 0.3256361172987759\n",
      "Iteration: 7884000, loss: 0.10652215844317163, gradient norm: 0.42765351185182027\n",
      "Iteration: 7885000, loss: 0.10652204699937798, gradient norm: 0.13947580039772595\n",
      "Iteration: 7886000, loss: 0.1065217516836525, gradient norm: 0.7812244281556239\n",
      "Iteration: 7887000, loss: 0.10652159919328548, gradient norm: 0.1833014194369294\n",
      "Iteration: 7888000, loss: 0.10652139759125745, gradient norm: 0.09378003623285046\n",
      "Iteration: 7889000, loss: 0.10652108790711615, gradient norm: 0.4648014993023322\n",
      "Iteration: 7890000, loss: 0.10652101057305546, gradient norm: 0.7687004249282438\n",
      "Iteration: 7891000, loss: 0.10652063059706926, gradient norm: 0.44448981673355387\n",
      "Iteration: 7892000, loss: 0.10652066822202085, gradient norm: 0.2546191483136274\n",
      "Iteration: 7893000, loss: 0.10652020761113977, gradient norm: 0.11224658549772291\n",
      "Iteration: 7894000, loss: 0.10652024442707601, gradient norm: 1.005693208334351\n",
      "Iteration: 7895000, loss: 0.10651985786874203, gradient norm: 0.29640415109638535\n",
      "Iteration: 7896000, loss: 0.10651976136331451, gradient norm: 0.6235882508325273\n",
      "Iteration: 7897000, loss: 0.10651960110697131, gradient norm: 0.28651644336782417\n",
      "Iteration: 7898000, loss: 0.10651941341713897, gradient norm: 0.10489043152388897\n",
      "Iteration: 7899000, loss: 0.10651913880287114, gradient norm: 0.20728431058603639\n",
      "Iteration: 7900000, loss: 0.10651882458991122, gradient norm: 0.02418694277392489\n",
      "Iteration: 7901000, loss: 0.10651880862978345, gradient norm: 0.052379366856302366\n",
      "Iteration: 7902000, loss: 0.10651852254273919, gradient norm: 0.525647792341052\n",
      "Iteration: 7903000, loss: 0.10651819748396574, gradient norm: 0.17703434684883737\n",
      "Iteration: 7904000, loss: 0.10651810348112639, gradient norm: 0.062187660082330926\n",
      "Iteration: 7905000, loss: 0.10651795185576172, gradient norm: 0.4389386055760517\n",
      "Iteration: 7906000, loss: 0.10651761992564124, gradient norm: 0.24879240568090627\n",
      "Iteration: 7907000, loss: 0.10651774565587385, gradient norm: 0.26932799400298985\n",
      "Iteration: 7908000, loss: 0.10651711887456526, gradient norm: 0.1553151069781952\n",
      "Iteration: 7909000, loss: 0.10651719523119298, gradient norm: 0.6847465938057028\n",
      "Iteration: 7910000, loss: 0.10651674977962375, gradient norm: 0.3380400829156685\n",
      "Iteration: 7911000, loss: 0.10651678736403625, gradient norm: 0.2315289681486905\n",
      "Iteration: 7912000, loss: 0.10651634033641619, gradient norm: 0.3806715943641652\n",
      "Iteration: 7913000, loss: 0.10651636203097681, gradient norm: 0.6708593814094604\n",
      "Iteration: 7914000, loss: 0.1065160154390289, gradient norm: 0.5013829610989622\n",
      "Iteration: 7915000, loss: 0.10651577106400936, gradient norm: 0.13597401949776147\n",
      "Iteration: 7916000, loss: 0.10651579120931506, gradient norm: 0.4620953251257564\n",
      "Iteration: 7917000, loss: 0.10651539803460648, gradient norm: 0.17638338390121708\n",
      "Iteration: 7918000, loss: 0.10651518601257869, gradient norm: 0.22241611657137775\n",
      "Iteration: 7919000, loss: 0.10651509070696434, gradient norm: 0.24878924848005463\n",
      "Iteration: 7920000, loss: 0.10651479997535927, gradient norm: 0.22043259385618327\n",
      "Iteration: 7921000, loss: 0.10651469940161098, gradient norm: 0.11737203973730291\n",
      "Iteration: 7922000, loss: 0.10651441780333451, gradient norm: 0.191368367228855\n",
      "Iteration: 7923000, loss: 0.10651422889357215, gradient norm: 0.49158070004941645\n",
      "Iteration: 7924000, loss: 0.10651401504060329, gradient norm: 0.25276910410399067\n",
      "Iteration: 7925000, loss: 0.1065138960303261, gradient norm: 0.47740836986389246\n",
      "Iteration: 7926000, loss: 0.10651367004557599, gradient norm: 0.2289656071622582\n",
      "Iteration: 7927000, loss: 0.10651331665799028, gradient norm: 0.09590507620600723\n",
      "Iteration: 7928000, loss: 0.10651328901332248, gradient norm: 0.7078059707116806\n",
      "Iteration: 7929000, loss: 0.10651293429849983, gradient norm: 0.33753937309117654\n",
      "Iteration: 7930000, loss: 0.10651285286800673, gradient norm: 0.16020624475933487\n",
      "Iteration: 7931000, loss: 0.10651258693325727, gradient norm: 0.21023215371761275\n",
      "Iteration: 7932000, loss: 0.10651245831975586, gradient norm: 0.4944965455988646\n",
      "Iteration: 7933000, loss: 0.10651231704784879, gradient norm: 0.36402639883952415\n",
      "Iteration: 7934000, loss: 0.1065118878049977, gradient norm: 0.13554415254201996\n",
      "Iteration: 7935000, loss: 0.10651179479955503, gradient norm: 0.026662286636566508\n",
      "Iteration: 7936000, loss: 0.10651157444492526, gradient norm: 0.18115497943520603\n",
      "Iteration: 7937000, loss: 0.10651151397012398, gradient norm: 0.2678215396247096\n",
      "Iteration: 7938000, loss: 0.10651116596612778, gradient norm: 0.8230416433897556\n",
      "Iteration: 7939000, loss: 0.10651098856976025, gradient norm: 0.17600587656509536\n",
      "Iteration: 7940000, loss: 0.10651076748706802, gradient norm: 0.5137858726767035\n",
      "Iteration: 7941000, loss: 0.10651058313215618, gradient norm: 0.26382607641515876\n",
      "Iteration: 7942000, loss: 0.1065104205551894, gradient norm: 1.1607161447270145\n",
      "Iteration: 7943000, loss: 0.10651009999462786, gradient norm: 0.31211945617324294\n",
      "Iteration: 7944000, loss: 0.10651007817719499, gradient norm: 0.3489399548532752\n",
      "Iteration: 7945000, loss: 0.1065096682207616, gradient norm: 0.21568642870726118\n",
      "Iteration: 7946000, loss: 0.10650961412853234, gradient norm: 0.6440081145175843\n",
      "Iteration: 7947000, loss: 0.106509421418235, gradient norm: 0.05868084713569671\n",
      "Iteration: 7948000, loss: 0.10650913203382448, gradient norm: 0.6420384405046605\n",
      "Iteration: 7949000, loss: 0.10650891225435595, gradient norm: 0.06709662689453864\n",
      "Iteration: 7950000, loss: 0.10650888556344919, gradient norm: 0.09850680805767642\n",
      "Iteration: 7951000, loss: 0.10650854535479473, gradient norm: 0.09654774034316708\n",
      "Iteration: 7952000, loss: 0.10650839100362795, gradient norm: 0.36246101819004267\n",
      "Iteration: 7953000, loss: 0.10650819644536585, gradient norm: 0.8676687280471596\n",
      "Iteration: 7954000, loss: 0.1065078941858864, gradient norm: 0.17033213890297347\n",
      "Iteration: 7955000, loss: 0.1065078058989271, gradient norm: 0.1320988865593019\n",
      "Iteration: 7956000, loss: 0.1065074637899703, gradient norm: 0.2695456711429305\n",
      "Iteration: 7957000, loss: 0.10650735856499227, gradient norm: 0.14270412677586594\n",
      "Iteration: 7958000, loss: 0.10650719003096352, gradient norm: 0.09989978571360231\n",
      "Iteration: 7959000, loss: 0.1065069502909497, gradient norm: 0.17421053851173804\n",
      "Iteration: 7960000, loss: 0.10650668249451833, gradient norm: 0.18180373644343184\n",
      "Iteration: 7961000, loss: 0.10650657297780947, gradient norm: 0.6039074895165344\n",
      "Iteration: 7962000, loss: 0.10650634429166736, gradient norm: 0.019759654360108212\n",
      "Iteration: 7963000, loss: 0.10650609739402234, gradient norm: 0.09600251807997277\n",
      "Iteration: 7964000, loss: 0.10650599454765909, gradient norm: 0.08631202991245947\n",
      "Iteration: 7965000, loss: 0.10650573363064789, gradient norm: 0.49669900502725917\n",
      "Iteration: 7966000, loss: 0.10650555372809828, gradient norm: 0.30503285677936604\n",
      "Iteration: 7967000, loss: 0.10650528226717328, gradient norm: 0.175938573390584\n",
      "Iteration: 7968000, loss: 0.1065050607755563, gradient norm: 0.36486714434239015\n",
      "Iteration: 7969000, loss: 0.10650501264577364, gradient norm: 0.18806810191574916\n",
      "Iteration: 7970000, loss: 0.10650469074700783, gradient norm: 0.2669926947145842\n",
      "Iteration: 7971000, loss: 0.10650456490953666, gradient norm: 0.0510816627462456\n",
      "Iteration: 7972000, loss: 0.10650434635589316, gradient norm: 0.03175353850900501\n",
      "Iteration: 7973000, loss: 0.10650422412335685, gradient norm: 0.5772280588915301\n",
      "Iteration: 7974000, loss: 0.10650392387580257, gradient norm: 0.3021552565535694\n",
      "Iteration: 7975000, loss: 0.10650365787013341, gradient norm: 0.08546560127220551\n",
      "Iteration: 7976000, loss: 0.10650361623663396, gradient norm: 0.21887028623801463\n",
      "Iteration: 7977000, loss: 0.10650332398499507, gradient norm: 0.23391668454277545\n",
      "Iteration: 7978000, loss: 0.10650309563036248, gradient norm: 0.3171053607191561\n",
      "Iteration: 7979000, loss: 0.10650290944674082, gradient norm: 0.7883007954857403\n",
      "Iteration: 7980000, loss: 0.10650272295683683, gradient norm: 0.7803080287317266\n",
      "Iteration: 7981000, loss: 0.10650251812856627, gradient norm: 0.04559985536809197\n",
      "Iteration: 7982000, loss: 0.10650229549604709, gradient norm: 0.3555301207758336\n",
      "Iteration: 7983000, loss: 0.10650221829500672, gradient norm: 0.365445036650995\n",
      "Iteration: 7984000, loss: 0.10650188292891417, gradient norm: 0.14361975559118906\n",
      "Iteration: 7985000, loss: 0.10650182967112595, gradient norm: 0.2092303182399703\n",
      "Iteration: 7986000, loss: 0.10650138776047466, gradient norm: 0.7948353956175119\n",
      "Iteration: 7987000, loss: 0.10650135306476122, gradient norm: 0.13463613917854594\n",
      "Iteration: 7988000, loss: 0.10650111911799871, gradient norm: 0.012720743086204156\n",
      "Iteration: 7989000, loss: 0.10650096763969627, gradient norm: 0.3111878073799367\n",
      "Iteration: 7990000, loss: 0.1065006335976793, gradient norm: 0.05401035111237615\n",
      "Iteration: 7991000, loss: 0.10650053113438758, gradient norm: 0.47219244705873203\n",
      "Iteration: 7992000, loss: 0.10650039485242442, gradient norm: 0.16296004056723296\n",
      "Iteration: 7993000, loss: 0.10650011987565564, gradient norm: 0.5775073571696164\n",
      "Iteration: 7994000, loss: 0.10649988138762181, gradient norm: 0.14074305080132773\n",
      "Iteration: 7995000, loss: 0.10649974082633712, gradient norm: 0.29231162927861015\n",
      "Iteration: 7996000, loss: 0.10649947742996321, gradient norm: 0.168079926155228\n",
      "Iteration: 7997000, loss: 0.1064992637110519, gradient norm: 0.1295180375718773\n",
      "Iteration: 7998000, loss: 0.10649925458372414, gradient norm: 0.13665004472975262\n",
      "Iteration: 7999000, loss: 0.10649880801775512, gradient norm: 0.08915118991159775\n",
      "Iteration: 8000000, loss: 0.10649873764522327, gradient norm: 0.22323303628670213\n",
      "Iteration: 8001000, loss: 0.10649853324340008, gradient norm: 0.10973374989230192\n",
      "Iteration: 8002000, loss: 0.1064982443784212, gradient norm: 0.7852162070682184\n",
      "Iteration: 8003000, loss: 0.10649808874519937, gradient norm: 0.4103755150732551\n",
      "Iteration: 8004000, loss: 0.10649801189200361, gradient norm: 0.13627249461606003\n",
      "Iteration: 8005000, loss: 0.10649764786733067, gradient norm: 0.3652949388215641\n",
      "Iteration: 8006000, loss: 0.10649770882132327, gradient norm: 0.06501245264328404\n",
      "Iteration: 8007000, loss: 0.10649722836335695, gradient norm: 0.5928170226817345\n",
      "Iteration: 8008000, loss: 0.10649703782988774, gradient norm: 0.28716307529710955\n",
      "Iteration: 8009000, loss: 0.10649696925391602, gradient norm: 0.06987164984679686\n",
      "Iteration: 8010000, loss: 0.1064967110972219, gradient norm: 0.3484610528454533\n",
      "Iteration: 8011000, loss: 0.10649655709748723, gradient norm: 0.29206109782052814\n",
      "Iteration: 8012000, loss: 0.1064963625674758, gradient norm: 0.8323721523779477\n",
      "Iteration: 8013000, loss: 0.10649604266564965, gradient norm: 0.4033689110354124\n",
      "Iteration: 8014000, loss: 0.10649606189266349, gradient norm: 0.17999680133366558\n",
      "Iteration: 8015000, loss: 0.10649565451859112, gradient norm: 0.03539060692919081\n",
      "Iteration: 8016000, loss: 0.10649558023603535, gradient norm: 0.06578590647097951\n",
      "Iteration: 8017000, loss: 0.10649528804842284, gradient norm: 0.21806555216496007\n",
      "Iteration: 8018000, loss: 0.10649502903091436, gradient norm: 0.4138586803816764\n",
      "Iteration: 8019000, loss: 0.1064950639902972, gradient norm: 0.7038273784134335\n",
      "Iteration: 8020000, loss: 0.10649457695275942, gradient norm: 0.18971076368902198\n",
      "Iteration: 8021000, loss: 0.10649460101470595, gradient norm: 0.12057880965359409\n",
      "Iteration: 8022000, loss: 0.10649435130809741, gradient norm: 0.3175063004575388\n",
      "Iteration: 8023000, loss: 0.10649422743580715, gradient norm: 0.1602129864217896\n",
      "Iteration: 8024000, loss: 0.1064938197447717, gradient norm: 0.2735985576304805\n",
      "Iteration: 8025000, loss: 0.10649374232724056, gradient norm: 0.13264499999208437\n",
      "Iteration: 8026000, loss: 0.10649357674809201, gradient norm: 0.4412705271131643\n",
      "Iteration: 8027000, loss: 0.10649317630045729, gradient norm: 0.3961092007815721\n",
      "Iteration: 8028000, loss: 0.10649323217401173, gradient norm: 0.15574427204686347\n",
      "Iteration: 8029000, loss: 0.10649293407638062, gradient norm: 0.24350065209402294\n",
      "Iteration: 8030000, loss: 0.10649274100446533, gradient norm: 0.12939380481072188\n",
      "Iteration: 8031000, loss: 0.10649268340508485, gradient norm: 0.7912197250473488\n",
      "Iteration: 8032000, loss: 0.1064923298982362, gradient norm: 0.05663556064060223\n",
      "Iteration: 8033000, loss: 0.10649206971108367, gradient norm: 0.7365982177368675\n",
      "Iteration: 8034000, loss: 0.106491875030155, gradient norm: 0.33269174899301057\n",
      "Iteration: 8035000, loss: 0.10649185875214542, gradient norm: 0.4109072806409071\n",
      "Iteration: 8036000, loss: 0.10649150210529101, gradient norm: 0.21081003585085853\n",
      "Iteration: 8037000, loss: 0.10649135020040783, gradient norm: 0.9897962324945513\n",
      "Iteration: 8038000, loss: 0.10649120685097332, gradient norm: 0.5427536414167228\n",
      "Iteration: 8039000, loss: 0.10649090313322418, gradient norm: 0.35829825976328206\n",
      "Iteration: 8040000, loss: 0.10649076160630382, gradient norm: 0.4005907906821127\n",
      "Iteration: 8041000, loss: 0.10649052570010133, gradient norm: 0.08643617822119128\n",
      "Iteration: 8042000, loss: 0.1064903395554245, gradient norm: 0.5636083092616054\n",
      "Iteration: 8043000, loss: 0.10649016059756436, gradient norm: 0.11540122383170726\n",
      "Iteration: 8044000, loss: 0.10649004550497959, gradient norm: 0.32000703091894106\n",
      "Iteration: 8045000, loss: 0.10648982692827134, gradient norm: 0.746246496252338\n",
      "Iteration: 8046000, loss: 0.10648946631895544, gradient norm: 0.17846279845647028\n",
      "Iteration: 8047000, loss: 0.10648937158263044, gradient norm: 0.04908691790180315\n",
      "Iteration: 8048000, loss: 0.10648919337400818, gradient norm: 0.21284437531493586\n",
      "Iteration: 8049000, loss: 0.10648890286633023, gradient norm: 0.2932133707223146\n",
      "Iteration: 8050000, loss: 0.1064888357838784, gradient norm: 0.2318351521866223\n",
      "Iteration: 8051000, loss: 0.10648861257266407, gradient norm: 0.26052149899285115\n",
      "Iteration: 8052000, loss: 0.10648833758964893, gradient norm: 0.6937632707508682\n",
      "Iteration: 8053000, loss: 0.10648832134753618, gradient norm: 0.8054136284956632\n",
      "Iteration: 8054000, loss: 0.10648785671067246, gradient norm: 0.040333726046711066\n",
      "Iteration: 8055000, loss: 0.10648782164849473, gradient norm: 0.13819985597606976\n",
      "Iteration: 8056000, loss: 0.10648763599708201, gradient norm: 0.4602753951377941\n",
      "Iteration: 8057000, loss: 0.10648743333233322, gradient norm: 0.07217936175399586\n",
      "Iteration: 8058000, loss: 0.10648723102806924, gradient norm: 0.36178705545300405\n",
      "Iteration: 8059000, loss: 0.10648690039723117, gradient norm: 0.21365282125242072\n",
      "Iteration: 8060000, loss: 0.10648677635794937, gradient norm: 0.23815434330293309\n",
      "Iteration: 8061000, loss: 0.10648671274671723, gradient norm: 0.5425835057690179\n",
      "Iteration: 8062000, loss: 0.10648651599148365, gradient norm: 0.06390824364736904\n",
      "Iteration: 8063000, loss: 0.10648598652761158, gradient norm: 0.21898043741046347\n",
      "Iteration: 8064000, loss: 0.10648611161155429, gradient norm: 0.22225279033733547\n",
      "Iteration: 8065000, loss: 0.1064857574353438, gradient norm: 0.39662781084087184\n",
      "Iteration: 8066000, loss: 0.10648566251722456, gradient norm: 0.13833732657481537\n",
      "Iteration: 8067000, loss: 0.10648539854508447, gradient norm: 0.33454488138878186\n",
      "Iteration: 8068000, loss: 0.10648522109843661, gradient norm: 0.7841386029503119\n",
      "Iteration: 8069000, loss: 0.10648510661766114, gradient norm: 0.2015217876186429\n",
      "Iteration: 8070000, loss: 0.10648484216543608, gradient norm: 0.061810652960464195\n",
      "Iteration: 8071000, loss: 0.10648459233169971, gradient norm: 0.27720935658814666\n",
      "Iteration: 8072000, loss: 0.10648440220568725, gradient norm: 0.28203130909646396\n",
      "Iteration: 8073000, loss: 0.10648424138110721, gradient norm: 0.5481209708338075\n",
      "Iteration: 8074000, loss: 0.10648406650984026, gradient norm: 0.12051785804134162\n",
      "Iteration: 8075000, loss: 0.10648380888207717, gradient norm: 0.16488859952436682\n",
      "Iteration: 8076000, loss: 0.1064836190259068, gradient norm: 0.8503866517262145\n",
      "Iteration: 8077000, loss: 0.10648349233058645, gradient norm: 0.5445891948227508\n",
      "Iteration: 8078000, loss: 0.1064831952284461, gradient norm: 0.4426286038195363\n",
      "Iteration: 8079000, loss: 0.10648308867658669, gradient norm: 0.3123387210183835\n",
      "Iteration: 8080000, loss: 0.1064828691732283, gradient norm: 0.40415280620420857\n",
      "Iteration: 8081000, loss: 0.10648284691742907, gradient norm: 0.4783529052071252\n",
      "Iteration: 8082000, loss: 0.10648231310965053, gradient norm: 0.935811479775236\n",
      "Iteration: 8083000, loss: 0.10648217478087155, gradient norm: 0.12157940521812641\n",
      "Iteration: 8084000, loss: 0.10648211150019349, gradient norm: 0.38046603766366355\n",
      "Iteration: 8085000, loss: 0.10648201914548013, gradient norm: 0.8967471307094176\n",
      "Iteration: 8086000, loss: 0.10648159277316858, gradient norm: 0.5280048472855096\n",
      "Iteration: 8087000, loss: 0.10648150372051568, gradient norm: 0.07663259233192822\n",
      "Iteration: 8088000, loss: 0.10648132731636019, gradient norm: 0.3501971321078978\n",
      "Iteration: 8089000, loss: 0.10648102412460325, gradient norm: 0.2332486554662026\n",
      "Iteration: 8090000, loss: 0.1064809151157873, gradient norm: 0.07458496556721667\n",
      "Iteration: 8091000, loss: 0.10648060573486379, gradient norm: 0.06367268059025825\n",
      "Iteration: 8092000, loss: 0.1064805854592299, gradient norm: 0.33435650578439646\n",
      "Iteration: 8093000, loss: 0.10648034695578505, gradient norm: 1.0058969942509288\n",
      "Iteration: 8094000, loss: 0.10647998474977263, gradient norm: 0.12929381423962258\n",
      "Iteration: 8095000, loss: 0.10647991197179621, gradient norm: 0.5015242089535168\n",
      "Iteration: 8096000, loss: 0.10647976430714161, gradient norm: 0.3886226870131918\n",
      "Iteration: 8097000, loss: 0.10647959368224841, gradient norm: 0.25084937133338603\n",
      "Iteration: 8098000, loss: 0.10647925574729408, gradient norm: 0.45048655558750117\n",
      "Iteration: 8099000, loss: 0.10647932464344258, gradient norm: 0.11004454853298523\n",
      "Iteration: 8100000, loss: 0.10647875924957685, gradient norm: 0.2648900852096303\n",
      "Iteration: 8101000, loss: 0.10647876168281226, gradient norm: 0.3174883156541221\n",
      "Iteration: 8102000, loss: 0.10647867532298085, gradient norm: 0.2858891319637985\n",
      "Iteration: 8103000, loss: 0.10647814957568207, gradient norm: 0.2095993690852137\n",
      "Iteration: 8104000, loss: 0.10647819126414752, gradient norm: 0.2940054285223194\n",
      "Iteration: 8105000, loss: 0.10647809717367929, gradient norm: 0.12650700419888036\n",
      "Iteration: 8106000, loss: 0.10647770209117612, gradient norm: 0.6810265993912015\n",
      "Iteration: 8107000, loss: 0.10647763553297916, gradient norm: 0.1383182175103578\n",
      "Iteration: 8108000, loss: 0.10647750090416755, gradient norm: 0.2654954319714311\n",
      "Iteration: 8109000, loss: 0.10647696373264402, gradient norm: 0.09982569152860435\n",
      "Iteration: 8110000, loss: 0.1064770702843787, gradient norm: 0.01633508907938642\n",
      "Iteration: 8111000, loss: 0.10647679914587414, gradient norm: 0.09171594873887275\n",
      "Iteration: 8112000, loss: 0.10647665143410806, gradient norm: 0.23521181363801605\n",
      "Iteration: 8113000, loss: 0.10647629414462423, gradient norm: 0.1718514086845268\n",
      "Iteration: 8114000, loss: 0.10647635244263563, gradient norm: 0.030505318936529347\n",
      "Iteration: 8115000, loss: 0.10647595161903287, gradient norm: 0.3980592296686764\n",
      "Iteration: 8116000, loss: 0.10647590318222615, gradient norm: 0.4342429199384806\n",
      "Iteration: 8117000, loss: 0.10647544102686825, gradient norm: 0.30274952272966815\n",
      "Iteration: 8118000, loss: 0.10647545349716048, gradient norm: 0.710552981676425\n",
      "Iteration: 8119000, loss: 0.1064752515387964, gradient norm: 0.3187698390504214\n",
      "Iteration: 8120000, loss: 0.10647499555725884, gradient norm: 0.304688830385354\n",
      "Iteration: 8121000, loss: 0.10647477593757884, gradient norm: 0.40326909606242534\n",
      "Iteration: 8122000, loss: 0.106474702200829, gradient norm: 0.6845843370307906\n",
      "Iteration: 8123000, loss: 0.10647459696004408, gradient norm: 0.252489875354515\n",
      "Iteration: 8124000, loss: 0.10647407920332924, gradient norm: 0.06919097308354719\n",
      "Iteration: 8125000, loss: 0.10647405888655549, gradient norm: 0.044033220884263854\n",
      "Iteration: 8126000, loss: 0.10647392890388264, gradient norm: 0.3290260856367714\n",
      "Iteration: 8127000, loss: 0.10647364907659276, gradient norm: 0.10447909644848566\n",
      "Iteration: 8128000, loss: 0.1064732874823957, gradient norm: 0.10239429498147704\n",
      "Iteration: 8129000, loss: 0.10647331398508456, gradient norm: 0.04868571401876673\n",
      "Iteration: 8130000, loss: 0.106473202523156, gradient norm: 0.45509745280034536\n",
      "Iteration: 8131000, loss: 0.10647293174375105, gradient norm: 0.08197779020790148\n",
      "Iteration: 8132000, loss: 0.10647263452476391, gradient norm: 0.5152616728011316\n",
      "Iteration: 8133000, loss: 0.10647253219699185, gradient norm: 0.6243332642656723\n",
      "Iteration: 8134000, loss: 0.10647246527948455, gradient norm: 0.08859683077283788\n",
      "Iteration: 8135000, loss: 0.10647204044929628, gradient norm: 0.6826001148113527\n",
      "Iteration: 8136000, loss: 0.10647184203654618, gradient norm: 0.27331477194664927\n",
      "Iteration: 8137000, loss: 0.1064717354603118, gradient norm: 0.26121368603734446\n",
      "Iteration: 8138000, loss: 0.10647149519291582, gradient norm: 0.28187963411830375\n",
      "Iteration: 8139000, loss: 0.10647134862094967, gradient norm: 0.20090065480193833\n",
      "Iteration: 8140000, loss: 0.10647117943092266, gradient norm: 0.3059076079355124\n",
      "Iteration: 8141000, loss: 0.10647088858975172, gradient norm: 0.08903762083277318\n",
      "Iteration: 8142000, loss: 0.10647091077834728, gradient norm: 0.06903892677669761\n",
      "Iteration: 8143000, loss: 0.10647048271665739, gradient norm: 0.09702508360154737\n",
      "Iteration: 8144000, loss: 0.10647052935538, gradient norm: 0.25821428965497645\n",
      "Iteration: 8145000, loss: 0.10647010172986818, gradient norm: 0.4239377108781297\n",
      "Iteration: 8146000, loss: 0.10646989975427101, gradient norm: 0.03624093744008755\n",
      "Iteration: 8147000, loss: 0.106469949615334, gradient norm: 0.709764119966874\n",
      "Iteration: 8148000, loss: 0.10646953811696132, gradient norm: 0.418033345909094\n",
      "Iteration: 8149000, loss: 0.10646958761970685, gradient norm: 0.4387448767616164\n",
      "Iteration: 8150000, loss: 0.106469004829608, gradient norm: 0.23741160408849357\n",
      "Iteration: 8151000, loss: 0.10646901798603513, gradient norm: 0.5908549557215206\n",
      "Iteration: 8152000, loss: 0.10646877437588899, gradient norm: 0.21403302511893574\n",
      "Iteration: 8153000, loss: 0.10646870534669893, gradient norm: 0.2775417230835042\n",
      "Iteration: 8154000, loss: 0.10646836070381951, gradient norm: 1.1078394820764073\n",
      "Iteration: 8155000, loss: 0.10646825806641043, gradient norm: 0.12495795722975575\n",
      "Iteration: 8156000, loss: 0.10646795018669435, gradient norm: 0.5031249562051536\n",
      "Iteration: 8157000, loss: 0.10646797087074751, gradient norm: 0.3658454084624043\n",
      "Iteration: 8158000, loss: 0.10646757177477437, gradient norm: 0.42299477763877724\n",
      "Iteration: 8159000, loss: 0.10646766723265252, gradient norm: 0.3671337285611807\n",
      "Iteration: 8160000, loss: 0.1064671395860183, gradient norm: 0.17485896939981027\n",
      "Iteration: 8161000, loss: 0.10646721645736575, gradient norm: 0.1394082943892633\n",
      "Iteration: 8162000, loss: 0.10646663667184635, gradient norm: 0.15213112604499884\n",
      "Iteration: 8163000, loss: 0.10646674441551886, gradient norm: 0.17868217749909268\n",
      "Iteration: 8164000, loss: 0.10646652114052023, gradient norm: 0.6565635193400446\n",
      "Iteration: 8165000, loss: 0.1064663873448684, gradient norm: 0.15018128489650606\n",
      "Iteration: 8166000, loss: 0.10646605307507562, gradient norm: 0.42441180264408523\n",
      "Iteration: 8167000, loss: 0.10646592357885314, gradient norm: 0.7599566731479231\n",
      "Iteration: 8168000, loss: 0.10646576815803821, gradient norm: 0.0626592646595106\n",
      "Iteration: 8169000, loss: 0.10646557448131265, gradient norm: 0.2510186658895481\n",
      "Iteration: 8170000, loss: 0.1064652291691602, gradient norm: 0.23321037273760362\n",
      "Iteration: 8171000, loss: 0.10646519662987403, gradient norm: 0.0817936020350979\n",
      "Iteration: 8172000, loss: 0.10646490736611074, gradient norm: 0.11334758682117974\n",
      "Iteration: 8173000, loss: 0.10646499158330386, gradient norm: 0.6564930914570134\n",
      "Iteration: 8174000, loss: 0.1064644842664279, gradient norm: 0.41968903729178786\n",
      "Iteration: 8175000, loss: 0.10646431094100484, gradient norm: 0.09968567151145209\n",
      "Iteration: 8176000, loss: 0.10646413341516191, gradient norm: 0.2957167644497613\n",
      "Iteration: 8177000, loss: 0.10646400689886146, gradient norm: 0.9761898490702757\n",
      "Iteration: 8178000, loss: 0.1064638000007937, gradient norm: 0.5969526361437786\n",
      "Iteration: 8179000, loss: 0.10646379522759179, gradient norm: 0.1831135309043312\n",
      "Iteration: 8180000, loss: 0.10646330516428025, gradient norm: 0.14358589104964203\n",
      "Iteration: 8181000, loss: 0.10646316912568418, gradient norm: 0.18562037129886363\n",
      "Iteration: 8182000, loss: 0.10646308296906629, gradient norm: 0.03813967364894055\n",
      "Iteration: 8183000, loss: 0.10646276222119956, gradient norm: 0.07708993242110035\n",
      "Iteration: 8184000, loss: 0.106462706889466, gradient norm: 0.02425209216800558\n",
      "Iteration: 8185000, loss: 0.1064623222239722, gradient norm: 0.16707660554012033\n",
      "Iteration: 8186000, loss: 0.10646236126166239, gradient norm: 0.09267759950153834\n",
      "Iteration: 8187000, loss: 0.10646201976735852, gradient norm: 0.19540501914718766\n",
      "Iteration: 8188000, loss: 0.1064618844160187, gradient norm: 0.20187703003288868\n",
      "Iteration: 8189000, loss: 0.10646164560344422, gradient norm: 0.5304612038607927\n",
      "Iteration: 8190000, loss: 0.10646154009428677, gradient norm: 0.24659412493040891\n",
      "Iteration: 8191000, loss: 0.10646126100193869, gradient norm: 0.5578724268888904\n",
      "Iteration: 8192000, loss: 0.1064610813282714, gradient norm: 0.36505567262469935\n",
      "Iteration: 8193000, loss: 0.10646093973163236, gradient norm: 0.07129129271692804\n",
      "Iteration: 8194000, loss: 0.10646074492225321, gradient norm: 0.4190921618462015\n",
      "Iteration: 8195000, loss: 0.10646065163218432, gradient norm: 0.2426647612451876\n",
      "Iteration: 8196000, loss: 0.10646028263831948, gradient norm: 0.27223194737173595\n",
      "Iteration: 8197000, loss: 0.10645999148910693, gradient norm: 0.21106189312454438\n",
      "Iteration: 8198000, loss: 0.10645993220036204, gradient norm: 0.1776149747086569\n",
      "Iteration: 8199000, loss: 0.10645995531797718, gradient norm: 0.45375534032257103\n",
      "Iteration: 8200000, loss: 0.10645943791699675, gradient norm: 0.23795515728387395\n",
      "Iteration: 8201000, loss: 0.10645939172408836, gradient norm: 0.10104486948018217\n",
      "Iteration: 8202000, loss: 0.10645916943582807, gradient norm: 0.08420196857401789\n",
      "Iteration: 8203000, loss: 0.10645885517270996, gradient norm: 0.37757348755713316\n",
      "Iteration: 8204000, loss: 0.10645891723710689, gradient norm: 0.1341240088999341\n",
      "Iteration: 8205000, loss: 0.10645859444758982, gradient norm: 0.045089200921778885\n",
      "Iteration: 8206000, loss: 0.10645844642885086, gradient norm: 0.22393537765292162\n",
      "Iteration: 8207000, loss: 0.10645825829590314, gradient norm: 0.5407113979685438\n",
      "Iteration: 8208000, loss: 0.1064580971035898, gradient norm: 0.10679569382261674\n",
      "Iteration: 8209000, loss: 0.1064577389947292, gradient norm: 0.06433738844738959\n",
      "Iteration: 8210000, loss: 0.10645765373840504, gradient norm: 0.24535320758483206\n",
      "Iteration: 8211000, loss: 0.10645745364460354, gradient norm: 0.13903108919420384\n",
      "Iteration: 8212000, loss: 0.10645733181598092, gradient norm: 0.1726060034229986\n",
      "Iteration: 8213000, loss: 0.10645708690502752, gradient norm: 0.09505716470702535\n",
      "Iteration: 8214000, loss: 0.1064568299251691, gradient norm: 0.28425841694516935\n",
      "Iteration: 8215000, loss: 0.10645655947118254, gradient norm: 0.1246382218794288\n",
      "Iteration: 8216000, loss: 0.10645680583363216, gradient norm: 0.1724427871083762\n",
      "Iteration: 8217000, loss: 0.10645621895673844, gradient norm: 0.5741274031783457\n",
      "Iteration: 8218000, loss: 0.10645604168061845, gradient norm: 0.03431231553494436\n",
      "Iteration: 8219000, loss: 0.10645600444332583, gradient norm: 0.22709131470864244\n",
      "Iteration: 8220000, loss: 0.10645570066991875, gradient norm: 0.1337344887009515\n",
      "Iteration: 8221000, loss: 0.10645539900811073, gradient norm: 0.19660611817435425\n",
      "Iteration: 8222000, loss: 0.10645546174757559, gradient norm: 0.22328622424239888\n",
      "Iteration: 8223000, loss: 0.10645512001255047, gradient norm: 0.41589423178870255\n",
      "Iteration: 8224000, loss: 0.10645498671625378, gradient norm: 0.3119891814566938\n",
      "Iteration: 8225000, loss: 0.1064547722583738, gradient norm: 0.400621364010216\n",
      "Iteration: 8226000, loss: 0.10645450755311953, gradient norm: 0.5615558672903548\n",
      "Iteration: 8227000, loss: 0.10645436090106362, gradient norm: 0.11811025042677525\n",
      "Iteration: 8228000, loss: 0.10645418173584792, gradient norm: 0.13301258192531906\n",
      "Iteration: 8229000, loss: 0.10645410928543382, gradient norm: 0.08331275381019386\n",
      "Iteration: 8230000, loss: 0.10645373982574427, gradient norm: 0.5708627092904235\n",
      "Iteration: 8231000, loss: 0.10645368806429197, gradient norm: 0.14135804094419085\n",
      "Iteration: 8232000, loss: 0.10645338997352181, gradient norm: 0.12936486405276473\n",
      "Iteration: 8233000, loss: 0.10645333382401222, gradient norm: 0.04334371923314774\n",
      "Iteration: 8234000, loss: 0.106453018796241, gradient norm: 0.12092737188155506\n",
      "Iteration: 8235000, loss: 0.10645292516003056, gradient norm: 0.09259867140901672\n",
      "Iteration: 8236000, loss: 0.10645265808546703, gradient norm: 0.18467803018453258\n",
      "Iteration: 8237000, loss: 0.10645243109499482, gradient norm: 0.2599758978788149\n",
      "Iteration: 8238000, loss: 0.10645237187772938, gradient norm: 0.3993215328584307\n",
      "Iteration: 8239000, loss: 0.10645207044021036, gradient norm: 0.06395497082716159\n",
      "Iteration: 8240000, loss: 0.10645186261746488, gradient norm: 0.49474680139755867\n",
      "Iteration: 8241000, loss: 0.10645178615937217, gradient norm: 0.15820647411468933\n",
      "Iteration: 8242000, loss: 0.1064515300160771, gradient norm: 0.18190448566269235\n",
      "Iteration: 8243000, loss: 0.10645135464948345, gradient norm: 0.3118118331330474\n",
      "Iteration: 8244000, loss: 0.10645119975953966, gradient norm: 0.37835591058664764\n",
      "Iteration: 8245000, loss: 0.10645089111391372, gradient norm: 0.0935567123160313\n",
      "Iteration: 8246000, loss: 0.10645087071319342, gradient norm: 0.6543433480316345\n",
      "Iteration: 8247000, loss: 0.1064505009905458, gradient norm: 0.06324014099690974\n",
      "Iteration: 8248000, loss: 0.10645041142610559, gradient norm: 0.13228721602442897\n",
      "Iteration: 8249000, loss: 0.10645024576925985, gradient norm: 0.31182588528460475\n",
      "Iteration: 8250000, loss: 0.10644992375657908, gradient norm: 0.43623310501817997\n",
      "Iteration: 8251000, loss: 0.10644992616665033, gradient norm: 0.16702379288749478\n",
      "Iteration: 8252000, loss: 0.10644971741573926, gradient norm: 0.07126803262409563\n",
      "Iteration: 8253000, loss: 0.10644930202035387, gradient norm: 0.05394032477349619\n",
      "Iteration: 8254000, loss: 0.10644934141994428, gradient norm: 0.20720103968619139\n",
      "Iteration: 8255000, loss: 0.10644913900851063, gradient norm: 0.4727856608653466\n",
      "Iteration: 8256000, loss: 0.1064488979595771, gradient norm: 0.45837118275168764\n",
      "Iteration: 8257000, loss: 0.10644866945656875, gradient norm: 0.21810155942295284\n",
      "Iteration: 8258000, loss: 0.1064484695696549, gradient norm: 0.1511607782954213\n",
      "Iteration: 8259000, loss: 0.10644834527119228, gradient norm: 0.16702131654275545\n",
      "Iteration: 8260000, loss: 0.1064479962997805, gradient norm: 0.5687985526256598\n",
      "Iteration: 8261000, loss: 0.1064479728467277, gradient norm: 0.5322630640950708\n",
      "Iteration: 8262000, loss: 0.1064476467667731, gradient norm: 0.20812091376613412\n",
      "Iteration: 8263000, loss: 0.10644772452746426, gradient norm: 0.3948229393676423\n",
      "Iteration: 8264000, loss: 0.10644720948610345, gradient norm: 0.46968765518364436\n",
      "Iteration: 8265000, loss: 0.10644735366572138, gradient norm: 0.3108647504159119\n",
      "Iteration: 8266000, loss: 0.10644683250588013, gradient norm: 0.2966786964678405\n",
      "Iteration: 8267000, loss: 0.1064467798684638, gradient norm: 0.09735594406011487\n",
      "Iteration: 8268000, loss: 0.10644660588721233, gradient norm: 0.10261438946901652\n",
      "Iteration: 8269000, loss: 0.10644648457384105, gradient norm: 0.5644642831586838\n",
      "Iteration: 8270000, loss: 0.10644614670884257, gradient norm: 0.6104801258167456\n",
      "Iteration: 8271000, loss: 0.10644603746210929, gradient norm: 0.37207575587782843\n",
      "Iteration: 8272000, loss: 0.10644579939502219, gradient norm: 0.8759467887579964\n",
      "Iteration: 8273000, loss: 0.10644579073864219, gradient norm: 0.2588199319421616\n",
      "Iteration: 8274000, loss: 0.10644536693799896, gradient norm: 0.34642830420114457\n",
      "Iteration: 8275000, loss: 0.10644544556352904, gradient norm: 0.08112060240777898\n",
      "Iteration: 8276000, loss: 0.10644502995279229, gradient norm: 0.365626851990267\n",
      "Iteration: 8277000, loss: 0.10644487349631425, gradient norm: 0.17482259067507816\n",
      "Iteration: 8278000, loss: 0.10644473799597833, gradient norm: 0.3253916962918006\n",
      "Iteration: 8279000, loss: 0.10644446646909982, gradient norm: 0.009148938795794117\n",
      "Iteration: 8280000, loss: 0.10644434953034178, gradient norm: 0.6641839038365096\n",
      "Iteration: 8281000, loss: 0.10644417671488202, gradient norm: 0.6329263074478076\n",
      "Iteration: 8282000, loss: 0.10644381484891735, gradient norm: 0.042781492576907665\n",
      "Iteration: 8283000, loss: 0.10644375578235056, gradient norm: 0.28758367441903065\n",
      "Iteration: 8284000, loss: 0.10644363212608321, gradient norm: 0.2457574496421406\n",
      "Iteration: 8285000, loss: 0.10644340048727399, gradient norm: 0.24654441288518095\n",
      "Iteration: 8286000, loss: 0.10644319192858341, gradient norm: 0.6589529058686436\n",
      "Iteration: 8287000, loss: 0.10644313837810264, gradient norm: 0.5613554334136381\n",
      "Iteration: 8288000, loss: 0.10644265884830886, gradient norm: 0.10251105596239288\n",
      "Iteration: 8289000, loss: 0.10644268790014703, gradient norm: 0.6800640021921616\n",
      "Iteration: 8290000, loss: 0.10644235611119257, gradient norm: 0.07760725171932012\n",
      "Iteration: 8291000, loss: 0.10644237390641542, gradient norm: 0.25611593466012517\n",
      "Iteration: 8292000, loss: 0.10644200810009255, gradient norm: 0.3987257652027326\n",
      "Iteration: 8293000, loss: 0.1064419941743336, gradient norm: 0.32290646066868733\n",
      "Iteration: 8294000, loss: 0.1064416765075149, gradient norm: 0.19325871111306728\n",
      "Iteration: 8295000, loss: 0.10644155267554573, gradient norm: 0.573305447617028\n",
      "Iteration: 8296000, loss: 0.10644126000803833, gradient norm: 0.9559632005106323\n",
      "Iteration: 8297000, loss: 0.10644102500651631, gradient norm: 0.4482734143917467\n",
      "Iteration: 8298000, loss: 0.10644096872855192, gradient norm: 0.06093624250249698\n",
      "Iteration: 8299000, loss: 0.10644075992578376, gradient norm: 0.39301112618965856\n",
      "Iteration: 8300000, loss: 0.10644068845169702, gradient norm: 0.3722794130902577\n",
      "Iteration: 8301000, loss: 0.10644039340310359, gradient norm: 0.16702980053177158\n",
      "Iteration: 8302000, loss: 0.10644010062642048, gradient norm: 0.19168692019551226\n",
      "Iteration: 8303000, loss: 0.106440108726629, gradient norm: 0.06362898555032072\n",
      "Iteration: 8304000, loss: 0.10643966586875521, gradient norm: 0.41507297232053325\n",
      "Iteration: 8305000, loss: 0.10643960919166431, gradient norm: 0.06546580193421303\n",
      "Iteration: 8306000, loss: 0.10643942509284328, gradient norm: 0.0969247819050042\n",
      "Iteration: 8307000, loss: 0.10643936359574815, gradient norm: 0.44177056908206974\n",
      "Iteration: 8308000, loss: 0.10643907909553721, gradient norm: 0.08483835391851294\n",
      "Iteration: 8309000, loss: 0.10643891022335421, gradient norm: 0.4029365085632101\n",
      "Iteration: 8310000, loss: 0.10643859862895522, gradient norm: 0.21726323789650034\n",
      "Iteration: 8311000, loss: 0.10643848120256257, gradient norm: 0.009618404848141249\n",
      "Iteration: 8312000, loss: 0.1064384108505099, gradient norm: 0.36355495783792147\n",
      "Iteration: 8313000, loss: 0.1064379975542609, gradient norm: 0.2576654862534703\n",
      "Iteration: 8314000, loss: 0.10643783527285007, gradient norm: 0.4707156475012898\n",
      "Iteration: 8315000, loss: 0.10643783806823215, gradient norm: 0.012473059842777892\n",
      "Iteration: 8316000, loss: 0.1064376104697133, gradient norm: 0.4115137082396364\n",
      "Iteration: 8317000, loss: 0.10643736406197862, gradient norm: 0.08050378848371996\n",
      "Iteration: 8318000, loss: 0.106437164779619, gradient norm: 0.5453238757925829\n",
      "Iteration: 8319000, loss: 0.10643703024250506, gradient norm: 0.16573407430827927\n",
      "Iteration: 8320000, loss: 0.1064367679835753, gradient norm: 0.34692563960130146\n",
      "Iteration: 8321000, loss: 0.10643670214994203, gradient norm: 0.6194035041755448\n",
      "Iteration: 8322000, loss: 0.10643639645600746, gradient norm: 0.4606114734530243\n",
      "Iteration: 8323000, loss: 0.10643607925412675, gradient norm: 0.3441593711086948\n",
      "Iteration: 8324000, loss: 0.1064361637215351, gradient norm: 0.46517480539595235\n",
      "Iteration: 8325000, loss: 0.106436016984521, gradient norm: 0.1425225166230256\n",
      "Iteration: 8326000, loss: 0.10643563180226086, gradient norm: 0.8087330364360992\n",
      "Iteration: 8327000, loss: 0.10643551272673452, gradient norm: 0.6076608389306164\n",
      "Iteration: 8328000, loss: 0.10643528001300123, gradient norm: 0.36019486974552434\n",
      "Iteration: 8329000, loss: 0.10643523111328557, gradient norm: 0.41905051990331393\n",
      "Iteration: 8330000, loss: 0.10643492379615753, gradient norm: 0.2757941797903227\n",
      "Iteration: 8331000, loss: 0.10643472165826137, gradient norm: 0.6139260482958795\n",
      "Iteration: 8332000, loss: 0.10643475860551607, gradient norm: 0.19839785623575582\n",
      "Iteration: 8333000, loss: 0.10643428471561424, gradient norm: 0.5975031043208076\n",
      "Iteration: 8334000, loss: 0.1064341387703904, gradient norm: 0.12235846625691559\n",
      "Iteration: 8335000, loss: 0.10643405240216108, gradient norm: 0.7606384537712916\n",
      "Iteration: 8336000, loss: 0.10643382987508802, gradient norm: 0.08949934769031756\n",
      "Iteration: 8337000, loss: 0.10643357080920177, gradient norm: 0.5405685648114348\n",
      "Iteration: 8338000, loss: 0.10643346138370394, gradient norm: 0.39513780634789053\n",
      "Iteration: 8339000, loss: 0.10643323428531627, gradient norm: 0.17028782569929363\n",
      "Iteration: 8340000, loss: 0.10643316692721368, gradient norm: 0.4702656641894739\n",
      "Iteration: 8341000, loss: 0.10643292811505614, gradient norm: 0.5617443950116647\n",
      "Iteration: 8342000, loss: 0.10643261062347753, gradient norm: 0.24545157267143755\n",
      "Iteration: 8343000, loss: 0.10643235945378347, gradient norm: 0.15218822983332286\n",
      "Iteration: 8344000, loss: 0.10643248229742924, gradient norm: 0.11214013681770263\n",
      "Iteration: 8345000, loss: 0.10643206430137277, gradient norm: 0.11660181894139272\n",
      "Iteration: 8346000, loss: 0.10643207665118938, gradient norm: 0.35569563902503815\n",
      "Iteration: 8347000, loss: 0.10643165932200688, gradient norm: 0.16638217409922088\n",
      "Iteration: 8348000, loss: 0.10643162533064424, gradient norm: 0.1241270521418416\n",
      "Iteration: 8349000, loss: 0.10643125160888221, gradient norm: 0.45422257363493673\n",
      "Iteration: 8350000, loss: 0.10643127848552482, gradient norm: 0.4219119504745417\n",
      "Iteration: 8351000, loss: 0.10643113480683303, gradient norm: 0.26610890940439186\n",
      "Iteration: 8352000, loss: 0.1064308051683877, gradient norm: 0.15020357353975186\n",
      "Iteration: 8353000, loss: 0.10643058851888787, gradient norm: 0.7227941502548774\n",
      "Iteration: 8354000, loss: 0.10643042159593985, gradient norm: 0.39869258884192565\n",
      "Iteration: 8355000, loss: 0.10643033182595983, gradient norm: 0.36036198430471683\n",
      "Iteration: 8356000, loss: 0.10643020364334126, gradient norm: 0.4014507456761742\n",
      "Iteration: 8357000, loss: 0.10642978829858232, gradient norm: 0.06248495856464886\n",
      "Iteration: 8358000, loss: 0.10642967915477536, gradient norm: 0.1187008803066419\n",
      "Iteration: 8359000, loss: 0.10642966734120711, gradient norm: 0.13351821433333602\n",
      "Iteration: 8360000, loss: 0.10642931236918061, gradient norm: 0.272121120547645\n",
      "Iteration: 8361000, loss: 0.10642906839765234, gradient norm: 0.099982937263086\n",
      "Iteration: 8362000, loss: 0.1064289370385639, gradient norm: 0.09365513651778376\n",
      "Iteration: 8363000, loss: 0.1064289042110506, gradient norm: 0.5336680285547348\n",
      "Iteration: 8364000, loss: 0.1064286254668002, gradient norm: 0.6430790442653749\n",
      "Iteration: 8365000, loss: 0.10642844307034653, gradient norm: 0.18150326259004426\n",
      "Iteration: 8366000, loss: 0.10642818923086378, gradient norm: 0.20685116571279602\n",
      "Iteration: 8367000, loss: 0.10642810383434141, gradient norm: 0.07746877347042924\n",
      "Iteration: 8368000, loss: 0.10642772556343866, gradient norm: 0.5835279530863404\n",
      "Iteration: 8369000, loss: 0.10642788330681723, gradient norm: 0.5095373475050318\n",
      "Iteration: 8370000, loss: 0.10642751346745127, gradient norm: 0.8726082448652844\n",
      "Iteration: 8371000, loss: 0.10642722625130571, gradient norm: 0.45816845527038647\n",
      "Iteration: 8372000, loss: 0.10642706319850774, gradient norm: 0.28218286185078223\n",
      "Iteration: 8373000, loss: 0.10642702495454816, gradient norm: 0.2558480081611729\n",
      "Iteration: 8374000, loss: 0.10642666948239742, gradient norm: 0.3787641812235404\n",
      "Iteration: 8375000, loss: 0.10642669264287036, gradient norm: 0.4682196915255338\n",
      "Iteration: 8376000, loss: 0.10642629584732322, gradient norm: 0.09501014288513593\n",
      "Iteration: 8377000, loss: 0.10642621411614467, gradient norm: 0.35918970582778975\n",
      "Iteration: 8378000, loss: 0.10642593273280315, gradient norm: 0.10121455359231225\n",
      "Iteration: 8379000, loss: 0.10642586418424155, gradient norm: 0.06846953739506359\n",
      "Iteration: 8380000, loss: 0.10642559458475748, gradient norm: 0.302227012894441\n",
      "Iteration: 8381000, loss: 0.1064256353464041, gradient norm: 0.35698589027614236\n",
      "Iteration: 8382000, loss: 0.10642518828367227, gradient norm: 0.1513058027263953\n",
      "Iteration: 8383000, loss: 0.10642508361593224, gradient norm: 0.547843961957219\n",
      "Iteration: 8384000, loss: 0.10642489719439817, gradient norm: 0.8581622475722033\n",
      "Iteration: 8385000, loss: 0.10642464196927502, gradient norm: 0.3843763898510336\n",
      "Iteration: 8386000, loss: 0.1064245981237159, gradient norm: 0.09237979200089409\n",
      "Iteration: 8387000, loss: 0.1064244186970529, gradient norm: 0.3422600582156463\n",
      "Iteration: 8388000, loss: 0.10642409719828064, gradient norm: 0.4642206430783022\n",
      "Iteration: 8389000, loss: 0.1064239633359397, gradient norm: 0.4091211009204318\n",
      "Iteration: 8390000, loss: 0.10642375113204504, gradient norm: 0.3929922767805713\n",
      "Iteration: 8391000, loss: 0.10642356288512614, gradient norm: 0.8344467900347762\n",
      "Iteration: 8392000, loss: 0.10642359182949204, gradient norm: 0.4151106977592646\n",
      "Iteration: 8393000, loss: 0.10642315799835961, gradient norm: 0.48072405152520126\n",
      "Iteration: 8394000, loss: 0.10642294947206568, gradient norm: 0.18952118156357614\n",
      "Iteration: 8395000, loss: 0.10642287463036312, gradient norm: 0.11675230163749409\n",
      "Iteration: 8396000, loss: 0.1064226647239062, gradient norm: 0.5393957314533493\n",
      "Iteration: 8397000, loss: 0.10642252671551833, gradient norm: 0.7754429743032794\n",
      "Iteration: 8398000, loss: 0.10642227196375113, gradient norm: 0.07751259643165238\n",
      "Iteration: 8399000, loss: 0.1064220497637598, gradient norm: 0.06435848277606337\n",
      "Iteration: 8400000, loss: 0.10642207974721109, gradient norm: 0.43263536340312847\n",
      "Iteration: 8401000, loss: 0.10642178096172193, gradient norm: 0.6909802946234463\n",
      "Iteration: 8402000, loss: 0.10642147970575631, gradient norm: 0.3021134421249526\n",
      "Iteration: 8403000, loss: 0.1064214376854064, gradient norm: 0.1465097022601759\n",
      "Iteration: 8404000, loss: 0.10642115479816228, gradient norm: 0.162188201468819\n",
      "Iteration: 8405000, loss: 0.10642117041258303, gradient norm: 0.0654679157208489\n",
      "Iteration: 8406000, loss: 0.10642090875210226, gradient norm: 0.2413324496255644\n",
      "Iteration: 8407000, loss: 0.10642046105237964, gradient norm: 0.6711252279927973\n",
      "Iteration: 8408000, loss: 0.10642048570289238, gradient norm: 0.4327404087694924\n",
      "Iteration: 8409000, loss: 0.1064203585188952, gradient norm: 0.26475823385642594\n",
      "Iteration: 8410000, loss: 0.10642008462317289, gradient norm: 0.3770715600401412\n",
      "Iteration: 8411000, loss: 0.10641981417928878, gradient norm: 0.5652073824721963\n",
      "Iteration: 8412000, loss: 0.10641986588711139, gradient norm: 0.1770214982356131\n",
      "Iteration: 8413000, loss: 0.10641958100438137, gradient norm: 0.1882246755021826\n",
      "Iteration: 8414000, loss: 0.10641929577184664, gradient norm: 0.3825098523212863\n",
      "Iteration: 8415000, loss: 0.1064191737603222, gradient norm: 0.2568535441894914\n",
      "Iteration: 8416000, loss: 0.10641907660646581, gradient norm: 0.32301362602886297\n",
      "Iteration: 8417000, loss: 0.10641871594855616, gradient norm: 0.14247605414968254\n",
      "Iteration: 8418000, loss: 0.10641860014495205, gradient norm: 0.2510740212694558\n",
      "Iteration: 8419000, loss: 0.10641854671536588, gradient norm: 0.30289698357724903\n",
      "Iteration: 8420000, loss: 0.10641818432728493, gradient norm: 0.0926628807544799\n",
      "Iteration: 8421000, loss: 0.10641816033713675, gradient norm: 0.10773940618160405\n",
      "Iteration: 8422000, loss: 0.10641777914108419, gradient norm: 0.21610054766825285\n",
      "Iteration: 8423000, loss: 0.10641761657771685, gradient norm: 0.690544762517889\n",
      "Iteration: 8424000, loss: 0.10641772543417871, gradient norm: 0.8273918857305261\n",
      "Iteration: 8425000, loss: 0.10641734835919303, gradient norm: 0.7076455722967118\n",
      "Iteration: 8426000, loss: 0.10641718377224112, gradient norm: 0.386367208347546\n",
      "Iteration: 8427000, loss: 0.10641702677685819, gradient norm: 0.2700941793490425\n",
      "Iteration: 8428000, loss: 0.10641679493758699, gradient norm: 0.5082498159329158\n",
      "Iteration: 8429000, loss: 0.10641654459769055, gradient norm: 0.5258293226849751\n",
      "Iteration: 8430000, loss: 0.10641631578300584, gradient norm: 0.024779131714861773\n",
      "Iteration: 8431000, loss: 0.10641626679919683, gradient norm: 0.08498781211931387\n",
      "Iteration: 8432000, loss: 0.10641607228753196, gradient norm: 0.07471011016401245\n",
      "Iteration: 8433000, loss: 0.10641594998480633, gradient norm: 0.13044853674511822\n",
      "Iteration: 8434000, loss: 0.10641568420141884, gradient norm: 0.1399439719180492\n",
      "Iteration: 8435000, loss: 0.10641556633934723, gradient norm: 0.26305216202297105\n",
      "Iteration: 8436000, loss: 0.10641537368358393, gradient norm: 0.12248839294403735\n",
      "Iteration: 8437000, loss: 0.10641512672913254, gradient norm: 0.5615676680714318\n",
      "Iteration: 8438000, loss: 0.10641496394966543, gradient norm: 0.194796652745787\n",
      "Iteration: 8439000, loss: 0.10641473180361224, gradient norm: 0.11691685312717814\n",
      "Iteration: 8440000, loss: 0.1064145561701046, gradient norm: 0.4529322664505869\n",
      "Iteration: 8441000, loss: 0.10641450235600561, gradient norm: 0.11654952572055276\n",
      "Iteration: 8442000, loss: 0.10641403932876028, gradient norm: 0.10439067159992392\n",
      "Iteration: 8443000, loss: 0.10641412073960223, gradient norm: 0.1940456446093289\n",
      "Iteration: 8444000, loss: 0.10641387165001716, gradient norm: 0.14221044573649294\n",
      "Iteration: 8445000, loss: 0.10641363256171388, gradient norm: 0.10951417832698941\n",
      "Iteration: 8446000, loss: 0.1064136299232439, gradient norm: 0.22253927011263477\n",
      "Iteration: 8447000, loss: 0.10641318676769256, gradient norm: 0.20341701926528966\n",
      "Iteration: 8448000, loss: 0.10641314497461798, gradient norm: 0.07787042997532158\n",
      "Iteration: 8449000, loss: 0.10641298317113472, gradient norm: 0.31662519451219856\n",
      "Iteration: 8450000, loss: 0.10641264733559902, gradient norm: 0.4629967064836978\n",
      "Iteration: 8451000, loss: 0.10641262928224711, gradient norm: 0.46935533349951036\n",
      "Iteration: 8452000, loss: 0.10641236568820622, gradient norm: 0.3548813136138935\n",
      "Iteration: 8453000, loss: 0.10641232224671424, gradient norm: 0.21275983403773538\n",
      "Iteration: 8454000, loss: 0.10641203590047428, gradient norm: 0.4607583421710624\n",
      "Iteration: 8455000, loss: 0.1064117931706512, gradient norm: 0.4899813422280408\n",
      "Iteration: 8456000, loss: 0.10641171771023199, gradient norm: 0.7658225973490684\n",
      "Iteration: 8457000, loss: 0.10641155709877911, gradient norm: 0.38405050952047776\n",
      "Iteration: 8458000, loss: 0.10641116301701069, gradient norm: 0.10964108948322301\n",
      "Iteration: 8459000, loss: 0.10641119924855298, gradient norm: 0.15807868238170564\n",
      "Iteration: 8460000, loss: 0.10641086249752366, gradient norm: 0.3684611144908781\n",
      "Iteration: 8461000, loss: 0.1064107538909383, gradient norm: 0.705132212559833\n",
      "Iteration: 8462000, loss: 0.10641065583754321, gradient norm: 0.163583014939835\n",
      "Iteration: 8463000, loss: 0.10641036184575718, gradient norm: 0.2902136180136427\n",
      "Iteration: 8464000, loss: 0.10641024575202018, gradient norm: 0.25575549678154896\n",
      "Iteration: 8465000, loss: 0.10640999944680708, gradient norm: 0.06556456962944589\n",
      "Iteration: 8466000, loss: 0.10640977175111424, gradient norm: 0.19664620698113272\n",
      "Iteration: 8467000, loss: 0.10640975369340162, gradient norm: 0.1666579573223238\n",
      "Iteration: 8468000, loss: 0.10640950292515004, gradient norm: 0.3031947993185036\n",
      "Iteration: 8469000, loss: 0.1064092006625588, gradient norm: 0.3125966146648648\n",
      "Iteration: 8470000, loss: 0.10640912036515204, gradient norm: 0.21566922303525196\n",
      "Iteration: 8471000, loss: 0.1064090074562131, gradient norm: 0.33961292166729107\n",
      "Iteration: 8472000, loss: 0.10640869111523406, gradient norm: 0.4345848733100013\n",
      "Iteration: 8473000, loss: 0.106408532121279, gradient norm: 0.2839273981778071\n",
      "Iteration: 8474000, loss: 0.10640843895212007, gradient norm: 0.03809459356115097\n",
      "Iteration: 8475000, loss: 0.10640819611059697, gradient norm: 0.2956677999286264\n",
      "Iteration: 8476000, loss: 0.10640804017343451, gradient norm: 0.2133918864743226\n",
      "Iteration: 8477000, loss: 0.10640785450952393, gradient norm: 0.48151693445436217\n",
      "Iteration: 8478000, loss: 0.10640756213803373, gradient norm: 0.5589835661826901\n",
      "Iteration: 8479000, loss: 0.10640748843778174, gradient norm: 0.8442109642188994\n",
      "Iteration: 8480000, loss: 0.10640740471150349, gradient norm: 0.2258703390190971\n",
      "Iteration: 8481000, loss: 0.10640699783109636, gradient norm: 0.5278347858542909\n",
      "Iteration: 8482000, loss: 0.10640691691966504, gradient norm: 0.41793901486093793\n",
      "Iteration: 8483000, loss: 0.10640678180412408, gradient norm: 0.7277717793160243\n",
      "Iteration: 8484000, loss: 0.10640662980996911, gradient norm: 0.2228234850089751\n",
      "Iteration: 8485000, loss: 0.10640640192466681, gradient norm: 0.5039489187784079\n",
      "Iteration: 8486000, loss: 0.10640626516170018, gradient norm: 0.2539426569939474\n",
      "Iteration: 8487000, loss: 0.1064060451126437, gradient norm: 0.5862655642169207\n",
      "Iteration: 8488000, loss: 0.1064058357304595, gradient norm: 0.2095134467709839\n",
      "Iteration: 8489000, loss: 0.10640549302555911, gradient norm: 0.44521163400398966\n",
      "Iteration: 8490000, loss: 0.1064055920334236, gradient norm: 0.08583109865587545\n",
      "Iteration: 8491000, loss: 0.10640526182966308, gradient norm: 0.849902142887646\n",
      "Iteration: 8492000, loss: 0.1064051086290745, gradient norm: 0.024096154571959284\n",
      "Iteration: 8493000, loss: 0.10640500673245647, gradient norm: 0.10496397572011143\n",
      "Iteration: 8494000, loss: 0.10640487307771017, gradient norm: 0.25244125056665107\n",
      "Iteration: 8495000, loss: 0.10640439648343587, gradient norm: 0.1945562073529812\n",
      "Iteration: 8496000, loss: 0.10640449975257585, gradient norm: 0.47044645260823187\n",
      "Iteration: 8497000, loss: 0.10640415415704695, gradient norm: 0.31682966427423015\n",
      "Iteration: 8498000, loss: 0.10640394865105039, gradient norm: 0.2616523472986731\n",
      "Iteration: 8499000, loss: 0.10640403253246485, gradient norm: 0.04110416639967821\n",
      "Iteration: 8500000, loss: 0.106403619760166, gradient norm: 0.500616936683738\n",
      "Iteration: 8501000, loss: 0.10640362560807713, gradient norm: 0.2504111601180097\n",
      "Iteration: 8502000, loss: 0.1064031723317432, gradient norm: 1.0664560201882567\n",
      "Iteration: 8503000, loss: 0.1064031565817478, gradient norm: 0.14619262417965148\n",
      "Iteration: 8504000, loss: 0.10640295581219163, gradient norm: 0.2901158336698502\n",
      "Iteration: 8505000, loss: 0.10640266030588026, gradient norm: 0.5425579534137746\n",
      "Iteration: 8506000, loss: 0.10640261236439746, gradient norm: 0.2511260428920314\n",
      "Iteration: 8507000, loss: 0.10640246563743642, gradient norm: 0.12383646927230446\n",
      "Iteration: 8508000, loss: 0.10640220475177227, gradient norm: 0.6364789114559094\n",
      "Iteration: 8509000, loss: 0.10640204118933548, gradient norm: 0.2436489928200956\n",
      "Iteration: 8510000, loss: 0.10640170790050946, gradient norm: 0.14666833340530952\n",
      "Iteration: 8511000, loss: 0.10640171242104873, gradient norm: 0.12319976012169417\n",
      "Iteration: 8512000, loss: 0.10640157705450862, gradient norm: 0.512299826472115\n",
      "Iteration: 8513000, loss: 0.10640141825992376, gradient norm: 0.1331504424198898\n",
      "Iteration: 8514000, loss: 0.10640104047090004, gradient norm: 0.28784759540885607\n",
      "Iteration: 8515000, loss: 0.10640090593078602, gradient norm: 0.18579673090245263\n",
      "Iteration: 8516000, loss: 0.10640086686943698, gradient norm: 0.018179422434673256\n",
      "Iteration: 8517000, loss: 0.10640056840397176, gradient norm: 0.254969384323745\n",
      "Iteration: 8518000, loss: 0.10640037340732472, gradient norm: 0.2521725751993615\n",
      "Iteration: 8519000, loss: 0.10640037847103274, gradient norm: 0.049761987196883344\n",
      "Iteration: 8520000, loss: 0.10640006445610625, gradient norm: 0.3162718827949326\n",
      "Iteration: 8521000, loss: 0.10639980676493808, gradient norm: 0.624261754054517\n",
      "Iteration: 8522000, loss: 0.10639973484989575, gradient norm: 0.0698428532465941\n",
      "Iteration: 8523000, loss: 0.10639945973507574, gradient norm: 0.32357462021438255\n",
      "Iteration: 8524000, loss: 0.10639928169354662, gradient norm: 0.31914638973050397\n",
      "Iteration: 8525000, loss: 0.1063991093635098, gradient norm: 0.7107495434801987\n",
      "Iteration: 8526000, loss: 0.10639911543205252, gradient norm: 0.2731700896524583\n",
      "Iteration: 8527000, loss: 0.10639863830037571, gradient norm: 0.16523098637098152\n",
      "Iteration: 8528000, loss: 0.10639865435620624, gradient norm: 0.4435817853780366\n",
      "Iteration: 8529000, loss: 0.10639847892349533, gradient norm: 0.17356276047107808\n",
      "Iteration: 8530000, loss: 0.10639831159220668, gradient norm: 0.22929373831948272\n",
      "Iteration: 8531000, loss: 0.1063980194816775, gradient norm: 0.0705768336493222\n",
      "Iteration: 8532000, loss: 0.10639790910182555, gradient norm: 0.4786903814273131\n",
      "Iteration: 8533000, loss: 0.10639763434928756, gradient norm: 0.3883414982385248\n",
      "Iteration: 8534000, loss: 0.10639744930036389, gradient norm: 0.5950795730188456\n",
      "Iteration: 8535000, loss: 0.10639744475685285, gradient norm: 0.3555504708201115\n",
      "Iteration: 8536000, loss: 0.10639714022791015, gradient norm: 0.2090478101754237\n",
      "Iteration: 8537000, loss: 0.10639695501688048, gradient norm: 0.16114624664665436\n",
      "Iteration: 8538000, loss: 0.10639685659563723, gradient norm: 0.008065540649392664\n",
      "Iteration: 8539000, loss: 0.10639666626335754, gradient norm: 0.024838331362556552\n",
      "Iteration: 8540000, loss: 0.10639632437675817, gradient norm: 0.5314970121754948\n",
      "Iteration: 8541000, loss: 0.10639630437633636, gradient norm: 0.4890146614842343\n",
      "Iteration: 8542000, loss: 0.10639613555953216, gradient norm: 0.5425602012513614\n",
      "Iteration: 8543000, loss: 0.10639585422688262, gradient norm: 0.1483404050342988\n",
      "Iteration: 8544000, loss: 0.10639566911495237, gradient norm: 0.7112038982612235\n",
      "Iteration: 8545000, loss: 0.10639555607890804, gradient norm: 0.49493431403471555\n",
      "Iteration: 8546000, loss: 0.10639544878363927, gradient norm: 0.019829281255684884\n",
      "Iteration: 8547000, loss: 0.10639534833956046, gradient norm: 0.06515905944903849\n",
      "Iteration: 8548000, loss: 0.10639483594624724, gradient norm: 0.43899330949037035\n",
      "Iteration: 8549000, loss: 0.10639498103749503, gradient norm: 0.35404280000026783\n",
      "Iteration: 8550000, loss: 0.10639474743142617, gradient norm: 0.34126071897728594\n",
      "Iteration: 8551000, loss: 0.10639443341619749, gradient norm: 0.4597620878399151\n",
      "Iteration: 8552000, loss: 0.10639429183497572, gradient norm: 0.17809851537614865\n",
      "Iteration: 8553000, loss: 0.10639411484648313, gradient norm: 0.10918108873022817\n",
      "Iteration: 8554000, loss: 0.10639392970924627, gradient norm: 0.7500482830093348\n",
      "Iteration: 8555000, loss: 0.10639380381287745, gradient norm: 0.08684553433574968\n",
      "Iteration: 8556000, loss: 0.10639353588925213, gradient norm: 1.049502057306285\n",
      "Iteration: 8557000, loss: 0.1063934206722186, gradient norm: 0.05395807596997905\n",
      "Iteration: 8558000, loss: 0.1063932769669211, gradient norm: 0.14601415972853796\n",
      "Iteration: 8559000, loss: 0.10639294325568693, gradient norm: 0.4307727614812714\n",
      "Iteration: 8560000, loss: 0.10639288894027459, gradient norm: 0.1827050512918134\n",
      "Iteration: 8561000, loss: 0.10639271659006706, gradient norm: 0.16630118270743127\n",
      "Iteration: 8562000, loss: 0.10639245883334707, gradient norm: 0.38521301487808635\n",
      "Iteration: 8563000, loss: 0.10639226972663368, gradient norm: 0.5257875119284721\n",
      "Iteration: 8564000, loss: 0.10639218478305469, gradient norm: 0.11955519918960522\n",
      "Iteration: 8565000, loss: 0.1063920133543374, gradient norm: 0.16421285720322132\n",
      "Iteration: 8566000, loss: 0.10639177221116951, gradient norm: 0.18661205708806367\n",
      "Iteration: 8567000, loss: 0.10639165702404071, gradient norm: 0.7660264848229258\n",
      "Iteration: 8568000, loss: 0.10639150264503507, gradient norm: 0.20820561398826837\n",
      "Iteration: 8569000, loss: 0.10639114713958232, gradient norm: 0.12502689812580078\n",
      "Iteration: 8570000, loss: 0.10639104898968159, gradient norm: 0.18646721932653182\n",
      "Iteration: 8571000, loss: 0.10639093503622113, gradient norm: 0.3529192388890028\n",
      "Iteration: 8572000, loss: 0.10639061588480363, gradient norm: 0.12970454481656526\n",
      "Iteration: 8573000, loss: 0.10639062774422418, gradient norm: 0.29701111745737374\n",
      "Iteration: 8574000, loss: 0.10639030104263024, gradient norm: 0.5965140995683064\n",
      "Iteration: 8575000, loss: 0.10639035679236425, gradient norm: 0.054780736020004175\n",
      "Iteration: 8576000, loss: 0.10638983576644767, gradient norm: 0.40876808172193607\n",
      "Iteration: 8577000, loss: 0.10638997812312985, gradient norm: 0.12701699644903622\n",
      "Iteration: 8578000, loss: 0.10638961932714269, gradient norm: 0.492069921886252\n",
      "Iteration: 8579000, loss: 0.10638960421240184, gradient norm: 0.6327802566826793\n",
      "Iteration: 8580000, loss: 0.10638918804956961, gradient norm: 0.3128625888009764\n",
      "Iteration: 8581000, loss: 0.10638918880711504, gradient norm: 0.22705447797366038\n",
      "Iteration: 8582000, loss: 0.10638886422772031, gradient norm: 0.6464876607942439\n",
      "Iteration: 8583000, loss: 0.10638877378126262, gradient norm: 0.9927623382408642\n",
      "Iteration: 8584000, loss: 0.10638853929179194, gradient norm: 0.1825658204229647\n",
      "Iteration: 8585000, loss: 0.10638846808187385, gradient norm: 0.4053493240419868\n",
      "Iteration: 8586000, loss: 0.10638817447972336, gradient norm: 0.2813111692311966\n",
      "Iteration: 8587000, loss: 0.10638801332425472, gradient norm: 0.2173537039978312\n",
      "Iteration: 8588000, loss: 0.10638795149793631, gradient norm: 0.17367925048185123\n",
      "Iteration: 8589000, loss: 0.10638765201066093, gradient norm: 0.06872464913013539\n",
      "Iteration: 8590000, loss: 0.10638762313586844, gradient norm: 0.0877386345591168\n",
      "Iteration: 8591000, loss: 0.10638715978798749, gradient norm: 0.5590778711419161\n",
      "Iteration: 8592000, loss: 0.10638727256834414, gradient norm: 0.8144308094104415\n",
      "Iteration: 8593000, loss: 0.10638692676268996, gradient norm: 0.595478009514137\n",
      "Iteration: 8594000, loss: 0.1063867668175522, gradient norm: 0.17840295498987463\n",
      "Iteration: 8595000, loss: 0.10638672909748519, gradient norm: 0.18378503283253775\n",
      "Iteration: 8596000, loss: 0.10638635490107176, gradient norm: 0.673836705000346\n",
      "Iteration: 8597000, loss: 0.1063863023280146, gradient norm: 0.5168024901594468\n",
      "Iteration: 8598000, loss: 0.10638607664997343, gradient norm: 0.20114615702058716\n",
      "Iteration: 8599000, loss: 0.10638594566229195, gradient norm: 0.3491466191308323\n",
      "Iteration: 8600000, loss: 0.10638581512060868, gradient norm: 0.1378154207311268\n",
      "Iteration: 8601000, loss: 0.10638533970497509, gradient norm: 0.16267561440975714\n",
      "Iteration: 8602000, loss: 0.10638540496038537, gradient norm: 0.6025516242837219\n",
      "Iteration: 8603000, loss: 0.10638539480149947, gradient norm: 0.1353755604282358\n",
      "Iteration: 8604000, loss: 0.10638510304361451, gradient norm: 0.1525266513249251\n",
      "Iteration: 8605000, loss: 0.10638479358664137, gradient norm: 0.1787259723313294\n",
      "Iteration: 8606000, loss: 0.10638467625940352, gradient norm: 0.42595206528324103\n",
      "Iteration: 8607000, loss: 0.10638446053695165, gradient norm: 0.38539168242823835\n",
      "Iteration: 8608000, loss: 0.10638428745361296, gradient norm: 0.4476501049769348\n",
      "Iteration: 8609000, loss: 0.10638409011818717, gradient norm: 0.839533139169168\n",
      "Iteration: 8610000, loss: 0.10638408825521685, gradient norm: 0.04837401681671326\n",
      "Iteration: 8611000, loss: 0.10638365099650904, gradient norm: 0.1860900573421479\n",
      "Iteration: 8612000, loss: 0.10638362683653872, gradient norm: 0.27291406629757187\n",
      "Iteration: 8613000, loss: 0.10638339789291823, gradient norm: 0.33517262294983596\n",
      "Iteration: 8614000, loss: 0.10638322279621763, gradient norm: 0.3569944383991832\n",
      "Iteration: 8615000, loss: 0.10638323099024549, gradient norm: 0.2028001091272843\n",
      "Iteration: 8616000, loss: 0.10638273838499, gradient norm: 0.4136214857343417\n",
      "Iteration: 8617000, loss: 0.10638299869271851, gradient norm: 0.5081904825117167\n",
      "Iteration: 8618000, loss: 0.10638248146224327, gradient norm: 0.13058255857281542\n",
      "Iteration: 8619000, loss: 0.10638226007973578, gradient norm: 0.13682861017789674\n",
      "Iteration: 8620000, loss: 0.10638218648751416, gradient norm: 0.5636775199780554\n",
      "Iteration: 8621000, loss: 0.10638208245856484, gradient norm: 0.7243616385859392\n",
      "Iteration: 8622000, loss: 0.10638192294624983, gradient norm: 0.5472373870659158\n",
      "Iteration: 8623000, loss: 0.10638169377403371, gradient norm: 0.3105080262066276\n",
      "Iteration: 8624000, loss: 0.10638137516088522, gradient norm: 0.477629743642371\n",
      "Iteration: 8625000, loss: 0.10638133917081377, gradient norm: 0.38642410377217334\n",
      "Iteration: 8626000, loss: 0.10638114877578256, gradient norm: 0.5260436634786413\n",
      "Iteration: 8627000, loss: 0.10638096110622376, gradient norm: 0.21736349781490535\n",
      "Iteration: 8628000, loss: 0.10638081593734709, gradient norm: 0.06838008664843714\n",
      "Iteration: 8629000, loss: 0.10638045897993792, gradient norm: 0.11316991281630831\n",
      "Iteration: 8630000, loss: 0.10638042580306223, gradient norm: 0.12499264387216344\n",
      "Iteration: 8631000, loss: 0.10638038919892619, gradient norm: 0.5274733504008381\n",
      "Iteration: 8632000, loss: 0.1063799564068616, gradient norm: 0.24170230260639372\n",
      "Iteration: 8633000, loss: 0.10637997738187593, gradient norm: 0.14437339720995188\n",
      "Iteration: 8634000, loss: 0.10637961557496906, gradient norm: 0.10468051217870396\n",
      "Iteration: 8635000, loss: 0.10637955391750578, gradient norm: 0.2698166864829504\n",
      "Iteration: 8636000, loss: 0.10637929633152865, gradient norm: 0.34519478962037803\n",
      "Iteration: 8637000, loss: 0.10637916894911865, gradient norm: 0.3042585179798529\n",
      "Iteration: 8638000, loss: 0.10637901374659228, gradient norm: 0.08621581719660512\n",
      "Iteration: 8639000, loss: 0.1063788427832346, gradient norm: 0.7001632315520425\n",
      "Iteration: 8640000, loss: 0.10637863842930995, gradient norm: 0.11471508373767803\n",
      "Iteration: 8641000, loss: 0.10637861532262485, gradient norm: 0.3246003046033691\n",
      "Iteration: 8642000, loss: 0.10637833943881471, gradient norm: 0.12172711010658228\n",
      "Iteration: 8643000, loss: 0.10637807732841943, gradient norm: 0.531796192073966\n",
      "Iteration: 8644000, loss: 0.10637792206675968, gradient norm: 0.31812609327783214\n",
      "Iteration: 8645000, loss: 0.10637774778710896, gradient norm: 0.7021062533364887\n",
      "Iteration: 8646000, loss: 0.10637764823114634, gradient norm: 0.177784628034395\n",
      "Iteration: 8647000, loss: 0.10637730690430157, gradient norm: 0.34793295178094175\n",
      "Iteration: 8648000, loss: 0.10637730271458025, gradient norm: 0.26509496934778576\n",
      "Iteration: 8649000, loss: 0.10637709345470503, gradient norm: 0.6817626872555219\n",
      "Iteration: 8650000, loss: 0.10637685201835859, gradient norm: 0.2982796595933278\n",
      "Iteration: 8651000, loss: 0.10637673758723792, gradient norm: 0.779370896217839\n",
      "Iteration: 8652000, loss: 0.10637670242284986, gradient norm: 0.02871929165669281\n",
      "Iteration: 8653000, loss: 0.10637616412752092, gradient norm: 0.1296712835030068\n",
      "Iteration: 8654000, loss: 0.1063762628524814, gradient norm: 0.11888724190052158\n",
      "Iteration: 8655000, loss: 0.10637606068066457, gradient norm: 0.1547524365338772\n",
      "Iteration: 8656000, loss: 0.10637568992031356, gradient norm: 0.16640907896709872\n",
      "Iteration: 8657000, loss: 0.10637573819327818, gradient norm: 0.11348927739732928\n",
      "Iteration: 8658000, loss: 0.10637552453522975, gradient norm: 0.6649355277933773\n",
      "Iteration: 8659000, loss: 0.1063753201487182, gradient norm: 0.4041972527581982\n",
      "Iteration: 8660000, loss: 0.10637505764601536, gradient norm: 0.14275637563976615\n",
      "Iteration: 8661000, loss: 0.1063749809877668, gradient norm: 0.19928631189659451\n",
      "Iteration: 8662000, loss: 0.10637473323845513, gradient norm: 0.1296365243133065\n",
      "Iteration: 8663000, loss: 0.10637465211943424, gradient norm: 0.1909076160640871\n",
      "Iteration: 8664000, loss: 0.10637437169445858, gradient norm: 0.8638937859572394\n",
      "Iteration: 8665000, loss: 0.10637435078626689, gradient norm: 0.12853155296643065\n",
      "Iteration: 8666000, loss: 0.10637405967950991, gradient norm: 0.1720226537316848\n",
      "Iteration: 8667000, loss: 0.10637394873446541, gradient norm: 0.09490597356500179\n",
      "Iteration: 8668000, loss: 0.10637374509990909, gradient norm: 0.358736138402285\n",
      "Iteration: 8669000, loss: 0.10637359594738376, gradient norm: 0.061962289996906166\n",
      "Iteration: 8670000, loss: 0.10637327322527693, gradient norm: 0.35636447122039633\n",
      "Iteration: 8671000, loss: 0.1063733281752931, gradient norm: 0.05552706204912525\n",
      "Iteration: 8672000, loss: 0.10637300604532371, gradient norm: 0.6223375684383138\n",
      "Iteration: 8673000, loss: 0.10637289457120294, gradient norm: 0.15385017276956736\n",
      "Iteration: 8674000, loss: 0.10637262273626527, gradient norm: 0.6484501916611809\n",
      "Iteration: 8675000, loss: 0.10637246052047994, gradient norm: 0.7967910535805223\n",
      "Iteration: 8676000, loss: 0.10637233832408251, gradient norm: 0.011408823504327134\n",
      "Iteration: 8677000, loss: 0.10637209942060873, gradient norm: 0.3306261929703034\n",
      "Iteration: 8678000, loss: 0.10637193172013393, gradient norm: 0.7977626453743346\n",
      "Iteration: 8679000, loss: 0.10637185227151108, gradient norm: 0.27280683608793943\n",
      "Iteration: 8680000, loss: 0.10637172955036045, gradient norm: 0.039328852308208165\n",
      "Iteration: 8681000, loss: 0.10637157925454949, gradient norm: 0.03468687329300812\n",
      "Iteration: 8682000, loss: 0.10637112786554914, gradient norm: 0.19324971227111076\n",
      "Iteration: 8683000, loss: 0.10637107265119722, gradient norm: 0.23749201290837432\n",
      "Iteration: 8684000, loss: 0.10637110710811129, gradient norm: 0.2574176273963582\n",
      "Iteration: 8685000, loss: 0.10637079874068087, gradient norm: 0.49311237532156865\n",
      "Iteration: 8686000, loss: 0.10637048308363861, gradient norm: 0.3853515305821261\n",
      "Iteration: 8687000, loss: 0.10637037321302008, gradient norm: 0.41189274535197823\n",
      "Iteration: 8688000, loss: 0.10637024502779494, gradient norm: 0.8060004894302988\n",
      "Iteration: 8689000, loss: 0.10637010862747599, gradient norm: 0.32881261069600404\n",
      "Iteration: 8690000, loss: 0.10636973491764175, gradient norm: 0.31682364739117436\n",
      "Iteration: 8691000, loss: 0.10636983629404181, gradient norm: 0.21071260214077517\n",
      "Iteration: 8692000, loss: 0.10636953240172907, gradient norm: 0.37576825814515646\n",
      "Iteration: 8693000, loss: 0.10636929439724341, gradient norm: 0.3491572502175481\n",
      "Iteration: 8694000, loss: 0.10636917271857825, gradient norm: 0.4705546275297481\n",
      "Iteration: 8695000, loss: 0.10636894619448191, gradient norm: 0.3726358258279772\n",
      "Iteration: 8696000, loss: 0.1063689592389792, gradient norm: 0.23279368441096157\n",
      "Iteration: 8697000, loss: 0.10636863409125234, gradient norm: 0.5332745344463696\n",
      "Iteration: 8698000, loss: 0.10636839566609516, gradient norm: 0.547076050257987\n",
      "Iteration: 8699000, loss: 0.10636854895604592, gradient norm: 0.048757299779199424\n",
      "Iteration: 8700000, loss: 0.1063680103606998, gradient norm: 0.08139865364634562\n",
      "Iteration: 8701000, loss: 0.10636784239713995, gradient norm: 0.18738513608095145\n",
      "Iteration: 8702000, loss: 0.10636789966079542, gradient norm: 0.01558694726368226\n",
      "Iteration: 8703000, loss: 0.10636761802279117, gradient norm: 0.2839388230338094\n",
      "Iteration: 8704000, loss: 0.10636743433370205, gradient norm: 0.7576906172100687\n",
      "Iteration: 8705000, loss: 0.10636733051834237, gradient norm: 0.11198898228750391\n",
      "Iteration: 8706000, loss: 0.10636701206186219, gradient norm: 0.3411944400767163\n",
      "Iteration: 8707000, loss: 0.10636688254090243, gradient norm: 0.799176245075492\n",
      "Iteration: 8708000, loss: 0.10636676524724485, gradient norm: 0.27377479057721876\n",
      "Iteration: 8709000, loss: 0.10636645557964007, gradient norm: 0.2084459674209301\n",
      "Iteration: 8710000, loss: 0.10636648770204495, gradient norm: 0.11730390299461535\n",
      "Iteration: 8711000, loss: 0.10636613654328755, gradient norm: 0.13987382755239403\n",
      "Iteration: 8712000, loss: 0.1063661200559032, gradient norm: 0.5542918488422478\n",
      "Iteration: 8713000, loss: 0.10636588958104887, gradient norm: 0.16058984288059935\n",
      "Iteration: 8714000, loss: 0.10636564845395192, gradient norm: 0.2937589559886846\n",
      "Iteration: 8715000, loss: 0.1063654664359003, gradient norm: 0.44590271380276614\n",
      "Iteration: 8716000, loss: 0.10636541798900269, gradient norm: 0.09923574561984165\n",
      "Iteration: 8717000, loss: 0.1063651107422479, gradient norm: 0.15689376627892002\n",
      "Iteration: 8718000, loss: 0.10636497530925826, gradient norm: 0.1860728740023276\n",
      "Iteration: 8719000, loss: 0.10636494112506045, gradient norm: 0.3258527881132514\n",
      "Iteration: 8720000, loss: 0.10636462678246712, gradient norm: 0.08979002238226239\n",
      "Iteration: 8721000, loss: 0.10636444452141029, gradient norm: 0.3998410542885608\n",
      "Iteration: 8722000, loss: 0.1063643210389634, gradient norm: 0.1942362007542144\n",
      "Iteration: 8723000, loss: 0.10636405610598691, gradient norm: 0.10544241942945466\n",
      "Iteration: 8724000, loss: 0.10636396741227488, gradient norm: 0.2593400189800434\n",
      "Iteration: 8725000, loss: 0.10636384878849772, gradient norm: 0.27669189300634045\n",
      "Iteration: 8726000, loss: 0.10636361619222699, gradient norm: 0.25166159334324173\n",
      "Iteration: 8727000, loss: 0.10636335069496723, gradient norm: 0.12911440353360615\n",
      "Iteration: 8728000, loss: 0.10636343111197093, gradient norm: 0.2446876284263388\n",
      "Iteration: 8729000, loss: 0.10636316466227845, gradient norm: 0.284770343185156\n",
      "Iteration: 8730000, loss: 0.10636284661868917, gradient norm: 0.544946936993695\n",
      "Iteration: 8731000, loss: 0.10636285214852338, gradient norm: 0.7097019384658029\n",
      "Iteration: 8732000, loss: 0.10636259455143547, gradient norm: 0.14581368972506603\n",
      "Iteration: 8733000, loss: 0.10636226135309454, gradient norm: 0.12399704281021938\n",
      "Iteration: 8734000, loss: 0.10636226049980242, gradient norm: 0.2715826611145917\n",
      "Iteration: 8735000, loss: 0.10636195452212523, gradient norm: 0.21985538151456105\n",
      "Iteration: 8736000, loss: 0.10636191867377667, gradient norm: 0.6601067727377518\n",
      "Iteration: 8737000, loss: 0.10636178246254023, gradient norm: 0.6767039745202534\n",
      "Iteration: 8738000, loss: 0.10636147277889148, gradient norm: 0.28760348442944544\n",
      "Iteration: 8739000, loss: 0.10636121436625946, gradient norm: 0.03180831729797302\n",
      "Iteration: 8740000, loss: 0.10636133308586976, gradient norm: 0.7498890157155329\n",
      "Iteration: 8741000, loss: 0.1063611143061226, gradient norm: 0.14793598504341393\n",
      "Iteration: 8742000, loss: 0.1063607129836362, gradient norm: 0.10993631297620816\n",
      "Iteration: 8743000, loss: 0.10636068710747945, gradient norm: 0.13709579824615037\n",
      "Iteration: 8744000, loss: 0.10636049444576792, gradient norm: 0.11081268985124994\n",
      "Iteration: 8745000, loss: 0.10636025754616278, gradient norm: 0.4518699960864672\n",
      "Iteration: 8746000, loss: 0.10636021927029202, gradient norm: 0.15975136731849526\n",
      "Iteration: 8747000, loss: 0.10636008444728455, gradient norm: 0.41515640508024404\n",
      "Iteration: 8748000, loss: 0.10635970269734334, gradient norm: 0.15210816610205274\n",
      "Iteration: 8749000, loss: 0.10635961206122099, gradient norm: 0.22103138430919436\n",
      "Iteration: 8750000, loss: 0.10635949735668485, gradient norm: 1.1776624680709544\n",
      "Iteration: 8751000, loss: 0.10635931270548407, gradient norm: 0.1605996915513363\n",
      "Iteration: 8752000, loss: 0.1063593783799586, gradient norm: 0.1585646015373384\n",
      "Iteration: 8753000, loss: 0.10635867418732664, gradient norm: 0.0230096455401344\n",
      "Iteration: 8754000, loss: 0.1063587503587164, gradient norm: 0.184411643511058\n",
      "Iteration: 8755000, loss: 0.10635852500794354, gradient norm: 0.4971015653791129\n",
      "Iteration: 8756000, loss: 0.10635847164179554, gradient norm: 0.11896405723803148\n",
      "Iteration: 8757000, loss: 0.10635827228305311, gradient norm: 0.48242185423282247\n",
      "Iteration: 8758000, loss: 0.10635802400000376, gradient norm: 0.3433600517400435\n",
      "Iteration: 8759000, loss: 0.10635793319244627, gradient norm: 0.09941653941517935\n",
      "Iteration: 8760000, loss: 0.10635765936946032, gradient norm: 0.14611388922098859\n",
      "Iteration: 8761000, loss: 0.10635768245035876, gradient norm: 0.05315054745008387\n",
      "Iteration: 8762000, loss: 0.10635724117699753, gradient norm: 0.06743021809274395\n",
      "Iteration: 8763000, loss: 0.1063573739101662, gradient norm: 0.3619748920167667\n",
      "Iteration: 8764000, loss: 0.10635694733877489, gradient norm: 0.14076859604048328\n",
      "Iteration: 8765000, loss: 0.10635701290988052, gradient norm: 0.36009934877402144\n",
      "Iteration: 8766000, loss: 0.10635654422603105, gradient norm: 0.7166585833912479\n",
      "Iteration: 8767000, loss: 0.10635659751306416, gradient norm: 0.7731598448756198\n",
      "Iteration: 8768000, loss: 0.10635637303099148, gradient norm: 0.1437264788876248\n",
      "Iteration: 8769000, loss: 0.10635632067022495, gradient norm: 0.14344861434299014\n",
      "Iteration: 8770000, loss: 0.1063559117568002, gradient norm: 0.7783521011596\n",
      "Iteration: 8771000, loss: 0.10635584263277632, gradient norm: 0.2989634579042948\n",
      "Iteration: 8772000, loss: 0.10635570079004407, gradient norm: 0.15924904633337658\n",
      "Iteration: 8773000, loss: 0.10635536515616517, gradient norm: 0.021201191638530997\n",
      "Iteration: 8774000, loss: 0.1063553941212048, gradient norm: 0.36408302451272934\n",
      "Iteration: 8775000, loss: 0.10635530319697827, gradient norm: 0.12723219449871295\n",
      "Iteration: 8776000, loss: 0.10635487335584072, gradient norm: 0.7647009445660095\n",
      "Iteration: 8777000, loss: 0.10635476320641463, gradient norm: 0.26876843364645786\n",
      "Iteration: 8778000, loss: 0.10635458343394291, gradient norm: 0.622663569837852\n",
      "Iteration: 8779000, loss: 0.10635454144309398, gradient norm: 0.23589718746084837\n",
      "Iteration: 8780000, loss: 0.10635435731686063, gradient norm: 0.7261221187894558\n",
      "Iteration: 8781000, loss: 0.10635391908579293, gradient norm: 0.23032627426376245\n",
      "Iteration: 8782000, loss: 0.10635396931135088, gradient norm: 0.5853639650901151\n",
      "Iteration: 8783000, loss: 0.10635386232292529, gradient norm: 0.13134678182821144\n",
      "Iteration: 8784000, loss: 0.10635356868745177, gradient norm: 0.6626347774241279\n",
      "Iteration: 8785000, loss: 0.10635335112427098, gradient norm: 0.1346012284274552\n",
      "Iteration: 8786000, loss: 0.10635332769233728, gradient norm: 0.07711738273605163\n",
      "Iteration: 8787000, loss: 0.10635309067621258, gradient norm: 0.05814093920845231\n",
      "Iteration: 8788000, loss: 0.10635296614336102, gradient norm: 0.20138148442484144\n",
      "Iteration: 8789000, loss: 0.10635279238116861, gradient norm: 0.3767689781721064\n",
      "Iteration: 8790000, loss: 0.10635255541629422, gradient norm: 0.441382433731883\n",
      "Iteration: 8791000, loss: 0.10635242022636167, gradient norm: 0.16754318473808227\n",
      "Iteration: 8792000, loss: 0.10635221148324982, gradient norm: 0.3377133416963352\n",
      "Iteration: 8793000, loss: 0.10635193818673207, gradient norm: 0.09237490005494703\n",
      "Iteration: 8794000, loss: 0.10635195865837167, gradient norm: 0.00786143652824295\n",
      "Iteration: 8795000, loss: 0.10635170159976667, gradient norm: 0.5023328524621056\n",
      "Iteration: 8796000, loss: 0.10635150407139744, gradient norm: 0.4795722943504565\n",
      "Iteration: 8797000, loss: 0.10635144637138215, gradient norm: 0.04698073459178264\n",
      "Iteration: 8798000, loss: 0.10635117865462891, gradient norm: 0.877791915101039\n",
      "Iteration: 8799000, loss: 0.10635120929915798, gradient norm: 0.4967511118835757\n",
      "Iteration: 8800000, loss: 0.10635076193777389, gradient norm: 0.11735198371077189\n",
      "Iteration: 8801000, loss: 0.10635053021159553, gradient norm: 0.44903697210283083\n",
      "Iteration: 8802000, loss: 0.10635064611893551, gradient norm: 0.8957412357908555\n",
      "Iteration: 8803000, loss: 0.10635050414813148, gradient norm: 0.46049590539143653\n",
      "Iteration: 8804000, loss: 0.1063500994907722, gradient norm: 0.49972786909565886\n",
      "Iteration: 8805000, loss: 0.10634988305018865, gradient norm: 0.028309213912628767\n",
      "Iteration: 8806000, loss: 0.1063500902805648, gradient norm: 0.605127666137626\n",
      "Iteration: 8807000, loss: 0.10634942152954044, gradient norm: 0.16288210537066486\n",
      "Iteration: 8808000, loss: 0.10634954523185546, gradient norm: 0.4433038021082873\n",
      "Iteration: 8809000, loss: 0.106349273101929, gradient norm: 0.5601708327914872\n",
      "Iteration: 8810000, loss: 0.10634923608093465, gradient norm: 0.16863321665452238\n",
      "Iteration: 8811000, loss: 0.10634910092906295, gradient norm: 0.5981016594432712\n",
      "Iteration: 8812000, loss: 0.10634873634300009, gradient norm: 0.4482777568301007\n",
      "Iteration: 8813000, loss: 0.10634852631284969, gradient norm: 0.6504243354615942\n",
      "Iteration: 8814000, loss: 0.10634843895701526, gradient norm: 0.07772105511510957\n",
      "Iteration: 8815000, loss: 0.10634842232147904, gradient norm: 0.11626112190977406\n",
      "Iteration: 8816000, loss: 0.10634812380775957, gradient norm: 0.18896759874367028\n",
      "Iteration: 8817000, loss: 0.10634786326900253, gradient norm: 0.1256189911909789\n",
      "Iteration: 8818000, loss: 0.10634779361640873, gradient norm: 0.5588439632600204\n",
      "Iteration: 8819000, loss: 0.10634770810076541, gradient norm: 0.24690257068129018\n",
      "Iteration: 8820000, loss: 0.10634733983759598, gradient norm: 0.5190734978194205\n",
      "Iteration: 8821000, loss: 0.10634735555994083, gradient norm: 0.46857074177540636\n",
      "Iteration: 8822000, loss: 0.10634714977800053, gradient norm: 0.42579910596015697\n",
      "Iteration: 8823000, loss: 0.10634694483687564, gradient norm: 0.3038580917262139\n",
      "Iteration: 8824000, loss: 0.1063466898170063, gradient norm: 0.32865293625043557\n",
      "Iteration: 8825000, loss: 0.1063466143911298, gradient norm: 0.725776194725237\n",
      "Iteration: 8826000, loss: 0.10634652661456247, gradient norm: 0.03962559961142613\n",
      "Iteration: 8827000, loss: 0.10634607967445711, gradient norm: 0.22466907732126554\n",
      "Iteration: 8828000, loss: 0.10634603479286056, gradient norm: 0.1290619718587962\n",
      "Iteration: 8829000, loss: 0.10634584362358951, gradient norm: 0.23057094206979162\n",
      "Iteration: 8830000, loss: 0.10634585594669821, gradient norm: 0.021474131023941707\n",
      "Iteration: 8831000, loss: 0.10634554761306887, gradient norm: 0.13550667433886326\n",
      "Iteration: 8832000, loss: 0.10634555062512431, gradient norm: 0.2012161737640181\n",
      "Iteration: 8833000, loss: 0.10634506085956402, gradient norm: 0.3221441676572761\n",
      "Iteration: 8834000, loss: 0.10634499380210637, gradient norm: 0.6479825800739338\n",
      "Iteration: 8835000, loss: 0.10634509601464477, gradient norm: 0.0521421199655973\n",
      "Iteration: 8836000, loss: 0.10634453072883877, gradient norm: 0.6553945114421507\n",
      "Iteration: 8837000, loss: 0.10634459400574318, gradient norm: 0.3398926081904325\n",
      "Iteration: 8838000, loss: 0.10634434146571056, gradient norm: 0.26426075951560873\n",
      "Iteration: 8839000, loss: 0.10634416991528857, gradient norm: 0.25867516566995996\n",
      "Iteration: 8840000, loss: 0.10634418383859104, gradient norm: 0.12875845101999928\n",
      "Iteration: 8841000, loss: 0.10634377643350275, gradient norm: 0.2569830580600279\n",
      "Iteration: 8842000, loss: 0.10634375090249766, gradient norm: 0.47933029166751034\n",
      "Iteration: 8843000, loss: 0.10634361373733521, gradient norm: 0.5477508892602512\n",
      "Iteration: 8844000, loss: 0.10634330962066658, gradient norm: 0.09100401159551524\n",
      "Iteration: 8845000, loss: 0.10634312542576771, gradient norm: 0.05437711309114496\n",
      "Iteration: 8846000, loss: 0.10634312046644367, gradient norm: 0.1891299753258048\n",
      "Iteration: 8847000, loss: 0.10634267909908178, gradient norm: 0.1266823405768298\n",
      "Iteration: 8848000, loss: 0.10634273372403452, gradient norm: 0.12357163053206147\n",
      "Iteration: 8849000, loss: 0.10634262816155299, gradient norm: 0.2486081969613149\n",
      "Iteration: 8850000, loss: 0.10634219437749996, gradient norm: 0.5633180026790495\n",
      "Iteration: 8851000, loss: 0.10634225642116857, gradient norm: 0.10572772799301629\n",
      "Iteration: 8852000, loss: 0.10634191457470257, gradient norm: 0.18173196898637922\n",
      "Iteration: 8853000, loss: 0.10634179036998033, gradient norm: 0.20368155097690085\n",
      "Iteration: 8854000, loss: 0.10634180703769415, gradient norm: 0.0800373465270826\n",
      "Iteration: 8855000, loss: 0.10634145265212455, gradient norm: 0.7744293273973556\n",
      "Iteration: 8856000, loss: 0.10634130512765581, gradient norm: 0.6775811863840617\n",
      "Iteration: 8857000, loss: 0.1063410790330477, gradient norm: 0.5725857316726379\n",
      "Iteration: 8858000, loss: 0.1063409818658596, gradient norm: 0.05195577605268903\n",
      "Iteration: 8859000, loss: 0.1063407489612994, gradient norm: 0.39045993197378126\n",
      "Iteration: 8860000, loss: 0.10634070207161475, gradient norm: 0.14046918932188493\n",
      "Iteration: 8861000, loss: 0.10634056990902016, gradient norm: 0.16196364612393396\n",
      "Iteration: 8862000, loss: 0.1063402157586309, gradient norm: 0.2236601933087734\n",
      "Iteration: 8863000, loss: 0.10634029112094782, gradient norm: 0.43330371723151967\n",
      "Iteration: 8864000, loss: 0.10633972657278125, gradient norm: 0.1790677327145877\n",
      "Iteration: 8865000, loss: 0.10633999077468345, gradient norm: 0.3265778548521812\n",
      "Iteration: 8866000, loss: 0.10633956063052412, gradient norm: 0.14591381003386594\n",
      "Iteration: 8867000, loss: 0.1063394227600644, gradient norm: 0.4315157122196737\n",
      "Iteration: 8868000, loss: 0.10633933177158997, gradient norm: 0.04686911396467813\n",
      "Iteration: 8869000, loss: 0.10633903958120805, gradient norm: 0.3545578968594852\n",
      "Iteration: 8870000, loss: 0.1063388981373861, gradient norm: 0.2330208042942224\n",
      "Iteration: 8871000, loss: 0.10633892104250302, gradient norm: 0.11589850891010171\n",
      "Iteration: 8872000, loss: 0.1063385698301068, gradient norm: 0.18514786228198768\n",
      "Iteration: 8873000, loss: 0.10633834541285389, gradient norm: 0.5651418599965601\n",
      "Iteration: 8874000, loss: 0.10633833595108381, gradient norm: 0.3374262869706786\n",
      "Iteration: 8875000, loss: 0.10633806753315153, gradient norm: 0.2199778346379585\n",
      "Iteration: 8876000, loss: 0.10633793946066832, gradient norm: 0.10246763151128674\n",
      "Iteration: 8877000, loss: 0.1063379271541674, gradient norm: 0.539792219871589\n",
      "Iteration: 8878000, loss: 0.10633746462753932, gradient norm: 0.29392855688478814\n",
      "Iteration: 8879000, loss: 0.1063375208463831, gradient norm: 0.14016111686148827\n",
      "Iteration: 8880000, loss: 0.10633720525264803, gradient norm: 0.6696659417235211\n",
      "Iteration: 8881000, loss: 0.10633711025322609, gradient norm: 0.13141328712521513\n",
      "Iteration: 8882000, loss: 0.10633690433635837, gradient norm: 0.05636332762250843\n",
      "Iteration: 8883000, loss: 0.10633676729722155, gradient norm: 0.3572179133770324\n",
      "Iteration: 8884000, loss: 0.10633642761352495, gradient norm: 0.3698318400328073\n",
      "Iteration: 8885000, loss: 0.10633638885877543, gradient norm: 0.14386050705256231\n",
      "Iteration: 8886000, loss: 0.10633633668429701, gradient norm: 0.4504377695797686\n",
      "Iteration: 8887000, loss: 0.10633613925241954, gradient norm: 0.2838675565429112\n",
      "Iteration: 8888000, loss: 0.10633576180972472, gradient norm: 0.45471273249287253\n",
      "Iteration: 8889000, loss: 0.10633582137565023, gradient norm: 0.2788951309530854\n",
      "Iteration: 8890000, loss: 0.10633567316908202, gradient norm: 0.10665835325694327\n",
      "Iteration: 8891000, loss: 0.10633527440148156, gradient norm: 0.15045467621882197\n",
      "Iteration: 8892000, loss: 0.10633524322740828, gradient norm: 0.07359734540481351\n",
      "Iteration: 8893000, loss: 0.10633519630107002, gradient norm: 0.1020825025465858\n",
      "Iteration: 8894000, loss: 0.1063348928912223, gradient norm: 0.822380819810914\n",
      "Iteration: 8895000, loss: 0.10633458988330312, gradient norm: 0.15568917409210353\n",
      "Iteration: 8896000, loss: 0.1063347300965569, gradient norm: 0.028540077065126655\n",
      "Iteration: 8897000, loss: 0.1063341923157468, gradient norm: 0.12009762034169362\n",
      "Iteration: 8898000, loss: 0.10633429857014971, gradient norm: 0.41689573449488465\n",
      "Iteration: 8899000, loss: 0.10633407323027172, gradient norm: 0.12077884325557754\n",
      "Iteration: 8900000, loss: 0.1063339663728675, gradient norm: 0.5893678247567624\n",
      "Iteration: 8901000, loss: 0.10633358108018015, gradient norm: 0.6365008771708247\n",
      "Iteration: 8902000, loss: 0.10633356532593587, gradient norm: 0.11433164062819354\n",
      "Iteration: 8903000, loss: 0.10633348093021742, gradient norm: 0.13559250737302248\n",
      "Iteration: 8904000, loss: 0.10633310858704947, gradient norm: 0.7127805873692917\n",
      "Iteration: 8905000, loss: 0.10633305773265131, gradient norm: 0.8788202318166698\n",
      "Iteration: 8906000, loss: 0.1063328034775568, gradient norm: 0.1613774476031764\n",
      "Iteration: 8907000, loss: 0.10633274459304343, gradient norm: 0.18979884370156253\n",
      "Iteration: 8908000, loss: 0.10633251646469728, gradient norm: 0.2083532511725131\n",
      "Iteration: 8909000, loss: 0.10633247961770738, gradient norm: 0.2646117123746145\n",
      "Iteration: 8910000, loss: 0.1063322040952526, gradient norm: 0.619773496460967\n",
      "Iteration: 8911000, loss: 0.1063319864206132, gradient norm: 0.19170757195607463\n",
      "Iteration: 8912000, loss: 0.10633183673559689, gradient norm: 0.19581027630124548\n",
      "Iteration: 8913000, loss: 0.1063318695527891, gradient norm: 0.4476694839502024\n",
      "Iteration: 8914000, loss: 0.1063313505872551, gradient norm: 0.4435798724240228\n",
      "Iteration: 8915000, loss: 0.1063314064747602, gradient norm: 0.1726473431811583\n",
      "Iteration: 8916000, loss: 0.10633112039756441, gradient norm: 0.8846188403813369\n",
      "Iteration: 8917000, loss: 0.10633118754789359, gradient norm: 0.18744075628105653\n",
      "Iteration: 8918000, loss: 0.10633079510584276, gradient norm: 0.9437552277772675\n",
      "Iteration: 8919000, loss: 0.10633060704358578, gradient norm: 0.5990979400412809\n",
      "Iteration: 8920000, loss: 0.10633058524754464, gradient norm: 0.7055664711291705\n",
      "Iteration: 8921000, loss: 0.10633038353421059, gradient norm: 0.4122217530862545\n",
      "Iteration: 8922000, loss: 0.10633011881130928, gradient norm: 0.33253532428744087\n",
      "Iteration: 8923000, loss: 0.10632992947282259, gradient norm: 0.6352051462585889\n",
      "Iteration: 8924000, loss: 0.10632985973940154, gradient norm: 0.40845439721255294\n",
      "Iteration: 8925000, loss: 0.10632970324819897, gradient norm: 0.25308060713341207\n",
      "Iteration: 8926000, loss: 0.10632952490571067, gradient norm: 0.14678183220151061\n",
      "Iteration: 8927000, loss: 0.10632938521192788, gradient norm: 0.12467585651277818\n",
      "Iteration: 8928000, loss: 0.10632922413090842, gradient norm: 0.5633413262935705\n",
      "Iteration: 8929000, loss: 0.1063290035293828, gradient norm: 0.17630951239824458\n",
      "Iteration: 8930000, loss: 0.1063287769514217, gradient norm: 0.05939694434100329\n",
      "Iteration: 8931000, loss: 0.10632871935938708, gradient norm: 0.5216614445449471\n",
      "Iteration: 8932000, loss: 0.10632842322400002, gradient norm: 0.4269585819926312\n",
      "Iteration: 8933000, loss: 0.1063283945498078, gradient norm: 0.40609098190721904\n",
      "Iteration: 8934000, loss: 0.10632820228963069, gradient norm: 0.9574369644825398\n",
      "Iteration: 8935000, loss: 0.1063280691115982, gradient norm: 0.1679029804452278\n",
      "Iteration: 8936000, loss: 0.10632785406332353, gradient norm: 0.4419636449266\n",
      "Iteration: 8937000, loss: 0.10632763977910926, gradient norm: 0.6828287638265352\n",
      "Iteration: 8938000, loss: 0.10632757333423264, gradient norm: 0.5924957080451865\n",
      "Iteration: 8939000, loss: 0.10632732260158798, gradient norm: 0.13548540084096822\n",
      "Iteration: 8940000, loss: 0.10632717516532632, gradient norm: 0.7885780773513806\n",
      "Iteration: 8941000, loss: 0.10632703015842003, gradient norm: 0.5039596680244527\n",
      "Iteration: 8942000, loss: 0.10632673035892483, gradient norm: 0.1792063284099361\n",
      "Iteration: 8943000, loss: 0.10632678691354346, gradient norm: 0.37570583105737587\n",
      "Iteration: 8944000, loss: 0.10632642602724687, gradient norm: 0.4873003794655722\n",
      "Iteration: 8945000, loss: 0.10632641903418906, gradient norm: 0.24082176447446335\n",
      "Iteration: 8946000, loss: 0.106326110580487, gradient norm: 0.21302454858949552\n",
      "Iteration: 8947000, loss: 0.10632603092489654, gradient norm: 0.30682846890555876\n",
      "Iteration: 8948000, loss: 0.10632577716807928, gradient norm: 0.17117507004772312\n",
      "Iteration: 8949000, loss: 0.10632576674178686, gradient norm: 0.5312988881527431\n",
      "Iteration: 8950000, loss: 0.10632548674000566, gradient norm: 0.09694100815390701\n",
      "Iteration: 8951000, loss: 0.10632537193003418, gradient norm: 0.17149150114079129\n",
      "Iteration: 8952000, loss: 0.10632509711938917, gradient norm: 0.3995841187153543\n",
      "Iteration: 8953000, loss: 0.10632511844872934, gradient norm: 0.20917163904435077\n",
      "Iteration: 8954000, loss: 0.1063248592368345, gradient norm: 0.3617227193652263\n",
      "Iteration: 8955000, loss: 0.1063245385158206, gradient norm: 0.8050265421871379\n",
      "Iteration: 8956000, loss: 0.1063246317245474, gradient norm: 0.1601695487379633\n",
      "Iteration: 8957000, loss: 0.10632428971154047, gradient norm: 0.48791285919201044\n",
      "Iteration: 8958000, loss: 0.10632431087560602, gradient norm: 0.2547200833254266\n",
      "Iteration: 8959000, loss: 0.10632397971196568, gradient norm: 0.6183300725973505\n",
      "Iteration: 8960000, loss: 0.10632373197734041, gradient norm: 0.060065338986254914\n",
      "Iteration: 8961000, loss: 0.10632367308184067, gradient norm: 0.15537825749742293\n",
      "Iteration: 8962000, loss: 0.10632350083420308, gradient norm: 0.2904823483036114\n",
      "Iteration: 8963000, loss: 0.10632342508497294, gradient norm: 0.8023718173708418\n",
      "Iteration: 8964000, loss: 0.10632310978267603, gradient norm: 0.42303985085619855\n",
      "Iteration: 8965000, loss: 0.10632305431415436, gradient norm: 0.2123771759761594\n",
      "Iteration: 8966000, loss: 0.10632284206693388, gradient norm: 0.06439686104799285\n",
      "Iteration: 8967000, loss: 0.10632261434759496, gradient norm: 0.20852994590857146\n",
      "Iteration: 8968000, loss: 0.1063225063660782, gradient norm: 0.17372246022023582\n",
      "Iteration: 8969000, loss: 0.10632242907392421, gradient norm: 0.024637131947674543\n",
      "Iteration: 8970000, loss: 0.10632215964132954, gradient norm: 0.3857297518464691\n",
      "Iteration: 8971000, loss: 0.1063220493705985, gradient norm: 0.2507215442207544\n",
      "Iteration: 8972000, loss: 0.10632177499639527, gradient norm: 0.024081144800755832\n",
      "Iteration: 8973000, loss: 0.10632166973297912, gradient norm: 0.3323055478418842\n",
      "Iteration: 8974000, loss: 0.10632157577999739, gradient norm: 0.20353242794135168\n",
      "Iteration: 8975000, loss: 0.10632137147852357, gradient norm: 0.46603138537397526\n",
      "Iteration: 8976000, loss: 0.10632123474266358, gradient norm: 0.24662943028153764\n",
      "Iteration: 8977000, loss: 0.10632096426693824, gradient norm: 0.11553428999862751\n",
      "Iteration: 8978000, loss: 0.1063207905770764, gradient norm: 0.5186047106225782\n",
      "Iteration: 8979000, loss: 0.10632068705981047, gradient norm: 0.10706538156703914\n",
      "Iteration: 8980000, loss: 0.10632046757374272, gradient norm: 0.16759436733035804\n",
      "Iteration: 8981000, loss: 0.10632041389142416, gradient norm: 0.3094203284112209\n",
      "Iteration: 8982000, loss: 0.10632025413281213, gradient norm: 0.14694635136474504\n",
      "Iteration: 8983000, loss: 0.10631982316239105, gradient norm: 0.46821955314690383\n",
      "Iteration: 8984000, loss: 0.10632001587929094, gradient norm: 0.26487184778965894\n",
      "Iteration: 8985000, loss: 0.10631969352916058, gradient norm: 0.3684830034617993\n",
      "Iteration: 8986000, loss: 0.10631942821664955, gradient norm: 0.28372146940584203\n",
      "Iteration: 8987000, loss: 0.10631941577857901, gradient norm: 0.26084823060661005\n",
      "Iteration: 8988000, loss: 0.10631918154148225, gradient norm: 0.6001021106584317\n",
      "Iteration: 8989000, loss: 0.10631895191244906, gradient norm: 0.21795677543168843\n",
      "Iteration: 8990000, loss: 0.10631891605345176, gradient norm: 0.730005336121307\n",
      "Iteration: 8991000, loss: 0.1063188353151496, gradient norm: 0.04791922493022906\n",
      "Iteration: 8992000, loss: 0.10631836952355082, gradient norm: 0.08893536530395306\n",
      "Iteration: 8993000, loss: 0.1063184434269071, gradient norm: 0.171308250062419\n",
      "Iteration: 8994000, loss: 0.10631813922091443, gradient norm: 0.29951168498417396\n",
      "Iteration: 8995000, loss: 0.10631803566913142, gradient norm: 0.15320695117305608\n",
      "Iteration: 8996000, loss: 0.10631782588054005, gradient norm: 0.4348632853753843\n",
      "Iteration: 8997000, loss: 0.10631757648838805, gradient norm: 0.4079637864876913\n",
      "Iteration: 8998000, loss: 0.1063177413896297, gradient norm: 0.49492156569293183\n",
      "Iteration: 8999000, loss: 0.10631728036346769, gradient norm: 0.16783746312534614\n",
      "Iteration: 9000000, loss: 0.10631727264767162, gradient norm: 0.13949275679036788\n",
      "Iteration: 9001000, loss: 0.10631711481964376, gradient norm: 0.7094131609216707\n",
      "Iteration: 9002000, loss: 0.10631672068628578, gradient norm: 0.23998390186563373\n",
      "Iteration: 9003000, loss: 0.10631677573333016, gradient norm: 0.06964061787001923\n",
      "Iteration: 9004000, loss: 0.10631658987161015, gradient norm: 0.06904247384207386\n",
      "Iteration: 9005000, loss: 0.10631628968538397, gradient norm: 0.47514664138881935\n",
      "Iteration: 9006000, loss: 0.10631615844656712, gradient norm: 0.6961037921097047\n",
      "Iteration: 9007000, loss: 0.10631615893353362, gradient norm: 0.3369677058822859\n",
      "Iteration: 9008000, loss: 0.10631586097740839, gradient norm: 0.23281738396466978\n",
      "Iteration: 9009000, loss: 0.10631575661710792, gradient norm: 0.22248126219339068\n",
      "Iteration: 9010000, loss: 0.10631558474242528, gradient norm: 0.705434589834814\n",
      "Iteration: 9011000, loss: 0.1063153875501394, gradient norm: 0.6529682598037851\n",
      "Iteration: 9012000, loss: 0.10631513705993637, gradient norm: 0.3290337178800473\n",
      "Iteration: 9013000, loss: 0.10631496434044632, gradient norm: 0.30826667618102527\n",
      "Iteration: 9014000, loss: 0.10631504604192403, gradient norm: 0.4580220829453582\n",
      "Iteration: 9015000, loss: 0.10631466911838106, gradient norm: 0.159085983294429\n",
      "Iteration: 9016000, loss: 0.10631448636366113, gradient norm: 0.19046354870734564\n",
      "Iteration: 9017000, loss: 0.1063145177766918, gradient norm: 0.5108873273742568\n",
      "Iteration: 9018000, loss: 0.10631416656600794, gradient norm: 0.47202398687418\n",
      "Iteration: 9019000, loss: 0.10631410938877263, gradient norm: 0.32396888495102094\n",
      "Iteration: 9020000, loss: 0.10631389480709694, gradient norm: 0.03571970502739719\n",
      "Iteration: 9021000, loss: 0.10631376401375639, gradient norm: 0.6329232863522783\n",
      "Iteration: 9022000, loss: 0.10631355208555428, gradient norm: 0.02977983036686448\n",
      "Iteration: 9023000, loss: 0.10631350720264385, gradient norm: 0.25907155820076416\n",
      "Iteration: 9024000, loss: 0.10631316587950494, gradient norm: 0.8433613670715371\n",
      "Iteration: 9025000, loss: 0.10631307666963911, gradient norm: 0.40059814396786325\n",
      "Iteration: 9026000, loss: 0.10631286123663199, gradient norm: 0.1697842119233627\n",
      "Iteration: 9027000, loss: 0.10631273657315808, gradient norm: 0.355607413495485\n",
      "Iteration: 9028000, loss: 0.10631269791624427, gradient norm: 0.6356997536490971\n",
      "Iteration: 9029000, loss: 0.10631236581326166, gradient norm: 0.17502848313120895\n",
      "Iteration: 9030000, loss: 0.10631224537473312, gradient norm: 0.5520216938742957\n",
      "Iteration: 9031000, loss: 0.10631213367290912, gradient norm: 0.1663791408528687\n",
      "Iteration: 9032000, loss: 0.10631199637933864, gradient norm: 0.5590549741649052\n",
      "Iteration: 9033000, loss: 0.10631163760070537, gradient norm: 0.04121093519830113\n",
      "Iteration: 9034000, loss: 0.10631158672465682, gradient norm: 0.6069148393736105\n",
      "Iteration: 9035000, loss: 0.10631147194382959, gradient norm: 0.1504223152417879\n",
      "Iteration: 9036000, loss: 0.10631124999826634, gradient norm: 0.27046969169967305\n",
      "Iteration: 9037000, loss: 0.10631108773386484, gradient norm: 0.3104720780032\n",
      "Iteration: 9038000, loss: 0.10631097355007907, gradient norm: 0.4356075068233722\n",
      "Iteration: 9039000, loss: 0.10631088604911375, gradient norm: 0.11865687360195884\n",
      "Iteration: 9040000, loss: 0.10631061312949071, gradient norm: 0.1353316166631694\n",
      "Iteration: 9041000, loss: 0.10631036094319518, gradient norm: 0.48366715058528964\n",
      "Iteration: 9042000, loss: 0.10631024097163952, gradient norm: 0.3496309340621147\n",
      "Iteration: 9043000, loss: 0.1063102930332579, gradient norm: 0.2989519983667678\n",
      "Iteration: 9044000, loss: 0.10630980421761166, gradient norm: 0.20753701556817925\n",
      "Iteration: 9045000, loss: 0.10630989519903483, gradient norm: 0.2963704426204176\n",
      "Iteration: 9046000, loss: 0.10630972604445711, gradient norm: 0.0959179233437491\n",
      "Iteration: 9047000, loss: 0.10630940049460377, gradient norm: 0.15813295071564695\n",
      "Iteration: 9048000, loss: 0.10630927339731729, gradient norm: 0.13837915833989434\n",
      "Iteration: 9049000, loss: 0.10630910897407517, gradient norm: 0.5408675545008772\n",
      "Iteration: 9050000, loss: 0.10630885566208263, gradient norm: 0.36501816245752267\n",
      "Iteration: 9051000, loss: 0.10630890831773411, gradient norm: 0.48154358363942434\n",
      "Iteration: 9052000, loss: 0.10630858652007533, gradient norm: 0.3989597342038653\n",
      "Iteration: 9053000, loss: 0.10630853097961877, gradient norm: 0.46613117220377615\n",
      "Iteration: 9054000, loss: 0.10630837533025352, gradient norm: 0.8724953668246938\n",
      "Iteration: 9055000, loss: 0.10630804273430239, gradient norm: 0.1702554532079347\n",
      "Iteration: 9056000, loss: 0.10630799167869945, gradient norm: 0.1283600881312714\n",
      "Iteration: 9057000, loss: 0.10630788603032486, gradient norm: 0.09631948681648861\n",
      "Iteration: 9058000, loss: 0.10630772934094312, gradient norm: 0.08747406721985942\n",
      "Iteration: 9059000, loss: 0.10630746647998698, gradient norm: 0.14312055843875762\n",
      "Iteration: 9060000, loss: 0.10630741305029882, gradient norm: 0.46817495349445\n",
      "Iteration: 9061000, loss: 0.10630701629227875, gradient norm: 0.33995519481975206\n",
      "Iteration: 9062000, loss: 0.10630707254517961, gradient norm: 0.5235129642831922\n",
      "Iteration: 9063000, loss: 0.1063069645265198, gradient norm: 0.4544648332230654\n",
      "Iteration: 9064000, loss: 0.10630662061836535, gradient norm: 0.25030564792133203\n",
      "Iteration: 9065000, loss: 0.1063065653673963, gradient norm: 0.359699160495647\n",
      "Iteration: 9066000, loss: 0.10630635626390042, gradient norm: 0.48148566048384983\n",
      "Iteration: 9067000, loss: 0.10630609627301608, gradient norm: 0.1971066165788286\n",
      "Iteration: 9068000, loss: 0.10630606578289332, gradient norm: 0.053588091672098395\n",
      "Iteration: 9069000, loss: 0.10630596294300855, gradient norm: 0.2493548064911844\n",
      "Iteration: 9070000, loss: 0.10630560726987659, gradient norm: 0.5324087820170478\n",
      "Iteration: 9071000, loss: 0.10630564913306578, gradient norm: 0.4017106621919742\n",
      "Iteration: 9072000, loss: 0.10630525940720938, gradient norm: 0.40748345562574617\n",
      "Iteration: 9073000, loss: 0.10630524389018903, gradient norm: 0.2255654687125926\n",
      "Iteration: 9074000, loss: 0.1063051404217672, gradient norm: 0.14011663708128855\n",
      "Iteration: 9075000, loss: 0.10630488160887122, gradient norm: 0.35012538938705545\n",
      "Iteration: 9076000, loss: 0.10630457233552613, gradient norm: 0.20532705822277403\n",
      "Iteration: 9077000, loss: 0.10630465430102629, gradient norm: 0.41831772041417564\n",
      "Iteration: 9078000, loss: 0.10630439524700089, gradient norm: 0.2088376142495792\n",
      "Iteration: 9079000, loss: 0.10630416218652292, gradient norm: 0.5304922307935714\n",
      "Iteration: 9080000, loss: 0.10630417653335983, gradient norm: 0.2935313717706928\n",
      "Iteration: 9081000, loss: 0.1063039160868468, gradient norm: 0.11675464581134447\n",
      "Iteration: 9082000, loss: 0.10630369011537602, gradient norm: 0.3828464660466853\n",
      "Iteration: 9083000, loss: 0.10630369058440235, gradient norm: 0.752769571659706\n",
      "Iteration: 9084000, loss: 0.10630329742255719, gradient norm: 0.07665774089957299\n",
      "Iteration: 9085000, loss: 0.10630340949109122, gradient norm: 0.054688569704257596\n",
      "Iteration: 9086000, loss: 0.10630296991257426, gradient norm: 0.1294259854319564\n",
      "Iteration: 9087000, loss: 0.1063029451261549, gradient norm: 0.11108804401074533\n",
      "Iteration: 9088000, loss: 0.10630257040470027, gradient norm: 0.2708571322522825\n",
      "Iteration: 9089000, loss: 0.10630288820106477, gradient norm: 0.4553647324509203\n",
      "Iteration: 9090000, loss: 0.1063023646595856, gradient norm: 0.1500546845185269\n",
      "Iteration: 9091000, loss: 0.10630221188268557, gradient norm: 0.4815102936574336\n",
      "Iteration: 9092000, loss: 0.10630225701850196, gradient norm: 0.3911142086544206\n",
      "Iteration: 9093000, loss: 0.10630201256091625, gradient norm: 0.017801030051161373\n",
      "Iteration: 9094000, loss: 0.10630164903374055, gradient norm: 0.6622793352377604\n",
      "Iteration: 9095000, loss: 0.10630161534263535, gradient norm: 0.4131057650157087\n",
      "Iteration: 9096000, loss: 0.10630143032206874, gradient norm: 0.0897477768498573\n",
      "Iteration: 9097000, loss: 0.10630139621598403, gradient norm: 0.4601290279785763\n",
      "Iteration: 9098000, loss: 0.10630116817332115, gradient norm: 0.2212301936205934\n",
      "Iteration: 9099000, loss: 0.10630088813853346, gradient norm: 0.35953019871732256\n",
      "Iteration: 9100000, loss: 0.10630078538438194, gradient norm: 0.06740999577634799\n",
      "Iteration: 9101000, loss: 0.10630075091185938, gradient norm: 0.2598239961549634\n",
      "Iteration: 9102000, loss: 0.1063004744993671, gradient norm: 0.3493973744662553\n",
      "Iteration: 9103000, loss: 0.10630044691559176, gradient norm: 0.08110721757014708\n",
      "Iteration: 9104000, loss: 0.106300043183151, gradient norm: 0.037831316722370584\n",
      "Iteration: 9105000, loss: 0.10630002800516826, gradient norm: 0.0924219950314329\n",
      "Iteration: 9106000, loss: 0.1062997659797235, gradient norm: 0.15941846274525312\n",
      "Iteration: 9107000, loss: 0.1062998756297336, gradient norm: 0.8427532527411997\n",
      "Iteration: 9108000, loss: 0.10629961532004915, gradient norm: 0.32783826520444825\n",
      "Iteration: 9109000, loss: 0.10629923162875597, gradient norm: 0.331307389353932\n",
      "Iteration: 9110000, loss: 0.10629913446915512, gradient norm: 0.08454019758878575\n",
      "Iteration: 9111000, loss: 0.106299000349179, gradient norm: 0.35691383515702146\n",
      "Iteration: 9112000, loss: 0.10629906261123949, gradient norm: 0.6827781935664019\n",
      "Iteration: 9113000, loss: 0.10629871414880047, gradient norm: 0.9215560908436066\n",
      "Iteration: 9114000, loss: 0.1062984840786994, gradient norm: 0.3559504771114703\n",
      "Iteration: 9115000, loss: 0.10629830506519101, gradient norm: 0.10308140121979144\n",
      "Iteration: 9116000, loss: 0.10629818768605702, gradient norm: 0.6758737359663333\n",
      "Iteration: 9117000, loss: 0.1062982395181609, gradient norm: 0.18511056699349643\n",
      "Iteration: 9118000, loss: 0.10629775788838376, gradient norm: 0.27775661590454026\n",
      "Iteration: 9119000, loss: 0.1062978635057165, gradient norm: 0.3307061859097906\n",
      "Iteration: 9120000, loss: 0.10629771465486673, gradient norm: 0.6019940108064966\n",
      "Iteration: 9121000, loss: 0.10629723312224404, gradient norm: 0.14310193043286087\n",
      "Iteration: 9122000, loss: 0.10629723164386749, gradient norm: 0.34668438351622427\n",
      "Iteration: 9123000, loss: 0.10629709707435202, gradient norm: 0.2344418345326742\n",
      "Iteration: 9124000, loss: 0.10629687159357212, gradient norm: 0.8269598281271178\n",
      "Iteration: 9125000, loss: 0.10629689289961249, gradient norm: 0.5827853105014839\n",
      "Iteration: 9126000, loss: 0.10629656126110203, gradient norm: 0.14405816254898007\n",
      "Iteration: 9127000, loss: 0.10629652831986561, gradient norm: 0.9678517435424543\n",
      "Iteration: 9128000, loss: 0.1062961459601721, gradient norm: 0.06840897118958937\n",
      "Iteration: 9129000, loss: 0.10629608846064592, gradient norm: 0.2345493605374527\n",
      "Iteration: 9130000, loss: 0.10629608798367661, gradient norm: 0.3948247539191355\n",
      "Iteration: 9131000, loss: 0.10629578988600558, gradient norm: 0.10937822672941351\n",
      "Iteration: 9132000, loss: 0.1062955478302022, gradient norm: 0.48817906583836873\n",
      "Iteration: 9133000, loss: 0.10629558934849746, gradient norm: 0.36725337643006767\n",
      "Iteration: 9134000, loss: 0.1062951464149319, gradient norm: 0.571073593371629\n",
      "Iteration: 9135000, loss: 0.1062952587608779, gradient norm: 0.52569044884472\n",
      "Iteration: 9136000, loss: 0.10629494229878524, gradient norm: 0.36265229921016723\n",
      "Iteration: 9137000, loss: 0.10629484335732159, gradient norm: 0.059324841427401825\n",
      "Iteration: 9138000, loss: 0.10629470044945907, gradient norm: 0.13712022879888497\n",
      "Iteration: 9139000, loss: 0.10629453948868565, gradient norm: 1.0521347991200154\n",
      "Iteration: 9140000, loss: 0.10629429484527382, gradient norm: 0.35953331656424786\n",
      "Iteration: 9141000, loss: 0.10629422700921998, gradient norm: 0.053146966859187184\n",
      "Iteration: 9142000, loss: 0.10629407083126195, gradient norm: 0.45822100619092127\n",
      "Iteration: 9143000, loss: 0.10629384162476804, gradient norm: 0.24945060142399542\n",
      "Iteration: 9144000, loss: 0.1062937483099115, gradient norm: 0.45045413518470345\n",
      "Iteration: 9145000, loss: 0.10629365023115497, gradient norm: 0.1211464092570137\n",
      "Iteration: 9146000, loss: 0.1062931789077449, gradient norm: 0.12112509722171338\n",
      "Iteration: 9147000, loss: 0.1062932224510273, gradient norm: 0.45383338231915166\n",
      "Iteration: 9148000, loss: 0.10629302966595663, gradient norm: 0.09199364738022928\n",
      "Iteration: 9149000, loss: 0.10629289778271696, gradient norm: 0.17796173836511484\n",
      "Iteration: 9150000, loss: 0.10629278317179461, gradient norm: 0.2926485044955253\n",
      "Iteration: 9151000, loss: 0.10629251886846067, gradient norm: 0.3019190124181597\n",
      "Iteration: 9152000, loss: 0.10629241060632146, gradient norm: 0.07164584475024116\n",
      "Iteration: 9153000, loss: 0.10629222908046608, gradient norm: 0.3636749864839592\n",
      "Iteration: 9154000, loss: 0.10629205515339848, gradient norm: 0.1643684600970099\n",
      "Iteration: 9155000, loss: 0.10629196766916346, gradient norm: 0.18183832946726525\n",
      "Iteration: 9156000, loss: 0.10629168087733175, gradient norm: 0.2310879831456896\n",
      "Iteration: 9157000, loss: 0.10629172832972943, gradient norm: 0.9400916131558867\n",
      "Iteration: 9158000, loss: 0.10629132359825112, gradient norm: 0.1601199515267906\n",
      "Iteration: 9159000, loss: 0.10629143770265853, gradient norm: 0.34453884282444097\n",
      "Iteration: 9160000, loss: 0.10629106443617371, gradient norm: 0.44131432560179623\n",
      "Iteration: 9161000, loss: 0.1062909531372613, gradient norm: 0.18765837434822263\n",
      "Iteration: 9162000, loss: 0.10629085514333356, gradient norm: 0.5579529889527579\n",
      "Iteration: 9163000, loss: 0.1062905513063102, gradient norm: 0.337921387910103\n",
      "Iteration: 9164000, loss: 0.10629044567139917, gradient norm: 0.4644804408043979\n",
      "Iteration: 9165000, loss: 0.10629027342575821, gradient norm: 0.1472657714904733\n",
      "Iteration: 9166000, loss: 0.10629030486636604, gradient norm: 0.09389370977260235\n",
      "Iteration: 9167000, loss: 0.10628985996687286, gradient norm: 0.06996598076765799\n",
      "Iteration: 9168000, loss: 0.10628985075764057, gradient norm: 0.3928594697307366\n",
      "Iteration: 9169000, loss: 0.10628960400187173, gradient norm: 0.11365963262711218\n",
      "Iteration: 9170000, loss: 0.10628957499117153, gradient norm: 0.3114853745274089\n",
      "Iteration: 9171000, loss: 0.10628927590054615, gradient norm: 0.19250515689113096\n",
      "Iteration: 9172000, loss: 0.10628919089023416, gradient norm: 0.28912346892237795\n",
      "Iteration: 9173000, loss: 0.10628910863952543, gradient norm: 0.23044330043012212\n",
      "Iteration: 9174000, loss: 0.10628901737857771, gradient norm: 0.3512444739115584\n",
      "Iteration: 9175000, loss: 0.10628849770996203, gradient norm: 0.1054292349281817\n",
      "Iteration: 9176000, loss: 0.10628855785879805, gradient norm: 0.2687042514708424\n",
      "Iteration: 9177000, loss: 0.10628851350696007, gradient norm: 0.4576036183180133\n",
      "Iteration: 9178000, loss: 0.10628817079874509, gradient norm: 0.5113005459965639\n",
      "Iteration: 9179000, loss: 0.10628794486898545, gradient norm: 0.4860015362718701\n",
      "Iteration: 9180000, loss: 0.10628797935638024, gradient norm: 0.3850048353451019\n",
      "Iteration: 9181000, loss: 0.10628768027884246, gradient norm: 0.557532664764487\n",
      "Iteration: 9182000, loss: 0.10628760593473147, gradient norm: 0.06115079393229111\n",
      "Iteration: 9183000, loss: 0.10628756468749478, gradient norm: 0.3355663682892232\n",
      "Iteration: 9184000, loss: 0.1062871141786327, gradient norm: 0.20486786365689524\n",
      "Iteration: 9185000, loss: 0.10628720764497855, gradient norm: 0.37503365983615844\n",
      "Iteration: 9186000, loss: 0.10628692017354523, gradient norm: 0.18186811796197971\n",
      "Iteration: 9187000, loss: 0.10628682456689857, gradient norm: 0.14974332715720212\n",
      "Iteration: 9188000, loss: 0.10628660655585999, gradient norm: 0.22460929763912113\n",
      "Iteration: 9189000, loss: 0.10628640957444266, gradient norm: 0.022807517460626814\n",
      "Iteration: 9190000, loss: 0.10628641135360163, gradient norm: 0.058105226302592364\n",
      "Iteration: 9191000, loss: 0.10628604915306467, gradient norm: 0.27311583545075396\n",
      "Iteration: 9192000, loss: 0.10628604943512562, gradient norm: 0.9006448102677316\n",
      "Iteration: 9193000, loss: 0.1062857711667914, gradient norm: 0.15329347661137457\n",
      "Iteration: 9194000, loss: 0.10628566864580981, gradient norm: 0.4499924456839648\n",
      "Iteration: 9195000, loss: 0.10628561942512556, gradient norm: 0.1858032864839863\n",
      "Iteration: 9196000, loss: 0.10628543693425965, gradient norm: 0.5169596632309886\n",
      "Iteration: 9197000, loss: 0.10628510677747507, gradient norm: 0.3855984121021783\n",
      "Iteration: 9198000, loss: 0.106285000851816, gradient norm: 0.04554164392079378\n",
      "Iteration: 9199000, loss: 0.10628484797522207, gradient norm: 0.06555583776561996\n",
      "Iteration: 9200000, loss: 0.10628474055013241, gradient norm: 0.34495133253219057\n",
      "Iteration: 9201000, loss: 0.1062844453311339, gradient norm: 0.09406374302052078\n",
      "Iteration: 9202000, loss: 0.10628448245386919, gradient norm: 0.31470466554143806\n",
      "Iteration: 9203000, loss: 0.10628419251268852, gradient norm: 0.17155236364983312\n",
      "Iteration: 9204000, loss: 0.10628409845101666, gradient norm: 0.12222166993725352\n",
      "Iteration: 9205000, loss: 0.10628383851127887, gradient norm: 0.6147971022163079\n",
      "Iteration: 9206000, loss: 0.10628373955484983, gradient norm: 0.33535876003766135\n",
      "Iteration: 9207000, loss: 0.10628372686972862, gradient norm: 0.09164011415171314\n",
      "Iteration: 9208000, loss: 0.10628344160521762, gradient norm: 0.37330573813489687\n",
      "Iteration: 9209000, loss: 0.10628316882537905, gradient norm: 0.48724009299626403\n",
      "Iteration: 9210000, loss: 0.10628307783733706, gradient norm: 0.10455790260566962\n",
      "Iteration: 9211000, loss: 0.10628293440573577, gradient norm: 0.06387192503363018\n",
      "Iteration: 9212000, loss: 0.10628282832395908, gradient norm: 0.5794106368259778\n",
      "Iteration: 9213000, loss: 0.10628253177614257, gradient norm: 0.37028345656672546\n",
      "Iteration: 9214000, loss: 0.10628259008457723, gradient norm: 0.17793643623456468\n",
      "Iteration: 9215000, loss: 0.1062823020203839, gradient norm: 0.6873089881328166\n",
      "Iteration: 9216000, loss: 0.10628205231585035, gradient norm: 0.44682399912964255\n",
      "Iteration: 9217000, loss: 0.1062820042376348, gradient norm: 0.300793986510697\n",
      "Iteration: 9218000, loss: 0.10628181921135377, gradient norm: 0.19030359490558518\n",
      "Iteration: 9219000, loss: 0.10628173008367728, gradient norm: 0.16015597507670512\n",
      "Iteration: 9220000, loss: 0.10628147788570941, gradient norm: 0.4666387630394095\n",
      "Iteration: 9221000, loss: 0.10628142490491979, gradient norm: 0.20595090897959284\n",
      "Iteration: 9222000, loss: 0.10628116506118655, gradient norm: 0.11061045354902906\n",
      "Iteration: 9223000, loss: 0.10628091328220532, gradient norm: 0.15685310979946723\n",
      "Iteration: 9224000, loss: 0.10628103831291637, gradient norm: 0.44455801880464685\n",
      "Iteration: 9225000, loss: 0.1062806136624603, gradient norm: 0.12078335176461402\n",
      "Iteration: 9226000, loss: 0.10628070676606959, gradient norm: 0.666423099990937\n",
      "Iteration: 9227000, loss: 0.1062804387158038, gradient norm: 0.1893244321397353\n",
      "Iteration: 9228000, loss: 0.10628009052223263, gradient norm: 0.4471034859747317\n",
      "Iteration: 9229000, loss: 0.10628008020597608, gradient norm: 0.7486137382389724\n",
      "Iteration: 9230000, loss: 0.1062798492841188, gradient norm: 0.7828369065144984\n",
      "Iteration: 9231000, loss: 0.10627981353086605, gradient norm: 0.34964470227844724\n",
      "Iteration: 9232000, loss: 0.10627960895317705, gradient norm: 0.49595752227649736\n",
      "Iteration: 9233000, loss: 0.10627951437686343, gradient norm: 0.07033693413379252\n",
      "Iteration: 9234000, loss: 0.10627929032972543, gradient norm: 0.2505260773417322\n",
      "Iteration: 9235000, loss: 0.10627903777162936, gradient norm: 0.15238090067475002\n",
      "Iteration: 9236000, loss: 0.10627893408952538, gradient norm: 0.23821621600818876\n",
      "Iteration: 9237000, loss: 0.10627884168336849, gradient norm: 0.07701612393641982\n",
      "Iteration: 9238000, loss: 0.10627863545367214, gradient norm: 0.30541404516537624\n",
      "Iteration: 9239000, loss: 0.10627856388669342, gradient norm: 0.31754985014348686\n",
      "Iteration: 9240000, loss: 0.10627818945884045, gradient norm: 0.5013529183417559\n",
      "Iteration: 9241000, loss: 0.10627823536826103, gradient norm: 0.5775850220388526\n",
      "Iteration: 9242000, loss: 0.10627811627227428, gradient norm: 0.756917011669938\n",
      "Iteration: 9243000, loss: 0.10627780362255206, gradient norm: 0.14751377454974737\n",
      "Iteration: 9244000, loss: 0.10627762807132105, gradient norm: 0.566966937517568\n",
      "Iteration: 9245000, loss: 0.1062776545179084, gradient norm: 0.34319458921109974\n",
      "Iteration: 9246000, loss: 0.10627728161852525, gradient norm: 0.075785183394841\n",
      "Iteration: 9247000, loss: 0.10627728893227419, gradient norm: 0.14327256747073389\n",
      "Iteration: 9248000, loss: 0.10627708639732694, gradient norm: 0.35875793418154767\n",
      "Iteration: 9249000, loss: 0.1062768397170174, gradient norm: 0.2457106017876136\n",
      "Iteration: 9250000, loss: 0.10627677612381069, gradient norm: 0.18603512329591745\n",
      "Iteration: 9251000, loss: 0.10627658758945614, gradient norm: 0.19258314791977607\n",
      "Iteration: 9252000, loss: 0.10627648886443314, gradient norm: 0.14250311290724113\n",
      "Iteration: 9253000, loss: 0.10627630433890167, gradient norm: 0.24905986793659055\n",
      "Iteration: 9254000, loss: 0.1062759613864586, gradient norm: 0.09762295033106964\n",
      "Iteration: 9255000, loss: 0.10627614097791355, gradient norm: 0.1626008744865817\n",
      "Iteration: 9256000, loss: 0.10627566440530048, gradient norm: 0.2639860832559208\n",
      "Iteration: 9257000, loss: 0.10627579131821525, gradient norm: 0.37376317548803584\n",
      "Iteration: 9258000, loss: 0.10627552185749908, gradient norm: 0.16680841467129434\n",
      "Iteration: 9259000, loss: 0.10627530132510797, gradient norm: 0.18920748254562242\n",
      "Iteration: 9260000, loss: 0.10627521787514628, gradient norm: 0.1409959416573979\n",
      "Iteration: 9261000, loss: 0.10627492454607745, gradient norm: 0.7440829987490312\n",
      "Iteration: 9262000, loss: 0.10627488653207376, gradient norm: 0.3661028123454109\n",
      "Iteration: 9263000, loss: 0.10627470275980477, gradient norm: 0.23653808108034757\n",
      "Iteration: 9264000, loss: 0.10627458102455072, gradient norm: 0.12605318174136507\n",
      "Iteration: 9265000, loss: 0.10627418517278181, gradient norm: 0.4620779196221406\n",
      "Iteration: 9266000, loss: 0.10627430313054086, gradient norm: 0.19721303493503126\n",
      "Iteration: 9267000, loss: 0.10627397924320746, gradient norm: 0.06015284730096148\n",
      "Iteration: 9268000, loss: 0.10627392556606549, gradient norm: 0.5623306321885332\n",
      "Iteration: 9269000, loss: 0.10627374022049847, gradient norm: 0.23368695254500343\n",
      "Iteration: 9270000, loss: 0.10627359080317554, gradient norm: 0.8892165748950386\n",
      "Iteration: 9271000, loss: 0.10627341986258261, gradient norm: 0.4204928419488235\n",
      "Iteration: 9272000, loss: 0.1062732507647278, gradient norm: 0.3293497009248285\n",
      "Iteration: 9273000, loss: 0.10627328830218372, gradient norm: 0.2175722278527241\n",
      "Iteration: 9274000, loss: 0.10627299069149636, gradient norm: 0.21319802447799416\n",
      "Iteration: 9275000, loss: 0.10627262465534386, gradient norm: 0.20884283125095104\n",
      "Iteration: 9276000, loss: 0.10627261742194094, gradient norm: 0.2595534660800184\n",
      "Iteration: 9277000, loss: 0.10627254856864274, gradient norm: 0.7854349658925539\n",
      "Iteration: 9278000, loss: 0.10627239417610707, gradient norm: 0.0899872184296859\n",
      "Iteration: 9279000, loss: 0.10627211283901664, gradient norm: 0.13445945079978888\n",
      "Iteration: 9280000, loss: 0.10627198542681089, gradient norm: 0.3458805344092211\n",
      "Iteration: 9281000, loss: 0.10627193259949996, gradient norm: 0.29226596002845945\n",
      "Iteration: 9282000, loss: 0.10627171769546451, gradient norm: 0.2234638963250191\n",
      "Iteration: 9283000, loss: 0.10627152192539091, gradient norm: 0.5075502739918445\n",
      "Iteration: 9284000, loss: 0.10627135949875349, gradient norm: 0.01532241115382973\n",
      "Iteration: 9285000, loss: 0.10627125973719762, gradient norm: 0.17796304717107797\n",
      "Iteration: 9286000, loss: 0.10627103995543842, gradient norm: 0.2906314677465274\n",
      "Iteration: 9287000, loss: 0.1062710552158115, gradient norm: 0.3970276049359605\n",
      "Iteration: 9288000, loss: 0.10627071292755295, gradient norm: 0.5079256967998851\n",
      "Iteration: 9289000, loss: 0.1062705699109591, gradient norm: 0.3019310156226493\n",
      "Iteration: 9290000, loss: 0.10627031913729977, gradient norm: 0.1802720449999505\n",
      "Iteration: 9291000, loss: 0.10627020213061089, gradient norm: 0.26109380524814024\n",
      "Iteration: 9292000, loss: 0.10627018412149622, gradient norm: 0.08596663208045627\n",
      "Iteration: 9293000, loss: 0.1062699588629907, gradient norm: 0.10138759562858875\n",
      "Iteration: 9294000, loss: 0.10626994655578263, gradient norm: 0.36597654435496213\n",
      "Iteration: 9295000, loss: 0.1062696096273398, gradient norm: 0.12805702215436648\n",
      "Iteration: 9296000, loss: 0.10626945699999253, gradient norm: 0.27520839034085826\n",
      "Iteration: 9297000, loss: 0.10626928618647864, gradient norm: 0.3201966760423475\n",
      "Iteration: 9298000, loss: 0.1062691363903389, gradient norm: 0.8180618736600827\n",
      "Iteration: 9299000, loss: 0.10626898314207583, gradient norm: 0.06403568609238754\n",
      "Iteration: 9300000, loss: 0.10626896102797688, gradient norm: 0.7407917942274655\n",
      "Iteration: 9301000, loss: 0.10626872591240068, gradient norm: 0.269346310983316\n",
      "Iteration: 9302000, loss: 0.10626851102773983, gradient norm: 0.15500792085917167\n",
      "Iteration: 9303000, loss: 0.10626832560216964, gradient norm: 0.40188112102821466\n",
      "Iteration: 9304000, loss: 0.10626840256227309, gradient norm: 0.21199615000520045\n",
      "Iteration: 9305000, loss: 0.10626797982943353, gradient norm: 0.08082301191728938\n",
      "Iteration: 9306000, loss: 0.10626801304498469, gradient norm: 0.6263795695556187\n",
      "Iteration: 9307000, loss: 0.10626761561367508, gradient norm: 0.0738569604365159\n",
      "Iteration: 9308000, loss: 0.10626772135354848, gradient norm: 0.4183606638088223\n",
      "Iteration: 9309000, loss: 0.10626739295517817, gradient norm: 0.7696366644559312\n",
      "Iteration: 9310000, loss: 0.10626733136860468, gradient norm: 0.28743459102993235\n",
      "Iteration: 9311000, loss: 0.10626700003035555, gradient norm: 0.3353885485080455\n",
      "Iteration: 9312000, loss: 0.10626707825886, gradient norm: 0.12188388910949251\n",
      "Iteration: 9313000, loss: 0.1062668515143208, gradient norm: 0.28694772817360187\n",
      "Iteration: 9314000, loss: 0.10626656454392534, gradient norm: 0.19986977290902694\n",
      "Iteration: 9315000, loss: 0.10626652543574988, gradient norm: 0.19562982981698668\n",
      "Iteration: 9316000, loss: 0.10626653265383072, gradient norm: 0.31548765081922664\n",
      "Iteration: 9317000, loss: 0.10626601600774667, gradient norm: 0.6171438033789215\n",
      "Iteration: 9318000, loss: 0.10626610297443916, gradient norm: 0.3106140547840851\n",
      "Iteration: 9319000, loss: 0.10626587773385258, gradient norm: 0.7337312629544197\n",
      "Iteration: 9320000, loss: 0.10626563046915015, gradient norm: 0.10036037054368463\n",
      "Iteration: 9321000, loss: 0.10626563147019348, gradient norm: 0.5554994303934456\n",
      "Iteration: 9322000, loss: 0.10626547015836543, gradient norm: 0.5218983120761472\n",
      "Iteration: 9323000, loss: 0.10626522028974246, gradient norm: 0.6908077890098002\n",
      "Iteration: 9324000, loss: 0.1062653693103507, gradient norm: 0.28814149645973985\n",
      "Iteration: 9325000, loss: 0.10626474267758025, gradient norm: 0.28873075494966427\n",
      "Iteration: 9326000, loss: 0.10626464615474947, gradient norm: 0.422441182927819\n",
      "Iteration: 9327000, loss: 0.10626489418753368, gradient norm: 0.25545567083111487\n",
      "Iteration: 9328000, loss: 0.10626430418105175, gradient norm: 0.061331184857394985\n",
      "Iteration: 9329000, loss: 0.10626442326391206, gradient norm: 0.3931872640482292\n",
      "Iteration: 9330000, loss: 0.10626413363968291, gradient norm: 0.6077075136955576\n",
      "Iteration: 9331000, loss: 0.10626396086296319, gradient norm: 0.1669487960882428\n",
      "Iteration: 9332000, loss: 0.10626390518374583, gradient norm: 0.2405873655255104\n",
      "Iteration: 9333000, loss: 0.10626363891994585, gradient norm: 0.18964764414780286\n",
      "Iteration: 9334000, loss: 0.10626347475969562, gradient norm: 0.5073419854673079\n",
      "Iteration: 9335000, loss: 0.10626340395724869, gradient norm: 0.06925182577079882\n",
      "Iteration: 9336000, loss: 0.10626313345680512, gradient norm: 0.5181840965834119\n",
      "Iteration: 9337000, loss: 0.10626317538740854, gradient norm: 0.14148580177120557\n",
      "Iteration: 9338000, loss: 0.1062628753310661, gradient norm: 0.3887523606082645\n",
      "Iteration: 9339000, loss: 0.10626271423371139, gradient norm: 0.31122106712139996\n",
      "Iteration: 9340000, loss: 0.10626265074793284, gradient norm: 0.4760415341918962\n",
      "Iteration: 9341000, loss: 0.10626237997080502, gradient norm: 0.3562378805783743\n",
      "Iteration: 9342000, loss: 0.10626236485131983, gradient norm: 0.16657423626580983\n",
      "Iteration: 9343000, loss: 0.10626210786602923, gradient norm: 0.7193798324165943\n",
      "Iteration: 9344000, loss: 0.1062618912588865, gradient norm: 0.4040429535601171\n",
      "Iteration: 9345000, loss: 0.10626184041797447, gradient norm: 0.03235608255179978\n",
      "Iteration: 9346000, loss: 0.106261589593005, gradient norm: 0.2121078264720623\n",
      "Iteration: 9347000, loss: 0.10626153633552245, gradient norm: 0.473204097208712\n",
      "Iteration: 9348000, loss: 0.10626144919996097, gradient norm: 0.06813476265188527\n",
      "Iteration: 9349000, loss: 0.10626119292132052, gradient norm: 0.4639538727010962\n",
      "Iteration: 9350000, loss: 0.10626086234971532, gradient norm: 0.3394830539563564\n",
      "Iteration: 9351000, loss: 0.10626095171572342, gradient norm: 0.4025184014392179\n",
      "Iteration: 9352000, loss: 0.10626080389381377, gradient norm: 0.12734263638115084\n",
      "Iteration: 9353000, loss: 0.1062605005428852, gradient norm: 0.1753595185013106\n",
      "Iteration: 9354000, loss: 0.10626050486035876, gradient norm: 0.2819792726704556\n",
      "Iteration: 9355000, loss: 0.10626022082830158, gradient norm: 0.052657707263581585\n",
      "Iteration: 9356000, loss: 0.10626012438021593, gradient norm: 0.5318016634131769\n",
      "Iteration: 9357000, loss: 0.10625998502480738, gradient norm: 0.17574855218012186\n",
      "Iteration: 9358000, loss: 0.106259689391552, gradient norm: 0.30536482191283504\n",
      "Iteration: 9359000, loss: 0.10625945286123434, gradient norm: 0.1953660896008555\n",
      "Iteration: 9360000, loss: 0.10625956570512116, gradient norm: 0.5427832308516112\n",
      "Iteration: 9361000, loss: 0.10625942752257957, gradient norm: 0.2969010112147357\n",
      "Iteration: 9362000, loss: 0.10625912536022931, gradient norm: 0.35680089367170925\n",
      "Iteration: 9363000, loss: 0.10625916428737464, gradient norm: 0.08635909116209156\n",
      "Iteration: 9364000, loss: 0.10625864860757092, gradient norm: 0.2726925842732225\n",
      "Iteration: 9365000, loss: 0.10625883348565385, gradient norm: 0.7302607294400916\n",
      "Iteration: 9366000, loss: 0.10625844238437675, gradient norm: 0.14152668694648562\n",
      "Iteration: 9367000, loss: 0.10625838332769427, gradient norm: 0.25275969726043584\n",
      "Iteration: 9368000, loss: 0.1062582933074695, gradient norm: 0.36741967143842075\n",
      "Iteration: 9369000, loss: 0.1062580674714989, gradient norm: 0.18985491656789213\n",
      "Iteration: 9370000, loss: 0.10625800680878952, gradient norm: 0.1600428953684535\n",
      "Iteration: 9371000, loss: 0.10625776642097184, gradient norm: 0.5067605525629407\n",
      "Iteration: 9372000, loss: 0.1062575454603071, gradient norm: 0.44476302487489694\n",
      "Iteration: 9373000, loss: 0.1062574409089909, gradient norm: 0.7413983529845368\n",
      "Iteration: 9374000, loss: 0.10625741643902768, gradient norm: 0.05262703456894094\n",
      "Iteration: 9375000, loss: 0.10625701751431414, gradient norm: 0.16119421797366815\n",
      "Iteration: 9376000, loss: 0.10625705367745607, gradient norm: 0.19155801951179843\n",
      "Iteration: 9377000, loss: 0.10625690186237119, gradient norm: 0.5058266910864836\n",
      "Iteration: 9378000, loss: 0.10625661001804164, gradient norm: 0.3459878221025912\n",
      "Iteration: 9379000, loss: 0.10625662488252123, gradient norm: 0.2820087736350436\n",
      "Iteration: 9380000, loss: 0.10625626949287487, gradient norm: 0.6396391739884479\n",
      "Iteration: 9381000, loss: 0.10625632255595645, gradient norm: 0.4749280615438602\n",
      "Iteration: 9382000, loss: 0.1062560526405016, gradient norm: 0.45854219015553266\n",
      "Iteration: 9383000, loss: 0.10625585400120274, gradient norm: 0.06357121232739887\n",
      "Iteration: 9384000, loss: 0.10625572827223743, gradient norm: 0.8433010386909083\n",
      "Iteration: 9385000, loss: 0.10625549460518648, gradient norm: 0.36280836561236696\n",
      "Iteration: 9386000, loss: 0.1062554123417484, gradient norm: 0.6194844924385244\n",
      "Iteration: 9387000, loss: 0.10625533558241645, gradient norm: 0.4296867911722006\n",
      "Iteration: 9388000, loss: 0.10625518199591012, gradient norm: 0.06456276393938361\n",
      "Iteration: 9389000, loss: 0.10625493485727282, gradient norm: 0.7766876824901535\n",
      "Iteration: 9390000, loss: 0.1062548468863086, gradient norm: 0.2747291428218568\n",
      "Iteration: 9391000, loss: 0.10625478200539469, gradient norm: 0.43001157117953753\n",
      "Iteration: 9392000, loss: 0.1062543342470608, gradient norm: 0.14812495272332746\n",
      "Iteration: 9393000, loss: 0.1062544105294981, gradient norm: 0.03975939622956339\n",
      "Iteration: 9394000, loss: 0.1062541889971906, gradient norm: 0.4922899179418058\n",
      "Iteration: 9395000, loss: 0.10625411783850935, gradient norm: 0.2043789521692667\n",
      "Iteration: 9396000, loss: 0.10625385258118189, gradient norm: 0.2328423116279879\n",
      "Iteration: 9397000, loss: 0.10625373374458183, gradient norm: 0.6848667226291184\n",
      "Iteration: 9398000, loss: 0.1062535766044306, gradient norm: 0.11980340322948968\n",
      "Iteration: 9399000, loss: 0.10625336778940621, gradient norm: 0.6649376495711071\n",
      "Iteration: 9400000, loss: 0.10625333488940182, gradient norm: 0.08884554114601531\n",
      "Iteration: 9401000, loss: 0.10625315710811167, gradient norm: 0.2463013579608938\n",
      "Iteration: 9402000, loss: 0.10625282571043923, gradient norm: 0.5380448013003355\n",
      "Iteration: 9403000, loss: 0.10625294212517301, gradient norm: 0.11665056110970198\n",
      "Iteration: 9404000, loss: 0.1062527141308026, gradient norm: 0.18442086448366354\n",
      "Iteration: 9405000, loss: 0.10625244873295117, gradient norm: 0.17112248042380993\n",
      "Iteration: 9406000, loss: 0.10625238688819255, gradient norm: 0.3744009662581198\n",
      "Iteration: 9407000, loss: 0.10625219803037499, gradient norm: 0.27445287999213963\n",
      "Iteration: 9408000, loss: 0.10625202341936929, gradient norm: 0.10824268128080908\n",
      "Iteration: 9409000, loss: 0.10625195096738557, gradient norm: 0.044054324339058995\n",
      "Iteration: 9410000, loss: 0.1062517582120783, gradient norm: 0.3225194617161423\n",
      "Iteration: 9411000, loss: 0.10625148632501398, gradient norm: 0.366546618616614\n",
      "Iteration: 9412000, loss: 0.10625144562844363, gradient norm: 0.0970139760786535\n",
      "Iteration: 9413000, loss: 0.10625126892440781, gradient norm: 0.1961810093026914\n",
      "Iteration: 9414000, loss: 0.10625111305619832, gradient norm: 0.3021966956730386\n",
      "Iteration: 9415000, loss: 0.10625088915863296, gradient norm: 0.4568473370932892\n",
      "Iteration: 9416000, loss: 0.10625093781138607, gradient norm: 0.10584346653253564\n",
      "Iteration: 9417000, loss: 0.10625060621655641, gradient norm: 0.20551191436732025\n",
      "Iteration: 9418000, loss: 0.10625048412705555, gradient norm: 0.4645112510097237\n",
      "Iteration: 9419000, loss: 0.10625038033923462, gradient norm: 0.27236870745070463\n",
      "Iteration: 9420000, loss: 0.1062501030407768, gradient norm: 0.26591765712346815\n",
      "Iteration: 9421000, loss: 0.10625006844606805, gradient norm: 0.5069420282465228\n",
      "Iteration: 9422000, loss: 0.10624991794045645, gradient norm: 0.6310079603310411\n",
      "Iteration: 9423000, loss: 0.10624981128403092, gradient norm: 0.9074277268620805\n",
      "Iteration: 9424000, loss: 0.10624952299901395, gradient norm: 0.1496220914538432\n",
      "Iteration: 9425000, loss: 0.10624934418924928, gradient norm: 0.5942678852534249\n",
      "Iteration: 9426000, loss: 0.10624942864633065, gradient norm: 0.04121955296936222\n",
      "Iteration: 9427000, loss: 0.10624914398730274, gradient norm: 0.3150474204344111\n",
      "Iteration: 9428000, loss: 0.10624883321635588, gradient norm: 0.15844926794507244\n",
      "Iteration: 9429000, loss: 0.1062489502701038, gradient norm: 0.2037259048912792\n",
      "Iteration: 9430000, loss: 0.10624851624108991, gradient norm: 0.030069597919468522\n",
      "Iteration: 9431000, loss: 0.1062486010947293, gradient norm: 0.05142761492540286\n",
      "Iteration: 9432000, loss: 0.1062482829157682, gradient norm: 0.19828710268090496\n",
      "Iteration: 9433000, loss: 0.10624818715962725, gradient norm: 0.6086626380196928\n",
      "Iteration: 9434000, loss: 0.10624797128146829, gradient norm: 0.23698768913018523\n",
      "Iteration: 9435000, loss: 0.10624802769586578, gradient norm: 0.3410943250991087\n",
      "Iteration: 9436000, loss: 0.10624760000588995, gradient norm: 0.1871224157411682\n",
      "Iteration: 9437000, loss: 0.10624754053689552, gradient norm: 0.5097111586092069\n",
      "Iteration: 9438000, loss: 0.1062474873658328, gradient norm: 0.2636878022043859\n",
      "Iteration: 9439000, loss: 0.10624751640132492, gradient norm: 0.48868257157410144\n",
      "Iteration: 9440000, loss: 0.10624700156225754, gradient norm: 0.432677913152235\n",
      "Iteration: 9441000, loss: 0.10624691661179322, gradient norm: 0.32159059278848917\n",
      "Iteration: 9442000, loss: 0.10624666946606336, gradient norm: 0.18497436951653634\n",
      "Iteration: 9443000, loss: 0.10624675322880589, gradient norm: 0.09896606765668091\n",
      "Iteration: 9444000, loss: 0.10624648785071955, gradient norm: 0.3622314357821477\n",
      "Iteration: 9445000, loss: 0.10624635244990815, gradient norm: 0.28410973928757266\n",
      "Iteration: 9446000, loss: 0.10624619682389033, gradient norm: 0.22520287024035976\n",
      "Iteration: 9447000, loss: 0.1062460731630749, gradient norm: 0.537543746940482\n",
      "Iteration: 9448000, loss: 0.10624581647025329, gradient norm: 0.28607964127255625\n",
      "Iteration: 9449000, loss: 0.1062457481718537, gradient norm: 0.2729824949627271\n",
      "Iteration: 9450000, loss: 0.10624566443686463, gradient norm: 0.14481065307628604\n",
      "Iteration: 9451000, loss: 0.10624534045336205, gradient norm: 0.0804036510294016\n",
      "Iteration: 9452000, loss: 0.1062452785668706, gradient norm: 0.11592156974909613\n",
      "Iteration: 9453000, loss: 0.10624520518657266, gradient norm: 0.46150810043888296\n",
      "Iteration: 9454000, loss: 0.1062450331982024, gradient norm: 1.0194765415261617\n",
      "Iteration: 9455000, loss: 0.10624469881761225, gradient norm: 0.2545290889223314\n",
      "Iteration: 9456000, loss: 0.10624475253978824, gradient norm: 0.3814467521333384\n",
      "Iteration: 9457000, loss: 0.10624445979889623, gradient norm: 0.20728271078184118\n",
      "Iteration: 9458000, loss: 0.10624443241522372, gradient norm: 0.840987603098843\n",
      "Iteration: 9459000, loss: 0.10624416972650262, gradient norm: 0.9340324941347732\n",
      "Iteration: 9460000, loss: 0.10624406383911253, gradient norm: 0.44256818768870987\n",
      "Iteration: 9461000, loss: 0.10624373847302061, gradient norm: 0.16331352854911624\n",
      "Iteration: 9462000, loss: 0.10624387014338627, gradient norm: 0.2741115625331211\n",
      "Iteration: 9463000, loss: 0.10624369208309206, gradient norm: 0.09195450913806115\n",
      "Iteration: 9464000, loss: 0.10624330657486465, gradient norm: 0.35291254748634243\n",
      "Iteration: 9465000, loss: 0.10624337925777462, gradient norm: 0.2513956227164464\n",
      "Iteration: 9466000, loss: 0.10624310284554463, gradient norm: 0.2525873518143083\n",
      "Iteration: 9467000, loss: 0.1062429794526008, gradient norm: 0.2515359197714733\n",
      "Iteration: 9468000, loss: 0.10624285937518635, gradient norm: 0.1258553914113878\n",
      "Iteration: 9469000, loss: 0.10624264515983631, gradient norm: 0.4175457118469282\n",
      "Iteration: 9470000, loss: 0.10624244374471288, gradient norm: 0.13532179361521807\n",
      "Iteration: 9471000, loss: 0.10624248844310427, gradient norm: 0.40900550206595965\n",
      "Iteration: 9472000, loss: 0.10624209099753149, gradient norm: 0.5235057927958446\n",
      "Iteration: 9473000, loss: 0.10624206029143388, gradient norm: 0.28400217886927803\n",
      "Iteration: 9474000, loss: 0.10624195169541183, gradient norm: 0.15000791236503286\n",
      "Iteration: 9475000, loss: 0.10624197768715131, gradient norm: 0.10426767426484104\n",
      "Iteration: 9476000, loss: 0.10624144907457446, gradient norm: 0.6024754479425202\n",
      "Iteration: 9477000, loss: 0.1062414891742284, gradient norm: 0.3533482537711897\n",
      "Iteration: 9478000, loss: 0.10624137106168599, gradient norm: 0.24002137643820762\n",
      "Iteration: 9479000, loss: 0.1062410519698677, gradient norm: 0.6795284504836122\n",
      "Iteration: 9480000, loss: 0.10624103833031394, gradient norm: 0.38556283418017545\n",
      "Iteration: 9481000, loss: 0.10624083959186202, gradient norm: 0.5328571598010579\n",
      "Iteration: 9482000, loss: 0.10624052809037278, gradient norm: 0.7531964656452315\n",
      "Iteration: 9483000, loss: 0.10624065152729358, gradient norm: 0.23579116204564554\n",
      "Iteration: 9484000, loss: 0.10624039447244567, gradient norm: 0.6616277667277926\n",
      "Iteration: 9485000, loss: 0.10624030356569086, gradient norm: 0.1983175946706554\n",
      "Iteration: 9486000, loss: 0.10624010774726679, gradient norm: 0.44826767094869185\n",
      "Iteration: 9487000, loss: 0.10623982173634759, gradient norm: 0.1725535765533496\n",
      "Iteration: 9488000, loss: 0.10623978286630867, gradient norm: 0.43173013146545836\n",
      "Iteration: 9489000, loss: 0.10623961876200076, gradient norm: 0.14252858840857008\n",
      "Iteration: 9490000, loss: 0.10623948029941917, gradient norm: 0.2195072357399374\n",
      "Iteration: 9491000, loss: 0.10623918964867798, gradient norm: 0.4276818195025072\n",
      "Iteration: 9492000, loss: 0.10623932060121981, gradient norm: 0.576733744543989\n",
      "Iteration: 9493000, loss: 0.10623893577605274, gradient norm: 0.1261855074540734\n",
      "Iteration: 9494000, loss: 0.10623902087772755, gradient norm: 0.1369426229733463\n",
      "Iteration: 9495000, loss: 0.10623857023501673, gradient norm: 0.19377943301435452\n",
      "Iteration: 9496000, loss: 0.10623854695924845, gradient norm: 0.22936783657271212\n",
      "Iteration: 9497000, loss: 0.10623865152275806, gradient norm: 0.4676650426911165\n",
      "Iteration: 9498000, loss: 0.10623807712356584, gradient norm: 0.11648495444865072\n",
      "Iteration: 9499000, loss: 0.10623832974247627, gradient norm: 0.28793608841823864\n",
      "Iteration: 9500000, loss: 0.10623768593860586, gradient norm: 0.40415258742378124\n",
      "Iteration: 9501000, loss: 0.10623778207093106, gradient norm: 0.3228648946907627\n",
      "Iteration: 9502000, loss: 0.1062377391774626, gradient norm: 0.05154006783219998\n",
      "Iteration: 9503000, loss: 0.10623745312373485, gradient norm: 0.16624711061458364\n",
      "Iteration: 9504000, loss: 0.10623737532687955, gradient norm: 0.2024116479152963\n",
      "Iteration: 9505000, loss: 0.10623719363310093, gradient norm: 0.8695914459573172\n",
      "Iteration: 9506000, loss: 0.10623700979079505, gradient norm: 0.08175645446011959\n",
      "Iteration: 9507000, loss: 0.10623685668114112, gradient norm: 0.1969201210345201\n",
      "Iteration: 9508000, loss: 0.10623673475033192, gradient norm: 0.8333050040083075\n",
      "Iteration: 9509000, loss: 0.10623653988870393, gradient norm: 0.6895485298193519\n",
      "Iteration: 9510000, loss: 0.10623656351316334, gradient norm: 0.4144359316561687\n",
      "Iteration: 9511000, loss: 0.10623613104980036, gradient norm: 0.2705745317438667\n",
      "Iteration: 9512000, loss: 0.10623621171011917, gradient norm: 0.9197029219170949\n",
      "Iteration: 9513000, loss: 0.10623606970221545, gradient norm: 0.25775734493147257\n",
      "Iteration: 9514000, loss: 0.10623572419325433, gradient norm: 0.307687504913479\n",
      "Iteration: 9515000, loss: 0.10623581853920051, gradient norm: 0.3014415088860864\n",
      "Iteration: 9516000, loss: 0.10623543011629957, gradient norm: 0.25038751930182557\n",
      "Iteration: 9517000, loss: 0.10623544995301952, gradient norm: 0.810096706688611\n",
      "Iteration: 9518000, loss: 0.10623513126052196, gradient norm: 0.5317630050706714\n",
      "Iteration: 9519000, loss: 0.10623506562777062, gradient norm: 0.23367047345216194\n",
      "Iteration: 9520000, loss: 0.10623495553820055, gradient norm: 0.022832027660733282\n",
      "Iteration: 9521000, loss: 0.10623464540585403, gradient norm: 0.8510170419938501\n",
      "Iteration: 9522000, loss: 0.10623480449131845, gradient norm: 0.2199040169941942\n",
      "Iteration: 9523000, loss: 0.10623437258736962, gradient norm: 0.5229925393872062\n",
      "Iteration: 9524000, loss: 0.10623417266965299, gradient norm: 0.33594249272451104\n",
      "Iteration: 9525000, loss: 0.1062341600826092, gradient norm: 0.7189997166942524\n",
      "Iteration: 9526000, loss: 0.1062340801270278, gradient norm: 0.12008972779106578\n",
      "Iteration: 9527000, loss: 0.10623369796935334, gradient norm: 0.6432263467874768\n",
      "Iteration: 9528000, loss: 0.10623384655355664, gradient norm: 0.04373247252952966\n",
      "Iteration: 9529000, loss: 0.10623354682989924, gradient norm: 0.36414165067492416\n",
      "Iteration: 9530000, loss: 0.10623337627321881, gradient norm: 0.5981446903966636\n",
      "Iteration: 9531000, loss: 0.10623326231784051, gradient norm: 0.2710301043569034\n",
      "Iteration: 9532000, loss: 0.10623297879617204, gradient norm: 0.4152483993726678\n",
      "Iteration: 9533000, loss: 0.10623300716849876, gradient norm: 0.5094765844654779\n",
      "Iteration: 9534000, loss: 0.10623272106322142, gradient norm: 0.4773005527266247\n",
      "Iteration: 9535000, loss: 0.1062326141418697, gradient norm: 0.24697884775200224\n",
      "Iteration: 9536000, loss: 0.10623260028361325, gradient norm: 0.010840668950863006\n",
      "Iteration: 9537000, loss: 0.10623235223452329, gradient norm: 0.20880930029535863\n",
      "Iteration: 9538000, loss: 0.1062320657543814, gradient norm: 0.2649434661215387\n",
      "Iteration: 9539000, loss: 0.10623206233996904, gradient norm: 0.3308005934549667\n",
      "Iteration: 9540000, loss: 0.10623187200535164, gradient norm: 0.448794004958011\n",
      "Iteration: 9541000, loss: 0.10623182707190733, gradient norm: 0.8170450001225441\n",
      "Iteration: 9542000, loss: 0.10623141760243664, gradient norm: 0.10577900134760178\n",
      "Iteration: 9543000, loss: 0.10623143985961259, gradient norm: 0.4115827341810024\n",
      "Iteration: 9544000, loss: 0.10623134447620751, gradient norm: 0.240264038824426\n",
      "Iteration: 9545000, loss: 0.10623118848291332, gradient norm: 0.6627703050570584\n",
      "Iteration: 9546000, loss: 0.10623084238004703, gradient norm: 0.7502387599508223\n",
      "Iteration: 9547000, loss: 0.10623086046108242, gradient norm: 0.28879915162022385\n",
      "Iteration: 9548000, loss: 0.10623078605197309, gradient norm: 1.1476024732021959\n",
      "Iteration: 9549000, loss: 0.10623060248430996, gradient norm: 0.2379937583440433\n",
      "Iteration: 9550000, loss: 0.10623027368695548, gradient norm: 0.17438036071555202\n",
      "Iteration: 9551000, loss: 0.10623034650851397, gradient norm: 1.063027265481857\n",
      "Iteration: 9552000, loss: 0.10623009751155847, gradient norm: 0.3910247822505601\n",
      "Iteration: 9553000, loss: 0.10623000927836969, gradient norm: 0.15732731955574503\n",
      "Iteration: 9554000, loss: 0.10622968793732976, gradient norm: 0.030504528888968575\n",
      "Iteration: 9555000, loss: 0.10622960901614227, gradient norm: 0.5659311058750407\n",
      "Iteration: 9556000, loss: 0.10622949425025664, gradient norm: 0.4086732739709819\n",
      "Iteration: 9557000, loss: 0.10622935098797241, gradient norm: 0.19905686969819644\n",
      "Iteration: 9558000, loss: 0.10622910398012171, gradient norm: 0.39788230238217037\n",
      "Iteration: 9559000, loss: 0.10622896551874345, gradient norm: 0.49425907304921735\n",
      "Iteration: 9560000, loss: 0.10622895920592729, gradient norm: 0.04118457529394792\n",
      "Iteration: 9561000, loss: 0.10622882154867126, gradient norm: 0.6997023973591782\n",
      "Iteration: 9562000, loss: 0.10622844451706234, gradient norm: 0.1644935595544946\n",
      "Iteration: 9563000, loss: 0.10622838179929246, gradient norm: 0.146453296302438\n",
      "Iteration: 9564000, loss: 0.10622839557128089, gradient norm: 0.14275794239046838\n",
      "Iteration: 9565000, loss: 0.1062279166081446, gradient norm: 0.4358167990952052\n",
      "Iteration: 9566000, loss: 0.10622798930428895, gradient norm: 0.4296403896308406\n",
      "Iteration: 9567000, loss: 0.1062279290468445, gradient norm: 0.37470029569552293\n",
      "Iteration: 9568000, loss: 0.10622772277115518, gradient norm: 0.21259550417799494\n",
      "Iteration: 9569000, loss: 0.1062274550115045, gradient norm: 0.6832052956043523\n",
      "Iteration: 9570000, loss: 0.1062272084422934, gradient norm: 0.27801676863957203\n",
      "Iteration: 9571000, loss: 0.10622736725391173, gradient norm: 0.7475849511635393\n",
      "Iteration: 9572000, loss: 0.10622707882409396, gradient norm: 0.41018905122294547\n",
      "Iteration: 9573000, loss: 0.10622674755692503, gradient norm: 0.670546489745182\n",
      "Iteration: 9574000, loss: 0.1062267935856356, gradient norm: 0.4507288402348732\n",
      "Iteration: 9575000, loss: 0.10622663688547565, gradient norm: 0.12859934759973285\n",
      "Iteration: 9576000, loss: 0.10622641318021935, gradient norm: 0.30856946948728253\n",
      "Iteration: 9577000, loss: 0.10622630302609086, gradient norm: 0.7324859576235021\n",
      "Iteration: 9578000, loss: 0.10622629753854028, gradient norm: 0.6663632848694719\n",
      "Iteration: 9579000, loss: 0.10622605136592277, gradient norm: 0.1316521459840565\n",
      "Iteration: 9580000, loss: 0.10622568351501625, gradient norm: 0.21478249451865872\n",
      "Iteration: 9581000, loss: 0.10622586858254701, gradient norm: 0.4054852912549343\n",
      "Iteration: 9582000, loss: 0.10622542543921631, gradient norm: 0.129873089251674\n",
      "Iteration: 9583000, loss: 0.10622552957257382, gradient norm: 0.2484002000081281\n",
      "Iteration: 9584000, loss: 0.10622529803339216, gradient norm: 0.1723308336696552\n",
      "Iteration: 9585000, loss: 0.1062249580303202, gradient norm: 0.06057447920882416\n",
      "Iteration: 9586000, loss: 0.10622487742798033, gradient norm: 0.3218683123912922\n",
      "Iteration: 9587000, loss: 0.10622498769645386, gradient norm: 0.1281005197640651\n",
      "Iteration: 9588000, loss: 0.10622466950569373, gradient norm: 0.6502968114484883\n",
      "Iteration: 9589000, loss: 0.1062244837497822, gradient norm: 0.5842030921238944\n",
      "Iteration: 9590000, loss: 0.10622432001511595, gradient norm: 0.8747624762874157\n",
      "Iteration: 9591000, loss: 0.10622421851641674, gradient norm: 0.22616494859371106\n",
      "Iteration: 9592000, loss: 0.1062240718753554, gradient norm: 0.2409667583891286\n",
      "Iteration: 9593000, loss: 0.1062238819847208, gradient norm: 0.23526295560248245\n",
      "Iteration: 9594000, loss: 0.10622388823442339, gradient norm: 0.18788926211907483\n",
      "Iteration: 9595000, loss: 0.10622347492061397, gradient norm: 0.13325288581294642\n",
      "Iteration: 9596000, loss: 0.10622372830991708, gradient norm: 0.39823205142688445\n",
      "Iteration: 9597000, loss: 0.10622310154765062, gradient norm: 0.032441448336238\n",
      "Iteration: 9598000, loss: 0.10622320223025913, gradient norm: 0.5272230714706081\n",
      "Iteration: 9599000, loss: 0.1062229397918704, gradient norm: 0.3356803401458404\n",
      "Iteration: 9600000, loss: 0.10622296992741799, gradient norm: 0.13910727283807925\n",
      "Iteration: 9601000, loss: 0.10622278185033109, gradient norm: 0.02518967246172081\n",
      "Iteration: 9602000, loss: 0.10622236806882904, gradient norm: 0.06590695338805963\n",
      "Iteration: 9603000, loss: 0.1062224476368388, gradient norm: 0.09208340117473689\n",
      "Iteration: 9604000, loss: 0.10622242915475097, gradient norm: 0.38293393816600513\n",
      "Iteration: 9605000, loss: 0.10622204772622644, gradient norm: 0.05079569525535156\n",
      "Iteration: 9606000, loss: 0.10622205694469335, gradient norm: 0.19860412794389792\n",
      "Iteration: 9607000, loss: 0.10622174170562194, gradient norm: 0.294114988935442\n",
      "Iteration: 9608000, loss: 0.10622175222879901, gradient norm: 0.17628469603098837\n",
      "Iteration: 9609000, loss: 0.10622158311591659, gradient norm: 0.19108570631933589\n",
      "Iteration: 9610000, loss: 0.1062212362398949, gradient norm: 0.48640434399986976\n",
      "Iteration: 9611000, loss: 0.10622121275971133, gradient norm: 0.33382338018911994\n",
      "Iteration: 9612000, loss: 0.10622103946310289, gradient norm: 0.19425936877813943\n",
      "Iteration: 9613000, loss: 0.10622100846657749, gradient norm: 0.5190909734489583\n",
      "Iteration: 9614000, loss: 0.1062207544393878, gradient norm: 0.47141703379656286\n",
      "Iteration: 9615000, loss: 0.10622073323858268, gradient norm: 0.7095826563622094\n",
      "Iteration: 9616000, loss: 0.10622029618711167, gradient norm: 0.499322089273182\n",
      "Iteration: 9617000, loss: 0.10622031785614436, gradient norm: 0.30405269933692336\n",
      "Iteration: 9618000, loss: 0.10622038861123832, gradient norm: 0.09375573351034402\n",
      "Iteration: 9619000, loss: 0.10621995232835195, gradient norm: 0.27187019150535524\n",
      "Iteration: 9620000, loss: 0.1062198848079, gradient norm: 0.7645323617945621\n",
      "Iteration: 9621000, loss: 0.10621972706013706, gradient norm: 0.9303713287272619\n",
      "Iteration: 9622000, loss: 0.10621954676070754, gradient norm: 0.1775498479931389\n",
      "Iteration: 9623000, loss: 0.10621932055879892, gradient norm: 0.573877203637436\n",
      "Iteration: 9624000, loss: 0.10621939918610147, gradient norm: 0.13967893031261908\n",
      "Iteration: 9625000, loss: 0.10621906216639566, gradient norm: 0.13786177407710132\n",
      "Iteration: 9626000, loss: 0.10621916925082805, gradient norm: 0.37456719352674883\n",
      "Iteration: 9627000, loss: 0.1062186408852265, gradient norm: 0.25943707117355935\n",
      "Iteration: 9628000, loss: 0.1062187095759574, gradient norm: 0.1389839464710541\n",
      "Iteration: 9629000, loss: 0.10621852160185659, gradient norm: 0.44952610321981185\n",
      "Iteration: 9630000, loss: 0.10621853017051135, gradient norm: 0.07358697132593751\n",
      "Iteration: 9631000, loss: 0.1062181997892051, gradient norm: 0.4724191010086869\n",
      "Iteration: 9632000, loss: 0.10621800277098237, gradient norm: 0.6420537308057591\n",
      "Iteration: 9633000, loss: 0.1062179850432332, gradient norm: 0.6559837345764334\n",
      "Iteration: 9634000, loss: 0.1062177460748154, gradient norm: 0.14669543839161706\n",
      "Iteration: 9635000, loss: 0.1062177063257659, gradient norm: 0.15048596809236653\n",
      "Iteration: 9636000, loss: 0.10621749068553998, gradient norm: 0.07975791880377057\n",
      "Iteration: 9637000, loss: 0.10621747620299832, gradient norm: 0.42711333985498073\n",
      "Iteration: 9638000, loss: 0.10621699789605807, gradient norm: 0.4183898042563176\n",
      "Iteration: 9639000, loss: 0.1062170142470733, gradient norm: 0.025914986652169722\n",
      "Iteration: 9640000, loss: 0.10621696934623215, gradient norm: 0.16499796668052907\n",
      "Iteration: 9641000, loss: 0.10621661971816915, gradient norm: 0.19773851287700833\n",
      "Iteration: 9642000, loss: 0.10621680207282785, gradient norm: 0.1194869582213255\n",
      "Iteration: 9643000, loss: 0.1062163601808037, gradient norm: 0.32175552937765556\n",
      "Iteration: 9644000, loss: 0.10621634828528073, gradient norm: 0.03767193058745759\n",
      "Iteration: 9645000, loss: 0.10621618996060778, gradient norm: 0.7099763083450871\n",
      "Iteration: 9646000, loss: 0.10621602295968688, gradient norm: 0.16877083545428584\n",
      "Iteration: 9647000, loss: 0.10621577567653469, gradient norm: 0.128843584400823\n",
      "Iteration: 9648000, loss: 0.1062157954867401, gradient norm: 0.3754512351391789\n",
      "Iteration: 9649000, loss: 0.10621549023671678, gradient norm: 0.755913619513312\n",
      "Iteration: 9650000, loss: 0.1062153969266895, gradient norm: 0.5356860179225404\n",
      "Iteration: 9651000, loss: 0.10621524009927913, gradient norm: 0.7807621696990018\n",
      "Iteration: 9652000, loss: 0.10621510044269322, gradient norm: 0.25107657625660856\n",
      "Iteration: 9653000, loss: 0.10621490420248175, gradient norm: 0.746215701849899\n",
      "Iteration: 9654000, loss: 0.1062150521372407, gradient norm: 0.5228361746017332\n",
      "Iteration: 9655000, loss: 0.10621454470358668, gradient norm: 0.37374277789709603\n",
      "Iteration: 9656000, loss: 0.10621448975455837, gradient norm: 0.07589979008910985\n",
      "Iteration: 9657000, loss: 0.1062142824613235, gradient norm: 0.4650058337575519\n",
      "Iteration: 9658000, loss: 0.1062143843711709, gradient norm: 0.4147092439689167\n",
      "Iteration: 9659000, loss: 0.10621402370782349, gradient norm: 0.05903621675248233\n",
      "Iteration: 9660000, loss: 0.10621412955477946, gradient norm: 0.17325382253146437\n",
      "Iteration: 9661000, loss: 0.10621363707871632, gradient norm: 0.3580518209114433\n",
      "Iteration: 9662000, loss: 0.1062136858350982, gradient norm: 0.1029279277298118\n",
      "Iteration: 9663000, loss: 0.10621348286059581, gradient norm: 0.13127729073571848\n",
      "Iteration: 9664000, loss: 0.10621337325516514, gradient norm: 1.2981499485283292\n",
      "Iteration: 9665000, loss: 0.10621319198941732, gradient norm: 0.1414228600196811\n",
      "Iteration: 9666000, loss: 0.10621299504731002, gradient norm: 0.7498427550801433\n",
      "Iteration: 9667000, loss: 0.10621292192289755, gradient norm: 0.13474464035643352\n",
      "Iteration: 9668000, loss: 0.10621279485881464, gradient norm: 0.4026163305681329\n",
      "Iteration: 9669000, loss: 0.10621259173894165, gradient norm: 0.8264331213217304\n",
      "Iteration: 9670000, loss: 0.10621254845131849, gradient norm: 0.16369682438095556\n",
      "Iteration: 9671000, loss: 0.10621220071384584, gradient norm: 0.5127268033348136\n",
      "Iteration: 9672000, loss: 0.10621208492438623, gradient norm: 0.4196576458236833\n",
      "Iteration: 9673000, loss: 0.10621211233517251, gradient norm: 0.6252837153712079\n",
      "Iteration: 9674000, loss: 0.1062118316411935, gradient norm: 0.354431230965241\n",
      "Iteration: 9675000, loss: 0.10621160561539111, gradient norm: 0.2431832609837289\n",
      "Iteration: 9676000, loss: 0.10621164715419124, gradient norm: 0.3580582502025468\n",
      "Iteration: 9677000, loss: 0.10621133076483431, gradient norm: 0.902245496223241\n",
      "Iteration: 9678000, loss: 0.10621134715677727, gradient norm: 0.24340132382569388\n",
      "Iteration: 9679000, loss: 0.10621111027336345, gradient norm: 0.27688231885014725\n",
      "Iteration: 9680000, loss: 0.10621094247416414, gradient norm: 0.15124116325424347\n",
      "Iteration: 9681000, loss: 0.10621092382144155, gradient norm: 0.19519228007866227\n",
      "Iteration: 9682000, loss: 0.1062105997176878, gradient norm: 0.31925111130582723\n",
      "Iteration: 9683000, loss: 0.10621040862265678, gradient norm: 0.13568330145965038\n",
      "Iteration: 9684000, loss: 0.10621045784087989, gradient norm: 0.804687798371123\n",
      "Iteration: 9685000, loss: 0.10621024502462481, gradient norm: 0.19116307350400316\n",
      "Iteration: 9686000, loss: 0.10621005780583369, gradient norm: 0.26363568191342024\n",
      "Iteration: 9687000, loss: 0.10620991323504467, gradient norm: 0.7506556448465964\n",
      "Iteration: 9688000, loss: 0.10620984515273407, gradient norm: 0.10654971770466994\n",
      "Iteration: 9689000, loss: 0.10620960213073945, gradient norm: 0.1379653749533662\n",
      "Iteration: 9690000, loss: 0.106209473514998, gradient norm: 0.18951664038132662\n",
      "Iteration: 9691000, loss: 0.1062094840870713, gradient norm: 0.46175310080334114\n",
      "Iteration: 9692000, loss: 0.10620926523871571, gradient norm: 0.9630943188585612\n",
      "Iteration: 9693000, loss: 0.10620899272142997, gradient norm: 1.1077976659656783\n",
      "Iteration: 9694000, loss: 0.10620885422713644, gradient norm: 0.1420324516001359\n",
      "Iteration: 9695000, loss: 0.10620892292358815, gradient norm: 0.05742564330586765\n",
      "Iteration: 9696000, loss: 0.10620847946753818, gradient norm: 0.18749687705339504\n",
      "Iteration: 9697000, loss: 0.1062085509958213, gradient norm: 0.28056812344022086\n",
      "Iteration: 9698000, loss: 0.10620829647939686, gradient norm: 0.18885093995635985\n",
      "Iteration: 9699000, loss: 0.1062080383447657, gradient norm: 0.4034606810607582\n",
      "Iteration: 9700000, loss: 0.10620812300134033, gradient norm: 0.20430790370392335\n",
      "Iteration: 9701000, loss: 0.10620778609748091, gradient norm: 0.1431668834302913\n",
      "Iteration: 9702000, loss: 0.10620769348590481, gradient norm: 0.12980272373611654\n",
      "Iteration: 9703000, loss: 0.10620777897691008, gradient norm: 0.33396902146770685\n",
      "Iteration: 9704000, loss: 0.10620728890978154, gradient norm: 0.028589524258377432\n",
      "Iteration: 9705000, loss: 0.10620722558632668, gradient norm: 0.6174117846094175\n",
      "Iteration: 9706000, loss: 0.1062072139967221, gradient norm: 0.25996018565395335\n",
      "Iteration: 9707000, loss: 0.10620701262582552, gradient norm: 0.7455250672196243\n",
      "Iteration: 9708000, loss: 0.10620682714127741, gradient norm: 0.2903630403112378\n",
      "Iteration: 9709000, loss: 0.10620663664580321, gradient norm: 0.1958820803990907\n",
      "Iteration: 9710000, loss: 0.10620664553125, gradient norm: 0.27840695392571824\n",
      "Iteration: 9711000, loss: 0.10620642341522808, gradient norm: 0.4897804167905075\n",
      "Iteration: 9712000, loss: 0.10620617506846458, gradient norm: 0.188645012596984\n",
      "Iteration: 9713000, loss: 0.10620604175253717, gradient norm: 0.47009065822502405\n",
      "Iteration: 9714000, loss: 0.10620601451302793, gradient norm: 0.15904543115279218\n",
      "Iteration: 9715000, loss: 0.10620580296335967, gradient norm: 0.37577060975767423\n",
      "Iteration: 9716000, loss: 0.10620575907923302, gradient norm: 0.8014925341376314\n",
      "Iteration: 9717000, loss: 0.1062054455896261, gradient norm: 0.19226166602756586\n",
      "Iteration: 9718000, loss: 0.10620541958774503, gradient norm: 0.11166938831983653\n",
      "Iteration: 9719000, loss: 0.10620515589296092, gradient norm: 0.19316340428569745\n",
      "Iteration: 9720000, loss: 0.10620514806466846, gradient norm: 0.48411229203892003\n",
      "Iteration: 9721000, loss: 0.1062048467857193, gradient norm: 0.6724614775197315\n",
      "Iteration: 9722000, loss: 0.10620494453864945, gradient norm: 0.17971713101836836\n",
      "Iteration: 9723000, loss: 0.10620461635720574, gradient norm: 0.1305725604347081\n",
      "Iteration: 9724000, loss: 0.10620445836647374, gradient norm: 0.6063311107121697\n",
      "Iteration: 9725000, loss: 0.10620430173966301, gradient norm: 0.3403621247562235\n",
      "Iteration: 9726000, loss: 0.10620420157527925, gradient norm: 0.1650267114710797\n",
      "Iteration: 9727000, loss: 0.10620405621773116, gradient norm: 0.14131362948454282\n",
      "Iteration: 9728000, loss: 0.10620384215805548, gradient norm: 0.08222092796351897\n",
      "Iteration: 9729000, loss: 0.10620387402640313, gradient norm: 0.3032074203525839\n",
      "Iteration: 9730000, loss: 0.10620352067748598, gradient norm: 0.39685081422482776\n",
      "Iteration: 9731000, loss: 0.10620341767412836, gradient norm: 0.4488115802155105\n",
      "Iteration: 9732000, loss: 0.1062033590925349, gradient norm: 0.13570984809874798\n",
      "Iteration: 9733000, loss: 0.10620317523835092, gradient norm: 0.17312727983086965\n",
      "Iteration: 9734000, loss: 0.10620307687515708, gradient norm: 1.0026796986478073\n",
      "Iteration: 9735000, loss: 0.10620291607848382, gradient norm: 0.3556997911623302\n",
      "Iteration: 9736000, loss: 0.10620278800291469, gradient norm: 0.3883841735527237\n",
      "Iteration: 9737000, loss: 0.10620257603355242, gradient norm: 0.19733478265295787\n",
      "Iteration: 9738000, loss: 0.10620233319497183, gradient norm: 0.556012906833665\n",
      "Iteration: 9739000, loss: 0.10620234263747431, gradient norm: 0.3398965201399882\n",
      "Iteration: 9740000, loss: 0.10620244642322955, gradient norm: 0.4479307920068805\n",
      "Iteration: 9741000, loss: 0.10620175876393267, gradient norm: 0.7507658118680902\n",
      "Iteration: 9742000, loss: 0.10620192630005482, gradient norm: 0.2810895036995866\n",
      "Iteration: 9743000, loss: 0.10620170150139462, gradient norm: 0.34860136833675176\n",
      "Iteration: 9744000, loss: 0.10620151418961174, gradient norm: 0.09288935242277847\n",
      "Iteration: 9745000, loss: 0.10620146943162341, gradient norm: 0.26089487522055294\n",
      "Iteration: 9746000, loss: 0.10620136170692342, gradient norm: 0.24013227629179984\n",
      "Iteration: 9747000, loss: 0.10620095993604377, gradient norm: 0.9196685854727515\n",
      "Iteration: 9748000, loss: 0.10620111510789462, gradient norm: 0.16974635083248032\n",
      "Iteration: 9749000, loss: 0.10620081872696872, gradient norm: 0.1449450206753834\n",
      "Iteration: 9750000, loss: 0.10620064299005788, gradient norm: 0.5266015702695783\n",
      "Iteration: 9751000, loss: 0.10620048827922075, gradient norm: 0.6675568680446202\n",
      "Iteration: 9752000, loss: 0.10620048277187585, gradient norm: 0.15332305409139865\n",
      "Iteration: 9753000, loss: 0.10620026533577305, gradient norm: 0.04307343346983048\n",
      "Iteration: 9754000, loss: 0.10620003938970173, gradient norm: 0.29461722308354477\n",
      "Iteration: 9755000, loss: 0.10619988132313245, gradient norm: 0.22962669855688198\n",
      "Iteration: 9756000, loss: 0.10619986382625805, gradient norm: 0.19457546087685545\n",
      "Iteration: 9757000, loss: 0.10619965769497887, gradient norm: 0.39620060591119455\n",
      "Iteration: 9758000, loss: 0.10619969866221529, gradient norm: 0.3692379133650549\n",
      "Iteration: 9759000, loss: 0.1061992479155694, gradient norm: 0.2406883408720967\n",
      "Iteration: 9760000, loss: 0.10619914065666988, gradient norm: 0.017465529913020094\n",
      "Iteration: 9761000, loss: 0.10619915377157392, gradient norm: 0.09047751283171704\n",
      "Iteration: 9762000, loss: 0.1061989785746332, gradient norm: 0.2252729961866714\n",
      "Iteration: 9763000, loss: 0.106198735293401, gradient norm: 0.44769561744042596\n",
      "Iteration: 9764000, loss: 0.10619870363822113, gradient norm: 0.14477231463919982\n",
      "Iteration: 9765000, loss: 0.10619846949347385, gradient norm: 0.48617188673960393\n",
      "Iteration: 9766000, loss: 0.10619826822935785, gradient norm: 0.26299968157276515\n",
      "Iteration: 9767000, loss: 0.10619825962622172, gradient norm: 0.14602042461987844\n",
      "Iteration: 9768000, loss: 0.10619803465788086, gradient norm: 0.23489223354888045\n",
      "Iteration: 9769000, loss: 0.10619790267663756, gradient norm: 0.15867347232036408\n",
      "Iteration: 9770000, loss: 0.10619782980371878, gradient norm: 0.10487416933404811\n",
      "Iteration: 9771000, loss: 0.10619755341065537, gradient norm: 0.09397478185818453\n",
      "Iteration: 9772000, loss: 0.10619749829337474, gradient norm: 1.1501696289325456\n",
      "Iteration: 9773000, loss: 0.10619732626887068, gradient norm: 0.29052237215606275\n",
      "Iteration: 9774000, loss: 0.10619723863405377, gradient norm: 0.7874893357179177\n",
      "Iteration: 9775000, loss: 0.10619701014088113, gradient norm: 0.15848435388659102\n",
      "Iteration: 9776000, loss: 0.1061969109172808, gradient norm: 0.17623197103275537\n",
      "Iteration: 9777000, loss: 0.10619679495578026, gradient norm: 0.1481654833747459\n",
      "Iteration: 9778000, loss: 0.10619653520004718, gradient norm: 0.5152990279263436\n",
      "Iteration: 9779000, loss: 0.10619653892771859, gradient norm: 0.734093301140518\n",
      "Iteration: 9780000, loss: 0.10619640669391413, gradient norm: 0.42114687472845574\n",
      "Iteration: 9781000, loss: 0.10619597477677462, gradient norm: 0.3141342084654075\n",
      "Iteration: 9782000, loss: 0.10619601555017996, gradient norm: 0.1921124656608693\n",
      "Iteration: 9783000, loss: 0.10619597110099682, gradient norm: 0.2973922923963861\n",
      "Iteration: 9784000, loss: 0.10619573124171058, gradient norm: 0.015241048620324228\n",
      "Iteration: 9785000, loss: 0.10619557026256637, gradient norm: 0.2123071704514636\n",
      "Iteration: 9786000, loss: 0.10619537946078159, gradient norm: 0.3229813179740479\n",
      "Iteration: 9787000, loss: 0.10619526580908606, gradient norm: 0.3160692534755552\n",
      "Iteration: 9788000, loss: 0.10619526789361951, gradient norm: 0.27395579624617117\n",
      "Iteration: 9789000, loss: 0.10619494156937674, gradient norm: 0.21423254864414515\n",
      "Iteration: 9790000, loss: 0.10619484283160512, gradient norm: 0.521295226009929\n",
      "Iteration: 9791000, loss: 0.10619464375140591, gradient norm: 0.16981599017925572\n",
      "Iteration: 9792000, loss: 0.10619483634923148, gradient norm: 0.6287742948589307\n",
      "Iteration: 9793000, loss: 0.1061942132572595, gradient norm: 0.6505179815696405\n",
      "Iteration: 9794000, loss: 0.10619426572651705, gradient norm: 0.2904105866596203\n",
      "Iteration: 9795000, loss: 0.10619412056225527, gradient norm: 0.5093094676328086\n",
      "Iteration: 9796000, loss: 0.10619413321179545, gradient norm: 0.753180847905239\n",
      "Iteration: 9797000, loss: 0.10619375210917661, gradient norm: 0.24309999973304197\n",
      "Iteration: 9798000, loss: 0.10619379233341525, gradient norm: 0.18346463317797457\n",
      "Iteration: 9799000, loss: 0.10619339574694828, gradient norm: 0.3292272303173383\n",
      "Iteration: 9800000, loss: 0.10619333808031838, gradient norm: 0.17860634680166845\n",
      "Iteration: 9801000, loss: 0.10619347640629416, gradient norm: 0.26853239225602327\n",
      "Iteration: 9802000, loss: 0.10619294850584114, gradient norm: 0.538905191144639\n",
      "Iteration: 9803000, loss: 0.106193026937786, gradient norm: 0.37452656664844614\n",
      "Iteration: 9804000, loss: 0.10619286901348393, gradient norm: 0.22799018917056205\n",
      "Iteration: 9805000, loss: 0.10619266619234952, gradient norm: 0.10313213195777039\n",
      "Iteration: 9806000, loss: 0.10619251981520283, gradient norm: 0.5207767479083996\n",
      "Iteration: 9807000, loss: 0.1061923264703189, gradient norm: 0.293436739222468\n",
      "Iteration: 9808000, loss: 0.10619232411288075, gradient norm: 0.17241874639363597\n",
      "Iteration: 9809000, loss: 0.10619206410928694, gradient norm: 0.4220062593806248\n",
      "Iteration: 9810000, loss: 0.10619192498670736, gradient norm: 0.3704125951825327\n",
      "Iteration: 9811000, loss: 0.10619181067385713, gradient norm: 0.04965436044541109\n",
      "Iteration: 9812000, loss: 0.10619161421077795, gradient norm: 0.19462738038653735\n",
      "Iteration: 9813000, loss: 0.10619163124235592, gradient norm: 0.4605670194544635\n",
      "Iteration: 9814000, loss: 0.10619132374716427, gradient norm: 0.03663755412004549\n",
      "Iteration: 9815000, loss: 0.10619123580783534, gradient norm: 0.9783926404186928\n",
      "Iteration: 9816000, loss: 0.1061911534098231, gradient norm: 0.18274125189685572\n",
      "Iteration: 9817000, loss: 0.10619096931172393, gradient norm: 0.983718446585959\n",
      "Iteration: 9818000, loss: 0.10619080882196093, gradient norm: 0.35596069136093617\n",
      "Iteration: 9819000, loss: 0.10619064204107874, gradient norm: 0.6746748630633658\n",
      "Iteration: 9820000, loss: 0.10619058342490234, gradient norm: 0.12771203010814206\n",
      "Iteration: 9821000, loss: 0.10619033469824483, gradient norm: 0.3741561566887631\n",
      "Iteration: 9822000, loss: 0.10619000805673298, gradient norm: 0.18730712220367962\n",
      "Iteration: 9823000, loss: 0.10619016730695328, gradient norm: 0.11312531216770016\n",
      "Iteration: 9824000, loss: 0.10618999177543781, gradient norm: 0.539195927010302\n",
      "Iteration: 9825000, loss: 0.10618969727928083, gradient norm: 0.04877891741201023\n",
      "Iteration: 9826000, loss: 0.10618974341710224, gradient norm: 0.3197181357261698\n",
      "Iteration: 9827000, loss: 0.10618961706163305, gradient norm: 0.330137753907054\n",
      "Iteration: 9828000, loss: 0.10618917387114868, gradient norm: 0.14509761741945998\n",
      "Iteration: 9829000, loss: 0.10618926971795237, gradient norm: 0.43313786832870016\n",
      "Iteration: 9830000, loss: 0.10618905238586628, gradient norm: 0.40227280528825277\n",
      "Iteration: 9831000, loss: 0.10618880259582775, gradient norm: 0.31056522523921926\n",
      "Iteration: 9832000, loss: 0.10618879232306046, gradient norm: 0.08896264584217674\n",
      "Iteration: 9833000, loss: 0.10618871046693522, gradient norm: 0.0811776268818133\n",
      "Iteration: 9834000, loss: 0.10618830565567905, gradient norm: 0.2308483531166817\n",
      "Iteration: 9835000, loss: 0.10618847232757776, gradient norm: 0.24850101277317058\n",
      "Iteration: 9836000, loss: 0.10618812525152498, gradient norm: 0.5502617516847179\n",
      "Iteration: 9837000, loss: 0.1061881452873084, gradient norm: 0.045851093098178405\n",
      "Iteration: 9838000, loss: 0.10618783179005385, gradient norm: 0.7111190534220923\n",
      "Iteration: 9839000, loss: 0.10618774621803594, gradient norm: 0.16701905553163404\n",
      "Iteration: 9840000, loss: 0.10618764114756271, gradient norm: 0.3210378905611605\n",
      "Iteration: 9841000, loss: 0.10618764790085859, gradient norm: 0.3374803171085299\n",
      "Iteration: 9842000, loss: 0.10618721722722042, gradient norm: 0.04112139928440783\n",
      "Iteration: 9843000, loss: 0.1061872607825523, gradient norm: 0.4651029312805875\n",
      "Iteration: 9844000, loss: 0.1061869699103895, gradient norm: 0.275975883231362\n",
      "Iteration: 9845000, loss: 0.10618692680177978, gradient norm: 0.22668342435725172\n",
      "Iteration: 9846000, loss: 0.106186696612498, gradient norm: 0.16264155586606507\n",
      "Iteration: 9847000, loss: 0.10618677229776634, gradient norm: 0.22359326027252946\n",
      "Iteration: 9848000, loss: 0.10618628175890257, gradient norm: 0.38826527226636176\n",
      "Iteration: 9849000, loss: 0.10618628164053517, gradient norm: 0.21118125729303516\n",
      "Iteration: 9850000, loss: 0.10618620816400647, gradient norm: 0.31479950697110226\n",
      "Iteration: 9851000, loss: 0.10618605925685777, gradient norm: 0.6684232801413823\n",
      "Iteration: 9852000, loss: 0.10618595695227995, gradient norm: 0.4464315941859717\n",
      "Iteration: 9853000, loss: 0.1061857160336169, gradient norm: 0.03981554196255781\n",
      "Iteration: 9854000, loss: 0.10618552603066425, gradient norm: 0.7555342171124694\n",
      "Iteration: 9855000, loss: 0.10618545345221966, gradient norm: 0.5152200312094883\n",
      "Iteration: 9856000, loss: 0.10618551336625444, gradient norm: 0.08557703073335174\n",
      "Iteration: 9857000, loss: 0.10618501797268937, gradient norm: 0.5526996650021705\n",
      "Iteration: 9858000, loss: 0.10618511889265758, gradient norm: 0.40181778677054325\n",
      "Iteration: 9859000, loss: 0.10618493798211037, gradient norm: 0.14705798383580534\n",
      "Iteration: 9860000, loss: 0.10618468745115398, gradient norm: 0.2963958939632071\n",
      "Iteration: 9861000, loss: 0.10618456836861545, gradient norm: 0.11304854797963713\n",
      "Iteration: 9862000, loss: 0.10618444715693644, gradient norm: 0.45823567530797793\n",
      "Iteration: 9863000, loss: 0.10618424672216056, gradient norm: 0.31661171815263833\n",
      "Iteration: 9864000, loss: 0.10618427447655467, gradient norm: 0.29144704076254935\n",
      "Iteration: 9865000, loss: 0.10618393064639582, gradient norm: 0.3604024336133333\n",
      "Iteration: 9866000, loss: 0.1061838108823135, gradient norm: 0.265151399180608\n",
      "Iteration: 9867000, loss: 0.10618368316560275, gradient norm: 0.27913500956163\n",
      "Iteration: 9868000, loss: 0.10618368581796812, gradient norm: 0.29412431125282673\n",
      "Iteration: 9869000, loss: 0.10618340472906755, gradient norm: 0.7790233610343575\n",
      "Iteration: 9870000, loss: 0.10618337111848017, gradient norm: 0.3614323282971834\n",
      "Iteration: 9871000, loss: 0.1061831646609047, gradient norm: 0.9263682684529804\n",
      "Iteration: 9872000, loss: 0.10618289772123386, gradient norm: 0.4377233113344818\n",
      "Iteration: 9873000, loss: 0.10618305863221869, gradient norm: 0.4032777324838413\n",
      "Iteration: 9874000, loss: 0.10618259881140904, gradient norm: 0.2946375695849366\n",
      "Iteration: 9875000, loss: 0.1061826228544585, gradient norm: 0.1887388304707381\n",
      "Iteration: 9876000, loss: 0.10618241383951266, gradient norm: 0.10431754780152923\n",
      "Iteration: 9877000, loss: 0.10618237499680737, gradient norm: 0.5799137975459072\n",
      "Iteration: 9878000, loss: 0.10618216542904568, gradient norm: 0.42182670813520695\n",
      "Iteration: 9879000, loss: 0.10618194615033362, gradient norm: 0.29303505439775646\n",
      "Iteration: 9880000, loss: 0.10618204174301991, gradient norm: 0.6563579594942789\n",
      "Iteration: 9881000, loss: 0.10618161622665617, gradient norm: 0.33407732270758705\n",
      "Iteration: 9882000, loss: 0.10618138738483335, gradient norm: 0.3889369605443592\n",
      "Iteration: 9883000, loss: 0.10618180319214296, gradient norm: 0.42272351177256295\n",
      "Iteration: 9884000, loss: 0.10618107022747311, gradient norm: 0.24462208744832403\n",
      "Iteration: 9885000, loss: 0.10618117007669915, gradient norm: 1.083843370910032\n",
      "Iteration: 9886000, loss: 0.1061810603262558, gradient norm: 0.3077290913992468\n",
      "Iteration: 9887000, loss: 0.10618081101305023, gradient norm: 0.1550419857607618\n",
      "Iteration: 9888000, loss: 0.10618068139894883, gradient norm: 0.1548327088281647\n",
      "Iteration: 9889000, loss: 0.10618053651020051, gradient norm: 0.19705479115548238\n",
      "Iteration: 9890000, loss: 0.10618045796110288, gradient norm: 0.2959868344680162\n",
      "Iteration: 9891000, loss: 0.10618040792572514, gradient norm: 0.19010126861991275\n",
      "Iteration: 9892000, loss: 0.10618010289319284, gradient norm: 0.5821466603859536\n",
      "Iteration: 9893000, loss: 0.10617994335897489, gradient norm: 0.36676356580106523\n",
      "Iteration: 9894000, loss: 0.10617981592127773, gradient norm: 0.25039510356130507\n",
      "Iteration: 9895000, loss: 0.10617971928919101, gradient norm: 0.46842812514824606\n",
      "Iteration: 9896000, loss: 0.10617958527955154, gradient norm: 0.10670427398924694\n",
      "Iteration: 9897000, loss: 0.10617952392946721, gradient norm: 0.2712298784246601\n",
      "Iteration: 9898000, loss: 0.10617934582534676, gradient norm: 0.7115856627467566\n",
      "Iteration: 9899000, loss: 0.10617907351623615, gradient norm: 0.44518799076956833\n",
      "Iteration: 9900000, loss: 0.10617907703126718, gradient norm: 0.11226651698028572\n",
      "Iteration: 9901000, loss: 0.10617885807523746, gradient norm: 0.09605329841702781\n",
      "Iteration: 9902000, loss: 0.10617872910666902, gradient norm: 0.13082325599252043\n",
      "Iteration: 9903000, loss: 0.10617860544377364, gradient norm: 0.7858251607221189\n",
      "Iteration: 9904000, loss: 0.10617834695700784, gradient norm: 0.19697052703589463\n",
      "Iteration: 9905000, loss: 0.1061782970073805, gradient norm: 0.2378574944684978\n",
      "Iteration: 9906000, loss: 0.10617819546756659, gradient norm: 0.38342536831665064\n",
      "Iteration: 9907000, loss: 0.10617795829706052, gradient norm: 0.4880817679877613\n",
      "Iteration: 9908000, loss: 0.1061778170352828, gradient norm: 0.14340787876423636\n",
      "Iteration: 9909000, loss: 0.1061777582728544, gradient norm: 0.28200538772818445\n",
      "Iteration: 9910000, loss: 0.10617756672792997, gradient norm: 0.4919132918272345\n",
      "Iteration: 9911000, loss: 0.1061774147833325, gradient norm: 0.023826764034806148\n",
      "Iteration: 9912000, loss: 0.10617732606021726, gradient norm: 0.08807232373892818\n",
      "Iteration: 9913000, loss: 0.10617707169496661, gradient norm: 0.1788755128773513\n",
      "Iteration: 9914000, loss: 0.10617713730533677, gradient norm: 0.3864855983745876\n",
      "Iteration: 9915000, loss: 0.10617683437741837, gradient norm: 0.0766952455116576\n",
      "Iteration: 9916000, loss: 0.10617673618465874, gradient norm: 0.17323236294543679\n",
      "Iteration: 9917000, loss: 0.10617652826093445, gradient norm: 0.41899850006965395\n",
      "Iteration: 9918000, loss: 0.10617657388025362, gradient norm: 0.2387778774766328\n",
      "Iteration: 9919000, loss: 0.10617628837501378, gradient norm: 0.6187361315859552\n",
      "Iteration: 9920000, loss: 0.10617601099922976, gradient norm: 0.3824353304335442\n",
      "Iteration: 9921000, loss: 0.10617622522653912, gradient norm: 0.2680248320814838\n",
      "Iteration: 9922000, loss: 0.10617580303490742, gradient norm: 0.315403186313994\n",
      "Iteration: 9923000, loss: 0.10617569066780926, gradient norm: 0.6751800537916607\n",
      "Iteration: 9924000, loss: 0.1061755216751111, gradient norm: 0.21037935925504272\n",
      "Iteration: 9925000, loss: 0.1061754888229962, gradient norm: 0.20824919992534893\n",
      "Iteration: 9926000, loss: 0.10617534857832887, gradient norm: 0.5010795438135491\n",
      "Iteration: 9927000, loss: 0.10617512782124425, gradient norm: 0.2784661453166478\n",
      "Iteration: 9928000, loss: 0.10617496992772144, gradient norm: 0.15345439161322394\n",
      "Iteration: 9929000, loss: 0.10617497037979809, gradient norm: 0.30208936121030944\n",
      "Iteration: 9930000, loss: 0.10617479039958302, gradient norm: 0.14569365593086717\n",
      "Iteration: 9931000, loss: 0.1061744681327447, gradient norm: 0.22890464036422492\n",
      "Iteration: 9932000, loss: 0.10617445880089592, gradient norm: 0.3599226463952606\n",
      "Iteration: 9933000, loss: 0.10617431702291744, gradient norm: 0.32496928854791074\n",
      "Iteration: 9934000, loss: 0.10617419846156875, gradient norm: 0.2591927995086858\n",
      "Iteration: 9935000, loss: 0.10617405824700689, gradient norm: 0.43623606705379125\n",
      "Iteration: 9936000, loss: 0.10617393468260479, gradient norm: 0.0694612956770091\n",
      "Iteration: 9937000, loss: 0.10617363484286764, gradient norm: 0.28102871258696244\n",
      "Iteration: 9938000, loss: 0.106173556276077, gradient norm: 0.39734722759247626\n",
      "Iteration: 9939000, loss: 0.10617346523887891, gradient norm: 0.09530425679147003\n",
      "Iteration: 9940000, loss: 0.1061733211757929, gradient norm: 0.206890020763845\n",
      "Iteration: 9941000, loss: 0.1061731660920787, gradient norm: 0.2703138830029146\n",
      "Iteration: 9942000, loss: 0.10617295079320102, gradient norm: 0.4078162141545418\n",
      "Iteration: 9943000, loss: 0.10617301688487482, gradient norm: 0.5480983243671477\n",
      "Iteration: 9944000, loss: 0.10617270870113817, gradient norm: 0.337485052696153\n",
      "Iteration: 9945000, loss: 0.10617257470040907, gradient norm: 0.08430488826255975\n",
      "Iteration: 9946000, loss: 0.10617248931048932, gradient norm: 0.5761471403711104\n",
      "Iteration: 9947000, loss: 0.10617223134837986, gradient norm: 0.19066589920958207\n",
      "Iteration: 9948000, loss: 0.10617222078350304, gradient norm: 0.7914857959502817\n",
      "Iteration: 9949000, loss: 0.10617201647911949, gradient norm: 0.27623997682121193\n",
      "Iteration: 9950000, loss: 0.10617190225412054, gradient norm: 0.1472527273059353\n",
      "Iteration: 9951000, loss: 0.10617162319739179, gradient norm: 0.1814759095615667\n",
      "Iteration: 9952000, loss: 0.10617164712956365, gradient norm: 0.46400985546764184\n",
      "Iteration: 9953000, loss: 0.10617155189570593, gradient norm: 0.34098387790171714\n",
      "Iteration: 9954000, loss: 0.10617121062124894, gradient norm: 0.33931414735122417\n",
      "Iteration: 9955000, loss: 0.10617117083128057, gradient norm: 0.2932148953545242\n",
      "Iteration: 9956000, loss: 0.10617101560804393, gradient norm: 0.12993940939656828\n",
      "Iteration: 9957000, loss: 0.10617084870154926, gradient norm: 0.4801308010352947\n",
      "Iteration: 9958000, loss: 0.106170874545059, gradient norm: 0.24070891192337188\n",
      "Iteration: 9959000, loss: 0.10617052493887033, gradient norm: 0.023502279617625774\n",
      "Iteration: 9960000, loss: 0.10617052779442608, gradient norm: 0.199310659716556\n",
      "Iteration: 9961000, loss: 0.10617037091636669, gradient norm: 0.04525128532500291\n",
      "Iteration: 9962000, loss: 0.10617021587911263, gradient norm: 0.28955118758859666\n",
      "Iteration: 9963000, loss: 0.10616996299195154, gradient norm: 0.1739962163667473\n",
      "Iteration: 9964000, loss: 0.10616992340804543, gradient norm: 0.6878143842197177\n",
      "Iteration: 9965000, loss: 0.1061697904999578, gradient norm: 0.25178282351205633\n",
      "Iteration: 9966000, loss: 0.10616964607972106, gradient norm: 0.5357080886336691\n",
      "Iteration: 9967000, loss: 0.10616943754367766, gradient norm: 0.6525674550652508\n",
      "Iteration: 9968000, loss: 0.10616927525853408, gradient norm: 0.1418585780148622\n",
      "Iteration: 9969000, loss: 0.10616942272824385, gradient norm: 0.4747423250341592\n",
      "Iteration: 9970000, loss: 0.10616891434993875, gradient norm: 0.17480468634231214\n",
      "Iteration: 9971000, loss: 0.10616892644485418, gradient norm: 0.1373456997486372\n",
      "Iteration: 9972000, loss: 0.10616875632565399, gradient norm: 0.5038717872017459\n",
      "Iteration: 9973000, loss: 0.1061686608087152, gradient norm: 0.5606216682717979\n",
      "Iteration: 9974000, loss: 0.10616829215672484, gradient norm: 0.551970715932569\n",
      "Iteration: 9975000, loss: 0.10616852033517363, gradient norm: 0.050790808979695165\n",
      "Iteration: 9976000, loss: 0.10616821702970235, gradient norm: 0.15473628143507986\n",
      "Iteration: 9977000, loss: 0.10616800483889742, gradient norm: 0.3956073546767597\n",
      "Iteration: 9978000, loss: 0.1061680865702474, gradient norm: 0.2349967811741377\n",
      "Iteration: 9979000, loss: 0.1061676051819648, gradient norm: 0.15716931871279852\n",
      "Iteration: 9980000, loss: 0.10616766180877062, gradient norm: 0.2789646631340461\n",
      "Iteration: 9981000, loss: 0.10616739457206169, gradient norm: 0.13456722692738943\n",
      "Iteration: 9982000, loss: 0.10616746412376507, gradient norm: 0.8886803890128586\n",
      "Iteration: 9983000, loss: 0.10616724375923357, gradient norm: 0.1184957629838681\n",
      "Iteration: 9984000, loss: 0.10616703043879358, gradient norm: 0.6029107082640697\n",
      "Iteration: 9985000, loss: 0.10616703977960572, gradient norm: 0.36407015709114743\n",
      "Iteration: 9986000, loss: 0.10616667100557965, gradient norm: 0.23177606367166537\n",
      "Iteration: 9987000, loss: 0.10616696287644889, gradient norm: 0.6016587961397222\n",
      "Iteration: 9988000, loss: 0.10616637114356081, gradient norm: 0.1348756644105512\n",
      "Iteration: 9989000, loss: 0.10616616494292629, gradient norm: 0.07304477850978261\n",
      "Iteration: 9990000, loss: 0.10616647824027095, gradient norm: 0.9527478467250232\n",
      "Iteration: 9991000, loss: 0.10616590351272231, gradient norm: 0.5768210915541051\n",
      "Iteration: 9992000, loss: 0.10616599455280004, gradient norm: 0.3616276466630017\n",
      "Iteration: 9993000, loss: 0.10616585429317005, gradient norm: 0.2569305402132607\n",
      "Iteration: 9994000, loss: 0.10616562546684682, gradient norm: 0.18641460500885063\n",
      "Iteration: 9995000, loss: 0.10616556293122667, gradient norm: 0.1934721697792505\n",
      "Iteration: 9996000, loss: 0.10616538337623962, gradient norm: 1.3139652952330503\n",
      "Iteration: 9997000, loss: 0.10616532106949116, gradient norm: 0.3400828260202933\n",
      "Iteration: 9998000, loss: 0.10616505842704627, gradient norm: 0.21175537296154343\n",
      "Iteration: 9999000, loss: 0.10616490256050125, gradient norm: 0.1455785802945594\n",
      "Iteration: 10000000, loss: 0.1061648437732484, gradient norm: 0.23614903316201416\n",
      "Iteration: 10001000, loss: 0.1061646787287562, gradient norm: 0.4530411435432906\n",
      "Iteration: 10002000, loss: 0.10616450947236188, gradient norm: 0.08808968356859795\n",
      "Iteration: 10003000, loss: 0.10616451552842174, gradient norm: 0.30383229560593716\n",
      "Iteration: 10004000, loss: 0.10616416070446424, gradient norm: 0.183917127883234\n",
      "Iteration: 10005000, loss: 0.10616408810687201, gradient norm: 0.41215339764863146\n",
      "Iteration: 10006000, loss: 0.10616394665841236, gradient norm: 0.41187911704745633\n",
      "Iteration: 10007000, loss: 0.10616393456996619, gradient norm: 0.12084921271806366\n",
      "Iteration: 10008000, loss: 0.10616372505732155, gradient norm: 0.38111088223810746\n",
      "Iteration: 10009000, loss: 0.10616354155939783, gradient norm: 0.15214146900837783\n",
      "Iteration: 10010000, loss: 0.10616337061827551, gradient norm: 0.3178461935118672\n",
      "Iteration: 10011000, loss: 0.10616328774068215, gradient norm: 0.1872675330140298\n",
      "Iteration: 10012000, loss: 0.1061633625744559, gradient norm: 0.25487994416773835\n",
      "Iteration: 10013000, loss: 0.10616277229645792, gradient norm: 0.42043928936022323\n",
      "Iteration: 10014000, loss: 0.10616283295654783, gradient norm: 0.7395112372938708\n",
      "Iteration: 10015000, loss: 0.10616274980124488, gradient norm: 0.24708393861582623\n",
      "Iteration: 10016000, loss: 0.10616265135608516, gradient norm: 0.19001823339352175\n",
      "Iteration: 10017000, loss: 0.10616235102370365, gradient norm: 0.07070969104980408\n",
      "Iteration: 10018000, loss: 0.10616225614628069, gradient norm: 0.36739049441692134\n",
      "Iteration: 10019000, loss: 0.10616215326563337, gradient norm: 0.6468551717707236\n",
      "Iteration: 10020000, loss: 0.10616199241628768, gradient norm: 0.6665143853041618\n",
      "Iteration: 10021000, loss: 0.10616192780079464, gradient norm: 0.14488550395380922\n",
      "Iteration: 10022000, loss: 0.10616167455470167, gradient norm: 0.51421032027281\n",
      "Iteration: 10023000, loss: 0.10616171643411972, gradient norm: 0.31998352912279987\n",
      "Iteration: 10024000, loss: 0.10616137412232442, gradient norm: 0.11249422225825825\n",
      "Iteration: 10025000, loss: 0.1061613402940569, gradient norm: 0.6114898009632723\n",
      "Iteration: 10026000, loss: 0.10616115023812817, gradient norm: 0.2012172553667473\n",
      "Iteration: 10027000, loss: 0.10616102984922218, gradient norm: 0.6266156688555213\n",
      "Iteration: 10028000, loss: 0.10616082052977922, gradient norm: 0.69480640959505\n",
      "Iteration: 10029000, loss: 0.10616078523430582, gradient norm: 0.11851046683032705\n",
      "Iteration: 10030000, loss: 0.10616059130487064, gradient norm: 0.5891056849680798\n",
      "Iteration: 10031000, loss: 0.10616041548190523, gradient norm: 0.2544153792247343\n",
      "Iteration: 10032000, loss: 0.10616036645081024, gradient norm: 0.17268534940447938\n",
      "Iteration: 10033000, loss: 0.10616015652579838, gradient norm: 0.032083290111474\n",
      "Iteration: 10034000, loss: 0.10615998508777857, gradient norm: 0.23692228700862447\n",
      "Iteration: 10035000, loss: 0.10615991591016925, gradient norm: 0.801231664658601\n",
      "Iteration: 10036000, loss: 0.10615980508108978, gradient norm: 0.6582286743328188\n",
      "Iteration: 10037000, loss: 0.10615957448719499, gradient norm: 0.3974751100276731\n",
      "Iteration: 10038000, loss: 0.10615946325248456, gradient norm: 0.050150404329268775\n",
      "Iteration: 10039000, loss: 0.1061594024325453, gradient norm: 0.028179549933580468\n",
      "Iteration: 10040000, loss: 0.10615921319566855, gradient norm: 0.4901460334036805\n",
      "Iteration: 10041000, loss: 0.10615909290766029, gradient norm: 0.12781931323749204\n",
      "Iteration: 10042000, loss: 0.10615886110260545, gradient norm: 0.508882659972884\n",
      "Iteration: 10043000, loss: 0.10615880690185242, gradient norm: 0.5317734767833385\n",
      "Iteration: 10044000, loss: 0.10615868963694794, gradient norm: 0.30468637036367596\n",
      "Iteration: 10045000, loss: 0.10615853793761182, gradient norm: 0.4949785204328253\n",
      "Iteration: 10046000, loss: 0.10615828665780812, gradient norm: 0.36128968423817925\n",
      "Iteration: 10047000, loss: 0.106158531936579, gradient norm: 0.036209863677056434\n",
      "Iteration: 10048000, loss: 0.1061578910454323, gradient norm: 1.0106199552174655\n",
      "Iteration: 10049000, loss: 0.10615793417226489, gradient norm: 0.49112706088445884\n",
      "Iteration: 10050000, loss: 0.10615774553695954, gradient norm: 0.05627376385146525\n",
      "Iteration: 10051000, loss: 0.10615768133706653, gradient norm: 0.5018044082396255\n",
      "Iteration: 10052000, loss: 0.10615754237485611, gradient norm: 0.258751267326816\n",
      "Iteration: 10053000, loss: 0.10615745340044642, gradient norm: 0.04802991896480554\n",
      "Iteration: 10054000, loss: 0.10615719964047429, gradient norm: 0.5352464522026609\n",
      "Iteration: 10055000, loss: 0.10615708586468718, gradient norm: 0.34214439394967694\n",
      "Iteration: 10056000, loss: 0.10615691548581435, gradient norm: 0.11878309654631343\n",
      "Iteration: 10057000, loss: 0.1061569882122242, gradient norm: 0.1407204402616942\n",
      "Iteration: 10058000, loss: 0.1061565641637865, gradient norm: 0.12563853310578726\n",
      "Iteration: 10059000, loss: 0.10615666313407614, gradient norm: 0.2824002362767314\n",
      "Iteration: 10060000, loss: 0.1061563920201811, gradient norm: 0.842153414345526\n",
      "Iteration: 10061000, loss: 0.10615625605297684, gradient norm: 0.03362947854566955\n",
      "Iteration: 10062000, loss: 0.1061561355741619, gradient norm: 0.11541488946167126\n",
      "Iteration: 10063000, loss: 0.1061559863639353, gradient norm: 0.24614375042355388\n",
      "Iteration: 10064000, loss: 0.10615595907784738, gradient norm: 0.8044858477392184\n",
      "Iteration: 10065000, loss: 0.10615559548691891, gradient norm: 0.17539430950838406\n",
      "Iteration: 10066000, loss: 0.10615567626958036, gradient norm: 0.4085911770541947\n",
      "Iteration: 10067000, loss: 0.1061552611754457, gradient norm: 0.3924387069035779\n",
      "Iteration: 10068000, loss: 0.10615533215440141, gradient norm: 0.3765989430506433\n",
      "Iteration: 10069000, loss: 0.1061550652347261, gradient norm: 0.275170962826309\n",
      "Iteration: 10070000, loss: 0.10615516962881526, gradient norm: 0.20545370825613343\n",
      "Iteration: 10071000, loss: 0.10615479430614351, gradient norm: 0.40702811621972923\n",
      "Iteration: 10072000, loss: 0.10615466735546701, gradient norm: 0.6354074187972628\n",
      "Iteration: 10073000, loss: 0.10615463210573119, gradient norm: 0.36946839846622986\n",
      "Iteration: 10074000, loss: 0.10615443484816298, gradient norm: 0.12622510839433476\n",
      "Iteration: 10075000, loss: 0.1061543421822821, gradient norm: 0.47430634552085205\n",
      "Iteration: 10076000, loss: 0.10615414921727195, gradient norm: 0.17763645256221589\n",
      "Iteration: 10077000, loss: 0.10615400827831703, gradient norm: 0.18669643201144698\n",
      "Iteration: 10078000, loss: 0.10615398780818984, gradient norm: 0.22027996432735053\n",
      "Iteration: 10079000, loss: 0.10615357430388268, gradient norm: 0.3115547549451981\n",
      "Iteration: 10080000, loss: 0.10615377165220272, gradient norm: 0.07327170427505804\n",
      "Iteration: 10081000, loss: 0.10615341380402844, gradient norm: 0.3158749456511852\n",
      "Iteration: 10082000, loss: 0.1061533980535126, gradient norm: 0.1811572159144422\n",
      "Iteration: 10083000, loss: 0.10615315067145481, gradient norm: 0.23516270873886227\n",
      "Iteration: 10084000, loss: 0.10615299951127895, gradient norm: 0.3461197394973295\n",
      "Iteration: 10085000, loss: 0.10615309555785377, gradient norm: 0.3196124288582312\n",
      "Iteration: 10086000, loss: 0.10615256879607543, gradient norm: 0.502889933082203\n",
      "Iteration: 10087000, loss: 0.10615264582180277, gradient norm: 0.11866338070941163\n",
      "Iteration: 10088000, loss: 0.10615257936163036, gradient norm: 0.7891559028146081\n",
      "Iteration: 10089000, loss: 0.10615238768178464, gradient norm: 0.27954074200814594\n",
      "Iteration: 10090000, loss: 0.10615225576005505, gradient norm: 0.6118423944215557\n",
      "Iteration: 10091000, loss: 0.10615214575434753, gradient norm: 0.4014574435793656\n",
      "Iteration: 10092000, loss: 0.10615185435190327, gradient norm: 0.08796632449903392\n",
      "Iteration: 10093000, loss: 0.10615186762723709, gradient norm: 0.10822373871241646\n",
      "Iteration: 10094000, loss: 0.10615172180402152, gradient norm: 0.16726598055123415\n",
      "Iteration: 10095000, loss: 0.10615140878777612, gradient norm: 0.23323155873742651\n",
      "Iteration: 10096000, loss: 0.10615140965958328, gradient norm: 1.0364301523688193\n",
      "Iteration: 10097000, loss: 0.10615127262257901, gradient norm: 0.2090798652120265\n",
      "Iteration: 10098000, loss: 0.10615112017898139, gradient norm: 0.12803068178182733\n",
      "Iteration: 10099000, loss: 0.10615089434593364, gradient norm: 0.3133737346700637\n",
      "Iteration: 10100000, loss: 0.10615088653285858, gradient norm: 0.09083689209303761\n",
      "Iteration: 10101000, loss: 0.10615077038413179, gradient norm: 0.5805288171973598\n",
      "Iteration: 10102000, loss: 0.1061505154689013, gradient norm: 0.3217670556340837\n",
      "Iteration: 10103000, loss: 0.1061504969375596, gradient norm: 0.2772421383799278\n",
      "Iteration: 10104000, loss: 0.10615018652672964, gradient norm: 0.26636054839816703\n",
      "Iteration: 10105000, loss: 0.1061501341416189, gradient norm: 0.7884583198929759\n",
      "Iteration: 10106000, loss: 0.10615002033716647, gradient norm: 0.6194400936994753\n",
      "Iteration: 10107000, loss: 0.10614991520636215, gradient norm: 0.5390206662423852\n",
      "Iteration: 10108000, loss: 0.10614967516431273, gradient norm: 0.6900115099085771\n",
      "Iteration: 10109000, loss: 0.10614967925739842, gradient norm: 0.3701472724600226\n",
      "Iteration: 10110000, loss: 0.10614933858788254, gradient norm: 0.26518615628853\n",
      "Iteration: 10111000, loss: 0.1061493232412545, gradient norm: 0.13332703513603678\n",
      "Iteration: 10112000, loss: 0.1061492563695895, gradient norm: 0.050254931401725766\n",
      "Iteration: 10113000, loss: 0.10614897313250488, gradient norm: 0.20644568055626866\n",
      "Iteration: 10114000, loss: 0.10614895168980988, gradient norm: 0.6715992186970597\n",
      "Iteration: 10115000, loss: 0.10614879191029175, gradient norm: 0.4090471221651459\n",
      "Iteration: 10116000, loss: 0.10614851114288092, gradient norm: 0.2986233818133686\n",
      "Iteration: 10117000, loss: 0.10614844557039602, gradient norm: 1.0636996795962435\n",
      "Iteration: 10118000, loss: 0.10614829665448548, gradient norm: 0.23024948212590757\n",
      "Iteration: 10119000, loss: 0.1061482567244891, gradient norm: 0.2342170917236212\n",
      "Iteration: 10120000, loss: 0.10614820469064253, gradient norm: 0.4846784395951327\n",
      "Iteration: 10121000, loss: 0.10614787023434691, gradient norm: 0.2649942841801925\n",
      "Iteration: 10122000, loss: 0.10614771647454865, gradient norm: 0.39464305967172914\n",
      "Iteration: 10123000, loss: 0.1061476336301084, gradient norm: 0.21762068959510178\n",
      "Iteration: 10124000, loss: 0.10614742734314064, gradient norm: 0.3574860604470556\n",
      "Iteration: 10125000, loss: 0.10614741674741032, gradient norm: 0.48623301979442757\n",
      "Iteration: 10126000, loss: 0.10614729006392364, gradient norm: 0.12544213917122016\n",
      "Iteration: 10127000, loss: 0.10614728947883577, gradient norm: 0.3833897432672069\n",
      "Iteration: 10128000, loss: 0.10614679855557886, gradient norm: 0.5766418916482079\n",
      "Iteration: 10129000, loss: 0.10614684154531819, gradient norm: 0.03589837195888595\n",
      "Iteration: 10130000, loss: 0.10614658554204161, gradient norm: 0.028727788033997607\n",
      "Iteration: 10131000, loss: 0.10614663076010526, gradient norm: 0.2742584846944518\n",
      "Iteration: 10132000, loss: 0.10614630119835063, gradient norm: 0.5839416258732181\n",
      "Iteration: 10133000, loss: 0.10614634291286909, gradient norm: 0.16389293065758223\n",
      "Iteration: 10134000, loss: 0.1061461761001997, gradient norm: 0.3622246391342602\n",
      "Iteration: 10135000, loss: 0.10614611142790198, gradient norm: 0.08009361327947007\n",
      "Iteration: 10136000, loss: 0.1061457915480131, gradient norm: 0.4709615932152856\n",
      "Iteration: 10137000, loss: 0.10614581795511059, gradient norm: 0.3539373427483024\n",
      "Iteration: 10138000, loss: 0.1061454497185927, gradient norm: 0.29971467441669886\n",
      "Iteration: 10139000, loss: 0.10614536819276668, gradient norm: 0.3715433608508687\n",
      "Iteration: 10140000, loss: 0.10614531168410411, gradient norm: 0.4845280305737333\n",
      "Iteration: 10141000, loss: 0.10614511805775782, gradient norm: 0.14366735300476896\n",
      "Iteration: 10142000, loss: 0.10614530697067563, gradient norm: 0.47906261300310937\n",
      "Iteration: 10143000, loss: 0.10614476702878689, gradient norm: 0.032197830209868064\n",
      "Iteration: 10144000, loss: 0.10614460846103374, gradient norm: 0.3293313363225258\n",
      "Iteration: 10145000, loss: 0.10614473537680755, gradient norm: 0.20465296854315768\n",
      "Iteration: 10146000, loss: 0.10614454058350332, gradient norm: 0.32193851302255594\n",
      "Iteration: 10147000, loss: 0.10614422712649542, gradient norm: 0.1863356542394393\n",
      "Iteration: 10148000, loss: 0.10614411763854713, gradient norm: 0.34393241212645914\n",
      "Iteration: 10149000, loss: 0.1061441679375468, gradient norm: 0.05650917874531465\n",
      "Iteration: 10150000, loss: 0.10614391445065434, gradient norm: 0.16319706415085183\n",
      "Iteration: 10151000, loss: 0.10614374494125889, gradient norm: 0.16708665668909284\n",
      "Iteration: 10152000, loss: 0.10614358800544024, gradient norm: 0.9439379590134764\n",
      "Iteration: 10153000, loss: 0.10614367095188479, gradient norm: 0.2299262199995108\n",
      "Iteration: 10154000, loss: 0.10614331710293054, gradient norm: 0.6030879890949378\n",
      "Iteration: 10155000, loss: 0.10614311022984495, gradient norm: 0.26690607696970897\n",
      "Iteration: 10156000, loss: 0.10614332240185416, gradient norm: 0.4154801925211565\n",
      "Iteration: 10157000, loss: 0.10614287753255723, gradient norm: 0.08393268149787543\n",
      "Iteration: 10158000, loss: 0.10614273550344816, gradient norm: 0.21125408394495748\n",
      "Iteration: 10159000, loss: 0.10614267635636475, gradient norm: 0.7617437050145155\n",
      "Iteration: 10160000, loss: 0.10614262746056902, gradient norm: 0.6041149264398658\n",
      "Iteration: 10161000, loss: 0.1061423235418173, gradient norm: 0.3225461422746326\n",
      "Iteration: 10162000, loss: 0.10614229361889124, gradient norm: 0.28279917607099014\n",
      "Iteration: 10163000, loss: 0.10614217345013491, gradient norm: 0.24302254812118873\n",
      "Iteration: 10164000, loss: 0.1061419518345313, gradient norm: 0.23728210066552044\n",
      "Iteration: 10165000, loss: 0.10614181639803616, gradient norm: 0.4233041095932184\n",
      "Iteration: 10166000, loss: 0.10614166730309553, gradient norm: 0.11260491501644726\n",
      "Iteration: 10167000, loss: 0.10614157725053827, gradient norm: 0.6346985480749835\n",
      "Iteration: 10168000, loss: 0.10614153499705663, gradient norm: 0.14646457959910825\n",
      "Iteration: 10169000, loss: 0.10614136233335074, gradient norm: 0.4182641935390048\n",
      "Iteration: 10170000, loss: 0.1061409931309553, gradient norm: 0.8342900313187841\n",
      "Iteration: 10171000, loss: 0.10614123714873719, gradient norm: 0.3225832663962736\n",
      "Iteration: 10172000, loss: 0.10614073446240273, gradient norm: 0.11257245077451468\n",
      "Iteration: 10173000, loss: 0.10614081050922197, gradient norm: 0.6729069001151115\n",
      "Iteration: 10174000, loss: 0.10614065247579754, gradient norm: 0.09506880609915565\n",
      "Iteration: 10175000, loss: 0.10614046226381715, gradient norm: 0.3240876405854418\n",
      "Iteration: 10176000, loss: 0.10614052855603745, gradient norm: 0.5812359599545904\n",
      "Iteration: 10177000, loss: 0.10614007262352387, gradient norm: 0.14630014733183747\n",
      "Iteration: 10178000, loss: 0.10614001597777292, gradient norm: 0.16134330691677515\n",
      "Iteration: 10179000, loss: 0.10613996201252965, gradient norm: 0.17674364750450072\n",
      "Iteration: 10180000, loss: 0.10613974395600694, gradient norm: 0.3698434975780714\n",
      "Iteration: 10181000, loss: 0.10613962228749045, gradient norm: 0.25845985870183635\n",
      "Iteration: 10182000, loss: 0.10613978582572108, gradient norm: 0.16607930947410032\n",
      "Iteration: 10183000, loss: 0.1061392946084053, gradient norm: 0.631834093942776\n",
      "Iteration: 10184000, loss: 0.10613924971025761, gradient norm: 0.3307750376578208\n",
      "Iteration: 10185000, loss: 0.10613908143725109, gradient norm: 0.2965992003914733\n",
      "Iteration: 10186000, loss: 0.10613888824642788, gradient norm: 0.08914420673995227\n",
      "Iteration: 10187000, loss: 0.10613888810719704, gradient norm: 0.39795910796247747\n",
      "Iteration: 10188000, loss: 0.10613876314877049, gradient norm: 0.13731786618263042\n",
      "Iteration: 10189000, loss: 0.10613855147898976, gradient norm: 0.24379152451454328\n",
      "Iteration: 10190000, loss: 0.10613843254782683, gradient norm: 0.1609709306418143\n",
      "Iteration: 10191000, loss: 0.10613815121310859, gradient norm: 0.3515466174600342\n",
      "Iteration: 10192000, loss: 0.10613824475817916, gradient norm: 0.2023092014396528\n",
      "Iteration: 10193000, loss: 0.10613788835386886, gradient norm: 0.3501312360474576\n",
      "Iteration: 10194000, loss: 0.10613790123559388, gradient norm: 0.751586419280947\n",
      "Iteration: 10195000, loss: 0.10613792105391222, gradient norm: 0.7065235986819066\n",
      "Iteration: 10196000, loss: 0.106137533822926, gradient norm: 0.12188534869893453\n",
      "Iteration: 10197000, loss: 0.10613740868780129, gradient norm: 0.876316355054087\n",
      "Iteration: 10198000, loss: 0.10613735127529458, gradient norm: 0.6920121376634276\n",
      "Iteration: 10199000, loss: 0.10613716993305826, gradient norm: 0.45415788021495457\n",
      "Iteration: 10200000, loss: 0.10613710892984717, gradient norm: 0.6242107650369422\n",
      "Iteration: 10201000, loss: 0.10613684028831455, gradient norm: 0.498272646698043\n",
      "Iteration: 10202000, loss: 0.1061367346389294, gradient norm: 0.315414887704306\n",
      "Iteration: 10203000, loss: 0.10613672225519327, gradient norm: 0.456349593916616\n",
      "Iteration: 10204000, loss: 0.10613644124464199, gradient norm: 0.2858146502028284\n",
      "Iteration: 10205000, loss: 0.10613645220590014, gradient norm: 0.10620631198726264\n",
      "Iteration: 10206000, loss: 0.10613617573514021, gradient norm: 0.02229115665506828\n",
      "Iteration: 10207000, loss: 0.10613605387682035, gradient norm: 0.6294016183906869\n",
      "Iteration: 10208000, loss: 0.10613594482781297, gradient norm: 0.12520656930903531\n",
      "Iteration: 10209000, loss: 0.10613589204801907, gradient norm: 0.5151199308973213\n",
      "Iteration: 10210000, loss: 0.10613567919435454, gradient norm: 0.5331682745793741\n",
      "Iteration: 10211000, loss: 0.10613553997863201, gradient norm: 0.5379718723309647\n",
      "Iteration: 10212000, loss: 0.10613541691991145, gradient norm: 0.4464760269222557\n",
      "Iteration: 10213000, loss: 0.10613541406881725, gradient norm: 0.11636977265059269\n",
      "Iteration: 10214000, loss: 0.10613501682025449, gradient norm: 0.21073091261072846\n",
      "Iteration: 10215000, loss: 0.10613501197075559, gradient norm: 0.4022877476314423\n",
      "Iteration: 10216000, loss: 0.1061349066630615, gradient norm: 0.300625073176138\n",
      "Iteration: 10217000, loss: 0.10613478457973537, gradient norm: 0.25436568704197\n",
      "Iteration: 10218000, loss: 0.10613450981760014, gradient norm: 0.5399799182995022\n",
      "Iteration: 10219000, loss: 0.10613457249115, gradient norm: 0.4699455995354896\n",
      "Iteration: 10220000, loss: 0.10613424548765832, gradient norm: 0.3499046523278537\n",
      "Iteration: 10221000, loss: 0.10613421733244542, gradient norm: 0.3688214110866011\n",
      "Iteration: 10222000, loss: 0.10613399027816817, gradient norm: 0.16880323005736217\n",
      "Iteration: 10223000, loss: 0.10613387793774173, gradient norm: 0.13570158444221686\n",
      "Iteration: 10224000, loss: 0.10613378282656342, gradient norm: 0.3910834258966116\n",
      "Iteration: 10225000, loss: 0.10613355852186743, gradient norm: 0.11327647466825906\n",
      "Iteration: 10226000, loss: 0.10613364724734568, gradient norm: 0.1369644824512733\n",
      "Iteration: 10227000, loss: 0.10613341672522132, gradient norm: 0.4892222051332726\n",
      "Iteration: 10228000, loss: 0.10613319730252155, gradient norm: 0.2716652435993242\n",
      "Iteration: 10229000, loss: 0.10613310338213301, gradient norm: 0.3366728904130433\n",
      "Iteration: 10230000, loss: 0.10613286379450768, gradient norm: 0.506296405100068\n",
      "Iteration: 10231000, loss: 0.10613291783606467, gradient norm: 0.19308672597122128\n",
      "Iteration: 10232000, loss: 0.10613265226693996, gradient norm: 0.7945935644957371\n",
      "Iteration: 10233000, loss: 0.10613261567275671, gradient norm: 0.36703354689224577\n",
      "Iteration: 10234000, loss: 0.10613235599501007, gradient norm: 0.31492070993703425\n",
      "Iteration: 10235000, loss: 0.10613229739393613, gradient norm: 0.33863758367325203\n",
      "Iteration: 10236000, loss: 0.1061322274035897, gradient norm: 0.24223101901080415\n",
      "Iteration: 10237000, loss: 0.10613190443019374, gradient norm: 0.0827397672413659\n",
      "Iteration: 10238000, loss: 0.10613194510474196, gradient norm: 0.46827018318081587\n",
      "Iteration: 10239000, loss: 0.10613169387196457, gradient norm: 0.3345236046588998\n",
      "Iteration: 10240000, loss: 0.10613151179808675, gradient norm: 0.6699389874301449\n",
      "Iteration: 10241000, loss: 0.10613153505753789, gradient norm: 0.8217165434663616\n",
      "Iteration: 10242000, loss: 0.10613133713532816, gradient norm: 0.3556441475738426\n",
      "Iteration: 10243000, loss: 0.10613124860499616, gradient norm: 0.27231761548444455\n",
      "Iteration: 10244000, loss: 0.10613113053354471, gradient norm: 0.9212203357715937\n",
      "Iteration: 10245000, loss: 0.10613082015685787, gradient norm: 0.8406129752485009\n",
      "Iteration: 10246000, loss: 0.10613078509639769, gradient norm: 1.0720380735087494\n",
      "Iteration: 10247000, loss: 0.10613065461461699, gradient norm: 0.06866504089486296\n",
      "Iteration: 10248000, loss: 0.10613044746270855, gradient norm: 0.13154709267632472\n",
      "Iteration: 10249000, loss: 0.10613043095391603, gradient norm: 0.451057019881733\n",
      "Iteration: 10250000, loss: 0.10613026388156598, gradient norm: 0.5084757089866099\n",
      "Iteration: 10251000, loss: 0.10613006882663784, gradient norm: 0.19197100444887757\n",
      "Iteration: 10252000, loss: 0.10612988859586911, gradient norm: 0.3924405566887709\n",
      "Iteration: 10253000, loss: 0.10612999280308626, gradient norm: 0.08849663294127093\n",
      "Iteration: 10254000, loss: 0.10612962287326469, gradient norm: 0.6361015729610181\n",
      "Iteration: 10255000, loss: 0.10612964152698529, gradient norm: 0.055912409886981776\n",
      "Iteration: 10256000, loss: 0.10612937722945483, gradient norm: 0.46082064629095953\n",
      "Iteration: 10257000, loss: 0.10612931766459079, gradient norm: 0.3058011770280186\n",
      "Iteration: 10258000, loss: 0.10612907879576243, gradient norm: 0.14010190998247246\n",
      "Iteration: 10259000, loss: 0.10612914719602455, gradient norm: 0.5113452105947236\n",
      "Iteration: 10260000, loss: 0.10612876603520457, gradient norm: 0.5067724017893551\n",
      "Iteration: 10261000, loss: 0.10612874362801415, gradient norm: 0.5410273090074551\n",
      "Iteration: 10262000, loss: 0.10612865989653486, gradient norm: 0.5964016075142375\n",
      "Iteration: 10263000, loss: 0.10612857124077067, gradient norm: 0.1698022165029999\n",
      "Iteration: 10264000, loss: 0.10612819338814773, gradient norm: 0.06907319151520325\n",
      "Iteration: 10265000, loss: 0.10612826653306745, gradient norm: 0.601309419134948\n",
      "Iteration: 10266000, loss: 0.10612805076100887, gradient norm: 0.6119413780299927\n",
      "Iteration: 10267000, loss: 0.10612790004107106, gradient norm: 0.47641564083169874\n",
      "Iteration: 10268000, loss: 0.10612768712242945, gradient norm: 0.19834477660835576\n",
      "Iteration: 10269000, loss: 0.10612780460315122, gradient norm: 0.5682216695156026\n",
      "Iteration: 10270000, loss: 0.10612753803152382, gradient norm: 0.4708530840916553\n",
      "Iteration: 10271000, loss: 0.10612734473483304, gradient norm: 0.8453209439398797\n",
      "Iteration: 10272000, loss: 0.10612723240453575, gradient norm: 0.47917703158422936\n",
      "Iteration: 10273000, loss: 0.10612729313393118, gradient norm: 0.08590268696072065\n",
      "Iteration: 10274000, loss: 0.10612692400995714, gradient norm: 0.10155933446558413\n",
      "Iteration: 10275000, loss: 0.10612693276277475, gradient norm: 1.0275745951930932\n",
      "Iteration: 10276000, loss: 0.10612663438250042, gradient norm: 0.1679043967375051\n",
      "Iteration: 10277000, loss: 0.10612649289050978, gradient norm: 0.11994071834242705\n",
      "Iteration: 10278000, loss: 0.10612638468718776, gradient norm: 0.6841927115289623\n",
      "Iteration: 10279000, loss: 0.10612634899626416, gradient norm: 0.15783369104164405\n",
      "Iteration: 10280000, loss: 0.10612616439854977, gradient norm: 0.2584148994562194\n",
      "Iteration: 10281000, loss: 0.10612614946587942, gradient norm: 0.8348639544000569\n",
      "Iteration: 10282000, loss: 0.10612581872301445, gradient norm: 0.5617196522585158\n",
      "Iteration: 10283000, loss: 0.10612571752023865, gradient norm: 0.30263415368675317\n",
      "Iteration: 10284000, loss: 0.1061256154970751, gradient norm: 0.08970925778376526\n",
      "Iteration: 10285000, loss: 0.10612544833517487, gradient norm: 0.05119151439984148\n",
      "Iteration: 10286000, loss: 0.10612544768151273, gradient norm: 0.38112656264051276\n",
      "Iteration: 10287000, loss: 0.10612531847003771, gradient norm: 0.2877023893855149\n",
      "Iteration: 10288000, loss: 0.10612492374680965, gradient norm: 0.22742982625894279\n",
      "Iteration: 10289000, loss: 0.10612502676985251, gradient norm: 0.22410712250586237\n",
      "Iteration: 10290000, loss: 0.10612471114593325, gradient norm: 0.2766367275245406\n",
      "Iteration: 10291000, loss: 0.10612467139513959, gradient norm: 0.8298830411732241\n",
      "Iteration: 10292000, loss: 0.10612457738690666, gradient norm: 0.5476749044330756\n",
      "Iteration: 10293000, loss: 0.10612454068047472, gradient norm: 0.5284297276531473\n",
      "Iteration: 10294000, loss: 0.10612431593832261, gradient norm: 0.05658863707761269\n",
      "Iteration: 10295000, loss: 0.10612403499377496, gradient norm: 0.49564813589080875\n",
      "Iteration: 10296000, loss: 0.1061240863198711, gradient norm: 0.27771115151964165\n",
      "Iteration: 10297000, loss: 0.10612384697900985, gradient norm: 0.37156975342588616\n",
      "Iteration: 10298000, loss: 0.10612371635713641, gradient norm: 0.23949449350330917\n",
      "Iteration: 10299000, loss: 0.10612360447411906, gradient norm: 0.08082115847811293\n",
      "Iteration: 10300000, loss: 0.1061236508351441, gradient norm: 0.33638811365609333\n",
      "Iteration: 10301000, loss: 0.10612314116385838, gradient norm: 0.0946350955607211\n",
      "Iteration: 10302000, loss: 0.10612320358555834, gradient norm: 0.38614495958242945\n",
      "Iteration: 10303000, loss: 0.10612299735287327, gradient norm: 0.45781803754497724\n",
      "Iteration: 10304000, loss: 0.10612289908606745, gradient norm: 0.2315472297104656\n",
      "Iteration: 10305000, loss: 0.10612285411707156, gradient norm: 0.4991788475424917\n",
      "Iteration: 10306000, loss: 0.10612271133373873, gradient norm: 0.2733389321692781\n",
      "Iteration: 10307000, loss: 0.10612268098973489, gradient norm: 0.15914522040132445\n",
      "Iteration: 10308000, loss: 0.10612221611067213, gradient norm: 0.13886486437803136\n",
      "Iteration: 10309000, loss: 0.10612217359901735, gradient norm: 0.12322946429181991\n",
      "Iteration: 10310000, loss: 0.10612220369032839, gradient norm: 0.19802120066933038\n",
      "Iteration: 10311000, loss: 0.10612209763947777, gradient norm: 0.888856019417815\n",
      "Iteration: 10312000, loss: 0.10612183506953568, gradient norm: 0.18626878686608933\n",
      "Iteration: 10313000, loss: 0.10612181832781051, gradient norm: 0.15100099870607595\n",
      "Iteration: 10314000, loss: 0.1061214180879644, gradient norm: 0.10682832497923463\n",
      "Iteration: 10315000, loss: 0.10612144558874183, gradient norm: 0.14753539379705535\n",
      "Iteration: 10316000, loss: 0.10612137025921373, gradient norm: 0.17495374433530755\n",
      "Iteration: 10317000, loss: 0.10612113386409786, gradient norm: 0.2385294477212827\n",
      "Iteration: 10318000, loss: 0.1061210828225373, gradient norm: 0.19683806102041063\n",
      "Iteration: 10319000, loss: 0.10612097827610467, gradient norm: 0.28789884306585345\n",
      "Iteration: 10320000, loss: 0.10612065587941807, gradient norm: 0.19599121907753786\n",
      "Iteration: 10321000, loss: 0.1061206822771544, gradient norm: 0.5153194358555128\n",
      "Iteration: 10322000, loss: 0.10612056122166771, gradient norm: 0.11817013205269813\n",
      "Iteration: 10323000, loss: 0.10612036721805916, gradient norm: 0.32206543753948735\n",
      "Iteration: 10324000, loss: 0.10612024477698681, gradient norm: 0.2663458748288799\n",
      "Iteration: 10325000, loss: 0.10612002004704485, gradient norm: 0.33051484019953037\n",
      "Iteration: 10326000, loss: 0.10612018825960391, gradient norm: 0.6145461162145526\n",
      "Iteration: 10327000, loss: 0.10611970256028413, gradient norm: 0.23705704272310224\n",
      "Iteration: 10328000, loss: 0.10611966964320076, gradient norm: 0.507059587500441\n",
      "Iteration: 10329000, loss: 0.10611947606770375, gradient norm: 0.22020703403531625\n",
      "Iteration: 10330000, loss: 0.1061194996363596, gradient norm: 0.5105029168691746\n",
      "Iteration: 10331000, loss: 0.10611938242264739, gradient norm: 0.36695369980368736\n",
      "Iteration: 10332000, loss: 0.10611905559825689, gradient norm: 0.5281953620325122\n",
      "Iteration: 10333000, loss: 0.10611909153020667, gradient norm: 0.7912034412132104\n",
      "Iteration: 10334000, loss: 0.1061189923325397, gradient norm: 0.9655571545555267\n",
      "Iteration: 10335000, loss: 0.10611865627083523, gradient norm: 0.08471374395128145\n",
      "Iteration: 10336000, loss: 0.10611859608111113, gradient norm: 0.6649544366937732\n",
      "Iteration: 10337000, loss: 0.10611852395060847, gradient norm: 0.23296683954747607\n",
      "Iteration: 10338000, loss: 0.10611848642958177, gradient norm: 0.4682430759644734\n",
      "Iteration: 10339000, loss: 0.1061181589940928, gradient norm: 0.23963831751082817\n",
      "Iteration: 10340000, loss: 0.10611812487255685, gradient norm: 0.7443945106948945\n",
      "Iteration: 10341000, loss: 0.1061179733179965, gradient norm: 0.39387097873567467\n",
      "Iteration: 10342000, loss: 0.10611770941023091, gradient norm: 0.15456664260640018\n",
      "Iteration: 10343000, loss: 0.10611764559903457, gradient norm: 0.1574806872240835\n",
      "Iteration: 10344000, loss: 0.10611773910174208, gradient norm: 0.44357217558835355\n",
      "Iteration: 10345000, loss: 0.10611736972478582, gradient norm: 0.39793344417661636\n",
      "Iteration: 10346000, loss: 0.10611733438513352, gradient norm: 0.4397445019185639\n",
      "Iteration: 10347000, loss: 0.10611704091553215, gradient norm: 0.09571877329665991\n",
      "Iteration: 10348000, loss: 0.10611698385427457, gradient norm: 0.7297995597254414\n",
      "Iteration: 10349000, loss: 0.10611686143254404, gradient norm: 0.13002352258334207\n",
      "Iteration: 10350000, loss: 0.1061168025533415, gradient norm: 0.20737342991194133\n",
      "Iteration: 10351000, loss: 0.10611668922468792, gradient norm: 0.17841050496941502\n",
      "Iteration: 10352000, loss: 0.10611631172311607, gradient norm: 0.6302028328825381\n",
      "Iteration: 10353000, loss: 0.1061164817976217, gradient norm: 0.1802571671631793\n",
      "Iteration: 10354000, loss: 0.10611612153069556, gradient norm: 0.4081619995836142\n",
      "Iteration: 10355000, loss: 0.10611608622037741, gradient norm: 0.5935100711657688\n",
      "Iteration: 10356000, loss: 0.10611601314020878, gradient norm: 0.27351435466519897\n",
      "Iteration: 10357000, loss: 0.10611582838454188, gradient norm: 0.26749968054055434\n",
      "Iteration: 10358000, loss: 0.10611563922628418, gradient norm: 0.11529166397723992\n",
      "Iteration: 10359000, loss: 0.10611560789709926, gradient norm: 0.07126350594599\n",
      "Iteration: 10360000, loss: 0.10611533011130543, gradient norm: 0.21455868590077232\n",
      "Iteration: 10361000, loss: 0.10611526770989194, gradient norm: 0.5831324156562628\n",
      "Iteration: 10362000, loss: 0.10611521990358276, gradient norm: 0.18307729596947211\n",
      "Iteration: 10363000, loss: 0.1061151450356549, gradient norm: 0.08257968985030839\n",
      "Iteration: 10364000, loss: 0.10611472386507714, gradient norm: 0.34762365316802896\n",
      "Iteration: 10365000, loss: 0.10611489600346798, gradient norm: 0.30021945514818665\n",
      "Iteration: 10366000, loss: 0.1061144604358922, gradient norm: 0.33493812932078276\n",
      "Iteration: 10367000, loss: 0.10611452603137779, gradient norm: 0.03854471072121912\n",
      "Iteration: 10368000, loss: 0.10611424452282238, gradient norm: 0.5622806006441752\n",
      "Iteration: 10369000, loss: 0.10611425954158943, gradient norm: 0.1942829842790841\n",
      "Iteration: 10370000, loss: 0.10611399842787739, gradient norm: 0.45606422946180813\n",
      "Iteration: 10371000, loss: 0.10611393787941444, gradient norm: 0.0829599073703088\n",
      "Iteration: 10372000, loss: 0.10611396646872356, gradient norm: 0.22332732994744672\n",
      "Iteration: 10373000, loss: 0.10611360966670302, gradient norm: 0.06856965940359491\n",
      "Iteration: 10374000, loss: 0.10611348410561576, gradient norm: 0.1572056330357766\n",
      "Iteration: 10375000, loss: 0.10611336254836544, gradient norm: 0.5474736054043667\n",
      "Iteration: 10376000, loss: 0.10611351545304357, gradient norm: 0.21979524611611667\n",
      "Iteration: 10377000, loss: 0.10611309680998285, gradient norm: 0.5501687517579537\n",
      "Iteration: 10378000, loss: 0.10611299486886774, gradient norm: 0.4514815051826458\n",
      "Iteration: 10379000, loss: 0.10611290242548913, gradient norm: 0.15841821813377555\n",
      "Iteration: 10380000, loss: 0.10611276244019123, gradient norm: 0.5667946953254429\n",
      "Iteration: 10381000, loss: 0.10611261378771045, gradient norm: 0.10449930366346219\n",
      "Iteration: 10382000, loss: 0.10611240460991621, gradient norm: 0.9489472562787339\n",
      "Iteration: 10383000, loss: 0.10611231570280999, gradient norm: 0.40659552807141286\n",
      "Iteration: 10384000, loss: 0.10611231507394905, gradient norm: 0.181618700022295\n",
      "Iteration: 10385000, loss: 0.106111898423474, gradient norm: 0.08642170886589237\n",
      "Iteration: 10386000, loss: 0.10611200626819703, gradient norm: 0.6617964453302755\n",
      "Iteration: 10387000, loss: 0.10611184902841767, gradient norm: 0.25122715720740946\n",
      "Iteration: 10388000, loss: 0.10611153652664111, gradient norm: 0.1590689698617883\n",
      "Iteration: 10389000, loss: 0.10611161957655582, gradient norm: 0.08984417836301357\n",
      "Iteration: 10390000, loss: 0.10611148434263322, gradient norm: 0.504969990749443\n",
      "Iteration: 10391000, loss: 0.10611124280601723, gradient norm: 0.2114352418047475\n",
      "Iteration: 10392000, loss: 0.10611112014588599, gradient norm: 0.3476630294670386\n",
      "Iteration: 10393000, loss: 0.10611102431390837, gradient norm: 0.31637309445596645\n",
      "Iteration: 10394000, loss: 0.10611087906717549, gradient norm: 0.6994462220688455\n",
      "Iteration: 10395000, loss: 0.10611075810858599, gradient norm: 0.21561066638541726\n",
      "Iteration: 10396000, loss: 0.10611056315930552, gradient norm: 0.3629177665095166\n",
      "Iteration: 10397000, loss: 0.10611045196917436, gradient norm: 0.11858999508688275\n",
      "Iteration: 10398000, loss: 0.10611036545018601, gradient norm: 0.07518174057840907\n",
      "Iteration: 10399000, loss: 0.10611039783047209, gradient norm: 0.37622348794611593\n",
      "Iteration: 10400000, loss: 0.10610996244858528, gradient norm: 0.27989481004901234\n",
      "Iteration: 10401000, loss: 0.10611013243743804, gradient norm: 0.11460600707940194\n",
      "Iteration: 10402000, loss: 0.10610965785428556, gradient norm: 0.6844066155048014\n",
      "Iteration: 10403000, loss: 0.10610972190309499, gradient norm: 1.0112186913877024\n",
      "Iteration: 10404000, loss: 0.10610959293161158, gradient norm: 0.23583893306976642\n",
      "Iteration: 10405000, loss: 0.10610939035213451, gradient norm: 0.46570168779201415\n",
      "Iteration: 10406000, loss: 0.10610928632799288, gradient norm: 0.2884073358916533\n",
      "Iteration: 10407000, loss: 0.10610922667880227, gradient norm: 0.3404981222108987\n",
      "Iteration: 10408000, loss: 0.10610897225143795, gradient norm: 0.19853582589060795\n",
      "Iteration: 10409000, loss: 0.10610895735789162, gradient norm: 0.6740269381477034\n",
      "Iteration: 10410000, loss: 0.10610872347769391, gradient norm: 0.4407448118876128\n",
      "Iteration: 10411000, loss: 0.10610849799823137, gradient norm: 0.5886509002959681\n",
      "Iteration: 10412000, loss: 0.10610854162115592, gradient norm: 0.3308358662222128\n",
      "Iteration: 10413000, loss: 0.10610850021185693, gradient norm: 0.17998401826988922\n",
      "Iteration: 10414000, loss: 0.10610813172825036, gradient norm: 0.23616350535248878\n",
      "Iteration: 10415000, loss: 0.10610807706282745, gradient norm: 0.46540773933393237\n",
      "Iteration: 10416000, loss: 0.10610796566969168, gradient norm: 0.01896148699073332\n",
      "Iteration: 10417000, loss: 0.10610784845532754, gradient norm: 0.33664060212869884\n",
      "Iteration: 10418000, loss: 0.10610765109367307, gradient norm: 0.048492517576524154\n",
      "Iteration: 10419000, loss: 0.10610750579237424, gradient norm: 0.3725956342067042\n",
      "Iteration: 10420000, loss: 0.10610749569402657, gradient norm: 0.28521689793997324\n",
      "Iteration: 10421000, loss: 0.10610728545902663, gradient norm: 0.2884040630331691\n",
      "Iteration: 10422000, loss: 0.10610717082347204, gradient norm: 0.3129362594080658\n",
      "Iteration: 10423000, loss: 0.10610700354656441, gradient norm: 0.601860342186045\n",
      "Iteration: 10424000, loss: 0.10610696981256322, gradient norm: 0.13052647715443214\n",
      "Iteration: 10425000, loss: 0.10610672903233088, gradient norm: 0.868640118681198\n",
      "Iteration: 10426000, loss: 0.10610667728562297, gradient norm: 0.3452976250183213\n",
      "Iteration: 10427000, loss: 0.10610648923521435, gradient norm: 0.2429339279646529\n",
      "Iteration: 10428000, loss: 0.1061062745905255, gradient norm: 0.33568107988123314\n",
      "Iteration: 10429000, loss: 0.10610620932736609, gradient norm: 0.07293609268994426\n",
      "Iteration: 10430000, loss: 0.10610631042076127, gradient norm: 0.13506690416311695\n",
      "Iteration: 10431000, loss: 0.10610580524955292, gradient norm: 0.16232281811674298\n",
      "Iteration: 10432000, loss: 0.10610596655716469, gradient norm: 0.11556132363240487\n",
      "Iteration: 10433000, loss: 0.10610569302391056, gradient norm: 0.11157453326395182\n",
      "Iteration: 10434000, loss: 0.10610561144518531, gradient norm: 0.7174918258183188\n",
      "Iteration: 10435000, loss: 0.10610548877233499, gradient norm: 0.30413185790393793\n",
      "Iteration: 10436000, loss: 0.1061051777879112, gradient norm: 0.3211325013093982\n",
      "Iteration: 10437000, loss: 0.10610523144172046, gradient norm: 0.4120291737462293\n",
      "Iteration: 10438000, loss: 0.10610496293768018, gradient norm: 0.5481710647187075\n",
      "Iteration: 10439000, loss: 0.10610498983599444, gradient norm: 0.5166852933977071\n",
      "Iteration: 10440000, loss: 0.10610487886756478, gradient norm: 1.1587924673682999\n",
      "Iteration: 10441000, loss: 0.106104622701653, gradient norm: 0.4111698585670175\n",
      "Iteration: 10442000, loss: 0.10610437921345878, gradient norm: 0.2688266030498219\n",
      "Iteration: 10443000, loss: 0.10610442202765427, gradient norm: 0.1235116439876902\n",
      "Iteration: 10444000, loss: 0.106104173203671, gradient norm: 0.6178334155777606\n",
      "Iteration: 10445000, loss: 0.10610421415745533, gradient norm: 0.3854613450601564\n",
      "Iteration: 10446000, loss: 0.10610400779206797, gradient norm: 0.20035578895054748\n",
      "Iteration: 10447000, loss: 0.10610376249102996, gradient norm: 0.182541798947049\n",
      "Iteration: 10448000, loss: 0.10610387349035723, gradient norm: 0.19960511151422994\n",
      "Iteration: 10449000, loss: 0.10610354683494572, gradient norm: 0.02079447678113039\n",
      "Iteration: 10450000, loss: 0.1061034576512146, gradient norm: 0.2689033716897418\n",
      "Iteration: 10451000, loss: 0.10610338630033764, gradient norm: 0.5038699161054024\n",
      "Iteration: 10452000, loss: 0.10610324667076915, gradient norm: 0.3520332704309627\n",
      "Iteration: 10453000, loss: 0.10610303180899332, gradient norm: 0.11879014499970283\n",
      "Iteration: 10454000, loss: 0.10610281898995728, gradient norm: 0.21730194090397814\n",
      "Iteration: 10455000, loss: 0.1061029001926681, gradient norm: 0.5065651466981618\n",
      "Iteration: 10456000, loss: 0.10610255492680334, gradient norm: 0.1198088126642194\n",
      "Iteration: 10457000, loss: 0.10610251062722141, gradient norm: 0.6092011547759476\n",
      "Iteration: 10458000, loss: 0.10610245318825261, gradient norm: 0.4558161914271338\n",
      "Iteration: 10459000, loss: 0.1061023509114342, gradient norm: 0.6148714298310526\n",
      "Iteration: 10460000, loss: 0.10610215405455052, gradient norm: 0.10370544244147287\n",
      "Iteration: 10461000, loss: 0.10610208531143542, gradient norm: 0.24035359713458954\n",
      "Iteration: 10462000, loss: 0.10610186795833135, gradient norm: 0.6464327734855397\n",
      "Iteration: 10463000, loss: 0.10610173121379166, gradient norm: 0.37033205061868313\n",
      "Iteration: 10464000, loss: 0.1061016095326186, gradient norm: 0.036678573138362126\n",
      "Iteration: 10465000, loss: 0.10610140842809446, gradient norm: 0.048997558875065385\n",
      "Iteration: 10466000, loss: 0.10610133406892965, gradient norm: 0.3544228951829969\n",
      "Iteration: 10467000, loss: 0.10610127577661516, gradient norm: 0.20895168900610547\n",
      "Iteration: 10468000, loss: 0.10610110648811757, gradient norm: 0.3984745889780466\n",
      "Iteration: 10469000, loss: 0.10610094726017023, gradient norm: 0.20280419251953852\n",
      "Iteration: 10470000, loss: 0.10610092875664064, gradient norm: 0.35082819699033324\n",
      "Iteration: 10471000, loss: 0.1061005541668323, gradient norm: 0.0810791967075199\n",
      "Iteration: 10472000, loss: 0.10610069641026425, gradient norm: 0.23452446215464612\n",
      "Iteration: 10473000, loss: 0.1061004132707913, gradient norm: 0.28510281356777645\n",
      "Iteration: 10474000, loss: 0.10610025444529785, gradient norm: 0.32091155080707895\n",
      "Iteration: 10475000, loss: 0.10610020243852689, gradient norm: 0.29178963477168657\n",
      "Iteration: 10476000, loss: 0.10610003135803285, gradient norm: 0.0954949643676148\n",
      "Iteration: 10477000, loss: 0.1060999295321672, gradient norm: 0.4163066644538707\n",
      "Iteration: 10478000, loss: 0.10609973155694127, gradient norm: 0.12734400400816975\n",
      "Iteration: 10479000, loss: 0.10609966321252465, gradient norm: 0.3960675410608546\n",
      "Iteration: 10480000, loss: 0.106099487365892, gradient norm: 0.7670831237503977\n",
      "Iteration: 10481000, loss: 0.10609938369066971, gradient norm: 0.22430626387045138\n",
      "Iteration: 10482000, loss: 0.10609916219600109, gradient norm: 0.2687295673308546\n",
      "Iteration: 10483000, loss: 0.10609916097036699, gradient norm: 0.6945509669176504\n",
      "Iteration: 10484000, loss: 0.1060989144137959, gradient norm: 0.2989331580841762\n",
      "Iteration: 10485000, loss: 0.10609895123964254, gradient norm: 0.46913087905100365\n",
      "Iteration: 10486000, loss: 0.10609861303802731, gradient norm: 0.41982536114352526\n",
      "Iteration: 10487000, loss: 0.10609857773505021, gradient norm: 0.2816075701456507\n",
      "Iteration: 10488000, loss: 0.10609862637864007, gradient norm: 0.4017818464253942\n",
      "Iteration: 10489000, loss: 0.10609827919139396, gradient norm: 0.0772063061338353\n",
      "Iteration: 10490000, loss: 0.10609825883857045, gradient norm: 0.15763383289231123\n",
      "Iteration: 10491000, loss: 0.10609800317330498, gradient norm: 0.23317334078991167\n",
      "Iteration: 10492000, loss: 0.10609785704135812, gradient norm: 0.6563411956344012\n",
      "Iteration: 10493000, loss: 0.1060978440796432, gradient norm: 0.2691239739427633\n",
      "Iteration: 10494000, loss: 0.10609766092629541, gradient norm: 0.07643943722273995\n",
      "Iteration: 10495000, loss: 0.10609771352222484, gradient norm: 0.23813474659673428\n",
      "Iteration: 10496000, loss: 0.10609731787985674, gradient norm: 0.342885356423251\n",
      "Iteration: 10497000, loss: 0.10609720970018896, gradient norm: 0.3057427829123769\n",
      "Iteration: 10498000, loss: 0.10609738825001838, gradient norm: 0.5994242526663978\n",
      "Iteration: 10499000, loss: 0.10609675841683429, gradient norm: 0.42069825302709857\n",
      "Iteration: 10500000, loss: 0.10609697607955128, gradient norm: 0.6146787069103198\n",
      "Iteration: 10501000, loss: 0.10609674416062612, gradient norm: 0.4216320568861061\n",
      "Iteration: 10502000, loss: 0.10609659883386072, gradient norm: 0.5576886535210737\n",
      "Iteration: 10503000, loss: 0.10609637668816103, gradient norm: 0.3602479532609145\n",
      "Iteration: 10504000, loss: 0.10609640208343092, gradient norm: 0.35244914132641036\n",
      "Iteration: 10505000, loss: 0.10609622033447152, gradient norm: 0.46541243681816286\n",
      "Iteration: 10506000, loss: 0.10609619374435483, gradient norm: 0.49609464275575155\n",
      "Iteration: 10507000, loss: 0.10609590802605616, gradient norm: 0.3348577586136543\n",
      "Iteration: 10508000, loss: 0.10609584565133813, gradient norm: 0.05631996849130776\n",
      "Iteration: 10509000, loss: 0.10609570990333034, gradient norm: 0.14197617402180415\n",
      "Iteration: 10510000, loss: 0.10609558201500395, gradient norm: 0.3581229434952639\n",
      "Iteration: 10511000, loss: 0.10609539785812319, gradient norm: 0.16116576266092475\n",
      "Iteration: 10512000, loss: 0.10609547000553561, gradient norm: 0.2882197656796532\n",
      "Iteration: 10513000, loss: 0.10609510356172672, gradient norm: 0.2375816476493064\n",
      "Iteration: 10514000, loss: 0.10609510326462794, gradient norm: 0.3937700491017132\n",
      "Iteration: 10515000, loss: 0.10609476591909434, gradient norm: 0.20115660657528012\n",
      "Iteration: 10516000, loss: 0.1060948432243439, gradient norm: 0.854397288223002\n",
      "Iteration: 10517000, loss: 0.10609470317794512, gradient norm: 0.4625505396663617\n",
      "Iteration: 10518000, loss: 0.1060944279830337, gradient norm: 0.5273111096156289\n",
      "Iteration: 10519000, loss: 0.10609450769770493, gradient norm: 0.47939892631866415\n",
      "Iteration: 10520000, loss: 0.10609425959434078, gradient norm: 0.10458557054341047\n",
      "Iteration: 10521000, loss: 0.10609407163109442, gradient norm: 0.4214710143448764\n",
      "Iteration: 10522000, loss: 0.10609399809820468, gradient norm: 0.12247954695861996\n",
      "Iteration: 10523000, loss: 0.10609388744416313, gradient norm: 0.6347112379157522\n",
      "Iteration: 10524000, loss: 0.10609377907418764, gradient norm: 0.5912358401677605\n",
      "Iteration: 10525000, loss: 0.10609353292704098, gradient norm: 0.32097099096258647\n",
      "Iteration: 10526000, loss: 0.106093555804652, gradient norm: 0.7246210318694732\n",
      "Iteration: 10527000, loss: 0.10609330853889963, gradient norm: 1.0041502548486982\n",
      "Iteration: 10528000, loss: 0.1060933031681059, gradient norm: 0.18178097129853438\n",
      "Iteration: 10529000, loss: 0.10609323086937587, gradient norm: 0.6147800961393058\n",
      "Iteration: 10530000, loss: 0.10609273594976495, gradient norm: 0.141543693432106\n",
      "Iteration: 10531000, loss: 0.10609288279900853, gradient norm: 0.18405195539839278\n",
      "Iteration: 10532000, loss: 0.10609273228638824, gradient norm: 0.5556638182621391\n",
      "Iteration: 10533000, loss: 0.10609256791489764, gradient norm: 0.15916673931582703\n",
      "Iteration: 10534000, loss: 0.10609251471434411, gradient norm: 0.29370447462848487\n",
      "Iteration: 10535000, loss: 0.10609232237681834, gradient norm: 0.10854622847074386\n",
      "Iteration: 10536000, loss: 0.10609214518673446, gradient norm: 0.3954897693091085\n",
      "Iteration: 10537000, loss: 0.10609205600879247, gradient norm: 0.6112719516458458\n",
      "Iteration: 10538000, loss: 0.10609195888421327, gradient norm: 0.7526389490822325\n",
      "Iteration: 10539000, loss: 0.10609172489456775, gradient norm: 0.2672670583608218\n",
      "Iteration: 10540000, loss: 0.1060916606130241, gradient norm: 0.9094499591900246\n",
      "Iteration: 10541000, loss: 0.10609148035276192, gradient norm: 0.2988577502912354\n",
      "Iteration: 10542000, loss: 0.10609162931454141, gradient norm: 0.25059534748264656\n",
      "Iteration: 10543000, loss: 0.10609109682915485, gradient norm: 0.35179655320887737\n",
      "Iteration: 10544000, loss: 0.10609144909549355, gradient norm: 0.40353152770889494\n",
      "Iteration: 10545000, loss: 0.10609086812492931, gradient norm: 0.16047254575511036\n",
      "Iteration: 10546000, loss: 0.10609078384284107, gradient norm: 0.8431359380508184\n",
      "Iteration: 10547000, loss: 0.10609078881234978, gradient norm: 0.6955287274006698\n",
      "Iteration: 10548000, loss: 0.10609048817927129, gradient norm: 0.1661774387980937\n",
      "Iteration: 10549000, loss: 0.10609052894491756, gradient norm: 0.7534853413845731\n",
      "Iteration: 10550000, loss: 0.10609046847549707, gradient norm: 0.22532857040888402\n",
      "Iteration: 10551000, loss: 0.10609028412925597, gradient norm: 0.38142968850208253\n",
      "Iteration: 10552000, loss: 0.10609010287646253, gradient norm: 0.21647803930912082\n",
      "Iteration: 10553000, loss: 0.10608994880446712, gradient norm: 0.3826201605083319\n",
      "Iteration: 10554000, loss: 0.10608996357626202, gradient norm: 0.553452577104846\n",
      "Iteration: 10555000, loss: 0.10608961924653969, gradient norm: 0.03792701472961477\n",
      "Iteration: 10556000, loss: 0.10608958716246315, gradient norm: 0.2604619529669112\n",
      "Iteration: 10557000, loss: 0.10608943889018531, gradient norm: 0.2140778610919612\n",
      "Iteration: 10558000, loss: 0.1060893698732912, gradient norm: 0.25024743999014676\n",
      "Iteration: 10559000, loss: 0.1060890356191591, gradient norm: 0.7494565305904608\n",
      "Iteration: 10560000, loss: 0.1060891803239707, gradient norm: 0.299094981417426\n",
      "Iteration: 10561000, loss: 0.10608898713736271, gradient norm: 0.3180066099055818\n",
      "Iteration: 10562000, loss: 0.1060887057450851, gradient norm: 0.1604069037894577\n",
      "Iteration: 10563000, loss: 0.10608857854944913, gradient norm: 0.4740042385954214\n",
      "Iteration: 10564000, loss: 0.10608865813616215, gradient norm: 0.24073556472977667\n",
      "Iteration: 10565000, loss: 0.1060884106553203, gradient norm: 0.11976559842709014\n",
      "Iteration: 10566000, loss: 0.10608830445069643, gradient norm: 0.25521090868673324\n",
      "Iteration: 10567000, loss: 0.10608816249916096, gradient norm: 0.4743613302748812\n",
      "Iteration: 10568000, loss: 0.10608793391672804, gradient norm: 0.206639580799621\n",
      "Iteration: 10569000, loss: 0.10608805637173135, gradient norm: 0.248531104941502\n",
      "Iteration: 10570000, loss: 0.10608771061178178, gradient norm: 0.20345933323584756\n",
      "Iteration: 10571000, loss: 0.10608760964903456, gradient norm: 0.5699740125264746\n",
      "Iteration: 10572000, loss: 0.10608751610744127, gradient norm: 0.14449220056943948\n",
      "Iteration: 10573000, loss: 0.10608747523502592, gradient norm: 0.10541157292828271\n",
      "Iteration: 10574000, loss: 0.10608713699113989, gradient norm: 0.89199133828674\n",
      "Iteration: 10575000, loss: 0.10608714780417361, gradient norm: 0.711593577699078\n",
      "Iteration: 10576000, loss: 0.10608705091909953, gradient norm: 0.09287647978801711\n",
      "Iteration: 10577000, loss: 0.10608687859145544, gradient norm: 0.8103365469826914\n",
      "Iteration: 10578000, loss: 0.10608659491882905, gradient norm: 0.027653822650283662\n",
      "Iteration: 10579000, loss: 0.10608667945179051, gradient norm: 0.16207845841013874\n",
      "Iteration: 10580000, loss: 0.10608646978039162, gradient norm: 0.207006488561326\n",
      "Iteration: 10581000, loss: 0.10608646149736986, gradient norm: 0.43785194440082315\n",
      "Iteration: 10582000, loss: 0.10608617035105496, gradient norm: 0.1347563593461022\n",
      "Iteration: 10583000, loss: 0.10608610105470075, gradient norm: 0.5287546717085316\n",
      "Iteration: 10584000, loss: 0.106085890341668, gradient norm: 0.11848141169125133\n",
      "Iteration: 10585000, loss: 0.10608575287107867, gradient norm: 0.2548183958361405\n",
      "Iteration: 10586000, loss: 0.10608579640396071, gradient norm: 0.7596426006306185\n",
      "Iteration: 10587000, loss: 0.10608558197729934, gradient norm: 0.4552390291904823\n",
      "Iteration: 10588000, loss: 0.1060854037636866, gradient norm: 0.1273040775428242\n",
      "Iteration: 10589000, loss: 0.10608538638829765, gradient norm: 0.6510299869403339\n",
      "Iteration: 10590000, loss: 0.10608506915587304, gradient norm: 0.059933753414538486\n",
      "Iteration: 10591000, loss: 0.10608508509654684, gradient norm: 0.261166074624368\n",
      "Iteration: 10592000, loss: 0.1060849311781461, gradient norm: 0.08891254114459408\n",
      "Iteration: 10593000, loss: 0.10608478187235328, gradient norm: 0.26754560356450496\n",
      "Iteration: 10594000, loss: 0.10608461397979903, gradient norm: 0.6343862891136866\n",
      "Iteration: 10595000, loss: 0.10608461545080716, gradient norm: 0.4060597923616147\n",
      "Iteration: 10596000, loss: 0.10608424316935068, gradient norm: 0.6645171804719968\n",
      "Iteration: 10597000, loss: 0.1060843624218022, gradient norm: 0.4759213810454261\n",
      "Iteration: 10598000, loss: 0.1060841639962242, gradient norm: 0.2265922747066759\n",
      "Iteration: 10599000, loss: 0.1060840279152615, gradient norm: 0.14659809784588343\n",
      "Iteration: 10600000, loss: 0.10608385359016277, gradient norm: 0.25843158112933867\n",
      "Iteration: 10601000, loss: 0.10608383993122046, gradient norm: 0.4357090504780934\n",
      "Iteration: 10602000, loss: 0.10608351173299387, gradient norm: 0.09320336052075018\n",
      "Iteration: 10603000, loss: 0.10608358899058709, gradient norm: 0.4413950776211794\n",
      "Iteration: 10604000, loss: 0.10608323188860012, gradient norm: 0.13450060737295827\n",
      "Iteration: 10605000, loss: 0.10608332717357258, gradient norm: 0.2726081925504374\n",
      "Iteration: 10606000, loss: 0.10608326519978727, gradient norm: 0.17024049018342644\n",
      "Iteration: 10607000, loss: 0.10608300351952861, gradient norm: 0.480833814530046\n",
      "Iteration: 10608000, loss: 0.10608268622656745, gradient norm: 0.7420940066042283\n",
      "Iteration: 10609000, loss: 0.1060827527641313, gradient norm: 0.1876055876293256\n",
      "Iteration: 10610000, loss: 0.10608260261332395, gradient norm: 0.10053484603046801\n",
      "Iteration: 10611000, loss: 0.10608244901659074, gradient norm: 0.23390006791097348\n",
      "Iteration: 10612000, loss: 0.10608242607009237, gradient norm: 0.28990082836461706\n",
      "Iteration: 10613000, loss: 0.10608212377589732, gradient norm: 0.5436098143585057\n",
      "Iteration: 10614000, loss: 0.10608207591333835, gradient norm: 0.5016942972069649\n",
      "Iteration: 10615000, loss: 0.10608203848540598, gradient norm: 0.8939841792402068\n",
      "Iteration: 10616000, loss: 0.1060817653199593, gradient norm: 0.5934568382439888\n",
      "Iteration: 10617000, loss: 0.10608163601165814, gradient norm: 0.2060821819326593\n",
      "Iteration: 10618000, loss: 0.1060815784868914, gradient norm: 0.44518748166308636\n",
      "Iteration: 10619000, loss: 0.10608147896993947, gradient norm: 0.5317137617057297\n",
      "Iteration: 10620000, loss: 0.10608118650869518, gradient norm: 0.2690528885070566\n",
      "Iteration: 10621000, loss: 0.10608122116218746, gradient norm: 0.13153235030065494\n",
      "Iteration: 10622000, loss: 0.10608108744626875, gradient norm: 0.18936469170366663\n",
      "Iteration: 10623000, loss: 0.10608093502423954, gradient norm: 0.12787689129310387\n",
      "Iteration: 10624000, loss: 0.10608077767927165, gradient norm: 0.16232113060121453\n",
      "Iteration: 10625000, loss: 0.10608065182720923, gradient norm: 0.06145228691837428\n",
      "Iteration: 10626000, loss: 0.10608062410733632, gradient norm: 0.31815929227197415\n",
      "Iteration: 10627000, loss: 0.1060803870236219, gradient norm: 0.358928117759846\n",
      "Iteration: 10628000, loss: 0.10608025407039412, gradient norm: 0.6989224493208644\n",
      "Iteration: 10629000, loss: 0.1060801214795157, gradient norm: 0.6207041041291176\n",
      "Iteration: 10630000, loss: 0.10608006625614158, gradient norm: 0.6641062343744464\n",
      "Iteration: 10631000, loss: 0.10607990597142429, gradient norm: 0.3406168906683815\n",
      "Iteration: 10632000, loss: 0.10607972128572282, gradient norm: 0.36005091186553034\n",
      "Iteration: 10633000, loss: 0.10607962876752523, gradient norm: 0.18455752551076204\n",
      "Iteration: 10634000, loss: 0.10607957107622898, gradient norm: 0.07013734047324455\n",
      "Iteration: 10635000, loss: 0.10607930731933668, gradient norm: 0.2281790862962459\n",
      "Iteration: 10636000, loss: 0.10607935542504515, gradient norm: 0.13850340080828716\n",
      "Iteration: 10637000, loss: 0.10607905092907571, gradient norm: 0.4171291341669078\n",
      "Iteration: 10638000, loss: 0.10607902875758542, gradient norm: 0.5487435556624816\n",
      "Iteration: 10639000, loss: 0.10607888688291231, gradient norm: 0.5411876565557957\n",
      "Iteration: 10640000, loss: 0.10607868445508846, gradient norm: 0.06585931213007537\n",
      "Iteration: 10641000, loss: 0.10607866105344545, gradient norm: 0.2854731600833712\n",
      "Iteration: 10642000, loss: 0.10607833554817037, gradient norm: 0.48433528937681103\n",
      "Iteration: 10643000, loss: 0.10607838891185775, gradient norm: 0.14149533801521533\n",
      "Iteration: 10644000, loss: 0.10607849616985476, gradient norm: 0.2605265990245242\n",
      "Iteration: 10645000, loss: 0.10607793196384278, gradient norm: 0.5117852414339258\n",
      "Iteration: 10646000, loss: 0.10607779879946165, gradient norm: 0.05993018855137316\n",
      "Iteration: 10647000, loss: 0.10607792152511643, gradient norm: 0.2771760860477951\n",
      "Iteration: 10648000, loss: 0.10607760635364884, gradient norm: 0.7426196820551455\n",
      "Iteration: 10649000, loss: 0.10607751374184718, gradient norm: 0.401572876776221\n",
      "Iteration: 10650000, loss: 0.10607756255347808, gradient norm: 0.5861244063140066\n",
      "Iteration: 10651000, loss: 0.10607745426507172, gradient norm: 0.3362977347081083\n",
      "Iteration: 10652000, loss: 0.10607710449474042, gradient norm: 0.20890815669584425\n",
      "Iteration: 10653000, loss: 0.1060770796020138, gradient norm: 0.47033061030766726\n",
      "Iteration: 10654000, loss: 0.10607692833062483, gradient norm: 0.2918516289675757\n",
      "Iteration: 10655000, loss: 0.1060767563989414, gradient norm: 0.03047391615608308\n",
      "Iteration: 10656000, loss: 0.10607677225873749, gradient norm: 0.32002857115515493\n",
      "Iteration: 10657000, loss: 0.10607641772561292, gradient norm: 0.29398635997297684\n",
      "Iteration: 10658000, loss: 0.10607651460979368, gradient norm: 0.5585018989829834\n",
      "Iteration: 10659000, loss: 0.10607629812583157, gradient norm: 0.7230307696913161\n",
      "Iteration: 10660000, loss: 0.10607614659775841, gradient norm: 0.11499227337948598\n",
      "Iteration: 10661000, loss: 0.10607603983890286, gradient norm: 0.7155075790094739\n",
      "Iteration: 10662000, loss: 0.10607583308359964, gradient norm: 0.16142671970148537\n",
      "Iteration: 10663000, loss: 0.10607595014030596, gradient norm: 1.0197702968199207\n",
      "Iteration: 10664000, loss: 0.10607561596857397, gradient norm: 0.08728428689692704\n",
      "Iteration: 10665000, loss: 0.10607543746279287, gradient norm: 0.033999308715184244\n",
      "Iteration: 10666000, loss: 0.1060755105399119, gradient norm: 0.09256434148727159\n",
      "Iteration: 10667000, loss: 0.10607520214686698, gradient norm: 0.16929224796321604\n",
      "Iteration: 10668000, loss: 0.1060751339096393, gradient norm: 0.3671393421657623\n",
      "Iteration: 10669000, loss: 0.10607505472466167, gradient norm: 0.5921744343588525\n",
      "Iteration: 10670000, loss: 0.10607496177505225, gradient norm: 0.5592350866735076\n",
      "Iteration: 10671000, loss: 0.10607483976731856, gradient norm: 0.43650433410756495\n",
      "Iteration: 10672000, loss: 0.10607453047341119, gradient norm: 0.3381449331999907\n",
      "Iteration: 10673000, loss: 0.10607447925200413, gradient norm: 0.7987103858309138\n",
      "Iteration: 10674000, loss: 0.10607439785270464, gradient norm: 0.3867898650257469\n",
      "Iteration: 10675000, loss: 0.10607428310396252, gradient norm: 0.34674430527972155\n",
      "Iteration: 10676000, loss: 0.10607418715159905, gradient norm: 0.4565744196617423\n",
      "Iteration: 10677000, loss: 0.10607393247988678, gradient norm: 0.31692785318729744\n",
      "Iteration: 10678000, loss: 0.1060737820342934, gradient norm: 0.7328523021336124\n",
      "Iteration: 10679000, loss: 0.106074000368149, gradient norm: 0.45098480650200085\n",
      "Iteration: 10680000, loss: 0.10607340433416035, gradient norm: 0.37824613982545874\n",
      "Iteration: 10681000, loss: 0.10607343901186683, gradient norm: 0.8771537436644736\n",
      "Iteration: 10682000, loss: 0.1060733517979825, gradient norm: 0.15885485815840691\n",
      "Iteration: 10683000, loss: 0.10607326599457832, gradient norm: 0.8598256872410246\n",
      "Iteration: 10684000, loss: 0.10607317513910829, gradient norm: 0.24088353366759388\n",
      "Iteration: 10685000, loss: 0.10607286496530575, gradient norm: 0.11841877820763869\n",
      "Iteration: 10686000, loss: 0.10607288395804415, gradient norm: 0.2695953446743908\n",
      "Iteration: 10687000, loss: 0.10607282704009728, gradient norm: 0.36562950093610525\n",
      "Iteration: 10688000, loss: 0.10607252459694687, gradient norm: 0.42725400411514525\n",
      "Iteration: 10689000, loss: 0.1060724580874052, gradient norm: 0.20554883296679521\n",
      "Iteration: 10690000, loss: 0.10607227194639306, gradient norm: 0.2225908355693793\n",
      "Iteration: 10691000, loss: 0.1060721902228251, gradient norm: 0.03662693738558025\n",
      "Iteration: 10692000, loss: 0.1060721051623438, gradient norm: 0.07452932369460934\n",
      "Iteration: 10693000, loss: 0.1060720286712762, gradient norm: 0.11406905542128361\n",
      "Iteration: 10694000, loss: 0.10607182415068099, gradient norm: 0.4931164677052237\n",
      "Iteration: 10695000, loss: 0.10607164452599244, gradient norm: 0.43917261790939466\n",
      "Iteration: 10696000, loss: 0.10607154272152411, gradient norm: 0.22767257283744818\n",
      "Iteration: 10697000, loss: 0.10607141215738729, gradient norm: 0.6275950428949472\n",
      "Iteration: 10698000, loss: 0.10607130978869216, gradient norm: 0.7706704333334188\n",
      "Iteration: 10699000, loss: 0.10607121051103219, gradient norm: 0.558209207598982\n",
      "Iteration: 10700000, loss: 0.10607116340527514, gradient norm: 0.17949671070281337\n",
      "Iteration: 10701000, loss: 0.10607085364258592, gradient norm: 0.3126994347753394\n",
      "Iteration: 10702000, loss: 0.10607100649878376, gradient norm: 0.5291773711208568\n",
      "Iteration: 10703000, loss: 0.10607056473032792, gradient norm: 0.22337841168862843\n",
      "Iteration: 10704000, loss: 0.10607040867876985, gradient norm: 0.10992222957412613\n",
      "Iteration: 10705000, loss: 0.10607059878853634, gradient norm: 0.24567050835223075\n",
      "Iteration: 10706000, loss: 0.10607014562005557, gradient norm: 0.5751412723342151\n",
      "Iteration: 10707000, loss: 0.10607020978064764, gradient norm: 0.9447710734403874\n",
      "Iteration: 10708000, loss: 0.10606996705187664, gradient norm: 0.17402638165064335\n",
      "Iteration: 10709000, loss: 0.10606997298964309, gradient norm: 0.5492859850949624\n",
      "Iteration: 10710000, loss: 0.10606982760765095, gradient norm: 0.38007644718822153\n",
      "Iteration: 10711000, loss: 0.10606962211948824, gradient norm: 0.5532963895777758\n",
      "Iteration: 10712000, loss: 0.10606952568050891, gradient norm: 0.1935166868834474\n",
      "Iteration: 10713000, loss: 0.10606943942914879, gradient norm: 0.49262920050324377\n",
      "Iteration: 10714000, loss: 0.10606936304287794, gradient norm: 0.3356672377563703\n",
      "Iteration: 10715000, loss: 0.10606899023997708, gradient norm: 0.8131448693140432\n",
      "Iteration: 10716000, loss: 0.1060691381698104, gradient norm: 0.18000174127360613\n",
      "Iteration: 10717000, loss: 0.10606896761229742, gradient norm: 0.5464850638590141\n",
      "Iteration: 10718000, loss: 0.1060688565598141, gradient norm: 0.35036017352438426\n",
      "Iteration: 10719000, loss: 0.10606849111298414, gradient norm: 0.41738528895843474\n",
      "Iteration: 10720000, loss: 0.10606861190404697, gradient norm: 0.09160868713040816\n",
      "Iteration: 10721000, loss: 0.10606819653756668, gradient norm: 0.2450458038431947\n",
      "Iteration: 10722000, loss: 0.10606843864914313, gradient norm: 0.19049026954224582\n",
      "Iteration: 10723000, loss: 0.1060680565432289, gradient norm: 0.255734264345483\n",
      "Iteration: 10724000, loss: 0.10606811768366402, gradient norm: 0.19985689700033865\n",
      "Iteration: 10725000, loss: 0.10606773898107305, gradient norm: 0.3840192207493797\n",
      "Iteration: 10726000, loss: 0.10606784847218131, gradient norm: 0.3424519015536953\n",
      "Iteration: 10727000, loss: 0.10606749574005082, gradient norm: 0.714758229529202\n",
      "Iteration: 10728000, loss: 0.10606763160342987, gradient norm: 0.271488421777448\n",
      "Iteration: 10729000, loss: 0.10606739385758072, gradient norm: 0.3703281952637067\n",
      "Iteration: 10730000, loss: 0.10606723892331653, gradient norm: 0.39903852319572036\n",
      "Iteration: 10731000, loss: 0.10606714698035538, gradient norm: 0.17589483567394412\n",
      "Iteration: 10732000, loss: 0.10606683826465138, gradient norm: 0.4264320861563744\n",
      "Iteration: 10733000, loss: 0.10606699480046373, gradient norm: 0.051900779363089476\n",
      "Iteration: 10734000, loss: 0.10606683817528396, gradient norm: 0.9162273729074911\n",
      "Iteration: 10735000, loss: 0.10606641783698936, gradient norm: 0.6953149363382246\n",
      "Iteration: 10736000, loss: 0.10606675715251108, gradient norm: 0.04044258830927353\n",
      "Iteration: 10737000, loss: 0.10606620428691924, gradient norm: 0.15381124382808212\n",
      "Iteration: 10738000, loss: 0.10606629756456994, gradient norm: 0.6464019998460139\n",
      "Iteration: 10739000, loss: 0.10606595194597358, gradient norm: 0.3122182238733834\n",
      "Iteration: 10740000, loss: 0.10606599145354505, gradient norm: 0.5201905597044354\n",
      "Iteration: 10741000, loss: 0.10606598173259094, gradient norm: 0.38861900949694617\n",
      "Iteration: 10742000, loss: 0.10606563352128873, gradient norm: 0.6522566526291922\n",
      "Iteration: 10743000, loss: 0.1060655908294751, gradient norm: 0.3607147883930625\n",
      "Iteration: 10744000, loss: 0.10606553844698513, gradient norm: 0.37463904438766465\n",
      "Iteration: 10745000, loss: 0.10606539418730829, gradient norm: 0.677406790096055\n",
      "Iteration: 10746000, loss: 0.10606513064940902, gradient norm: 0.26434026332551197\n",
      "Iteration: 10747000, loss: 0.1060651730833975, gradient norm: 0.35505433472646664\n",
      "Iteration: 10748000, loss: 0.10606498469928616, gradient norm: 0.23568040159448286\n",
      "Iteration: 10749000, loss: 0.10606481755763124, gradient norm: 0.3420849850386577\n",
      "Iteration: 10750000, loss: 0.10606464933674212, gradient norm: 0.1514479982953222\n",
      "Iteration: 10751000, loss: 0.10606465728684981, gradient norm: 0.12096372155002727\n",
      "Iteration: 10752000, loss: 0.1060644932201996, gradient norm: 0.36060352074919566\n",
      "Iteration: 10753000, loss: 0.10606448297799136, gradient norm: 0.7179611428242116\n",
      "Iteration: 10754000, loss: 0.10606411836017744, gradient norm: 0.3852393790291717\n",
      "Iteration: 10755000, loss: 0.10606403571085087, gradient norm: 0.2751879116733257\n",
      "Iteration: 10756000, loss: 0.10606401435318656, gradient norm: 0.5768316540351909\n",
      "Iteration: 10757000, loss: 0.10606391520219696, gradient norm: 0.6301338494334235\n",
      "Iteration: 10758000, loss: 0.10606364231771222, gradient norm: 0.3086688445675005\n",
      "Iteration: 10759000, loss: 0.10606354805722432, gradient norm: 0.18845333515335247\n",
      "Iteration: 10760000, loss: 0.1060635148762535, gradient norm: 0.059695169372291475\n",
      "Iteration: 10761000, loss: 0.10606328837422302, gradient norm: 0.10429769592952735\n",
      "Iteration: 10762000, loss: 0.10606318666889922, gradient norm: 0.3321733475676285\n",
      "Iteration: 10763000, loss: 0.10606304037644239, gradient norm: 0.34796895661661864\n",
      "Iteration: 10764000, loss: 0.10606299923859842, gradient norm: 0.2547236161140782\n",
      "Iteration: 10765000, loss: 0.10606276396706392, gradient norm: 0.4418764669341027\n",
      "Iteration: 10766000, loss: 0.10606283569207978, gradient norm: 0.5813102927693808\n",
      "Iteration: 10767000, loss: 0.1060624817280382, gradient norm: 0.44147524189394743\n",
      "Iteration: 10768000, loss: 0.10606231767983149, gradient norm: 0.2369283417500159\n",
      "Iteration: 10769000, loss: 0.10606243996528701, gradient norm: 0.2594640593193796\n",
      "Iteration: 10770000, loss: 0.10606220965464669, gradient norm: 0.22013818460342272\n",
      "Iteration: 10771000, loss: 0.10606197653958772, gradient norm: 0.11605801337831266\n",
      "Iteration: 10772000, loss: 0.10606195074813772, gradient norm: 0.256971576905079\n",
      "Iteration: 10773000, loss: 0.10606173337769235, gradient norm: 0.2982029599402216\n",
      "Iteration: 10774000, loss: 0.10606180850485596, gradient norm: 0.1380811221441923\n",
      "Iteration: 10775000, loss: 0.10606163476681917, gradient norm: 0.21587276705658262\n",
      "Iteration: 10776000, loss: 0.10606138951301111, gradient norm: 0.9042223413788724\n",
      "Iteration: 10777000, loss: 0.10606140617663547, gradient norm: 0.6269675769286965\n",
      "Iteration: 10778000, loss: 0.10606112109208975, gradient norm: 0.12221325983785641\n",
      "Iteration: 10779000, loss: 0.10606102724980847, gradient norm: 0.23357738674171852\n",
      "Iteration: 10780000, loss: 0.10606092631815851, gradient norm: 0.6962138160960184\n",
      "Iteration: 10781000, loss: 0.10606091161109524, gradient norm: 0.32732036983936985\n",
      "Iteration: 10782000, loss: 0.1060605658346845, gradient norm: 0.33207086855499374\n",
      "Iteration: 10783000, loss: 0.1060605660084436, gradient norm: 0.5546373489574907\n",
      "Iteration: 10784000, loss: 0.10606049104252535, gradient norm: 0.2717124261827773\n",
      "Iteration: 10785000, loss: 0.10606040431380964, gradient norm: 0.1320627548506581\n",
      "Iteration: 10786000, loss: 0.10606017981967836, gradient norm: 0.5308869513607831\n",
      "Iteration: 10787000, loss: 0.1060600082743163, gradient norm: 0.23494467433134472\n",
      "Iteration: 10788000, loss: 0.10606000707195291, gradient norm: 0.5334170080158751\n",
      "Iteration: 10789000, loss: 0.10605971775391555, gradient norm: 0.04854872109875306\n",
      "Iteration: 10790000, loss: 0.1060597571827592, gradient norm: 0.4307325838969759\n",
      "Iteration: 10791000, loss: 0.10605946044577275, gradient norm: 0.685274413027597\n",
      "Iteration: 10792000, loss: 0.10605948161421483, gradient norm: 0.47269948982969684\n",
      "Iteration: 10793000, loss: 0.1060592836398828, gradient norm: 0.36494281998866845\n",
      "Iteration: 10794000, loss: 0.1060592532364033, gradient norm: 0.33415984422269984\n",
      "Iteration: 10795000, loss: 0.10605891666172897, gradient norm: 0.286999238700445\n",
      "Iteration: 10796000, loss: 0.1060589364418062, gradient norm: 0.3375665719941919\n",
      "Iteration: 10797000, loss: 0.10605885753751719, gradient norm: 0.9206094622415517\n",
      "Iteration: 10798000, loss: 0.10605867440677116, gradient norm: 0.34778913242592935\n",
      "Iteration: 10799000, loss: 0.10605871993475362, gradient norm: 0.4125477030215011\n",
      "Iteration: 10800000, loss: 0.10605826390097996, gradient norm: 0.05742501999798305\n",
      "Iteration: 10801000, loss: 0.1060583339439327, gradient norm: 0.18124894375457587\n",
      "Iteration: 10802000, loss: 0.10605813780244354, gradient norm: 0.5988623391674988\n",
      "Iteration: 10803000, loss: 0.10605802358988348, gradient norm: 0.21111603745140906\n",
      "Iteration: 10804000, loss: 0.10605801226053461, gradient norm: 0.3892242599462443\n",
      "Iteration: 10805000, loss: 0.10605776836525455, gradient norm: 0.09388217988104315\n",
      "Iteration: 10806000, loss: 0.10605773074814757, gradient norm: 0.4536564567004847\n",
      "Iteration: 10807000, loss: 0.10605744083908265, gradient norm: 0.2828595734731674\n",
      "Iteration: 10808000, loss: 0.10605768419780366, gradient norm: 0.275018519470583\n",
      "Iteration: 10809000, loss: 0.10605717056795384, gradient norm: 0.2783403228698612\n",
      "Iteration: 10810000, loss: 0.10605724496800473, gradient norm: 0.022965032092548358\n",
      "Iteration: 10811000, loss: 0.1060570907445956, gradient norm: 0.16674978881508795\n",
      "Iteration: 10812000, loss: 0.10605688157972853, gradient norm: 0.23086261022767057\n",
      "Iteration: 10813000, loss: 0.10605677349313156, gradient norm: 0.9608932438250717\n",
      "Iteration: 10814000, loss: 0.10605668999204346, gradient norm: 0.17050927715815883\n",
      "Iteration: 10815000, loss: 0.10605678101227654, gradient norm: 0.3807291826392091\n",
      "Iteration: 10816000, loss: 0.10605608216334131, gradient norm: 0.2760031653803866\n",
      "Iteration: 10817000, loss: 0.1060564958415622, gradient norm: 0.13980960884490093\n",
      "Iteration: 10818000, loss: 0.10605605001387504, gradient norm: 0.6289422437008754\n",
      "Iteration: 10819000, loss: 0.10605607098949574, gradient norm: 0.04245767443118705\n",
      "Iteration: 10820000, loss: 0.10605594882667772, gradient norm: 0.08556235247924518\n",
      "Iteration: 10821000, loss: 0.10605586476386725, gradient norm: 0.7037427878400878\n",
      "Iteration: 10822000, loss: 0.10605586130618123, gradient norm: 0.4192725145849826\n",
      "Iteration: 10823000, loss: 0.10605536240732258, gradient norm: 0.6973988951550564\n",
      "Iteration: 10824000, loss: 0.1060554356331271, gradient norm: 0.44040825156527497\n",
      "Iteration: 10825000, loss: 0.1060553699244582, gradient norm: 0.7337946284074963\n",
      "Iteration: 10826000, loss: 0.10605502845379777, gradient norm: 0.41174237767438693\n",
      "Iteration: 10827000, loss: 0.10605511673667271, gradient norm: 0.40562890047962613\n",
      "Iteration: 10828000, loss: 0.10605486317492265, gradient norm: 0.4252998286044298\n",
      "Iteration: 10829000, loss: 0.10605481276117365, gradient norm: 0.34091663325320426\n",
      "Iteration: 10830000, loss: 0.10605468398460534, gradient norm: 0.3779611689194501\n",
      "Iteration: 10831000, loss: 0.10605443809247361, gradient norm: 0.811276065325863\n",
      "Iteration: 10832000, loss: 0.1060546685239125, gradient norm: 0.08104632548654395\n",
      "Iteration: 10833000, loss: 0.10605436396028349, gradient norm: 0.5664121640818494\n",
      "Iteration: 10834000, loss: 0.10605406762737767, gradient norm: 0.3748876433438237\n",
      "Iteration: 10835000, loss: 0.10605401597493502, gradient norm: 0.3494403140237559\n",
      "Iteration: 10836000, loss: 0.10605402174805206, gradient norm: 0.28546389177465575\n",
      "Iteration: 10837000, loss: 0.10605370251383689, gradient norm: 0.43126377257019305\n",
      "Iteration: 10838000, loss: 0.10605363023258889, gradient norm: 0.3114248131285472\n",
      "Iteration: 10839000, loss: 0.10605376003558788, gradient norm: 0.11595549219053648\n",
      "Iteration: 10840000, loss: 0.10605319642187475, gradient norm: 0.11209285470938209\n",
      "Iteration: 10841000, loss: 0.10605335717264873, gradient norm: 0.8368770146245464\n",
      "Iteration: 10842000, loss: 0.10605317019567938, gradient norm: 0.23567121932937296\n",
      "Iteration: 10843000, loss: 0.10605305133096081, gradient norm: 0.38353877049484475\n",
      "Iteration: 10844000, loss: 0.10605290214145072, gradient norm: 0.8242121828486344\n",
      "Iteration: 10845000, loss: 0.10605285056969226, gradient norm: 0.19776501758948034\n",
      "Iteration: 10846000, loss: 0.10605260515746451, gradient norm: 0.4505165309893095\n",
      "Iteration: 10847000, loss: 0.1060524735154904, gradient norm: 0.29943450720185827\n",
      "Iteration: 10848000, loss: 0.10605245196077058, gradient norm: 0.7983925250287545\n",
      "Iteration: 10849000, loss: 0.10605234594684612, gradient norm: 0.9069033793003305\n",
      "Iteration: 10850000, loss: 0.10605205818739068, gradient norm: 0.7616354290478292\n",
      "Iteration: 10851000, loss: 0.10605215614512829, gradient norm: 0.25680886643426243\n",
      "Iteration: 10852000, loss: 0.10605187467780224, gradient norm: 0.1747048939860942\n",
      "Iteration: 10853000, loss: 0.10605192126767048, gradient norm: 0.4010166694253217\n",
      "Iteration: 10854000, loss: 0.10605165549515813, gradient norm: 0.333295461132328\n",
      "Iteration: 10855000, loss: 0.10605147155273167, gradient norm: 1.1607610290398527\n",
      "Iteration: 10856000, loss: 0.10605137001010465, gradient norm: 0.17364235120098942\n",
      "Iteration: 10857000, loss: 0.10605137621716473, gradient norm: 0.285080651521161\n",
      "Iteration: 10858000, loss: 0.10605127235607191, gradient norm: 0.38799913629934807\n",
      "Iteration: 10859000, loss: 0.10605107814158353, gradient norm: 0.805131168202959\n",
      "Iteration: 10860000, loss: 0.10605091048537925, gradient norm: 0.4039695976005838\n",
      "Iteration: 10861000, loss: 0.10605077264338507, gradient norm: 0.39134968376066925\n",
      "Iteration: 10862000, loss: 0.10605064498069496, gradient norm: 0.059966475189320065\n",
      "Iteration: 10863000, loss: 0.10605061815468016, gradient norm: 0.19446219132648754\n",
      "Iteration: 10864000, loss: 0.10605042654925378, gradient norm: 0.3708079726419611\n",
      "Iteration: 10865000, loss: 0.10605024639749637, gradient norm: 0.18277799242477072\n",
      "Iteration: 10866000, loss: 0.10605019216899696, gradient norm: 0.17733610058086507\n",
      "Iteration: 10867000, loss: 0.10605015454140863, gradient norm: 0.5311477670004549\n",
      "Iteration: 10868000, loss: 0.10604979920518881, gradient norm: 0.2699649658834312\n",
      "Iteration: 10869000, loss: 0.10604982131312164, gradient norm: 0.2727369039840717\n",
      "Iteration: 10870000, loss: 0.10604972751516294, gradient norm: 0.3226122731968649\n",
      "Iteration: 10871000, loss: 0.10604960151121641, gradient norm: 0.8163687987798365\n",
      "Iteration: 10872000, loss: 0.10604936517329537, gradient norm: 0.15815488196934277\n",
      "Iteration: 10873000, loss: 0.10604940362649343, gradient norm: 0.179343401889994\n",
      "Iteration: 10874000, loss: 0.10604921686923169, gradient norm: 0.24036182268241724\n",
      "Iteration: 10875000, loss: 0.10604907715159557, gradient norm: 0.20812938138880702\n",
      "Iteration: 10876000, loss: 0.10604884586938661, gradient norm: 0.6594293226946077\n",
      "Iteration: 10877000, loss: 0.10604882975734974, gradient norm: 0.18730219655252947\n",
      "Iteration: 10878000, loss: 0.10604874947370506, gradient norm: 0.09195802074815154\n",
      "Iteration: 10879000, loss: 0.1060485767407619, gradient norm: 0.18996702731826795\n",
      "Iteration: 10880000, loss: 0.1060483122895022, gradient norm: 0.5510416200973399\n",
      "Iteration: 10881000, loss: 0.10604848229918744, gradient norm: 0.14748192680854025\n",
      "Iteration: 10882000, loss: 0.10604818400847528, gradient norm: 0.5427285767783752\n",
      "Iteration: 10883000, loss: 0.10604798594019997, gradient norm: 0.09365124177711673\n",
      "Iteration: 10884000, loss: 0.10604797679734713, gradient norm: 0.10166127266149015\n",
      "Iteration: 10885000, loss: 0.1060478036329742, gradient norm: 0.1850619362577799\n",
      "Iteration: 10886000, loss: 0.10604770199646174, gradient norm: 0.2243436611202264\n",
      "Iteration: 10887000, loss: 0.10604755032564472, gradient norm: 0.09198105014559042\n",
      "Iteration: 10888000, loss: 0.10604767070502158, gradient norm: 0.10277645225071003\n",
      "Iteration: 10889000, loss: 0.10604712551276228, gradient norm: 0.6061229984758082\n",
      "Iteration: 10890000, loss: 0.10604722538145844, gradient norm: 0.07806302665767341\n",
      "Iteration: 10891000, loss: 0.10604711213001312, gradient norm: 0.40244844882729575\n",
      "Iteration: 10892000, loss: 0.10604688871081336, gradient norm: 0.7057098450321804\n",
      "Iteration: 10893000, loss: 0.1060469501098307, gradient norm: 0.3034138610171153\n",
      "Iteration: 10894000, loss: 0.10604660771137914, gradient norm: 0.3301910620299092\n",
      "Iteration: 10895000, loss: 0.10604674958273051, gradient norm: 0.2696129017521981\n",
      "Iteration: 10896000, loss: 0.10604630382646693, gradient norm: 0.8650998142686501\n",
      "Iteration: 10897000, loss: 0.10604636877985532, gradient norm: 0.040603689689094745\n",
      "Iteration: 10898000, loss: 0.10604621047183468, gradient norm: 0.5340957970503966\n",
      "Iteration: 10899000, loss: 0.10604614395877633, gradient norm: 0.6438619835975898\n",
      "Iteration: 10900000, loss: 0.10604591337837924, gradient norm: 0.244523567422009\n",
      "Iteration: 10901000, loss: 0.10604597627467731, gradient norm: 0.447418506602399\n",
      "Iteration: 10902000, loss: 0.10604558951509153, gradient norm: 0.5414815769892231\n",
      "Iteration: 10903000, loss: 0.10604557020280239, gradient norm: 0.24588345127703728\n",
      "Iteration: 10904000, loss: 0.10604551057929319, gradient norm: 0.20920461177295813\n",
      "Iteration: 10905000, loss: 0.10604533672018296, gradient norm: 0.18403980583172352\n",
      "Iteration: 10906000, loss: 0.10604518569176966, gradient norm: 0.6300301653133746\n",
      "Iteration: 10907000, loss: 0.10604526354859573, gradient norm: 0.6077502990671405\n",
      "Iteration: 10908000, loss: 0.10604496797063524, gradient norm: 0.4380796555636971\n",
      "Iteration: 10909000, loss: 0.10604482375738175, gradient norm: 0.4859105582672154\n",
      "Iteration: 10910000, loss: 0.10604464224937225, gradient norm: 0.44813886836085576\n",
      "Iteration: 10911000, loss: 0.10604464847242061, gradient norm: 0.41110152062126315\n",
      "Iteration: 10912000, loss: 0.10604454761620119, gradient norm: 0.04012234432460342\n",
      "Iteration: 10913000, loss: 0.1060443644795111, gradient norm: 0.8698624715121607\n",
      "Iteration: 10914000, loss: 0.10604407549656462, gradient norm: 0.3076007883716889\n",
      "Iteration: 10915000, loss: 0.10604421291897803, gradient norm: 0.06862251175701238\n",
      "Iteration: 10916000, loss: 0.10604412351442125, gradient norm: 0.11581990172853385\n",
      "Iteration: 10917000, loss: 0.10604373568920036, gradient norm: 0.0579450254659787\n",
      "Iteration: 10918000, loss: 0.1060437329977937, gradient norm: 0.27020383390836844\n",
      "Iteration: 10919000, loss: 0.1060436203569934, gradient norm: 0.627459533888016\n",
      "Iteration: 10920000, loss: 0.1060433809370192, gradient norm: 0.14414261259233752\n",
      "Iteration: 10921000, loss: 0.10604338899993607, gradient norm: 0.4949922208221344\n",
      "Iteration: 10922000, loss: 0.10604332278777027, gradient norm: 0.39290339594557433\n",
      "Iteration: 10923000, loss: 0.10604309757183002, gradient norm: 0.4954341991170469\n",
      "Iteration: 10924000, loss: 0.1060430132026851, gradient norm: 0.1502310202724739\n",
      "Iteration: 10925000, loss: 0.10604275470383215, gradient norm: 0.20732208224667098\n",
      "Iteration: 10926000, loss: 0.10604292334093339, gradient norm: 0.5698055050019578\n",
      "Iteration: 10927000, loss: 0.10604248774093719, gradient norm: 0.26590972818818404\n",
      "Iteration: 10928000, loss: 0.10604266469747015, gradient norm: 0.14812283085607328\n",
      "Iteration: 10929000, loss: 0.10604240561486783, gradient norm: 0.16232410985538182\n",
      "Iteration: 10930000, loss: 0.10604219718391321, gradient norm: 0.40413182326788827\n",
      "Iteration: 10931000, loss: 0.10604214048736069, gradient norm: 0.10013429891508596\n",
      "Iteration: 10932000, loss: 0.10604205108589247, gradient norm: 0.0642849379107184\n",
      "Iteration: 10933000, loss: 0.10604186258708088, gradient norm: 0.08867902012893134\n",
      "Iteration: 10934000, loss: 0.10604174184577056, gradient norm: 0.6078380263571638\n",
      "Iteration: 10935000, loss: 0.10604173220689804, gradient norm: 0.05583080381605949\n",
      "Iteration: 10936000, loss: 0.10604148433934268, gradient norm: 0.34285835295848205\n",
      "Iteration: 10937000, loss: 0.10604134209832149, gradient norm: 0.14402436274286576\n",
      "Iteration: 10938000, loss: 0.1060414432290901, gradient norm: 0.28081700207434224\n",
      "Iteration: 10939000, loss: 0.10604102618070621, gradient norm: 0.6615069432073947\n",
      "Iteration: 10940000, loss: 0.10604111247997931, gradient norm: 0.4443001820715335\n",
      "Iteration: 10941000, loss: 0.1060406366059562, gradient norm: 0.21556910935353105\n",
      "Iteration: 10942000, loss: 0.1060409597665326, gradient norm: 0.25678332790280034\n",
      "Iteration: 10943000, loss: 0.10604067382579065, gradient norm: 0.0980610104209196\n",
      "Iteration: 10944000, loss: 0.10604049283953212, gradient norm: 0.08677662777794758\n",
      "Iteration: 10945000, loss: 0.1060404233388372, gradient norm: 0.700060809898337\n",
      "Iteration: 10946000, loss: 0.10604031188338635, gradient norm: 0.4939376814616001\n",
      "Iteration: 10947000, loss: 0.10604012249237087, gradient norm: 1.009959149703093\n",
      "Iteration: 10948000, loss: 0.1060401405782519, gradient norm: 1.0930046972780003\n",
      "Iteration: 10949000, loss: 0.10604004118337784, gradient norm: 0.07086998759512826\n",
      "Iteration: 10950000, loss: 0.10603975588024711, gradient norm: 0.5355921296508429\n",
      "Iteration: 10951000, loss: 0.1060395729609819, gradient norm: 0.6124342781433456\n",
      "Iteration: 10952000, loss: 0.10603955870228134, gradient norm: 0.24881826831222126\n",
      "Iteration: 10953000, loss: 0.10603933742052941, gradient norm: 0.3101604306278204\n",
      "Iteration: 10954000, loss: 0.10603946977409916, gradient norm: 0.26806553138566797\n",
      "Iteration: 10955000, loss: 0.10603912255512817, gradient norm: 0.4567667739444165\n",
      "Iteration: 10956000, loss: 0.10603914557724982, gradient norm: 0.3867031180691202\n",
      "Iteration: 10957000, loss: 0.10603887068730583, gradient norm: 0.3055041593005795\n",
      "Iteration: 10958000, loss: 0.1060386647142266, gradient norm: 0.4464517188138236\n",
      "Iteration: 10959000, loss: 0.10603876504133404, gradient norm: 0.8959572047137321\n",
      "Iteration: 10960000, loss: 0.10603852179394156, gradient norm: 0.24294636317564539\n",
      "Iteration: 10961000, loss: 0.10603844530195285, gradient norm: 0.2839171733492834\n",
      "Iteration: 10962000, loss: 0.10603843207735522, gradient norm: 0.42613115301588594\n",
      "Iteration: 10963000, loss: 0.10603816330012435, gradient norm: 0.11453933722242897\n",
      "Iteration: 10964000, loss: 0.10603803916933717, gradient norm: 0.22152269294175225\n",
      "Iteration: 10965000, loss: 0.10603790825647885, gradient norm: 0.39064507226542916\n",
      "Iteration: 10966000, loss: 0.10603779713136208, gradient norm: 0.25771380365024515\n",
      "Iteration: 10967000, loss: 0.10603773344247963, gradient norm: 0.6568690570185827\n",
      "Iteration: 10968000, loss: 0.10603750581926556, gradient norm: 0.2302253818642458\n",
      "Iteration: 10969000, loss: 0.10603751600344342, gradient norm: 0.07562502539143394\n",
      "Iteration: 10970000, loss: 0.10603746772006943, gradient norm: 0.28957074325532967\n",
      "Iteration: 10971000, loss: 0.10603726814075981, gradient norm: 0.2921605077538612\n",
      "Iteration: 10972000, loss: 0.10603705424022065, gradient norm: 0.36427313797196015\n",
      "Iteration: 10973000, loss: 0.10603705546003835, gradient norm: 0.18338448199116705\n",
      "Iteration: 10974000, loss: 0.10603667588405793, gradient norm: 0.34035701701283705\n",
      "Iteration: 10975000, loss: 0.10603675551189401, gradient norm: 0.22063519469459184\n",
      "Iteration: 10976000, loss: 0.10603675125256395, gradient norm: 0.27090388152239603\n",
      "Iteration: 10977000, loss: 0.10603643105111722, gradient norm: 0.3534675901749654\n",
      "Iteration: 10978000, loss: 0.106036286267645, gradient norm: 0.16963624785741804\n",
      "Iteration: 10979000, loss: 0.10603628799731521, gradient norm: 0.27307238666463457\n",
      "Iteration: 10980000, loss: 0.10603601470812078, gradient norm: 0.03246228625285205\n",
      "Iteration: 10981000, loss: 0.1060360666038386, gradient norm: 0.6154229226972179\n",
      "Iteration: 10982000, loss: 0.1060358554335123, gradient norm: 0.14333326200879154\n",
      "Iteration: 10983000, loss: 0.10603590966864213, gradient norm: 0.4168990221933311\n",
      "Iteration: 10984000, loss: 0.10603549067389338, gradient norm: 0.1078030324391917\n",
      "Iteration: 10985000, loss: 0.10603539990347816, gradient norm: 0.5498778906438234\n",
      "Iteration: 10986000, loss: 0.10603556984909579, gradient norm: 0.29168136811535716\n",
      "Iteration: 10987000, loss: 0.10603507766590997, gradient norm: 0.2903523645007116\n",
      "Iteration: 10988000, loss: 0.10603516726679092, gradient norm: 0.24580732135272884\n",
      "Iteration: 10989000, loss: 0.10603500936563268, gradient norm: 0.6472401953086716\n",
      "Iteration: 10990000, loss: 0.10603487451233891, gradient norm: 0.7529986077145093\n",
      "Iteration: 10991000, loss: 0.10603483627435707, gradient norm: 0.03589923155019226\n",
      "Iteration: 10992000, loss: 0.10603467490219172, gradient norm: 0.33986722615789505\n",
      "Iteration: 10993000, loss: 0.10603450341208001, gradient norm: 0.24942380984594215\n",
      "Iteration: 10994000, loss: 0.10603447661547688, gradient norm: 0.4743069970127065\n",
      "Iteration: 10995000, loss: 0.10603424030399833, gradient norm: 0.23419539693012187\n",
      "Iteration: 10996000, loss: 0.10603425079314523, gradient norm: 0.37536013702966936\n",
      "Iteration: 10997000, loss: 0.10603401688890605, gradient norm: 0.5141701333323877\n",
      "Iteration: 10998000, loss: 0.1060339236610549, gradient norm: 0.37685972123020167\n",
      "Iteration: 10999000, loss: 0.1060337633788769, gradient norm: 0.35534469833725807\n",
      "Iteration: 11000000, loss: 0.10603360209404783, gradient norm: 0.5670090818864253\n",
      "Iteration: 11001000, loss: 0.10603380558088356, gradient norm: 0.4726914993383431\n",
      "Iteration: 11002000, loss: 0.1060331669256062, gradient norm: 0.46865404220681894\n",
      "Iteration: 11003000, loss: 0.10603338630205679, gradient norm: 0.35380006807307546\n",
      "Iteration: 11004000, loss: 0.10603311322750128, gradient norm: 0.5547592190971872\n",
      "Iteration: 11005000, loss: 0.10603307723413656, gradient norm: 0.11663535523176982\n",
      "Iteration: 11006000, loss: 0.1060329288749608, gradient norm: 0.2664947292719809\n",
      "Iteration: 11007000, loss: 0.10603281929642161, gradient norm: 0.3454688172868006\n",
      "Iteration: 11008000, loss: 0.10603275445383295, gradient norm: 0.23283552509192687\n",
      "Iteration: 11009000, loss: 0.10603249485783103, gradient norm: 0.15096227389148084\n",
      "Iteration: 11010000, loss: 0.10603263760868993, gradient norm: 0.12289421512278852\n",
      "Iteration: 11011000, loss: 0.10603225553475472, gradient norm: 0.2625709106329677\n",
      "Iteration: 11012000, loss: 0.10603218662691222, gradient norm: 0.47095002268578234\n",
      "Iteration: 11013000, loss: 0.10603207438040266, gradient norm: 0.27302392523247426\n",
      "Iteration: 11014000, loss: 0.10603193680757378, gradient norm: 0.5535720650039595\n",
      "Iteration: 11015000, loss: 0.1060318714925349, gradient norm: 0.6089842299992066\n",
      "Iteration: 11016000, loss: 0.10603169955563692, gradient norm: 0.849112475861263\n",
      "Iteration: 11017000, loss: 0.10603168862977892, gradient norm: 0.6345994686464199\n",
      "Iteration: 11018000, loss: 0.10603146039941029, gradient norm: 0.24636879965505268\n",
      "Iteration: 11019000, loss: 0.10603146536819423, gradient norm: 0.19648289972113456\n",
      "Iteration: 11020000, loss: 0.10603104507380404, gradient norm: 0.3698247353760758\n",
      "Iteration: 11021000, loss: 0.10603111657174526, gradient norm: 0.3177567373377709\n",
      "Iteration: 11022000, loss: 0.10603106934007568, gradient norm: 0.6816406354638742\n",
      "Iteration: 11023000, loss: 0.10603086853303346, gradient norm: 0.1153297265827823\n",
      "Iteration: 11024000, loss: 0.10603060291112863, gradient norm: 0.6358277591786357\n",
      "Iteration: 11025000, loss: 0.10603064553522046, gradient norm: 0.489463953770065\n",
      "Iteration: 11026000, loss: 0.10603062835871847, gradient norm: 0.2480100851100467\n",
      "Iteration: 11027000, loss: 0.10603019141421785, gradient norm: 0.25230059201703986\n",
      "Iteration: 11028000, loss: 0.10603035089178701, gradient norm: 0.21829360689491348\n",
      "Iteration: 11029000, loss: 0.10603008038097406, gradient norm: 0.8153106532898106\n",
      "Iteration: 11030000, loss: 0.1060300555594057, gradient norm: 0.11969251677276002\n",
      "Iteration: 11031000, loss: 0.10602993964618725, gradient norm: 0.604129382337321\n",
      "Iteration: 11032000, loss: 0.10602974787310936, gradient norm: 0.22100044299085492\n",
      "Iteration: 11033000, loss: 0.10602972516547171, gradient norm: 0.5552719885333407\n",
      "Iteration: 11034000, loss: 0.10602954558126922, gradient norm: 0.44358866089235394\n",
      "Iteration: 11035000, loss: 0.10602933388784373, gradient norm: 0.26593567787635675\n",
      "Iteration: 11036000, loss: 0.10602935893039066, gradient norm: 0.12312098786588983\n",
      "Iteration: 11037000, loss: 0.10602914179089425, gradient norm: 0.43148961032420863\n",
      "Iteration: 11038000, loss: 0.10602883246698587, gradient norm: 0.525821933316255\n",
      "Iteration: 11039000, loss: 0.10602918450291307, gradient norm: 0.33737956342515374\n",
      "Iteration: 11040000, loss: 0.10602876926166997, gradient norm: 0.8926376270395834\n",
      "Iteration: 11041000, loss: 0.10602864385247271, gradient norm: 0.7494482408279926\n",
      "Iteration: 11042000, loss: 0.1060284797111648, gradient norm: 0.34712389903827556\n",
      "Iteration: 11043000, loss: 0.10602843975343511, gradient norm: 0.20508267234346672\n",
      "Iteration: 11044000, loss: 0.10602827559947753, gradient norm: 0.7914805327503435\n",
      "Iteration: 11045000, loss: 0.10602823309801002, gradient norm: 0.2884089265912768\n",
      "Iteration: 11046000, loss: 0.10602806491272834, gradient norm: 0.2064963289911945\n",
      "Iteration: 11047000, loss: 0.10602792923952863, gradient norm: 0.49246465288762076\n",
      "Iteration: 11048000, loss: 0.10602786247222376, gradient norm: 0.643261996056996\n",
      "Iteration: 11049000, loss: 0.10602768403532016, gradient norm: 0.23429678778166757\n",
      "Iteration: 11050000, loss: 0.10602757931342291, gradient norm: 0.7150674179154093\n",
      "Iteration: 11051000, loss: 0.10602756926363276, gradient norm: 0.016945072925144945\n",
      "Iteration: 11052000, loss: 0.10602729634325235, gradient norm: 0.07141568316053273\n",
      "Iteration: 11053000, loss: 0.10602715750021682, gradient norm: 0.47121731545931556\n",
      "Iteration: 11054000, loss: 0.10602719144183828, gradient norm: 0.076116066698803\n",
      "Iteration: 11055000, loss: 0.10602697208176538, gradient norm: 0.34239984366161846\n",
      "Iteration: 11056000, loss: 0.10602679198697704, gradient norm: 0.3736153043499974\n",
      "Iteration: 11057000, loss: 0.10602693989727938, gradient norm: 0.17565349700727592\n",
      "Iteration: 11058000, loss: 0.10602664048825916, gradient norm: 0.060906654262623115\n",
      "Iteration: 11059000, loss: 0.10602647004206453, gradient norm: 0.3866313850892331\n",
      "Iteration: 11060000, loss: 0.10602637982312765, gradient norm: 0.16979092247204008\n",
      "Iteration: 11061000, loss: 0.10602623447457447, gradient norm: 0.5574378705326766\n",
      "Iteration: 11062000, loss: 0.10602616019957313, gradient norm: 0.7721853698833513\n",
      "Iteration: 11063000, loss: 0.10602601552208231, gradient norm: 0.5066255544375787\n",
      "Iteration: 11064000, loss: 0.10602576917198224, gradient norm: 0.4616161653498144\n",
      "Iteration: 11065000, loss: 0.10602583391325653, gradient norm: 0.1637101733382384\n",
      "Iteration: 11066000, loss: 0.10602560545181591, gradient norm: 0.05828864473622957\n",
      "Iteration: 11067000, loss: 0.10602564162568472, gradient norm: 0.509136037502594\n",
      "Iteration: 11068000, loss: 0.10602530812712382, gradient norm: 0.05263136612955711\n",
      "Iteration: 11069000, loss: 0.10602531629298653, gradient norm: 0.6430235912399799\n",
      "Iteration: 11070000, loss: 0.10602520459875339, gradient norm: 0.33512925777146324\n",
      "Iteration: 11071000, loss: 0.1060249702094622, gradient norm: 0.6264586292868535\n",
      "Iteration: 11072000, loss: 0.1060248941291937, gradient norm: 0.2737290212930072\n",
      "Iteration: 11073000, loss: 0.10602490388667149, gradient norm: 0.521119194295602\n",
      "Iteration: 11074000, loss: 0.10602477051045134, gradient norm: 0.17204666281887954\n",
      "Iteration: 11075000, loss: 0.10602451340709738, gradient norm: 0.05636864083632544\n",
      "Iteration: 11076000, loss: 0.10602450084295725, gradient norm: 0.239752828268992\n",
      "Iteration: 11077000, loss: 0.10602421880480054, gradient norm: 0.4047642532298879\n",
      "Iteration: 11078000, loss: 0.10602424657017274, gradient norm: 0.16656737091227167\n",
      "Iteration: 11079000, loss: 0.1060240466065384, gradient norm: 0.09837425041346479\n",
      "Iteration: 11080000, loss: 0.10602395014599936, gradient norm: 0.3562416498697916\n",
      "Iteration: 11081000, loss: 0.10602383253682435, gradient norm: 0.2843972574696872\n",
      "Iteration: 11082000, loss: 0.10602370559794196, gradient norm: 0.27621381576747556\n",
      "Iteration: 11083000, loss: 0.10602352783640512, gradient norm: 0.09854618811467739\n",
      "Iteration: 11084000, loss: 0.10602355466931594, gradient norm: 0.1666508933426689\n",
      "Iteration: 11085000, loss: 0.10602328937436235, gradient norm: 0.3397850567416668\n",
      "Iteration: 11086000, loss: 0.10602322812749419, gradient norm: 0.428887258927655\n",
      "Iteration: 11087000, loss: 0.10602322623048425, gradient norm: 0.43987510173619115\n",
      "Iteration: 11088000, loss: 0.10602299424361558, gradient norm: 0.2685885156443505\n",
      "Iteration: 11089000, loss: 0.10602294677900564, gradient norm: 0.721906799382971\n",
      "Iteration: 11090000, loss: 0.10602259554125122, gradient norm: 0.7006837884681693\n",
      "Iteration: 11091000, loss: 0.10602262832263162, gradient norm: 0.2931825092815251\n",
      "Iteration: 11092000, loss: 0.1060225667247504, gradient norm: 0.4675329448455401\n",
      "Iteration: 11093000, loss: 0.10602235618469277, gradient norm: 1.1827065792580074\n",
      "Iteration: 11094000, loss: 0.10602224574219331, gradient norm: 0.23348196932365473\n",
      "Iteration: 11095000, loss: 0.10602218280169186, gradient norm: 0.3256292791645999\n",
      "Iteration: 11096000, loss: 0.1060221575292443, gradient norm: 0.39987991527488487\n",
      "Iteration: 11097000, loss: 0.10602177070437928, gradient norm: 0.21266728304205723\n",
      "Iteration: 11098000, loss: 0.10602177196874263, gradient norm: 0.2869753826358377\n",
      "Iteration: 11099000, loss: 0.1060215066771213, gradient norm: 0.4026027110395686\n",
      "Iteration: 11100000, loss: 0.10602162345145362, gradient norm: 0.08564853874616551\n",
      "Iteration: 11101000, loss: 0.10602145269421158, gradient norm: 0.32158830162176766\n",
      "Iteration: 11102000, loss: 0.10602141732850869, gradient norm: 0.6036973902892272\n",
      "Iteration: 11103000, loss: 0.10602115615053961, gradient norm: 0.37206345480536046\n",
      "Iteration: 11104000, loss: 0.10602090310794601, gradient norm: 0.03529468651352177\n",
      "Iteration: 11105000, loss: 0.10602103019173444, gradient norm: 0.5455849388044313\n",
      "Iteration: 11106000, loss: 0.106020802693066, gradient norm: 0.21035845548513446\n",
      "Iteration: 11107000, loss: 0.1060206547096241, gradient norm: 0.8544368231298275\n",
      "Iteration: 11108000, loss: 0.10602071930633823, gradient norm: 0.4374497468594016\n",
      "Iteration: 11109000, loss: 0.106020456981861, gradient norm: 0.14497755766125425\n",
      "Iteration: 11110000, loss: 0.10602028323511234, gradient norm: 0.471022853443871\n",
      "Iteration: 11111000, loss: 0.10602021523617162, gradient norm: 0.572395392309187\n",
      "Iteration: 11112000, loss: 0.10601998542220523, gradient norm: 0.2857211363545589\n",
      "Iteration: 11113000, loss: 0.10602013432974475, gradient norm: 0.379078973365818\n",
      "Iteration: 11114000, loss: 0.1060197672370527, gradient norm: 0.9416804917861987\n",
      "Iteration: 11115000, loss: 0.10601969419736923, gradient norm: 0.11309111305965595\n",
      "Iteration: 11116000, loss: 0.10601959931773734, gradient norm: 0.426106365911967\n",
      "Iteration: 11117000, loss: 0.10601953228772265, gradient norm: 0.4067841296796763\n",
      "Iteration: 11118000, loss: 0.10601935018443813, gradient norm: 0.3275674538857194\n",
      "Iteration: 11119000, loss: 0.10601929136137685, gradient norm: 0.10445010985606495\n",
      "Iteration: 11120000, loss: 0.1060192726557946, gradient norm: 1.042283982921719\n",
      "Iteration: 11121000, loss: 0.10601894516400347, gradient norm: 0.4326964414352583\n",
      "Iteration: 11122000, loss: 0.10601880983410247, gradient norm: 0.21757313961927763\n",
      "Iteration: 11123000, loss: 0.1060187852230359, gradient norm: 0.31708387494289997\n",
      "Iteration: 11124000, loss: 0.10601862428142846, gradient norm: 0.33257050798107257\n",
      "Iteration: 11125000, loss: 0.10601858814003834, gradient norm: 0.6866212717136285\n",
      "Iteration: 11126000, loss: 0.10601830526751237, gradient norm: 0.6253396505027092\n",
      "Iteration: 11127000, loss: 0.10601844400552216, gradient norm: 0.4260343061320659\n",
      "Iteration: 11128000, loss: 0.10601823804235588, gradient norm: 0.3035401402594377\n",
      "Iteration: 11129000, loss: 0.10601802077642457, gradient norm: 0.35748482949289706\n",
      "Iteration: 11130000, loss: 0.10601781993927674, gradient norm: 0.4213403984415819\n",
      "Iteration: 11131000, loss: 0.10601800046010343, gradient norm: 0.22852906141160786\n",
      "Iteration: 11132000, loss: 0.10601771505188427, gradient norm: 0.2521915672126131\n",
      "Iteration: 11133000, loss: 0.10601762179776822, gradient norm: 0.14336788356864247\n",
      "Iteration: 11134000, loss: 0.10601744121638708, gradient norm: 0.7203799409244299\n",
      "Iteration: 11135000, loss: 0.10601739801125759, gradient norm: 0.4693697193642662\n",
      "Iteration: 11136000, loss: 0.10601703290167423, gradient norm: 0.19035473759917462\n",
      "Iteration: 11137000, loss: 0.10601723876628744, gradient norm: 0.30119784675746\n",
      "Iteration: 11138000, loss: 0.1060169459853941, gradient norm: 0.3118949262214014\n",
      "Iteration: 11139000, loss: 0.10601679294032636, gradient norm: 0.40596405289679877\n",
      "Iteration: 11140000, loss: 0.10601686074427548, gradient norm: 0.29597743956293393\n",
      "Iteration: 11141000, loss: 0.10601662693886417, gradient norm: 0.16469442785539368\n",
      "Iteration: 11142000, loss: 0.10601641958538259, gradient norm: 0.5681344243616423\n",
      "Iteration: 11143000, loss: 0.10601636349341532, gradient norm: 0.2572949741723314\n",
      "Iteration: 11144000, loss: 0.10601629273222064, gradient norm: 0.06622427134739681\n",
      "Iteration: 11145000, loss: 0.10601609384497593, gradient norm: 0.14773108618970146\n",
      "Iteration: 11146000, loss: 0.106015992702292, gradient norm: 0.6313534660410642\n",
      "Iteration: 11147000, loss: 0.10601588912377986, gradient norm: 0.5222387352589707\n",
      "Iteration: 11148000, loss: 0.10601583534555681, gradient norm: 0.4430597854152754\n",
      "Iteration: 11149000, loss: 0.10601575798356001, gradient norm: 0.1715469210900873\n",
      "Iteration: 11150000, loss: 0.10601543557037683, gradient norm: 0.18413067890668997\n",
      "Iteration: 11151000, loss: 0.10601533214888001, gradient norm: 0.31677496391228127\n",
      "Iteration: 11152000, loss: 0.1060153824662943, gradient norm: 0.15056046322064906\n",
      "Iteration: 11153000, loss: 0.10601524554999253, gradient norm: 0.41959875558358084\n",
      "Iteration: 11154000, loss: 0.10601498456588584, gradient norm: 0.18592660434877253\n",
      "Iteration: 11155000, loss: 0.10601516042364689, gradient norm: 0.2668369283376356\n",
      "Iteration: 11156000, loss: 0.10601474743640449, gradient norm: 0.36443909117971873\n",
      "Iteration: 11157000, loss: 0.1060147868291528, gradient norm: 0.2394166881460514\n",
      "Iteration: 11158000, loss: 0.10601441732618641, gradient norm: 0.13009497411300713\n",
      "Iteration: 11159000, loss: 0.10601454148831878, gradient norm: 0.5232040650721403\n",
      "Iteration: 11160000, loss: 0.10601431092002886, gradient norm: 0.6073264387124463\n",
      "Iteration: 11161000, loss: 0.10601414887763917, gradient norm: 0.401475407594318\n",
      "Iteration: 11162000, loss: 0.10601421954222835, gradient norm: 0.18291587758516528\n",
      "Iteration: 11163000, loss: 0.10601394670236007, gradient norm: 0.37887028274923634\n",
      "Iteration: 11164000, loss: 0.10601401895930479, gradient norm: 0.5008211035202488\n",
      "Iteration: 11165000, loss: 0.10601366172254284, gradient norm: 0.2591362787369536\n",
      "Iteration: 11166000, loss: 0.10601354247860917, gradient norm: 0.3289794937554616\n",
      "Iteration: 11167000, loss: 0.10601348898219519, gradient norm: 0.31968994115713223\n",
      "Iteration: 11168000, loss: 0.10601345933143486, gradient norm: 0.08507648296382499\n",
      "Iteration: 11169000, loss: 0.10601329066612639, gradient norm: 0.7355005363553032\n",
      "Iteration: 11170000, loss: 0.10601331339153225, gradient norm: 0.4737496847015733\n",
      "Iteration: 11171000, loss: 0.10601290145125111, gradient norm: 0.9164139746872253\n",
      "Iteration: 11172000, loss: 0.10601291419734363, gradient norm: 0.5701388493111411\n",
      "Iteration: 11173000, loss: 0.10601281470221573, gradient norm: 0.19055381339734748\n",
      "Iteration: 11174000, loss: 0.10601269528381461, gradient norm: 0.1532160443338061\n",
      "Iteration: 11175000, loss: 0.10601249232814351, gradient norm: 0.09907042632162527\n",
      "Iteration: 11176000, loss: 0.10601245940238987, gradient norm: 0.40170284868719347\n",
      "Iteration: 11177000, loss: 0.10601243687989119, gradient norm: 0.39494737199866914\n",
      "Iteration: 11178000, loss: 0.10601233273632346, gradient norm: 0.4829091994408795\n",
      "Iteration: 11179000, loss: 0.10601185629558682, gradient norm: 0.08362735153031113\n",
      "Iteration: 11180000, loss: 0.10601203678641541, gradient norm: 0.43153859950045254\n",
      "Iteration: 11181000, loss: 0.10601174120481474, gradient norm: 0.047809597297317215\n",
      "Iteration: 11182000, loss: 0.10601170080278999, gradient norm: 0.13492325707713015\n",
      "Iteration: 11183000, loss: 0.10601168526065925, gradient norm: 0.3582043776089453\n",
      "Iteration: 11184000, loss: 0.10601156776201498, gradient norm: 0.2608055099717798\n",
      "Iteration: 11185000, loss: 0.10601129980942262, gradient norm: 0.15520422280951116\n",
      "Iteration: 11186000, loss: 0.1060113170241685, gradient norm: 0.5186555308155747\n",
      "Iteration: 11187000, loss: 0.10601102713358972, gradient norm: 0.38160547802165223\n",
      "Iteration: 11188000, loss: 0.1060110761446516, gradient norm: 0.12523129388080095\n",
      "Iteration: 11189000, loss: 0.10601091803157678, gradient norm: 0.13390203026508943\n",
      "Iteration: 11190000, loss: 0.10601087879070266, gradient norm: 0.7162641837749226\n",
      "Iteration: 11191000, loss: 0.1060105772842141, gradient norm: 0.07583852448305203\n",
      "Iteration: 11192000, loss: 0.10601061906621112, gradient norm: 0.3569328899031679\n",
      "Iteration: 11193000, loss: 0.1060103022433481, gradient norm: 0.4743435594606954\n",
      "Iteration: 11194000, loss: 0.10601033810146729, gradient norm: 0.3866829842248328\n",
      "Iteration: 11195000, loss: 0.10601014720790045, gradient norm: 0.5849803209355492\n",
      "Iteration: 11196000, loss: 0.10601012157125156, gradient norm: 0.1688051317994805\n",
      "Iteration: 11197000, loss: 0.10600990562480325, gradient norm: 0.2622580248977358\n",
      "Iteration: 11198000, loss: 0.1060098108461838, gradient norm: 0.0678741829487542\n",
      "Iteration: 11199000, loss: 0.10600982219133802, gradient norm: 0.2883263328666035\n",
      "Iteration: 11200000, loss: 0.10600942953799533, gradient norm: 0.5228795628976292\n",
      "Iteration: 11201000, loss: 0.10600948056627892, gradient norm: 0.3899229015752452\n",
      "Iteration: 11202000, loss: 0.10600937280364191, gradient norm: 0.45566239914807344\n",
      "Iteration: 11203000, loss: 0.1060091927225157, gradient norm: 0.24289326950636894\n",
      "Iteration: 11204000, loss: 0.10600918882023055, gradient norm: 0.4187476058477106\n",
      "Iteration: 11205000, loss: 0.10600893321797673, gradient norm: 0.13752121029828976\n",
      "Iteration: 11206000, loss: 0.10600884491236809, gradient norm: 0.49133134231526226\n",
      "Iteration: 11207000, loss: 0.10600879688469379, gradient norm: 0.12529030768172833\n",
      "Iteration: 11208000, loss: 0.10600860263276161, gradient norm: 0.3444401140201084\n",
      "Iteration: 11209000, loss: 0.10600854931583385, gradient norm: 0.7976340161877021\n",
      "Iteration: 11210000, loss: 0.10600834422121645, gradient norm: 0.2271019649781751\n",
      "Iteration: 11211000, loss: 0.10600834786017466, gradient norm: 0.9290672274396775\n",
      "Iteration: 11212000, loss: 0.10600822102908537, gradient norm: 0.05424467335631628\n",
      "Iteration: 11213000, loss: 0.10600798286838596, gradient norm: 0.8268970274358803\n",
      "Iteration: 11214000, loss: 0.10600795846456418, gradient norm: 0.4387181457397168\n",
      "Iteration: 11215000, loss: 0.10600782990094368, gradient norm: 0.17576764689891275\n",
      "Iteration: 11216000, loss: 0.10600753768583057, gradient norm: 0.16874962742236468\n",
      "Iteration: 11217000, loss: 0.10600781294238346, gradient norm: 0.2640782219636754\n",
      "Iteration: 11218000, loss: 0.10600718977614239, gradient norm: 0.5309816615034428\n",
      "Iteration: 11219000, loss: 0.10600748874492096, gradient norm: 0.1867419105888221\n",
      "Iteration: 11220000, loss: 0.10600718487543398, gradient norm: 0.3737233351109423\n",
      "Iteration: 11221000, loss: 0.10600719175525489, gradient norm: 0.06992813401848091\n",
      "Iteration: 11222000, loss: 0.10600677867831652, gradient norm: 0.48608140090622803\n",
      "Iteration: 11223000, loss: 0.10600693079493327, gradient norm: 0.2752950132223503\n",
      "Iteration: 11224000, loss: 0.10600668975806742, gradient norm: 0.149113372010841\n",
      "Iteration: 11225000, loss: 0.10600659693169334, gradient norm: 0.18014499606083145\n",
      "Iteration: 11226000, loss: 0.10600656379958044, gradient norm: 0.6154004240796522\n",
      "Iteration: 11227000, loss: 0.10600634638515635, gradient norm: 0.6869216607260482\n",
      "Iteration: 11228000, loss: 0.10600624616110754, gradient norm: 0.3019534973562608\n",
      "Iteration: 11229000, loss: 0.10600610689690021, gradient norm: 0.14393292470559058\n",
      "Iteration: 11230000, loss: 0.10600605625911425, gradient norm: 0.06496238297248903\n",
      "Iteration: 11231000, loss: 0.10600596053912563, gradient norm: 0.40930903420411535\n",
      "Iteration: 11232000, loss: 0.10600577854711098, gradient norm: 0.7298451121772795\n",
      "Iteration: 11233000, loss: 0.1060056320481786, gradient norm: 0.20393646193221188\n",
      "Iteration: 11234000, loss: 0.10600555862338797, gradient norm: 0.17777623373625917\n",
      "Iteration: 11235000, loss: 0.10600551158853799, gradient norm: 0.3481361287110223\n",
      "Iteration: 11236000, loss: 0.10600526883180388, gradient norm: 0.48814344905955026\n",
      "Iteration: 11237000, loss: 0.10600519872211345, gradient norm: 0.20334269535197957\n",
      "Iteration: 11238000, loss: 0.1060050874577274, gradient norm: 0.30006207590366174\n",
      "Iteration: 11239000, loss: 0.10600497043055758, gradient norm: 0.1197850755637703\n",
      "Iteration: 11240000, loss: 0.10600478620146132, gradient norm: 0.41028705921094943\n",
      "Iteration: 11241000, loss: 0.10600464585377224, gradient norm: 0.3609586885024914\n",
      "Iteration: 11242000, loss: 0.106004649970939, gradient norm: 0.6835468293165036\n",
      "Iteration: 11243000, loss: 0.10600437799756568, gradient norm: 0.47098725962602106\n",
      "Iteration: 11244000, loss: 0.10600451273207867, gradient norm: 0.5219810398020248\n",
      "Iteration: 11245000, loss: 0.10600416457341508, gradient norm: 0.3864414731502132\n",
      "Iteration: 11246000, loss: 0.10600415308230081, gradient norm: 0.5458446309217582\n",
      "Iteration: 11247000, loss: 0.10600398795924747, gradient norm: 0.6006130033996919\n",
      "Iteration: 11248000, loss: 0.10600397638108944, gradient norm: 0.6928824652818606\n",
      "Iteration: 11249000, loss: 0.10600383833586295, gradient norm: 0.27103326620062856\n",
      "Iteration: 11250000, loss: 0.10600345794793407, gradient norm: 0.3265942852711649\n",
      "Iteration: 11251000, loss: 0.10600360612209452, gradient norm: 0.20124706347290935\n",
      "Iteration: 11252000, loss: 0.10600359976755297, gradient norm: 0.11920161163511822\n",
      "Iteration: 11253000, loss: 0.10600321820574928, gradient norm: 0.35845143055582557\n",
      "Iteration: 11254000, loss: 0.10600344334498045, gradient norm: 0.5447569087848914\n",
      "Iteration: 11255000, loss: 0.1060029610356747, gradient norm: 0.5205402351135351\n",
      "Iteration: 11256000, loss: 0.1060028149169481, gradient norm: 0.6661321587676428\n",
      "Iteration: 11257000, loss: 0.10600278878615364, gradient norm: 0.3424738163272976\n",
      "Iteration: 11258000, loss: 0.10600272339652964, gradient norm: 0.29499942469033147\n",
      "Iteration: 11259000, loss: 0.10600268950683688, gradient norm: 0.38532371338486004\n",
      "Iteration: 11260000, loss: 0.10600237989045483, gradient norm: 0.18539934446447956\n",
      "Iteration: 11261000, loss: 0.10600241262525441, gradient norm: 0.09368885195602487\n",
      "Iteration: 11262000, loss: 0.10600220010584238, gradient norm: 0.20041107439819747\n",
      "Iteration: 11263000, loss: 0.10600220051409127, gradient norm: 0.17126472011819246\n",
      "Iteration: 11264000, loss: 0.10600196254013855, gradient norm: 0.38277111171971034\n",
      "Iteration: 11265000, loss: 0.10600183129998865, gradient norm: 0.8106323694407248\n",
      "Iteration: 11266000, loss: 0.10600179761863206, gradient norm: 0.18440035924438242\n",
      "Iteration: 11267000, loss: 0.10600179352110224, gradient norm: 0.3442910812781035\n",
      "Iteration: 11268000, loss: 0.1060014684029215, gradient norm: 0.4470243035013938\n",
      "Iteration: 11269000, loss: 0.1060013791678415, gradient norm: 0.08660758894539103\n",
      "Iteration: 11270000, loss: 0.10600135027166704, gradient norm: 0.20960391674385614\n",
      "Iteration: 11271000, loss: 0.10600115762202038, gradient norm: 0.1414050924149021\n",
      "Iteration: 11272000, loss: 0.10600120714183445, gradient norm: 0.16611465660100588\n",
      "Iteration: 11273000, loss: 0.10600077331980189, gradient norm: 0.3531186861510826\n",
      "Iteration: 11274000, loss: 0.10600098392394224, gradient norm: 0.4884638076607062\n",
      "Iteration: 11275000, loss: 0.10600062502565484, gradient norm: 0.2407550112848377\n",
      "Iteration: 11276000, loss: 0.10600067432035572, gradient norm: 0.4650581278185836\n",
      "Iteration: 11277000, loss: 0.1060005345447015, gradient norm: 0.6652654229223179\n",
      "Iteration: 11278000, loss: 0.10600030418034251, gradient norm: 0.48440621594202415\n",
      "Iteration: 11279000, loss: 0.10600016293563272, gradient norm: 0.4657166613430486\n",
      "Iteration: 11280000, loss: 0.1060002981446367, gradient norm: 0.17688306969152923\n",
      "Iteration: 11281000, loss: 0.10599985140004585, gradient norm: 0.3915103698387928\n",
      "Iteration: 11282000, loss: 0.10599983830027766, gradient norm: 0.5221900352909188\n",
      "Iteration: 11283000, loss: 0.10599978068530368, gradient norm: 0.27086906012391077\n",
      "Iteration: 11284000, loss: 0.1059997020712263, gradient norm: 0.1955640270340762\n",
      "Iteration: 11285000, loss: 0.10599960977843248, gradient norm: 0.3325231942549496\n",
      "Iteration: 11286000, loss: 0.10599929978184176, gradient norm: 0.2098545286869327\n",
      "Iteration: 11287000, loss: 0.10599928748191623, gradient norm: 0.06925577562315287\n",
      "Iteration: 11288000, loss: 0.10599920733416274, gradient norm: 0.32101110946631356\n",
      "Iteration: 11289000, loss: 0.10599915467564211, gradient norm: 1.066685733481442\n",
      "Iteration: 11290000, loss: 0.105998858960163, gradient norm: 0.35260041875431375\n",
      "Iteration: 11291000, loss: 0.10599880235126759, gradient norm: 0.3148175463690659\n",
      "Iteration: 11292000, loss: 0.10599865858807697, gradient norm: 0.09553487277134654\n",
      "Iteration: 11293000, loss: 0.10599883191829862, gradient norm: 0.34151212541215875\n",
      "Iteration: 11294000, loss: 0.10599829918165858, gradient norm: 0.2668770797654878\n",
      "Iteration: 11295000, loss: 0.10599843247727549, gradient norm: 0.1853652867196213\n",
      "Iteration: 11296000, loss: 0.10599833378623674, gradient norm: 0.2262969684506618\n",
      "Iteration: 11297000, loss: 0.10599818919929475, gradient norm: 0.23844283810030537\n",
      "Iteration: 11298000, loss: 0.10599785306345079, gradient norm: 0.47977534021302826\n",
      "Iteration: 11299000, loss: 0.1059980059372107, gradient norm: 0.8240389787832602\n",
      "Iteration: 11300000, loss: 0.10599766981506321, gradient norm: 0.3035588624195801\n",
      "Iteration: 11301000, loss: 0.10599787846826993, gradient norm: 0.24333808592844366\n",
      "Iteration: 11302000, loss: 0.10599743846525227, gradient norm: 0.42473860499195354\n",
      "Iteration: 11303000, loss: 0.1059973762020762, gradient norm: 0.26765755644076383\n",
      "Iteration: 11304000, loss: 0.10599726444005805, gradient norm: 0.22708322402781841\n",
      "Iteration: 11305000, loss: 0.10599718835015012, gradient norm: 0.23793497230779959\n",
      "Iteration: 11306000, loss: 0.10599706908094876, gradient norm: 0.17048920359404393\n",
      "Iteration: 11307000, loss: 0.10599705435627023, gradient norm: 0.14939981547361303\n",
      "Iteration: 11308000, loss: 0.10599683965136769, gradient norm: 0.28739402494731114\n",
      "Iteration: 11309000, loss: 0.10599671552382006, gradient norm: 0.31166318595283643\n",
      "Iteration: 11310000, loss: 0.10599656460494286, gradient norm: 0.7664089242002964\n",
      "Iteration: 11311000, loss: 0.10599653763748712, gradient norm: 0.5613619904669118\n",
      "Iteration: 11312000, loss: 0.10599638332715991, gradient norm: 0.20171726827912903\n",
      "Iteration: 11313000, loss: 0.10599621571975058, gradient norm: 0.3400102447535026\n",
      "Iteration: 11314000, loss: 0.10599622622028437, gradient norm: 0.5210569099401704\n",
      "Iteration: 11315000, loss: 0.10599590646389352, gradient norm: 0.6568025542532933\n",
      "Iteration: 11316000, loss: 0.10599590353819818, gradient norm: 0.5185730719373197\n",
      "Iteration: 11317000, loss: 0.10599591447924749, gradient norm: 0.8594112158916816\n",
      "Iteration: 11318000, loss: 0.10599558226868369, gradient norm: 0.5319386251134429\n",
      "Iteration: 11319000, loss: 0.10599545091878858, gradient norm: 0.45271887207379197\n",
      "Iteration: 11320000, loss: 0.10599561354232125, gradient norm: 0.6529294408187437\n",
      "Iteration: 11321000, loss: 0.10599514130695353, gradient norm: 0.333239251596572\n",
      "Iteration: 11322000, loss: 0.10599516929204236, gradient norm: 0.8370095804327042\n",
      "Iteration: 11323000, loss: 0.10599520697910834, gradient norm: 0.5973857043594878\n",
      "Iteration: 11324000, loss: 0.10599490751722188, gradient norm: 0.12275457780592103\n",
      "Iteration: 11325000, loss: 0.10599491707411539, gradient norm: 0.7203378708607214\n",
      "Iteration: 11326000, loss: 0.10599468969626127, gradient norm: 0.09635650997286369\n",
      "Iteration: 11327000, loss: 0.1059947074411677, gradient norm: 0.3083301485130251\n",
      "Iteration: 11328000, loss: 0.10599442434884979, gradient norm: 0.22120999096710423\n",
      "Iteration: 11329000, loss: 0.10599431921571108, gradient norm: 0.6572298780835991\n",
      "Iteration: 11330000, loss: 0.10599424974488551, gradient norm: 0.05793423449749183\n",
      "Iteration: 11331000, loss: 0.10599427585999632, gradient norm: 0.5791871520810268\n",
      "Iteration: 11332000, loss: 0.10599387614431623, gradient norm: 0.25325377945158634\n",
      "Iteration: 11333000, loss: 0.10599388205022053, gradient norm: 0.10470352761520935\n",
      "Iteration: 11334000, loss: 0.10599374971500819, gradient norm: 0.6847991959097383\n",
      "Iteration: 11335000, loss: 0.10599392166624275, gradient norm: 0.3669118969953625\n",
      "Iteration: 11336000, loss: 0.10599354091413889, gradient norm: 0.7254234817935042\n",
      "Iteration: 11337000, loss: 0.10599338306107962, gradient norm: 0.46239230739045245\n",
      "Iteration: 11338000, loss: 0.10599339881282308, gradient norm: 0.5224492057513866\n",
      "Iteration: 11339000, loss: 0.1059930768766374, gradient norm: 0.5538747179169784\n",
      "Iteration: 11340000, loss: 0.10599317702531665, gradient norm: 0.7758610495649053\n",
      "Iteration: 11341000, loss: 0.10599306098991432, gradient norm: 0.4927909626702144\n",
      "Iteration: 11342000, loss: 0.10599281768291603, gradient norm: 0.7222448491281204\n",
      "Iteration: 11343000, loss: 0.10599266291049514, gradient norm: 0.8339834546456814\n",
      "Iteration: 11344000, loss: 0.10599269950410574, gradient norm: 0.7252314864778482\n",
      "Iteration: 11345000, loss: 0.10599244403564288, gradient norm: 0.26983253089007286\n",
      "Iteration: 11346000, loss: 0.10599258360625533, gradient norm: 0.16782860198205626\n",
      "Iteration: 11347000, loss: 0.10599213122610812, gradient norm: 0.5784248623204831\n",
      "Iteration: 11348000, loss: 0.10599220193483756, gradient norm: 0.2396140512648092\n",
      "Iteration: 11349000, loss: 0.10599199652848147, gradient norm: 0.3304275120957176\n",
      "Iteration: 11350000, loss: 0.10599200875777093, gradient norm: 0.26522386463345493\n",
      "Iteration: 11351000, loss: 0.10599182681362666, gradient norm: 0.21913603398183043\n",
      "Iteration: 11352000, loss: 0.10599161536066214, gradient norm: 0.2562334581915066\n",
      "Iteration: 11353000, loss: 0.10599154082315695, gradient norm: 0.11550678672851189\n",
      "Iteration: 11354000, loss: 0.10599157333478167, gradient norm: 0.6625363106807954\n",
      "Iteration: 11355000, loss: 0.10599134875473039, gradient norm: 0.3954399769550943\n",
      "Iteration: 11356000, loss: 0.10599122069434756, gradient norm: 0.08224776702902095\n",
      "Iteration: 11357000, loss: 0.10599111763586783, gradient norm: 0.1110992079022038\n",
      "Iteration: 11358000, loss: 0.10599096894817663, gradient norm: 0.5785223956223601\n",
      "Iteration: 11359000, loss: 0.10599085448036658, gradient norm: 0.5808537699698646\n",
      "Iteration: 11360000, loss: 0.1059907767537466, gradient norm: 0.5888180983614331\n",
      "Iteration: 11361000, loss: 0.10599069634829297, gradient norm: 0.1587223956380204\n",
      "Iteration: 11362000, loss: 0.10599062013875427, gradient norm: 0.34562474393752934\n",
      "Iteration: 11363000, loss: 0.10599033500112426, gradient norm: 0.0628395334328497\n",
      "Iteration: 11364000, loss: 0.10599019370964698, gradient norm: 0.19136364448903528\n",
      "Iteration: 11365000, loss: 0.10599021976880829, gradient norm: 0.24869347721665716\n",
      "Iteration: 11366000, loss: 0.10599004684190894, gradient norm: 0.6764556405972386\n",
      "Iteration: 11367000, loss: 0.1059900174195182, gradient norm: 0.30844692281242264\n",
      "Iteration: 11368000, loss: 0.10598985711405952, gradient norm: 0.4088424863164573\n",
      "Iteration: 11369000, loss: 0.10598967291801634, gradient norm: 0.12093216955729102\n",
      "Iteration: 11370000, loss: 0.10598953790489915, gradient norm: 0.11368194846904862\n",
      "Iteration: 11371000, loss: 0.10598966135907745, gradient norm: 0.08954371180932855\n",
      "Iteration: 11372000, loss: 0.10598934953319827, gradient norm: 0.2789421943710575\n",
      "Iteration: 11373000, loss: 0.10598922584327047, gradient norm: 0.36225330835139213\n",
      "Iteration: 11374000, loss: 0.10598909765414176, gradient norm: 0.2125681816214035\n",
      "Iteration: 11375000, loss: 0.10598900587310489, gradient norm: 0.2862360795654854\n",
      "Iteration: 11376000, loss: 0.10598895190393952, gradient norm: 1.0706097043233904\n",
      "Iteration: 11377000, loss: 0.10598883722335109, gradient norm: 0.20097349597537192\n",
      "Iteration: 11378000, loss: 0.10598866128740715, gradient norm: 0.25912717691837983\n",
      "Iteration: 11379000, loss: 0.10598852382806125, gradient norm: 0.2582839373844129\n",
      "Iteration: 11380000, loss: 0.10598850942886841, gradient norm: 0.5454190285623702\n",
      "Iteration: 11381000, loss: 0.10598834136080913, gradient norm: 0.3164274369688681\n",
      "Iteration: 11382000, loss: 0.10598813371935419, gradient norm: 0.3748656282341659\n",
      "Iteration: 11383000, loss: 0.10598821827859432, gradient norm: 0.15560789972687025\n",
      "Iteration: 11384000, loss: 0.10598786890943003, gradient norm: 0.2789918254172133\n",
      "Iteration: 11385000, loss: 0.10598792073880417, gradient norm: 0.44674806942215517\n",
      "Iteration: 11386000, loss: 0.10598762421984324, gradient norm: 0.04322119468189249\n",
      "Iteration: 11387000, loss: 0.10598779413132985, gradient norm: 0.37043121920698213\n",
      "Iteration: 11388000, loss: 0.10598746093327281, gradient norm: 0.3017015320857879\n",
      "Iteration: 11389000, loss: 0.10598749073032357, gradient norm: 0.20148302825309847\n",
      "Iteration: 11390000, loss: 0.10598723967880484, gradient norm: 0.34343135329059005\n",
      "Iteration: 11391000, loss: 0.10598709215316998, gradient norm: 0.513588135833745\n",
      "Iteration: 11392000, loss: 0.10598709550719285, gradient norm: 0.5244307966293013\n",
      "Iteration: 11393000, loss: 0.10598703619521214, gradient norm: 0.17066628134961032\n",
      "Iteration: 11394000, loss: 0.1059866351317137, gradient norm: 0.7684617894198403\n",
      "Iteration: 11395000, loss: 0.10598670448260902, gradient norm: 0.2148626024024036\n",
      "Iteration: 11396000, loss: 0.10598662644405735, gradient norm: 0.31268256710509484\n",
      "Iteration: 11397000, loss: 0.10598651891719862, gradient norm: 0.19575954486287483\n",
      "Iteration: 11398000, loss: 0.10598627467844625, gradient norm: 0.39740679731909906\n",
      "Iteration: 11399000, loss: 0.10598629277030122, gradient norm: 0.52645184353251\n",
      "Iteration: 11400000, loss: 0.10598627652045653, gradient norm: 0.5211469253966522\n",
      "Iteration: 11401000, loss: 0.10598595760234798, gradient norm: 0.39427114701114147\n",
      "Iteration: 11402000, loss: 0.10598597678719476, gradient norm: 0.24827385736884797\n",
      "Iteration: 11403000, loss: 0.10598571285955478, gradient norm: 0.12459282826951025\n",
      "Iteration: 11404000, loss: 0.10598562508078746, gradient norm: 0.3005010188245084\n",
      "Iteration: 11405000, loss: 0.10598560252488108, gradient norm: 0.41845547705369\n",
      "Iteration: 11406000, loss: 0.10598537058563871, gradient norm: 0.12983164978097742\n",
      "Iteration: 11407000, loss: 0.10598536742037025, gradient norm: 0.4198557734007897\n",
      "Iteration: 11408000, loss: 0.105985118327189, gradient norm: 0.30193584533901013\n",
      "Iteration: 11409000, loss: 0.1059850352231529, gradient norm: 0.13418579966899896\n",
      "Iteration: 11410000, loss: 0.10598512247227657, gradient norm: 0.5068025851972496\n",
      "Iteration: 11411000, loss: 0.10598481485643496, gradient norm: 0.7090232936924525\n",
      "Iteration: 11412000, loss: 0.1059847023570552, gradient norm: 0.16697893892200028\n",
      "Iteration: 11413000, loss: 0.10598447927946161, gradient norm: 0.33446128704844885\n",
      "Iteration: 11414000, loss: 0.10598456390520937, gradient norm: 0.1547911500430439\n",
      "Iteration: 11415000, loss: 0.10598446114768362, gradient norm: 0.3107963152638628\n",
      "Iteration: 11416000, loss: 0.10598424211442202, gradient norm: 0.15155373119131407\n",
      "Iteration: 11417000, loss: 0.10598414464798958, gradient norm: 0.1365394718225218\n",
      "Iteration: 11418000, loss: 0.10598404834658819, gradient norm: 0.4641378500489413\n",
      "Iteration: 11419000, loss: 0.1059839811154144, gradient norm: 0.235943244797886\n",
      "Iteration: 11420000, loss: 0.10598377296558317, gradient norm: 0.35738940636144784\n",
      "Iteration: 11421000, loss: 0.1059837545607632, gradient norm: 0.1213697045414133\n",
      "Iteration: 11422000, loss: 0.10598356130213905, gradient norm: 0.3487735541725122\n",
      "Iteration: 11423000, loss: 0.10598348908322232, gradient norm: 0.520513592394294\n",
      "Iteration: 11424000, loss: 0.10598328866895977, gradient norm: 0.6817353974560525\n",
      "Iteration: 11425000, loss: 0.10598331306308177, gradient norm: 0.14216073444968172\n",
      "Iteration: 11426000, loss: 0.10598315075389178, gradient norm: 0.12156129761059872\n",
      "Iteration: 11427000, loss: 0.10598297233181467, gradient norm: 0.13224602428936616\n",
      "Iteration: 11428000, loss: 0.10598294220144876, gradient norm: 0.19426408493404657\n",
      "Iteration: 11429000, loss: 0.10598274512182562, gradient norm: 0.20502151316030787\n",
      "Iteration: 11430000, loss: 0.10598262312376758, gradient norm: 0.3418876617105699\n",
      "Iteration: 11431000, loss: 0.10598259728366438, gradient norm: 0.7984726778321891\n",
      "Iteration: 11432000, loss: 0.10598261229595299, gradient norm: 0.11817047401380089\n",
      "Iteration: 11433000, loss: 0.1059821585061812, gradient norm: 0.7522419855234741\n",
      "Iteration: 11434000, loss: 0.10598209442679259, gradient norm: 0.28141453385245335\n",
      "Iteration: 11435000, loss: 0.10598228945430109, gradient norm: 0.6876443229927489\n",
      "Iteration: 11436000, loss: 0.10598195558038451, gradient norm: 0.14136903931214037\n",
      "Iteration: 11437000, loss: 0.10598181196479965, gradient norm: 0.31630374839495945\n",
      "Iteration: 11438000, loss: 0.10598175232555027, gradient norm: 0.41444548601368847\n",
      "Iteration: 11439000, loss: 0.10598164260472416, gradient norm: 0.18823914422406623\n",
      "Iteration: 11440000, loss: 0.10598161931399402, gradient norm: 0.17896092812789596\n",
      "Iteration: 11441000, loss: 0.10598115842626445, gradient norm: 0.8516186959413257\n",
      "Iteration: 11442000, loss: 0.10598129044167831, gradient norm: 0.08807091818921876\n",
      "Iteration: 11443000, loss: 0.10598125727495308, gradient norm: 0.4732519909538643\n",
      "Iteration: 11444000, loss: 0.1059809746184728, gradient norm: 0.15018763080836728\n",
      "Iteration: 11445000, loss: 0.10598096142914576, gradient norm: 0.22997729454675936\n",
      "Iteration: 11446000, loss: 0.10598082487244287, gradient norm: 0.29245241767166635\n",
      "Iteration: 11447000, loss: 0.10598054240486546, gradient norm: 0.35503685833668625\n",
      "Iteration: 11448000, loss: 0.10598063424984454, gradient norm: 0.3559677497461353\n",
      "Iteration: 11449000, loss: 0.10598056736520241, gradient norm: 0.31831558447671604\n",
      "Iteration: 11450000, loss: 0.10598037057465659, gradient norm: 0.2718821555665459\n",
      "Iteration: 11451000, loss: 0.10598023535866691, gradient norm: 0.36053370490451386\n",
      "Iteration: 11452000, loss: 0.10598009589764973, gradient norm: 0.18844141125767747\n",
      "Iteration: 11453000, loss: 0.10598010551580188, gradient norm: 0.6027808426417732\n",
      "Iteration: 11454000, loss: 0.10597996918177491, gradient norm: 0.28882628770864943\n",
      "Iteration: 11455000, loss: 0.1059797759950446, gradient norm: 0.10618700074954894\n",
      "Iteration: 11456000, loss: 0.10597961610208477, gradient norm: 0.5461750882043244\n",
      "Iteration: 11457000, loss: 0.10597959537188566, gradient norm: 0.4365186787706296\n",
      "Iteration: 11458000, loss: 0.10597931776471552, gradient norm: 0.43155301452823197\n",
      "Iteration: 11459000, loss: 0.10597938814548692, gradient norm: 0.3312358635321341\n",
      "Iteration: 11460000, loss: 0.10597936855743201, gradient norm: 0.21773936507027247\n",
      "Iteration: 11461000, loss: 0.10597881155649802, gradient norm: 0.2903057107991981\n",
      "Iteration: 11462000, loss: 0.10597908403897732, gradient norm: 0.8415604528842654\n",
      "Iteration: 11463000, loss: 0.1059790145278376, gradient norm: 0.10371667569729855\n",
      "Iteration: 11464000, loss: 0.1059786264744145, gradient norm: 0.159517929719437\n",
      "Iteration: 11465000, loss: 0.10597899756593551, gradient norm: 0.5907368627788565\n",
      "Iteration: 11466000, loss: 0.10597843018387541, gradient norm: 0.19951846243970178\n",
      "Iteration: 11467000, loss: 0.10597823142436592, gradient norm: 0.5797656940626323\n",
      "Iteration: 11468000, loss: 0.10597831996914689, gradient norm: 0.2310843432144251\n",
      "Iteration: 11469000, loss: 0.10597811665201595, gradient norm: 0.5600947194276722\n",
      "Iteration: 11470000, loss: 0.10597805448988223, gradient norm: 0.39451197529559723\n",
      "Iteration: 11471000, loss: 0.10597806100481004, gradient norm: 0.37552868169273634\n",
      "Iteration: 11472000, loss: 0.10597782925258954, gradient norm: 0.404671034506998\n",
      "Iteration: 11473000, loss: 0.10597770654576925, gradient norm: 0.05045912748857113\n",
      "Iteration: 11474000, loss: 0.10597767977992831, gradient norm: 0.09778013324294622\n",
      "Iteration: 11475000, loss: 0.10597734556260414, gradient norm: 0.1374340400002194\n",
      "Iteration: 11476000, loss: 0.10597748246270616, gradient norm: 0.9808005369007456\n",
      "Iteration: 11477000, loss: 0.10597718285515316, gradient norm: 0.3082854990849464\n",
      "Iteration: 11478000, loss: 0.10597730065528144, gradient norm: 0.5243875301160196\n",
      "Iteration: 11479000, loss: 0.10597687967294644, gradient norm: 0.2889924253336123\n",
      "Iteration: 11480000, loss: 0.10597706984045177, gradient norm: 0.44179927979591077\n",
      "Iteration: 11481000, loss: 0.10597672229315959, gradient norm: 0.24194481043310853\n",
      "Iteration: 11482000, loss: 0.10597677179642055, gradient norm: 0.06861285816329386\n",
      "Iteration: 11483000, loss: 0.10597651366029075, gradient norm: 0.3063686331434273\n",
      "Iteration: 11484000, loss: 0.10597650186206363, gradient norm: 0.5015719318524736\n",
      "Iteration: 11485000, loss: 0.10597628241477813, gradient norm: 0.5266241574900191\n",
      "Iteration: 11486000, loss: 0.10597613830579815, gradient norm: 0.0596847137512392\n",
      "Iteration: 11487000, loss: 0.10597618824897881, gradient norm: 0.29281644951831687\n",
      "Iteration: 11488000, loss: 0.10597598459764589, gradient norm: 0.5939816110261354\n",
      "Iteration: 11489000, loss: 0.10597585085758097, gradient norm: 0.7762828009429439\n",
      "Iteration: 11490000, loss: 0.10597581878836117, gradient norm: 0.7694116818520865\n",
      "Iteration: 11491000, loss: 0.10597567651950324, gradient norm: 0.3820932902378577\n",
      "Iteration: 11492000, loss: 0.10597546128057389, gradient norm: 0.3882299641848055\n",
      "Iteration: 11493000, loss: 0.10597543580553893, gradient norm: 0.32358839248717464\n",
      "Iteration: 11494000, loss: 0.10597535026818862, gradient norm: 0.2413365153368238\n",
      "Iteration: 11495000, loss: 0.10597518037148433, gradient norm: 0.2575134966664902\n",
      "Iteration: 11496000, loss: 0.10597514229267956, gradient norm: 0.182966240908908\n",
      "Iteration: 11497000, loss: 0.10597494479298246, gradient norm: 0.7044610189040275\n",
      "Iteration: 11498000, loss: 0.1059747883105377, gradient norm: 0.412560838291996\n",
      "Iteration: 11499000, loss: 0.1059747413336374, gradient norm: 0.24537074666870512\n",
      "Iteration: 11500000, loss: 0.10597471157254303, gradient norm: 0.2176067159559669\n",
      "Iteration: 11501000, loss: 0.10597454802830254, gradient norm: 0.2348750206810149\n",
      "Iteration: 11502000, loss: 0.10597435455663151, gradient norm: 1.1156423209503399\n",
      "Iteration: 11503000, loss: 0.10597418002543965, gradient norm: 0.29354617346458756\n",
      "Iteration: 11504000, loss: 0.10597417075796335, gradient norm: 0.510567089483717\n",
      "Iteration: 11505000, loss: 0.10597412381749005, gradient norm: 0.18057734143361664\n",
      "Iteration: 11506000, loss: 0.10597417236557037, gradient norm: 0.9415878036639134\n",
      "Iteration: 11507000, loss: 0.10597367105188613, gradient norm: 0.2158132157727871\n",
      "Iteration: 11508000, loss: 0.10597367044648044, gradient norm: 0.7754889980883742\n",
      "Iteration: 11509000, loss: 0.10597365531095232, gradient norm: 0.5797572534159924\n",
      "Iteration: 11510000, loss: 0.10597338768169846, gradient norm: 0.8093674140039989\n",
      "Iteration: 11511000, loss: 0.10597349258203928, gradient norm: 0.29768650259968205\n",
      "Iteration: 11512000, loss: 0.10597319844771923, gradient norm: 0.4336973961710223\n",
      "Iteration: 11513000, loss: 0.10597313575062921, gradient norm: 0.5032951724441446\n",
      "Iteration: 11514000, loss: 0.10597309128275188, gradient norm: 0.4344857624856651\n",
      "Iteration: 11515000, loss: 0.10597278181054515, gradient norm: 0.20135084287330884\n",
      "Iteration: 11516000, loss: 0.1059729399995835, gradient norm: 0.43029447049699426\n",
      "Iteration: 11517000, loss: 0.10597261102934318, gradient norm: 0.3232683120499208\n",
      "Iteration: 11518000, loss: 0.1059727102999069, gradient norm: 0.21033708004734358\n",
      "Iteration: 11519000, loss: 0.1059722633828644, gradient norm: 0.3865294960969935\n",
      "Iteration: 11520000, loss: 0.10597244337137414, gradient norm: 0.4827186389631842\n",
      "Iteration: 11521000, loss: 0.10597222438579448, gradient norm: 0.18730616350299656\n",
      "Iteration: 11522000, loss: 0.10597210928283468, gradient norm: 0.42724071756932475\n",
      "Iteration: 11523000, loss: 0.10597196215122939, gradient norm: 0.5112440555271911\n",
      "Iteration: 11524000, loss: 0.10597195323746199, gradient norm: 0.18702440812399002\n",
      "Iteration: 11525000, loss: 0.10597166593004519, gradient norm: 0.172593658480363\n",
      "Iteration: 11526000, loss: 0.10597165587106123, gradient norm: 0.7227803293193744\n",
      "Iteration: 11527000, loss: 0.10597159068889693, gradient norm: 0.35473226544129033\n",
      "Iteration: 11528000, loss: 0.10597156666379626, gradient norm: 0.029660442501136378\n",
      "Iteration: 11529000, loss: 0.10597114424828495, gradient norm: 0.2405841813347778\n",
      "Iteration: 11530000, loss: 0.1059712812342789, gradient norm: 0.5740826273423949\n",
      "Iteration: 11531000, loss: 0.10597100269301137, gradient norm: 0.13757552146505608\n",
      "Iteration: 11532000, loss: 0.1059710654071392, gradient norm: 0.3939332309256212\n",
      "Iteration: 11533000, loss: 0.10597094917824677, gradient norm: 0.2890120744370912\n",
      "Iteration: 11534000, loss: 0.10597074358728903, gradient norm: 0.6449876532158855\n",
      "Iteration: 11535000, loss: 0.1059704901341344, gradient norm: 0.617666312928181\n",
      "Iteration: 11536000, loss: 0.10597059035114273, gradient norm: 0.23024045458340892\n",
      "Iteration: 11537000, loss: 0.10597036555465482, gradient norm: 0.57819350652656\n",
      "Iteration: 11538000, loss: 0.10597038324539748, gradient norm: 0.6948585435675341\n",
      "Iteration: 11539000, loss: 0.1059701075876485, gradient norm: 0.233266971100551\n",
      "Iteration: 11540000, loss: 0.10597000007737557, gradient norm: 0.5840949737967803\n",
      "Iteration: 11541000, loss: 0.10597000308048857, gradient norm: 0.4479625409038588\n",
      "Iteration: 11542000, loss: 0.1059698459483089, gradient norm: 0.36298774529975186\n",
      "Iteration: 11543000, loss: 0.10596985085203084, gradient norm: 0.28181692340743686\n",
      "Iteration: 11544000, loss: 0.10596960691638839, gradient norm: 0.48676000528186275\n",
      "Iteration: 11545000, loss: 0.10596960351233975, gradient norm: 0.3539554807428129\n",
      "Iteration: 11546000, loss: 0.10596928995142753, gradient norm: 0.24473395468275008\n",
      "Iteration: 11547000, loss: 0.10596922079440584, gradient norm: 0.3651564309880461\n",
      "Iteration: 11548000, loss: 0.1059691476652564, gradient norm: 0.3513763134897874\n",
      "Iteration: 11549000, loss: 0.10596907646649065, gradient norm: 0.54300687538156\n",
      "Iteration: 11550000, loss: 0.10596900359418836, gradient norm: 0.05079563822249461\n",
      "Iteration: 11551000, loss: 0.10596882707068164, gradient norm: 0.3216809085066771\n",
      "Iteration: 11552000, loss: 0.10596878450160414, gradient norm: 0.5502641203747258\n",
      "Iteration: 11553000, loss: 0.10596844502330573, gradient norm: 0.23820692942267643\n",
      "Iteration: 11554000, loss: 0.1059684720788798, gradient norm: 0.3335351956127721\n",
      "Iteration: 11555000, loss: 0.10596838370263882, gradient norm: 0.14660702916069954\n",
      "Iteration: 11556000, loss: 0.10596826237286373, gradient norm: 0.30747257620279106\n",
      "Iteration: 11557000, loss: 0.10596816898628299, gradient norm: 0.31262953566764684\n",
      "Iteration: 11558000, loss: 0.10596802541297048, gradient norm: 0.27639304402988607\n",
      "Iteration: 11559000, loss: 0.10596790415927675, gradient norm: 0.3030232191762687\n",
      "Iteration: 11560000, loss: 0.10596777428538724, gradient norm: 0.3664271197908939\n",
      "Iteration: 11561000, loss: 0.10596773261664151, gradient norm: 0.2389040952188122\n",
      "Iteration: 11562000, loss: 0.10596752914289934, gradient norm: 0.6920811952453274\n",
      "Iteration: 11563000, loss: 0.10596747398434829, gradient norm: 0.20636895316761564\n",
      "Iteration: 11564000, loss: 0.10596728130375808, gradient norm: 0.13267494073762173\n",
      "Iteration: 11565000, loss: 0.10596728333008934, gradient norm: 0.07714494804415323\n",
      "Iteration: 11566000, loss: 0.10596708872088183, gradient norm: 0.27531800809324924\n",
      "Iteration: 11567000, loss: 0.10596711789158812, gradient norm: 0.14030036622969744\n",
      "Iteration: 11568000, loss: 0.10596694383699637, gradient norm: 0.571689282781922\n",
      "Iteration: 11569000, loss: 0.10596659344351529, gradient norm: 0.3559919430389809\n",
      "Iteration: 11570000, loss: 0.10596672638378425, gradient norm: 0.1763904434470973\n",
      "Iteration: 11571000, loss: 0.10596647728356658, gradient norm: 0.18865782584060112\n",
      "Iteration: 11572000, loss: 0.10596654204427805, gradient norm: 1.0370529670538273\n",
      "Iteration: 11573000, loss: 0.10596629496214616, gradient norm: 0.4633385262909065\n",
      "Iteration: 11574000, loss: 0.10596614094379314, gradient norm: 0.7291484751627542\n",
      "Iteration: 11575000, loss: 0.10596611785126976, gradient norm: 0.7347327283285481\n",
      "Iteration: 11576000, loss: 0.10596596204858832, gradient norm: 0.04213142144079489\n",
      "Iteration: 11577000, loss: 0.10596592193471784, gradient norm: 0.19464402891600546\n",
      "Iteration: 11578000, loss: 0.10596578918149357, gradient norm: 0.23645950425664478\n",
      "Iteration: 11579000, loss: 0.10596553738233039, gradient norm: 0.7172193585571737\n",
      "Iteration: 11580000, loss: 0.10596550466469895, gradient norm: 0.16844021899664818\n",
      "Iteration: 11581000, loss: 0.10596542128754487, gradient norm: 0.19336860776894962\n",
      "Iteration: 11582000, loss: 0.10596536895893714, gradient norm: 0.13992232292181692\n",
      "Iteration: 11583000, loss: 0.10596526833101333, gradient norm: 0.37054953830548865\n",
      "Iteration: 11584000, loss: 0.10596494627273502, gradient norm: 0.261838704490283\n",
      "Iteration: 11585000, loss: 0.10596512960983155, gradient norm: 0.4678102644326347\n",
      "Iteration: 11586000, loss: 0.10596478932715701, gradient norm: 0.3676741020657436\n",
      "Iteration: 11587000, loss: 0.10596477573856984, gradient norm: 0.10693620349454541\n",
      "Iteration: 11588000, loss: 0.10596453021084554, gradient norm: 0.2842576570461693\n",
      "Iteration: 11589000, loss: 0.10596457765800787, gradient norm: 0.15152876378338656\n",
      "Iteration: 11590000, loss: 0.10596435216149823, gradient norm: 0.4715197836029743\n",
      "Iteration: 11591000, loss: 0.10596442147542427, gradient norm: 0.7479505816153206\n",
      "Iteration: 11592000, loss: 0.10596413853900005, gradient norm: 1.0758664290327373\n",
      "Iteration: 11593000, loss: 0.10596406800136006, gradient norm: 0.12703483413018868\n",
      "Iteration: 11594000, loss: 0.10596392672448246, gradient norm: 0.25071044575883694\n",
      "Iteration: 11595000, loss: 0.10596380627199388, gradient norm: 0.30834412629199\n",
      "Iteration: 11596000, loss: 0.10596379609796601, gradient norm: 0.37224434513367816\n",
      "Iteration: 11597000, loss: 0.10596369859301981, gradient norm: 0.5227061178784389\n",
      "Iteration: 11598000, loss: 0.10596338091054261, gradient norm: 0.12069331227514905\n",
      "Iteration: 11599000, loss: 0.10596344531852186, gradient norm: 0.361340551494662\n",
      "Iteration: 11600000, loss: 0.10596326122463853, gradient norm: 0.5812384251500305\n",
      "Iteration: 11601000, loss: 0.10596318074939319, gradient norm: 0.2255508220577617\n",
      "Iteration: 11602000, loss: 0.10596303557807538, gradient norm: 0.9358645609371251\n",
      "Iteration: 11603000, loss: 0.10596294975217352, gradient norm: 0.3644869746551857\n",
      "Iteration: 11604000, loss: 0.10596279995860436, gradient norm: 0.3972961477885865\n",
      "Iteration: 11605000, loss: 0.10596268010326691, gradient norm: 0.2768793511610578\n",
      "Iteration: 11606000, loss: 0.10596267090307612, gradient norm: 0.267853643049158\n",
      "Iteration: 11607000, loss: 0.10596252509798924, gradient norm: 0.23642634830758244\n",
      "Iteration: 11608000, loss: 0.10596222205865403, gradient norm: 0.2640902021426901\n",
      "Iteration: 11609000, loss: 0.10596228371514715, gradient norm: 0.4207670424940157\n",
      "Iteration: 11610000, loss: 0.10596219048624395, gradient norm: 0.051692641862099195\n",
      "Iteration: 11611000, loss: 0.10596213352356613, gradient norm: 0.32546910073194363\n",
      "Iteration: 11612000, loss: 0.10596181833868737, gradient norm: 0.33068846405622465\n",
      "Iteration: 11613000, loss: 0.10596179468149339, gradient norm: 0.7810746481597179\n",
      "Iteration: 11614000, loss: 0.10596168160596162, gradient norm: 0.1621869820344022\n",
      "Iteration: 11615000, loss: 0.1059616381546047, gradient norm: 0.676744974814535\n",
      "Iteration: 11616000, loss: 0.10596150233589105, gradient norm: 0.6422739664522211\n",
      "Iteration: 11617000, loss: 0.10596138146571184, gradient norm: 0.6744023746749993\n",
      "Iteration: 11618000, loss: 0.10596128879119973, gradient norm: 0.0417929437656099\n",
      "Iteration: 11619000, loss: 0.10596120843657067, gradient norm: 0.43415300388953937\n",
      "Iteration: 11620000, loss: 0.10596100166106501, gradient norm: 0.2761223524192774\n",
      "Iteration: 11621000, loss: 0.105961071623065, gradient norm: 0.5866324240147439\n",
      "Iteration: 11622000, loss: 0.10596055342462192, gradient norm: 0.45840624109460065\n",
      "Iteration: 11623000, loss: 0.10596071277495572, gradient norm: 0.7409135098995242\n",
      "Iteration: 11624000, loss: 0.10596062234731411, gradient norm: 0.38464034194628305\n",
      "Iteration: 11625000, loss: 0.10596043445038358, gradient norm: 0.27083200535530766\n",
      "Iteration: 11626000, loss: 0.10596024607293367, gradient norm: 0.5162153832886408\n",
      "Iteration: 11627000, loss: 0.10596043806756038, gradient norm: 0.17273554849842263\n",
      "Iteration: 11628000, loss: 0.10596013091307112, gradient norm: 0.19375702881890258\n",
      "Iteration: 11629000, loss: 0.10596000228740121, gradient norm: 0.6080649251580865\n",
      "Iteration: 11630000, loss: 0.10595998676429852, gradient norm: 0.07531357056080293\n",
      "Iteration: 11631000, loss: 0.10595965913575515, gradient norm: 1.0029110131449772\n",
      "Iteration: 11632000, loss: 0.10595971481061732, gradient norm: 0.06997324903384176\n",
      "Iteration: 11633000, loss: 0.10595955939643903, gradient norm: 0.26538045797669596\n",
      "Iteration: 11634000, loss: 0.10595928219077157, gradient norm: 0.26052647684383073\n",
      "Iteration: 11635000, loss: 0.10595945697491299, gradient norm: 0.4858247406491212\n",
      "Iteration: 11636000, loss: 0.10595921636419749, gradient norm: 0.6002520053415227\n",
      "Iteration: 11637000, loss: 0.10595915448747514, gradient norm: 0.07596344969254992\n",
      "Iteration: 11638000, loss: 0.10595883812183857, gradient norm: 0.51213520396454\n",
      "Iteration: 11639000, loss: 0.1059590260441059, gradient norm: 0.5164845620962221\n",
      "Iteration: 11640000, loss: 0.10595874277508574, gradient norm: 0.3201652656651899\n",
      "Iteration: 11641000, loss: 0.10595862852742333, gradient norm: 0.01262637253956834\n",
      "Iteration: 11642000, loss: 0.10595859674499002, gradient norm: 0.21172191750259697\n",
      "Iteration: 11643000, loss: 0.10595840424483365, gradient norm: 0.808426917235694\n",
      "Iteration: 11644000, loss: 0.10595836988283776, gradient norm: 0.23844068122874265\n",
      "Iteration: 11645000, loss: 0.1059582790529384, gradient norm: 0.9487436476001393\n",
      "Iteration: 11646000, loss: 0.10595799623088031, gradient norm: 0.9214473172264862\n",
      "Iteration: 11647000, loss: 0.1059580603217754, gradient norm: 0.6332159622807139\n",
      "Iteration: 11648000, loss: 0.10595791958938275, gradient norm: 0.043858188210122204\n",
      "Iteration: 11649000, loss: 0.10595759879745685, gradient norm: 0.25267545100435845\n",
      "Iteration: 11650000, loss: 0.10595784403140047, gradient norm: 0.7530397680548885\n",
      "Iteration: 11651000, loss: 0.10595745887386117, gradient norm: 0.15256717063131553\n",
      "Iteration: 11652000, loss: 0.10595745960391426, gradient norm: 0.7651259381925996\n",
      "Iteration: 11653000, loss: 0.10595739364204189, gradient norm: 0.15136192430041595\n",
      "Iteration: 11654000, loss: 0.10595726348419407, gradient norm: 0.0540809202042656\n",
      "Iteration: 11655000, loss: 0.10595696375532737, gradient norm: 0.10924311738953967\n",
      "Iteration: 11656000, loss: 0.10595706994632004, gradient norm: 0.5524957125301221\n",
      "Iteration: 11657000, loss: 0.10595690023863576, gradient norm: 0.8274978114268449\n",
      "Iteration: 11658000, loss: 0.10595664145921929, gradient norm: 0.0286361941209847\n",
      "Iteration: 11659000, loss: 0.10595676514164318, gradient norm: 0.916786159333375\n",
      "Iteration: 11660000, loss: 0.10595651494374514, gradient norm: 0.19463370735957208\n",
      "Iteration: 11661000, loss: 0.10595641348310304, gradient norm: 0.25549955982250233\n",
      "Iteration: 11662000, loss: 0.10595631898671798, gradient norm: 0.24568355142651074\n",
      "Iteration: 11663000, loss: 0.10595621552559467, gradient norm: 0.5780807613700526\n",
      "Iteration: 11664000, loss: 0.10595603354810008, gradient norm: 0.3222723043873327\n",
      "Iteration: 11665000, loss: 0.10595607383779335, gradient norm: 0.22494514655952738\n",
      "Iteration: 11666000, loss: 0.10595581415176245, gradient norm: 0.5183368886409236\n",
      "Iteration: 11667000, loss: 0.10595572988323726, gradient norm: 0.37176300448452787\n",
      "Iteration: 11668000, loss: 0.10595576622510353, gradient norm: 0.37603788246854614\n",
      "Iteration: 11669000, loss: 0.10595552830896543, gradient norm: 0.11058163897803273\n",
      "Iteration: 11670000, loss: 0.10595543163995533, gradient norm: 0.21989092862875692\n",
      "Iteration: 11671000, loss: 0.105955172830042, gradient norm: 0.10095773288999974\n",
      "Iteration: 11672000, loss: 0.10595533114228516, gradient norm: 0.4986015571809466\n",
      "Iteration: 11673000, loss: 0.10595499388777625, gradient norm: 0.5198012352132568\n",
      "Iteration: 11674000, loss: 0.10595497614526189, gradient norm: 0.08516331682281665\n",
      "Iteration: 11675000, loss: 0.10595501547523574, gradient norm: 0.34636348356983376\n",
      "Iteration: 11676000, loss: 0.10595467423626814, gradient norm: 0.872329687200702\n",
      "Iteration: 11677000, loss: 0.10595466663170251, gradient norm: 0.40134038114361165\n",
      "Iteration: 11678000, loss: 0.10595438959768322, gradient norm: 0.4101249399815522\n",
      "Iteration: 11679000, loss: 0.10595445212148469, gradient norm: 0.36539768505297127\n",
      "Iteration: 11680000, loss: 0.10595429711706873, gradient norm: 0.3184894782456174\n",
      "Iteration: 11681000, loss: 0.10595421184427585, gradient norm: 0.5083054051585135\n",
      "Iteration: 11682000, loss: 0.105954001233097, gradient norm: 0.35369680503203244\n",
      "Iteration: 11683000, loss: 0.10595397531930172, gradient norm: 0.8013757630972365\n",
      "Iteration: 11684000, loss: 0.10595400959351162, gradient norm: 1.1847219349270288\n",
      "Iteration: 11685000, loss: 0.10595369429547088, gradient norm: 0.7184550530821935\n",
      "Iteration: 11686000, loss: 0.10595362605253736, gradient norm: 0.28525806535262405\n",
      "Iteration: 11687000, loss: 0.10595347667284018, gradient norm: 0.4007242833878626\n",
      "Iteration: 11688000, loss: 0.10595342936856049, gradient norm: 0.3392296753698738\n",
      "Iteration: 11689000, loss: 0.10595319592864823, gradient norm: 0.243154542341605\n",
      "Iteration: 11690000, loss: 0.10595327191560057, gradient norm: 0.22590546104766848\n",
      "Iteration: 11691000, loss: 0.10595300401569567, gradient norm: 0.07984621965665924\n",
      "Iteration: 11692000, loss: 0.1059530712469782, gradient norm: 0.07439501098170936\n",
      "Iteration: 11693000, loss: 0.1059527590410886, gradient norm: 0.3569202770956033\n",
      "Iteration: 11694000, loss: 0.10595276292542521, gradient norm: 0.1529623311978268\n",
      "Iteration: 11695000, loss: 0.10595261365337075, gradient norm: 0.2645731604724913\n",
      "Iteration: 11696000, loss: 0.10595263820291329, gradient norm: 0.9939132966162988\n",
      "Iteration: 11697000, loss: 0.10595234005256435, gradient norm: 0.038769803459388255\n",
      "Iteration: 11698000, loss: 0.10595246663949508, gradient norm: 0.27204157990446426\n",
      "Iteration: 11699000, loss: 0.10595206705997197, gradient norm: 0.39711033528001394\n",
      "Iteration: 11700000, loss: 0.10595211551692517, gradient norm: 0.5248255187330831\n",
      "Iteration: 11701000, loss: 0.10595192207774283, gradient norm: 0.7505269754735489\n",
      "Iteration: 11702000, loss: 0.10595183414769693, gradient norm: 0.18771498783933788\n",
      "Iteration: 11703000, loss: 0.10595176471937759, gradient norm: 0.5363089664921844\n",
      "Iteration: 11704000, loss: 0.10595169030091264, gradient norm: 0.16473377409789036\n",
      "Iteration: 11705000, loss: 0.10595154199596238, gradient norm: 0.4397403643733375\n",
      "Iteration: 11706000, loss: 0.10595132015103181, gradient norm: 0.7400441392216613\n",
      "Iteration: 11707000, loss: 0.10595140454086753, gradient norm: 0.2688675152307129\n",
      "Iteration: 11708000, loss: 0.10595114039448943, gradient norm: 0.2502764073788987\n",
      "Iteration: 11709000, loss: 0.10595097133249674, gradient norm: 0.3225773435020594\n",
      "Iteration: 11710000, loss: 0.10595099007147958, gradient norm: 0.4184745873365738\n",
      "Iteration: 11711000, loss: 0.10595089314624605, gradient norm: 0.43917446491518697\n",
      "Iteration: 11712000, loss: 0.1059508241329411, gradient norm: 0.9384554504838725\n",
      "Iteration: 11713000, loss: 0.10595058115211314, gradient norm: 0.4200849017375986\n",
      "Iteration: 11714000, loss: 0.10595045219693876, gradient norm: 0.22348023106696482\n",
      "Iteration: 11715000, loss: 0.10595038697993962, gradient norm: 0.4519921269709815\n",
      "Iteration: 11716000, loss: 0.10595034256370067, gradient norm: 0.12242943099384347\n",
      "Iteration: 11717000, loss: 0.10595025989343117, gradient norm: 0.25922235113174236\n",
      "Iteration: 11718000, loss: 0.1059499178821887, gradient norm: 0.37556275944204\n",
      "Iteration: 11719000, loss: 0.10594999238770801, gradient norm: 0.049612810745824465\n",
      "Iteration: 11720000, loss: 0.10594991542023117, gradient norm: 0.3912997590970177\n",
      "Iteration: 11721000, loss: 0.10594964005857715, gradient norm: 0.445756255936495\n",
      "Iteration: 11722000, loss: 0.10594963791286477, gradient norm: 0.3168244138118949\n",
      "Iteration: 11723000, loss: 0.10594967114252386, gradient norm: 0.46370882983259276\n",
      "Iteration: 11724000, loss: 0.10594918974564503, gradient norm: 0.37432163358149656\n",
      "Iteration: 11725000, loss: 0.10594928669313862, gradient norm: 0.1471143024106314\n",
      "Iteration: 11726000, loss: 0.10594923895687204, gradient norm: 0.09777725915453024\n",
      "Iteration: 11727000, loss: 0.10594913235012399, gradient norm: 0.3893264913797387\n",
      "Iteration: 11728000, loss: 0.10594890690477228, gradient norm: 0.7924365289459969\n",
      "Iteration: 11729000, loss: 0.10594879435153783, gradient norm: 0.9178437366977933\n",
      "Iteration: 11730000, loss: 0.10594879230265843, gradient norm: 0.3672367125195734\n",
      "Iteration: 11731000, loss: 0.10594857355653294, gradient norm: 0.2295235405428447\n",
      "Iteration: 11732000, loss: 0.10594861438247653, gradient norm: 0.8932546805860274\n",
      "Iteration: 11733000, loss: 0.1059483623406463, gradient norm: 0.08142998089246607\n",
      "Iteration: 11734000, loss: 0.10594830754360635, gradient norm: 0.5960099657595562\n",
      "Iteration: 11735000, loss: 0.1059481304363638, gradient norm: 0.1602827342321065\n",
      "Iteration: 11736000, loss: 0.10594805936112103, gradient norm: 0.30262618649405354\n",
      "Iteration: 11737000, loss: 0.1059480182661716, gradient norm: 0.1193872624338573\n",
      "Iteration: 11738000, loss: 0.1059479141294319, gradient norm: 0.40188142796186\n",
      "Iteration: 11739000, loss: 0.10594777481582694, gradient norm: 0.7751951807840827\n",
      "Iteration: 11740000, loss: 0.10594753594225807, gradient norm: 0.6500087111879903\n",
      "Iteration: 11741000, loss: 0.10594744224538676, gradient norm: 0.037725616943736884\n",
      "Iteration: 11742000, loss: 0.1059475353533426, gradient norm: 0.5713042787707899\n",
      "Iteration: 11743000, loss: 0.1059473873834127, gradient norm: 0.2097197368686503\n",
      "Iteration: 11744000, loss: 0.10594709506655067, gradient norm: 0.13382337250742438\n",
      "Iteration: 11745000, loss: 0.10594709210126624, gradient norm: 0.07140755600513245\n",
      "Iteration: 11746000, loss: 0.10594690474883882, gradient norm: 0.460938761053907\n",
      "Iteration: 11747000, loss: 0.1059470164409013, gradient norm: 0.2412812933475115\n",
      "Iteration: 11748000, loss: 0.10594661990338, gradient norm: 0.36203888867717055\n",
      "Iteration: 11749000, loss: 0.10594662784192023, gradient norm: 0.3947404908870308\n",
      "Iteration: 11750000, loss: 0.10594658750076415, gradient norm: 0.3411634602483892\n",
      "Iteration: 11751000, loss: 0.10594646099215425, gradient norm: 0.5832944334649772\n",
      "Iteration: 11752000, loss: 0.10594634237747322, gradient norm: 0.14484125928407365\n",
      "Iteration: 11753000, loss: 0.10594612716271436, gradient norm: 0.16630231749538785\n",
      "Iteration: 11754000, loss: 0.10594610415847815, gradient norm: 0.527768796986204\n",
      "Iteration: 11755000, loss: 0.10594591357686857, gradient norm: 0.3297068275719432\n",
      "Iteration: 11756000, loss: 0.10594588124772747, gradient norm: 0.2554653764511338\n",
      "Iteration: 11757000, loss: 0.10594571712587515, gradient norm: 0.5012865067627537\n",
      "Iteration: 11758000, loss: 0.10594565823994251, gradient norm: 0.3911434877550133\n",
      "Iteration: 11759000, loss: 0.10594545399496286, gradient norm: 0.2541566761356368\n",
      "Iteration: 11760000, loss: 0.10594547626342267, gradient norm: 0.08375747045235755\n",
      "Iteration: 11761000, loss: 0.10594533350065535, gradient norm: 0.285297053708421\n",
      "Iteration: 11762000, loss: 0.1059451717441947, gradient norm: 0.21532398734493918\n",
      "Iteration: 11763000, loss: 0.10594514599785719, gradient norm: 0.07994310022939705\n",
      "Iteration: 11764000, loss: 0.10594486970107757, gradient norm: 0.2064657395199979\n",
      "Iteration: 11765000, loss: 0.10594507266908693, gradient norm: 0.1916915670669376\n",
      "Iteration: 11766000, loss: 0.1059446501698428, gradient norm: 0.1585473772714831\n",
      "Iteration: 11767000, loss: 0.10594461780348649, gradient norm: 0.2854567492158965\n",
      "Iteration: 11768000, loss: 0.10594450092303011, gradient norm: 0.2071887157738045\n",
      "Iteration: 11769000, loss: 0.10594446027950573, gradient norm: 0.7964718000753016\n",
      "Iteration: 11770000, loss: 0.10594432439386145, gradient norm: 0.6266494399644191\n",
      "Iteration: 11771000, loss: 0.10594411434846215, gradient norm: 0.1514605018671066\n",
      "Iteration: 11772000, loss: 0.10594419635703156, gradient norm: 0.40146403989343693\n",
      "Iteration: 11773000, loss: 0.10594405506933115, gradient norm: 0.325447488527079\n",
      "Iteration: 11774000, loss: 0.10594385645769137, gradient norm: 0.1609478602155763\n",
      "Iteration: 11775000, loss: 0.10594368369449648, gradient norm: 0.622877946607414\n",
      "Iteration: 11776000, loss: 0.1059436564334966, gradient norm: 0.1767457761971504\n",
      "Iteration: 11777000, loss: 0.10594359083055911, gradient norm: 0.844601462074587\n",
      "Iteration: 11778000, loss: 0.10594350299934138, gradient norm: 0.231854842775327\n",
      "Iteration: 11779000, loss: 0.10594329779498952, gradient norm: 0.0781070531497448\n",
      "Iteration: 11780000, loss: 0.10594316925186215, gradient norm: 0.15121008004947284\n",
      "Iteration: 11781000, loss: 0.10594319444078237, gradient norm: 0.27868403908130646\n",
      "Iteration: 11782000, loss: 0.105942913970686, gradient norm: 0.27173978345121746\n",
      "Iteration: 11783000, loss: 0.10594292830275182, gradient norm: 0.3805888521710783\n",
      "Iteration: 11784000, loss: 0.1059427470607697, gradient norm: 0.25350426699884787\n",
      "Iteration: 11785000, loss: 0.10594266172551917, gradient norm: 0.25300892711643375\n",
      "Iteration: 11786000, loss: 0.10594247762536939, gradient norm: 0.14314170063407172\n",
      "Iteration: 11787000, loss: 0.10594240391140722, gradient norm: 0.06124191313512123\n",
      "Iteration: 11788000, loss: 0.1059424767664514, gradient norm: 0.5763238800828587\n",
      "Iteration: 11789000, loss: 0.10594220572555975, gradient norm: 0.8738327308995786\n",
      "Iteration: 11790000, loss: 0.10594214181268906, gradient norm: 0.08384234632770092\n",
      "Iteration: 11791000, loss: 0.10594187968979932, gradient norm: 0.07908910868470319\n",
      "Iteration: 11792000, loss: 0.10594183968825865, gradient norm: 0.42372238091408393\n",
      "Iteration: 11793000, loss: 0.1059419580841374, gradient norm: 0.38396005957731605\n",
      "Iteration: 11794000, loss: 0.10594164515789786, gradient norm: 0.5094135189943418\n",
      "Iteration: 11795000, loss: 0.10594161672465877, gradient norm: 0.04086507012847512\n",
      "Iteration: 11796000, loss: 0.10594139050807753, gradient norm: 0.2370639390371952\n",
      "Iteration: 11797000, loss: 0.10594134142199564, gradient norm: 0.8522566100013074\n",
      "Iteration: 11798000, loss: 0.10594129210490616, gradient norm: 0.5458360617392928\n",
      "Iteration: 11799000, loss: 0.10594110891341485, gradient norm: 0.15030813208567007\n",
      "Iteration: 11800000, loss: 0.10594090524149649, gradient norm: 0.5912993565249537\n",
      "Iteration: 11801000, loss: 0.10594081280498713, gradient norm: 0.2263040170259061\n",
      "Iteration: 11802000, loss: 0.10594093990037891, gradient norm: 0.4086970064029139\n",
      "Iteration: 11803000, loss: 0.10594054772178922, gradient norm: 0.36538737806765204\n",
      "Iteration: 11804000, loss: 0.10594069320530536, gradient norm: 0.373319833274856\n",
      "Iteration: 11805000, loss: 0.10594037482296398, gradient norm: 0.20951192204661392\n",
      "Iteration: 11806000, loss: 0.10594030237023169, gradient norm: 0.2687010924902128\n",
      "Iteration: 11807000, loss: 0.10594026943646839, gradient norm: 0.4376918495383512\n",
      "Iteration: 11808000, loss: 0.10594008102932705, gradient norm: 0.3973093555264441\n",
      "Iteration: 11809000, loss: 0.10594018078337308, gradient norm: 0.14594603062726982\n",
      "Iteration: 11810000, loss: 0.10593988558795453, gradient norm: 0.03849042670002425\n",
      "Iteration: 11811000, loss: 0.10593968870233024, gradient norm: 0.6728318497966047\n",
      "Iteration: 11812000, loss: 0.1059397439493604, gradient norm: 0.45860931628996066\n",
      "Iteration: 11813000, loss: 0.1059397025007012, gradient norm: 0.34391356953093505\n",
      "Iteration: 11814000, loss: 0.10593924094964402, gradient norm: 0.6637306012347758\n",
      "Iteration: 11815000, loss: 0.10593944951738021, gradient norm: 0.0762745545397144\n",
      "Iteration: 11816000, loss: 0.10593930704648055, gradient norm: 0.3780263552625783\n",
      "Iteration: 11817000, loss: 0.10593905775003866, gradient norm: 0.25612297677717666\n",
      "Iteration: 11818000, loss: 0.1059390204449395, gradient norm: 0.7661507330646856\n",
      "Iteration: 11819000, loss: 0.10593910010572537, gradient norm: 0.8264174901093573\n",
      "Iteration: 11820000, loss: 0.10593862000156329, gradient norm: 0.4866758562268335\n",
      "Iteration: 11821000, loss: 0.1059387304731684, gradient norm: 0.5530883088913884\n",
      "Iteration: 11822000, loss: 0.10593871468133914, gradient norm: 0.791811360648571\n",
      "Iteration: 11823000, loss: 0.10593847669598569, gradient norm: 0.4681475272629134\n",
      "Iteration: 11824000, loss: 0.10593826085991623, gradient norm: 0.3691414729361\n",
      "Iteration: 11825000, loss: 0.10593852725435728, gradient norm: 0.03711288186484558\n",
      "Iteration: 11826000, loss: 0.10593798572812023, gradient norm: 0.601964270223621\n",
      "Iteration: 11827000, loss: 0.1059380176676725, gradient norm: 0.3691359870819831\n",
      "Iteration: 11828000, loss: 0.10593803851969383, gradient norm: 0.19818818973157143\n",
      "Iteration: 11829000, loss: 0.10593776584766505, gradient norm: 0.5554378234240105\n",
      "Iteration: 11830000, loss: 0.10593765034356141, gradient norm: 0.272982895775861\n",
      "Iteration: 11831000, loss: 0.10593775881916227, gradient norm: 0.9363889966202773\n",
      "Iteration: 11832000, loss: 0.10593740537574446, gradient norm: 0.3289963708494904\n",
      "Iteration: 11833000, loss: 0.10593737784713178, gradient norm: 0.5001295743187238\n",
      "Iteration: 11834000, loss: 0.10593728192658274, gradient norm: 0.7235319987271167\n",
      "Iteration: 11835000, loss: 0.10593721730597107, gradient norm: 0.7504920317938529\n",
      "Iteration: 11836000, loss: 0.10593705084677896, gradient norm: 0.7082634991389308\n",
      "Iteration: 11837000, loss: 0.10593684843008527, gradient norm: 0.1965058401351236\n",
      "Iteration: 11838000, loss: 0.10593692884581923, gradient norm: 0.1773102558780083\n",
      "Iteration: 11839000, loss: 0.10593663164315108, gradient norm: 0.34743807138669125\n",
      "Iteration: 11840000, loss: 0.10593672804632195, gradient norm: 0.24286400908640057\n",
      "Iteration: 11841000, loss: 0.10593656843224088, gradient norm: 0.5871216314571377\n",
      "Iteration: 11842000, loss: 0.10593645706902348, gradient norm: 0.19411529068138145\n",
      "Iteration: 11843000, loss: 0.10593617899771539, gradient norm: 0.6282124761940747\n",
      "Iteration: 11844000, loss: 0.10593623147552302, gradient norm: 0.6139458765636716\n",
      "Iteration: 11845000, loss: 0.10593608379417889, gradient norm: 0.03478023317541598\n",
      "Iteration: 11846000, loss: 0.10593598662305656, gradient norm: 0.3414501711129976\n",
      "Iteration: 11847000, loss: 0.10593590614630474, gradient norm: 0.37547551911626714\n",
      "Iteration: 11848000, loss: 0.10593558566618273, gradient norm: 0.3282741017715389\n",
      "Iteration: 11849000, loss: 0.1059357043736156, gradient norm: 0.49193616636324666\n",
      "Iteration: 11850000, loss: 0.10593549208899938, gradient norm: 0.08449815756787016\n",
      "Iteration: 11851000, loss: 0.10593538618274803, gradient norm: 0.5783638116981558\n",
      "Iteration: 11852000, loss: 0.10593531889213259, gradient norm: 0.33035519297953386\n",
      "Iteration: 11853000, loss: 0.10593516740695821, gradient norm: 0.29058883748906217\n",
      "Iteration: 11854000, loss: 0.10593514220183653, gradient norm: 0.07723628538280289\n",
      "Iteration: 11855000, loss: 0.105934961425077, gradient norm: 0.42120523438950597\n",
      "Iteration: 11856000, loss: 0.10593492072935384, gradient norm: 0.38004395917725753\n",
      "Iteration: 11857000, loss: 0.10593467260200866, gradient norm: 0.39269902779565513\n",
      "Iteration: 11858000, loss: 0.10593472735755176, gradient norm: 0.3637914434027767\n",
      "Iteration: 11859000, loss: 0.10593456218593549, gradient norm: 0.6504437237126236\n",
      "Iteration: 11860000, loss: 0.10593446847824407, gradient norm: 0.5031776480806761\n",
      "Iteration: 11861000, loss: 0.10593422772966674, gradient norm: 0.21303287375787172\n",
      "Iteration: 11862000, loss: 0.10593441502123593, gradient norm: 0.5927288118770435\n",
      "Iteration: 11863000, loss: 0.1059340070273947, gradient norm: 0.1185342350737683\n",
      "Iteration: 11864000, loss: 0.1059340014566477, gradient norm: 0.4082208556289624\n",
      "Iteration: 11865000, loss: 0.10593396813947828, gradient norm: 0.5161414587691799\n",
      "Iteration: 11866000, loss: 0.10593372703468282, gradient norm: 0.2737855305481459\n",
      "Iteration: 11867000, loss: 0.10593372355683696, gradient norm: 0.41453752452619635\n",
      "Iteration: 11868000, loss: 0.10593358872153033, gradient norm: 0.10591241968702601\n",
      "Iteration: 11869000, loss: 0.10593351847080745, gradient norm: 0.32830444701353173\n",
      "Iteration: 11870000, loss: 0.10593319908415433, gradient norm: 0.30781106249103685\n",
      "Iteration: 11871000, loss: 0.10593335864037287, gradient norm: 0.501685794363647\n",
      "Iteration: 11872000, loss: 0.10593297198642201, gradient norm: 0.33370139866871573\n",
      "Iteration: 11873000, loss: 0.10593303749672009, gradient norm: 0.5192513842036502\n",
      "Iteration: 11874000, loss: 0.10593295651453431, gradient norm: 0.4424758277003603\n",
      "Iteration: 11875000, loss: 0.10593274712543421, gradient norm: 0.4966531400655665\n",
      "Iteration: 11876000, loss: 0.1059327471403846, gradient norm: 0.4212368801100944\n",
      "Iteration: 11877000, loss: 0.1059325708262165, gradient norm: 0.1368610341442987\n",
      "Iteration: 11878000, loss: 0.10593241792138836, gradient norm: 0.21820939716412646\n",
      "Iteration: 11879000, loss: 0.10593249741079185, gradient norm: 0.1951541872251229\n",
      "Iteration: 11880000, loss: 0.10593230670388966, gradient norm: 0.2167864744267754\n",
      "Iteration: 11881000, loss: 0.10593229121386777, gradient norm: 0.2974961143741072\n",
      "Iteration: 11882000, loss: 0.10593193646230523, gradient norm: 0.4825124799727283\n",
      "Iteration: 11883000, loss: 0.10593191711787521, gradient norm: 0.5288073226326006\n",
      "Iteration: 11884000, loss: 0.10593183439117945, gradient norm: 0.2816988015691088\n",
      "Iteration: 11885000, loss: 0.10593163562206707, gradient norm: 0.8470267326978856\n",
      "Iteration: 11886000, loss: 0.10593158960542397, gradient norm: 0.2494926426748284\n",
      "Iteration: 11887000, loss: 0.10593153831800904, gradient norm: 0.38288949502090547\n",
      "Iteration: 11888000, loss: 0.10593157002500549, gradient norm: 0.10050855995754143\n",
      "Iteration: 11889000, loss: 0.1059312136362402, gradient norm: 0.468401566637582\n",
      "Iteration: 11890000, loss: 0.10593116341387437, gradient norm: 0.3068444256802993\n",
      "Iteration: 11891000, loss: 0.10593090879020961, gradient norm: 0.4284183778852971\n",
      "Iteration: 11892000, loss: 0.10593105082933092, gradient norm: 0.12390634674983664\n",
      "Iteration: 11893000, loss: 0.10593077243139092, gradient norm: 0.22559667610912712\n",
      "Iteration: 11894000, loss: 0.10593083189085242, gradient norm: 0.15792024205627556\n",
      "Iteration: 11895000, loss: 0.10593064631123306, gradient norm: 0.25440620844138595\n",
      "Iteration: 11896000, loss: 0.10593053791807366, gradient norm: 0.9042290552595679\n",
      "Iteration: 11897000, loss: 0.10593038676025898, gradient norm: 0.7593724247511221\n",
      "Iteration: 11898000, loss: 0.10593035247516357, gradient norm: 0.33517160115661454\n",
      "Iteration: 11899000, loss: 0.1059300994081975, gradient norm: 0.05529492730960812\n",
      "Iteration: 11900000, loss: 0.10593016010491638, gradient norm: 0.07504431099241848\n",
      "Iteration: 11901000, loss: 0.10592994888589277, gradient norm: 0.42555368900655927\n",
      "Iteration: 11902000, loss: 0.10592984658399934, gradient norm: 0.2031542382283595\n",
      "Iteration: 11903000, loss: 0.10592978711427725, gradient norm: 0.7198284799669588\n",
      "Iteration: 11904000, loss: 0.10592966334854384, gradient norm: 0.18457277553636728\n",
      "Iteration: 11905000, loss: 0.10592952628878825, gradient norm: 0.1820645616597957\n",
      "Iteration: 11906000, loss: 0.10592956775290306, gradient norm: 0.9136589864872527\n",
      "Iteration: 11907000, loss: 0.10592915636269555, gradient norm: 0.1677455927165185\n",
      "Iteration: 11908000, loss: 0.10592920204462913, gradient norm: 0.12977866945194058\n",
      "Iteration: 11909000, loss: 0.10592918283307393, gradient norm: 0.3299829949532214\n",
      "Iteration: 11910000, loss: 0.10592906585216455, gradient norm: 0.4773759711865423\n",
      "Iteration: 11911000, loss: 0.1059288224388851, gradient norm: 0.12367937184712022\n",
      "Iteration: 11912000, loss: 0.10592879630830788, gradient norm: 0.78952539659662\n",
      "Iteration: 11913000, loss: 0.10592870883869682, gradient norm: 0.15576794184524753\n",
      "Iteration: 11914000, loss: 0.10592847127096178, gradient norm: 0.1850737691131662\n",
      "Iteration: 11915000, loss: 0.10592843595639603, gradient norm: 0.6145234999313324\n",
      "Iteration: 11916000, loss: 0.10592858272660888, gradient norm: 0.1699450827165235\n",
      "Iteration: 11917000, loss: 0.10592821419939598, gradient norm: 0.8295400108646158\n",
      "Iteration: 11918000, loss: 0.10592814602690988, gradient norm: 0.026583060828149832\n",
      "Iteration: 11919000, loss: 0.10592781980666897, gradient norm: 0.20852343052505665\n",
      "Iteration: 11920000, loss: 0.10592797916234971, gradient norm: 0.8642431309586003\n",
      "Iteration: 11921000, loss: 0.10592789052935422, gradient norm: 0.4219886890601426\n",
      "Iteration: 11922000, loss: 0.1059276932729401, gradient norm: 0.21915473628256896\n",
      "Iteration: 11923000, loss: 0.1059276179691652, gradient norm: 0.5238600629450653\n",
      "Iteration: 11924000, loss: 0.10592750219678372, gradient norm: 0.2667492381113755\n",
      "Iteration: 11925000, loss: 0.10592733911154437, gradient norm: 0.3138854845008688\n",
      "Iteration: 11926000, loss: 0.10592724885626197, gradient norm: 0.2569176954491549\n",
      "Iteration: 11927000, loss: 0.10592713461861145, gradient norm: 0.32581658853389\n",
      "Iteration: 11928000, loss: 0.10592702145712005, gradient norm: 0.35648035354422614\n",
      "Iteration: 11929000, loss: 0.1059270142554967, gradient norm: 1.025261264955109\n",
      "Iteration: 11930000, loss: 0.10592679241052358, gradient norm: 0.5399884020110771\n",
      "Iteration: 11931000, loss: 0.10592679093724032, gradient norm: 0.32624843840447315\n",
      "Iteration: 11932000, loss: 0.10592654345466539, gradient norm: 0.2377337120237254\n",
      "Iteration: 11933000, loss: 0.10592649073356754, gradient norm: 0.7143152581112779\n",
      "Iteration: 11934000, loss: 0.10592634129788385, gradient norm: 0.11041815496361919\n",
      "Iteration: 11935000, loss: 0.10592659295399494, gradient norm: 0.14033510603419957\n",
      "Iteration: 11936000, loss: 0.10592595795763067, gradient norm: 0.7010658665189977\n",
      "Iteration: 11937000, loss: 0.10592620072556815, gradient norm: 0.14063665938660005\n",
      "Iteration: 11938000, loss: 0.1059258432570088, gradient norm: 0.6473063420918349\n",
      "Iteration: 11939000, loss: 0.10592596950363492, gradient norm: 0.19067760209543402\n",
      "Iteration: 11940000, loss: 0.10592571658476357, gradient norm: 0.44408833384888496\n",
      "Iteration: 11941000, loss: 0.10592571593089455, gradient norm: 0.49834553663148157\n",
      "Iteration: 11942000, loss: 0.10592541100748361, gradient norm: 0.7897101176911155\n",
      "Iteration: 11943000, loss: 0.10592539306397683, gradient norm: 0.4617404583779044\n",
      "Iteration: 11944000, loss: 0.10592530322825508, gradient norm: 0.19123173367105636\n",
      "Iteration: 11945000, loss: 0.10592532196896277, gradient norm: 0.37155381630138146\n",
      "Iteration: 11946000, loss: 0.10592502554108114, gradient norm: 0.4235146272512257\n",
      "Iteration: 11947000, loss: 0.10592502512617347, gradient norm: 0.5809249089504746\n",
      "Iteration: 11948000, loss: 0.10592507115751346, gradient norm: 0.3305906346021725\n",
      "Iteration: 11949000, loss: 0.10592464770101467, gradient norm: 0.2663090726790962\n",
      "Iteration: 11950000, loss: 0.10592456616226295, gradient norm: 0.2716899296783393\n",
      "Iteration: 11951000, loss: 0.10592454528945236, gradient norm: 0.43042119092460784\n",
      "Iteration: 11952000, loss: 0.10592454488272424, gradient norm: 0.16094454146309875\n",
      "Iteration: 11953000, loss: 0.10592430038243873, gradient norm: 0.05236835605001235\n",
      "Iteration: 11954000, loss: 0.10592421075644208, gradient norm: 0.06587765971084834\n",
      "Iteration: 11955000, loss: 0.10592417617156781, gradient norm: 0.4028054343038057\n",
      "Iteration: 11956000, loss: 0.1059240303651434, gradient norm: 1.1201306892837026\n",
      "Iteration: 11957000, loss: 0.10592387996269584, gradient norm: 0.22089309102415572\n",
      "Iteration: 11958000, loss: 0.10592387243684676, gradient norm: 0.9898854255136053\n",
      "Iteration: 11959000, loss: 0.1059236467733524, gradient norm: 0.285424872390802\n",
      "Iteration: 11960000, loss: 0.10592371764725989, gradient norm: 0.39574831946409744\n",
      "Iteration: 11961000, loss: 0.10592344883057976, gradient norm: 0.28153093561895454\n",
      "Iteration: 11962000, loss: 0.10592326027544711, gradient norm: 0.2595563837868677\n",
      "Iteration: 11963000, loss: 0.10592335871446898, gradient norm: 0.3800765550028179\n",
      "Iteration: 11964000, loss: 0.10592318942150962, gradient norm: 0.6321940327329849\n",
      "Iteration: 11965000, loss: 0.10592316021643162, gradient norm: 0.5336618694273217\n",
      "Iteration: 11966000, loss: 0.10592296085694111, gradient norm: 0.4612302716034099\n",
      "Iteration: 11967000, loss: 0.1059227100322448, gradient norm: 0.17969186098252302\n",
      "Iteration: 11968000, loss: 0.1059227555407528, gradient norm: 0.4659923483946628\n",
      "Iteration: 11969000, loss: 0.1059225939506891, gradient norm: 0.21620590081704283\n",
      "Iteration: 11970000, loss: 0.10592261350294174, gradient norm: 0.4056738875472928\n",
      "Iteration: 11971000, loss: 0.10592235707401164, gradient norm: 0.253283781904294\n",
      "Iteration: 11972000, loss: 0.10592222275345665, gradient norm: 0.354265961916038\n",
      "Iteration: 11973000, loss: 0.10592220696248401, gradient norm: 0.44779631818344817\n",
      "Iteration: 11974000, loss: 0.10592213991028285, gradient norm: 0.5219963583399903\n",
      "Iteration: 11975000, loss: 0.10592207234797663, gradient norm: 0.8424301669387745\n",
      "Iteration: 11976000, loss: 0.1059219132048608, gradient norm: 0.6170641123090967\n",
      "Iteration: 11977000, loss: 0.10592156573926104, gradient norm: 0.40783979808636955\n",
      "Iteration: 11978000, loss: 0.10592166051139891, gradient norm: 0.39648152868653935\n",
      "Iteration: 11979000, loss: 0.10592174895228129, gradient norm: 0.038023787544724706\n",
      "Iteration: 11980000, loss: 0.10592131009508546, gradient norm: 0.7573478332106096\n",
      "Iteration: 11981000, loss: 0.10592141769546089, gradient norm: 0.0972930439378116\n",
      "Iteration: 11982000, loss: 0.10592117697759376, gradient norm: 0.21674597725151037\n",
      "Iteration: 11983000, loss: 0.10592109598543184, gradient norm: 0.03466550514536994\n",
      "Iteration: 11984000, loss: 0.10592105393667069, gradient norm: 0.4649256136113029\n",
      "Iteration: 11985000, loss: 0.10592093165595927, gradient norm: 0.09908351416354497\n",
      "Iteration: 11986000, loss: 0.10592078639560458, gradient norm: 0.6172573010929288\n",
      "Iteration: 11987000, loss: 0.10592059725195609, gradient norm: 0.5389077946489592\n",
      "Iteration: 11988000, loss: 0.10592059134193121, gradient norm: 0.2430822838386729\n",
      "Iteration: 11989000, loss: 0.10592078877095, gradient norm: 0.913458829720829\n",
      "Iteration: 11990000, loss: 0.10592010578812369, gradient norm: 0.38263391205892555\n",
      "Iteration: 11991000, loss: 0.10592023693282633, gradient norm: 0.30757354367429734\n",
      "Iteration: 11992000, loss: 0.10592020578704213, gradient norm: 0.18672850373274594\n",
      "Iteration: 11993000, loss: 0.10592002148059243, gradient norm: 0.08112846409044461\n",
      "Iteration: 11994000, loss: 0.1059199831486394, gradient norm: 0.4750368801046516\n",
      "Iteration: 11995000, loss: 0.10591977854696322, gradient norm: 0.05008245283348581\n",
      "Iteration: 11996000, loss: 0.10591969369637866, gradient norm: 0.10413854801202534\n",
      "Iteration: 11997000, loss: 0.10591957992867214, gradient norm: 0.33097930001456305\n",
      "Iteration: 11998000, loss: 0.10591964718739545, gradient norm: 0.36549580056157965\n",
      "Iteration: 11999000, loss: 0.10591937341476174, gradient norm: 0.09043483284074287\n",
      "Iteration: 12000000, loss: 0.10591939606015802, gradient norm: 0.12058370406352249\n",
      "Iteration: 12001000, loss: 0.1059191296622266, gradient norm: 0.417550814147021\n",
      "Iteration: 12002000, loss: 0.105919048002298, gradient norm: 0.38627208905131727\n",
      "Iteration: 12003000, loss: 0.10591895161764166, gradient norm: 0.2652100501583197\n",
      "Iteration: 12004000, loss: 0.10591887385665143, gradient norm: 0.8611526627711688\n",
      "Iteration: 12005000, loss: 0.10591893018096224, gradient norm: 0.07114106918078274\n",
      "Iteration: 12006000, loss: 0.105918609873418, gradient norm: 0.8386637989437213\n",
      "Iteration: 12007000, loss: 0.10591843644828751, gradient norm: 0.12413330474462698\n",
      "Iteration: 12008000, loss: 0.1059184886243713, gradient norm: 0.32931151863829594\n",
      "Iteration: 12009000, loss: 0.10591825441881529, gradient norm: 0.12422634154380559\n",
      "Iteration: 12010000, loss: 0.10591826707923968, gradient norm: 0.5337233283476736\n",
      "Iteration: 12011000, loss: 0.10591813368945736, gradient norm: 0.8563015354696568\n",
      "Iteration: 12012000, loss: 0.10591804898130551, gradient norm: 0.16533647336310664\n",
      "Iteration: 12013000, loss: 0.10591788223134455, gradient norm: 0.3114152390342151\n",
      "Iteration: 12014000, loss: 0.1059179655217286, gradient norm: 0.48952832988315076\n",
      "Iteration: 12015000, loss: 0.10591751693867973, gradient norm: 0.4729525808284392\n",
      "Iteration: 12016000, loss: 0.10591759418642577, gradient norm: 0.18116596774429194\n",
      "Iteration: 12017000, loss: 0.10591747639240615, gradient norm: 0.25889530739533245\n",
      "Iteration: 12018000, loss: 0.10591745299972528, gradient norm: 0.22197023274183428\n",
      "Iteration: 12019000, loss: 0.10591725321126338, gradient norm: 0.31808695048354224\n",
      "Iteration: 12020000, loss: 0.10591717101928248, gradient norm: 0.2811047561625849\n",
      "Iteration: 12021000, loss: 0.10591698804811403, gradient norm: 0.132282625156191\n",
      "Iteration: 12022000, loss: 0.10591692716125979, gradient norm: 0.7404720967111664\n",
      "Iteration: 12023000, loss: 0.10591695118900646, gradient norm: 0.5814717940598108\n",
      "Iteration: 12024000, loss: 0.10591659504629504, gradient norm: 0.4145652860565667\n",
      "Iteration: 12025000, loss: 0.10591666328605254, gradient norm: 0.33545468835550746\n",
      "Iteration: 12026000, loss: 0.10591663171177554, gradient norm: 0.6365779148180566\n",
      "Iteration: 12027000, loss: 0.10591627650483525, gradient norm: 0.36699334379843346\n",
      "Iteration: 12028000, loss: 0.10591625344438478, gradient norm: 0.19969026164221285\n",
      "Iteration: 12029000, loss: 0.10591612009622578, gradient norm: 0.749296308761173\n",
      "Iteration: 12030000, loss: 0.10591636868295286, gradient norm: 0.5914985722631388\n",
      "Iteration: 12031000, loss: 0.1059158347870418, gradient norm: 0.040019234794088455\n",
      "Iteration: 12032000, loss: 0.10591599426952815, gradient norm: 0.3659161428932095\n",
      "Iteration: 12033000, loss: 0.10591559303989262, gradient norm: 0.23687676741740607\n",
      "Iteration: 12034000, loss: 0.10591566053693946, gradient norm: 0.4666682626570516\n",
      "Iteration: 12035000, loss: 0.10591568121758875, gradient norm: 0.12792224792391088\n",
      "Iteration: 12036000, loss: 0.10591540172747349, gradient norm: 0.14989095247821668\n",
      "Iteration: 12037000, loss: 0.105915559404172, gradient norm: 0.30598507469515096\n",
      "Iteration: 12038000, loss: 0.1059150891320516, gradient norm: 0.8336232078004049\n",
      "Iteration: 12039000, loss: 0.1059149952641051, gradient norm: 0.2188772980338215\n",
      "Iteration: 12040000, loss: 0.10591497841617588, gradient norm: 0.5419412736362873\n",
      "Iteration: 12041000, loss: 0.10591498050865616, gradient norm: 0.3979703469700807\n",
      "Iteration: 12042000, loss: 0.10591479054227841, gradient norm: 0.8282097171590481\n",
      "Iteration: 12043000, loss: 0.10591471853303415, gradient norm: 0.2910543486752219\n",
      "Iteration: 12044000, loss: 0.10591466030395895, gradient norm: 0.3053801154461147\n",
      "Iteration: 12045000, loss: 0.10591448915448737, gradient norm: 0.1423808496592814\n",
      "Iteration: 12046000, loss: 0.10591429181864978, gradient norm: 0.2931204890427713\n",
      "Iteration: 12047000, loss: 0.1059143768666666, gradient norm: 0.3527296208372279\n",
      "Iteration: 12048000, loss: 0.10591417181259492, gradient norm: 0.06594500273350586\n",
      "Iteration: 12049000, loss: 0.10591412081836787, gradient norm: 0.738948443223181\n",
      "Iteration: 12050000, loss: 0.10591389384942365, gradient norm: 0.17036451269354724\n",
      "Iteration: 12051000, loss: 0.10591388612252109, gradient norm: 0.3939782279482108\n",
      "Iteration: 12052000, loss: 0.1059137542113925, gradient norm: 0.43416710825942045\n",
      "Iteration: 12053000, loss: 0.10591349210138035, gradient norm: 0.6177792349496986\n",
      "Iteration: 12054000, loss: 0.1059135114455779, gradient norm: 0.06352376911356131\n",
      "Iteration: 12055000, loss: 0.10591342227447975, gradient norm: 0.21636797792737664\n",
      "Iteration: 12056000, loss: 0.10591327520515602, gradient norm: 0.6510970696023162\n",
      "Iteration: 12057000, loss: 0.10591323257483418, gradient norm: 0.6463361433792213\n",
      "Iteration: 12058000, loss: 0.10591310461982857, gradient norm: 0.0746550844237379\n",
      "Iteration: 12059000, loss: 0.10591310426917473, gradient norm: 0.3629718574170895\n",
      "Iteration: 12060000, loss: 0.10591282159763384, gradient norm: 0.2342780379899497\n",
      "Iteration: 12061000, loss: 0.10591272465495484, gradient norm: 0.19314942395854678\n",
      "Iteration: 12062000, loss: 0.10591274033725268, gradient norm: 0.29872627820823994\n",
      "Iteration: 12063000, loss: 0.10591255973410306, gradient norm: 0.3388821145327606\n",
      "Iteration: 12064000, loss: 0.10591244997209516, gradient norm: 0.2745619206117069\n",
      "Iteration: 12065000, loss: 0.10591236478906108, gradient norm: 0.3872456376706377\n",
      "Iteration: 12066000, loss: 0.10591217225949377, gradient norm: 0.8754219758301008\n",
      "Iteration: 12067000, loss: 0.10591223969483954, gradient norm: 1.0327006005111046\n",
      "Iteration: 12068000, loss: 0.10591189241033315, gradient norm: 0.14938287227786018\n",
      "Iteration: 12069000, loss: 0.10591200756847576, gradient norm: 0.30348405747330753\n",
      "Iteration: 12070000, loss: 0.10591193380367804, gradient norm: 0.19635250701976864\n",
      "Iteration: 12071000, loss: 0.10591156250725099, gradient norm: 0.06861765322396507\n",
      "Iteration: 12072000, loss: 0.10591166326024207, gradient norm: 0.3898838265576466\n",
      "Iteration: 12073000, loss: 0.10591149408587114, gradient norm: 0.411301134859035\n",
      "Iteration: 12074000, loss: 0.10591142500268769, gradient norm: 0.5309904710241788\n",
      "Iteration: 12075000, loss: 0.10591127402633034, gradient norm: 0.7860806083305611\n",
      "Iteration: 12076000, loss: 0.10591113157241279, gradient norm: 0.803452309124293\n",
      "Iteration: 12077000, loss: 0.10591100543107243, gradient norm: 0.17421513842086833\n",
      "Iteration: 12078000, loss: 0.10591100195660676, gradient norm: 0.09975703098169002\n",
      "Iteration: 12079000, loss: 0.10591103184690172, gradient norm: 0.8674860227335431\n",
      "Iteration: 12080000, loss: 0.1059106861033944, gradient norm: 0.16436019507742536\n",
      "Iteration: 12081000, loss: 0.10591072687181025, gradient norm: 0.15280335287487407\n",
      "Iteration: 12082000, loss: 0.10591045275837752, gradient norm: 0.26012538026685217\n",
      "Iteration: 12083000, loss: 0.10591036069505416, gradient norm: 0.511703772099866\n",
      "Iteration: 12084000, loss: 0.10591035069950795, gradient norm: 0.43537729140821135\n",
      "Iteration: 12085000, loss: 0.10591022274517177, gradient norm: 0.17636212074310273\n",
      "Iteration: 12086000, loss: 0.10591000235302848, gradient norm: 0.3857307427440472\n",
      "Iteration: 12087000, loss: 0.10591020183361063, gradient norm: 0.5927117463934439\n",
      "Iteration: 12088000, loss: 0.10590991983472162, gradient norm: 0.13977563718864186\n",
      "Iteration: 12089000, loss: 0.10590975482250622, gradient norm: 0.1012415128608825\n",
      "Iteration: 12090000, loss: 0.10590973959688928, gradient norm: 0.08302487318945978\n",
      "Iteration: 12091000, loss: 0.10590952410168901, gradient norm: 0.6745081075517352\n",
      "Iteration: 12092000, loss: 0.10590953866695399, gradient norm: 0.4644788675225497\n",
      "Iteration: 12093000, loss: 0.1059093768597582, gradient norm: 0.028598726282375135\n",
      "Iteration: 12094000, loss: 0.10590924641083757, gradient norm: 0.7340549215016091\n",
      "Iteration: 12095000, loss: 0.10590917751110332, gradient norm: 0.06704746216659183\n",
      "Iteration: 12096000, loss: 0.10590908788269732, gradient norm: 0.38066724662483203\n",
      "Iteration: 12097000, loss: 0.10590887522828393, gradient norm: 0.4727720884549498\n",
      "Iteration: 12098000, loss: 0.10590882717032159, gradient norm: 0.3968992292617307\n",
      "Iteration: 12099000, loss: 0.1059088413053985, gradient norm: 0.0666921689424653\n",
      "Iteration: 12100000, loss: 0.10590865627388277, gradient norm: 0.24221683021422327\n",
      "Iteration: 12101000, loss: 0.10590851765169898, gradient norm: 0.19863629541608258\n",
      "Iteration: 12102000, loss: 0.10590840150966663, gradient norm: 0.3657891566398683\n",
      "Iteration: 12103000, loss: 0.10590834714972126, gradient norm: 0.7221718438044715\n",
      "Iteration: 12104000, loss: 0.10590830149627924, gradient norm: 0.2266062616407026\n",
      "Iteration: 12105000, loss: 0.10590801431818303, gradient norm: 0.3326929696067766\n",
      "Iteration: 12106000, loss: 0.10590799757733424, gradient norm: 0.7682966791858596\n",
      "Iteration: 12107000, loss: 0.10590784285529779, gradient norm: 0.08307687779505571\n",
      "Iteration: 12108000, loss: 0.10590781576200052, gradient norm: 0.355435745434726\n",
      "Iteration: 12109000, loss: 0.10590768294621171, gradient norm: 0.5726752835792503\n",
      "Iteration: 12110000, loss: 0.10590762602642574, gradient norm: 0.47320407429289335\n",
      "Iteration: 12111000, loss: 0.10590736659766167, gradient norm: 0.13060754924312343\n",
      "Iteration: 12112000, loss: 0.10590753003905425, gradient norm: 0.22426857685829796\n",
      "Iteration: 12113000, loss: 0.10590713482548943, gradient norm: 0.14910637318928963\n",
      "Iteration: 12114000, loss: 0.10590726966752291, gradient norm: 0.2671827124587181\n",
      "Iteration: 12115000, loss: 0.10590687376338193, gradient norm: 0.5057700291613562\n",
      "Iteration: 12116000, loss: 0.10590700652058334, gradient norm: 0.21126093077900396\n",
      "Iteration: 12117000, loss: 0.10590682299325434, gradient norm: 0.15864371618935272\n",
      "Iteration: 12118000, loss: 0.10590671621550837, gradient norm: 0.11546631453910629\n",
      "Iteration: 12119000, loss: 0.10590663498593877, gradient norm: 0.5458748895628341\n",
      "Iteration: 12120000, loss: 0.10590655434585293, gradient norm: 0.13717253437458324\n",
      "Iteration: 12121000, loss: 0.10590642287869999, gradient norm: 0.9149227157579423\n",
      "Iteration: 12122000, loss: 0.10590623116820089, gradient norm: 0.6939280700536121\n",
      "Iteration: 12123000, loss: 0.10590616045580788, gradient norm: 0.12883394242985607\n",
      "Iteration: 12124000, loss: 0.10590618863577513, gradient norm: 0.3686192282964268\n",
      "Iteration: 12125000, loss: 0.10590595059594918, gradient norm: 0.3484781847803489\n",
      "Iteration: 12126000, loss: 0.10590583868629282, gradient norm: 0.15902563690733626\n",
      "Iteration: 12127000, loss: 0.10590582482301153, gradient norm: 0.3472609892457756\n",
      "Iteration: 12128000, loss: 0.10590582850305327, gradient norm: 0.5724985882953205\n",
      "Iteration: 12129000, loss: 0.10590539385274846, gradient norm: 1.1044804473009024\n",
      "Iteration: 12130000, loss: 0.10590545054639716, gradient norm: 0.6378061022646934\n",
      "Iteration: 12131000, loss: 0.10590541038050347, gradient norm: 0.10508342172203346\n",
      "Iteration: 12132000, loss: 0.10590534229839313, gradient norm: 0.48203682511472146\n",
      "Iteration: 12133000, loss: 0.10590520508713912, gradient norm: 0.36757831662635226\n",
      "Iteration: 12134000, loss: 0.10590487433443899, gradient norm: 0.4214196721898348\n",
      "Iteration: 12135000, loss: 0.10590509558638664, gradient norm: 0.4709888667964004\n",
      "Iteration: 12136000, loss: 0.10590473134252215, gradient norm: 0.20515745506797609\n",
      "Iteration: 12137000, loss: 0.1059047907087272, gradient norm: 0.06272718304686054\n",
      "Iteration: 12138000, loss: 0.10590448530049437, gradient norm: 0.2896345736198249\n",
      "Iteration: 12139000, loss: 0.10590472760166746, gradient norm: 0.40381352159855755\n",
      "Iteration: 12140000, loss: 0.1059043101826456, gradient norm: 0.293193466339067\n",
      "Iteration: 12141000, loss: 0.10590432696510055, gradient norm: 0.05986631439753229\n",
      "Iteration: 12142000, loss: 0.10590417283990089, gradient norm: 1.0109896308178998\n",
      "Iteration: 12143000, loss: 0.1059040003841265, gradient norm: 0.7437415740802724\n",
      "Iteration: 12144000, loss: 0.10590414374910921, gradient norm: 0.04517411186553394\n",
      "Iteration: 12145000, loss: 0.1059037629896013, gradient norm: 0.093928214244575\n",
      "Iteration: 12146000, loss: 0.10590399143822306, gradient norm: 0.9224460403008276\n",
      "Iteration: 12147000, loss: 0.10590347873486013, gradient norm: 0.46261557136383763\n",
      "Iteration: 12148000, loss: 0.10590360902499195, gradient norm: 0.28528379856896585\n",
      "Iteration: 12149000, loss: 0.10590330824580295, gradient norm: 0.02902309546546327\n",
      "Iteration: 12150000, loss: 0.10590347091197881, gradient norm: 0.39959037874187586\n",
      "Iteration: 12151000, loss: 0.10590336701491458, gradient norm: 0.29868260057375196\n",
      "Iteration: 12152000, loss: 0.10590325660453496, gradient norm: 0.5581747266107289\n",
      "Iteration: 12153000, loss: 0.10590277457065395, gradient norm: 0.05746353479478248\n",
      "Iteration: 12154000, loss: 0.10590298142374678, gradient norm: 0.33301325966874595\n",
      "Iteration: 12155000, loss: 0.10590290376895711, gradient norm: 0.24357094880946875\n",
      "Iteration: 12156000, loss: 0.10590277100557167, gradient norm: 0.21438564847776484\n",
      "Iteration: 12157000, loss: 0.1059025074930627, gradient norm: 0.8935062774802798\n",
      "Iteration: 12158000, loss: 0.10590251763773036, gradient norm: 0.22189899410240285\n",
      "Iteration: 12159000, loss: 0.10590246334664176, gradient norm: 0.6961127863833021\n",
      "Iteration: 12160000, loss: 0.10590234520513352, gradient norm: 0.17800394363547756\n",
      "Iteration: 12161000, loss: 0.1059021973011068, gradient norm: 0.4365997275266899\n",
      "Iteration: 12162000, loss: 0.10590212995298988, gradient norm: 0.47996270786748296\n",
      "Iteration: 12163000, loss: 0.10590198302627352, gradient norm: 0.1477255439566804\n",
      "Iteration: 12164000, loss: 0.10590185941393548, gradient norm: 0.9079826215782046\n",
      "Iteration: 12165000, loss: 0.10590177998840362, gradient norm: 0.3032553431257189\n",
      "Iteration: 12166000, loss: 0.1059017448094234, gradient norm: 0.09003032995892381\n",
      "Iteration: 12167000, loss: 0.10590145899657057, gradient norm: 0.11884929935894578\n",
      "Iteration: 12168000, loss: 0.1059014673040887, gradient norm: 0.46257977140125506\n",
      "Iteration: 12169000, loss: 0.10590148746424088, gradient norm: 0.5396390478336791\n",
      "Iteration: 12170000, loss: 0.10590119292994421, gradient norm: 0.4320314795063985\n",
      "Iteration: 12171000, loss: 0.10590106741402273, gradient norm: 0.43050133400985896\n",
      "Iteration: 12172000, loss: 0.10590110749222728, gradient norm: 0.18309196003319594\n",
      "Iteration: 12173000, loss: 0.10590083598677515, gradient norm: 0.48516301178720184\n",
      "Iteration: 12174000, loss: 0.10590098390980297, gradient norm: 0.5055759479249566\n",
      "Iteration: 12175000, loss: 0.10590069265406502, gradient norm: 0.2084338752810529\n",
      "Iteration: 12176000, loss: 0.1059006470748626, gradient norm: 0.7713429401965081\n",
      "Iteration: 12177000, loss: 0.10590053100938192, gradient norm: 0.8706544943165929\n",
      "Iteration: 12178000, loss: 0.10590045072464065, gradient norm: 0.22075838522405883\n",
      "Iteration: 12179000, loss: 0.1059002129415648, gradient norm: 0.5162081930824508\n",
      "Iteration: 12180000, loss: 0.10590022225106152, gradient norm: 0.6881569768621184\n",
      "Iteration: 12181000, loss: 0.10590003389399215, gradient norm: 0.2131863637577704\n",
      "Iteration: 12182000, loss: 0.1059001199791264, gradient norm: 0.7231397654424165\n",
      "Iteration: 12183000, loss: 0.10589974460789671, gradient norm: 0.37447432773054756\n",
      "Iteration: 12184000, loss: 0.1058998492899317, gradient norm: 0.28084991472835535\n",
      "Iteration: 12185000, loss: 0.10589960484975793, gradient norm: 0.38910976641137063\n",
      "Iteration: 12186000, loss: 0.10589988217788338, gradient norm: 0.10869959989335816\n",
      "Iteration: 12187000, loss: 0.10589926415873137, gradient norm: 0.20033230914134625\n",
      "Iteration: 12188000, loss: 0.10589937283948578, gradient norm: 0.4082307310413697\n",
      "Iteration: 12189000, loss: 0.10589914887436401, gradient norm: 0.415280394621361\n",
      "Iteration: 12190000, loss: 0.10589914576131407, gradient norm: 0.40878524363005525\n",
      "Iteration: 12191000, loss: 0.10589903472632117, gradient norm: 0.47358495594392364\n",
      "Iteration: 12192000, loss: 0.10589900821855691, gradient norm: 0.42801973750127276\n",
      "Iteration: 12193000, loss: 0.10589883462777612, gradient norm: 0.4876599809824319\n",
      "Iteration: 12194000, loss: 0.1058987593444024, gradient norm: 0.20549057979440993\n",
      "Iteration: 12195000, loss: 0.10589865824703834, gradient norm: 0.4703965773024624\n",
      "Iteration: 12196000, loss: 0.10589855861199854, gradient norm: 0.27285343660955624\n",
      "Iteration: 12197000, loss: 0.10589826074864064, gradient norm: 0.20257206119868465\n",
      "Iteration: 12198000, loss: 0.10589836155746238, gradient norm: 0.32804728045322334\n",
      "Iteration: 12199000, loss: 0.10589821047403482, gradient norm: 0.33300193336735895\n",
      "Iteration: 12200000, loss: 0.10589806852773109, gradient norm: 0.33490379277203736\n",
      "Iteration: 12201000, loss: 0.10589816730399038, gradient norm: 0.6170680257607534\n",
      "Iteration: 12202000, loss: 0.10589790264873605, gradient norm: 0.09070888298927751\n",
      "Iteration: 12203000, loss: 0.10589769430887987, gradient norm: 0.49321390790584985\n",
      "Iteration: 12204000, loss: 0.10589787309716149, gradient norm: 1.0407411194466287\n",
      "Iteration: 12205000, loss: 0.10589742694092645, gradient norm: 0.23705555140971252\n",
      "Iteration: 12206000, loss: 0.10589753750539134, gradient norm: 0.08001241796237409\n",
      "Iteration: 12207000, loss: 0.10589733295413316, gradient norm: 0.3579196611566755\n",
      "Iteration: 12208000, loss: 0.10589727778111557, gradient norm: 0.6138512305584648\n",
      "Iteration: 12209000, loss: 0.10589736420428633, gradient norm: 0.7870173442622442\n",
      "Iteration: 12210000, loss: 0.10589701034692482, gradient norm: 0.3296978122851908\n",
      "Iteration: 12211000, loss: 0.10589708813148369, gradient norm: 0.7964683927357862\n",
      "Iteration: 12212000, loss: 0.10589669368236491, gradient norm: 0.2286650204786543\n",
      "Iteration: 12213000, loss: 0.10589674129145506, gradient norm: 0.7720946061017133\n",
      "Iteration: 12214000, loss: 0.1058966753441335, gradient norm: 0.2714855307085544\n",
      "Iteration: 12215000, loss: 0.10589655056053927, gradient norm: 0.22909671886390817\n",
      "Iteration: 12216000, loss: 0.10589644565466935, gradient norm: 0.17881084710547285\n",
      "Iteration: 12217000, loss: 0.10589634962486344, gradient norm: 0.010716130517148046\n",
      "Iteration: 12218000, loss: 0.1058961835257707, gradient norm: 0.5118805372335897\n",
      "Iteration: 12219000, loss: 0.10589607686655131, gradient norm: 0.24278372211554053\n",
      "Iteration: 12220000, loss: 0.10589612955640522, gradient norm: 0.23582468268678694\n",
      "Iteration: 12221000, loss: 0.10589577395753996, gradient norm: 0.09510703831276116\n",
      "Iteration: 12222000, loss: 0.1058959175571191, gradient norm: 0.3400499911466584\n",
      "Iteration: 12223000, loss: 0.1058957390622528, gradient norm: 0.07627424513025279\n",
      "Iteration: 12224000, loss: 0.10589558677899603, gradient norm: 0.2299183256367894\n",
      "Iteration: 12225000, loss: 0.10589545016037488, gradient norm: 1.048805315681688\n",
      "Iteration: 12226000, loss: 0.10589529609449938, gradient norm: 0.24914746149057324\n",
      "Iteration: 12227000, loss: 0.10589539615016222, gradient norm: 0.13563304053012953\n",
      "Iteration: 12228000, loss: 0.10589499782903629, gradient norm: 0.6674551429257669\n",
      "Iteration: 12229000, loss: 0.10589521930214366, gradient norm: 0.2563366533563695\n",
      "Iteration: 12230000, loss: 0.10589495313335803, gradient norm: 0.09219844693766277\n",
      "Iteration: 12231000, loss: 0.1058949717780865, gradient norm: 0.6328483772194673\n",
      "Iteration: 12232000, loss: 0.10589473650250965, gradient norm: 0.3517803747514111\n",
      "Iteration: 12233000, loss: 0.1058947392541541, gradient norm: 0.696670312533108\n",
      "Iteration: 12234000, loss: 0.10589459334803696, gradient norm: 0.33893613448229787\n",
      "Iteration: 12235000, loss: 0.10589434723668811, gradient norm: 0.6393487675517149\n",
      "Iteration: 12236000, loss: 0.10589438930644701, gradient norm: 0.22191332275679457\n",
      "Iteration: 12237000, loss: 0.10589418139530322, gradient norm: 0.2526922590994761\n",
      "Iteration: 12238000, loss: 0.10589413982860972, gradient norm: 0.24688171562367003\n",
      "Iteration: 12239000, loss: 0.10589407743189298, gradient norm: 0.14377149374373155\n",
      "Iteration: 12240000, loss: 0.10589388800631036, gradient norm: 0.6712170574673468\n",
      "Iteration: 12241000, loss: 0.1058938513277378, gradient norm: 0.4213823191001741\n",
      "Iteration: 12242000, loss: 0.1058937777704472, gradient norm: 0.3403967942063149\n",
      "Iteration: 12243000, loss: 0.10589377366163222, gradient norm: 0.07533972811418148\n",
      "Iteration: 12244000, loss: 0.10589327605524425, gradient norm: 0.37189791671061767\n",
      "Iteration: 12245000, loss: 0.10589339785030824, gradient norm: 0.30525728745537156\n",
      "Iteration: 12246000, loss: 0.1058932699243522, gradient norm: 0.20729955353744053\n",
      "Iteration: 12247000, loss: 0.10589330346543122, gradient norm: 0.15761130482170538\n",
      "Iteration: 12248000, loss: 0.10589314867715867, gradient norm: 0.1035319515755625\n",
      "Iteration: 12249000, loss: 0.10589294094036512, gradient norm: 0.29233380697167866\n",
      "Iteration: 12250000, loss: 0.10589292168344609, gradient norm: 0.4137374692752605\n",
      "Iteration: 12251000, loss: 0.10589282183293355, gradient norm: 0.3048233096159093\n",
      "Iteration: 12252000, loss: 0.10589264220258618, gradient norm: 0.4493049313142904\n",
      "Iteration: 12253000, loss: 0.10589263646894571, gradient norm: 0.09298864921414347\n",
      "Iteration: 12254000, loss: 0.10589247632972343, gradient norm: 0.8093742244903878\n",
      "Iteration: 12255000, loss: 0.10589250699369569, gradient norm: 0.07619747287384006\n",
      "Iteration: 12256000, loss: 0.10589232565355054, gradient norm: 0.21160536133164673\n",
      "Iteration: 12257000, loss: 0.10589183975757575, gradient norm: 0.29410315477075805\n",
      "Iteration: 12258000, loss: 0.10589199972409608, gradient norm: 0.8099693762840439\n",
      "Iteration: 12259000, loss: 0.10589203537430955, gradient norm: 0.22754292134457985\n",
      "Iteration: 12260000, loss: 0.10589194223666049, gradient norm: 0.36739209265251843\n",
      "Iteration: 12261000, loss: 0.1058917553665959, gradient norm: 0.5849872615343841\n",
      "Iteration: 12262000, loss: 0.10589155447985911, gradient norm: 0.6736883719035792\n",
      "Iteration: 12263000, loss: 0.10589155904784973, gradient norm: 0.6965497731804081\n",
      "Iteration: 12264000, loss: 0.10589142922336554, gradient norm: 0.23491368244794522\n",
      "Iteration: 12265000, loss: 0.10589123828838273, gradient norm: 0.21703594080752395\n",
      "Iteration: 12266000, loss: 0.10589125604525602, gradient norm: 0.06298695148386699\n",
      "Iteration: 12267000, loss: 0.10589121200917495, gradient norm: 0.06252892074234745\n",
      "Iteration: 12268000, loss: 0.10589101264351689, gradient norm: 0.183616734545765\n",
      "Iteration: 12269000, loss: 0.10589097924166695, gradient norm: 0.6352592266225129\n",
      "Iteration: 12270000, loss: 0.10589066661118804, gradient norm: 0.28179641387216386\n",
      "Iteration: 12271000, loss: 0.10589082944155007, gradient norm: 0.727112165624806\n",
      "Iteration: 12272000, loss: 0.10589047572123032, gradient norm: 0.4646598706321767\n",
      "Iteration: 12273000, loss: 0.10589061307338106, gradient norm: 0.14602660212437626\n",
      "Iteration: 12274000, loss: 0.10589041274449654, gradient norm: 0.8603940014000034\n",
      "Iteration: 12275000, loss: 0.10589029559952204, gradient norm: 0.24285580330999382\n",
      "Iteration: 12276000, loss: 0.10589012081748134, gradient norm: 0.25629399885684556\n",
      "Iteration: 12277000, loss: 0.10588997024935205, gradient norm: 0.36872996291686855\n",
      "Iteration: 12278000, loss: 0.10589013732078825, gradient norm: 0.06809156675891112\n",
      "Iteration: 12279000, loss: 0.1058898172781523, gradient norm: 0.505699963405477\n",
      "Iteration: 12280000, loss: 0.10588991704335209, gradient norm: 0.10463224917315775\n",
      "Iteration: 12281000, loss: 0.10588973056335453, gradient norm: 0.9344443462029148\n",
      "Iteration: 12282000, loss: 0.10588957595799013, gradient norm: 0.3160840749920531\n",
      "Iteration: 12283000, loss: 0.10588937422069125, gradient norm: 0.4011622493214141\n",
      "Iteration: 12284000, loss: 0.10588929531476007, gradient norm: 0.4890781012457945\n",
      "Iteration: 12285000, loss: 0.1058893607862297, gradient norm: 0.6714649242738766\n",
      "Iteration: 12286000, loss: 0.10588910474207822, gradient norm: 0.21859525474988742\n",
      "Iteration: 12287000, loss: 0.10588908129297989, gradient norm: 0.22727801387785881\n",
      "Iteration: 12288000, loss: 0.10588899388889356, gradient norm: 0.22164398636561877\n",
      "Iteration: 12289000, loss: 0.10588874492663604, gradient norm: 0.4563113925830466\n",
      "Iteration: 12290000, loss: 0.10588876088820574, gradient norm: 0.7384102327278542\n",
      "Iteration: 12291000, loss: 0.10588865866967316, gradient norm: 0.062268193459391025\n",
      "Iteration: 12292000, loss: 0.10588854031237536, gradient norm: 0.4364532398562617\n",
      "Iteration: 12293000, loss: 0.10588835955644285, gradient norm: 0.14435953838615642\n",
      "Iteration: 12294000, loss: 0.10588831013914692, gradient norm: 0.7212042860573704\n",
      "Iteration: 12295000, loss: 0.1058883661917324, gradient norm: 0.19001035767711202\n",
      "Iteration: 12296000, loss: 0.10588808015943578, gradient norm: 0.2859631682952529\n",
      "Iteration: 12297000, loss: 0.10588804687239968, gradient norm: 0.16133267555299938\n",
      "Iteration: 12298000, loss: 0.10588785450839518, gradient norm: 0.23874950053242083\n",
      "Iteration: 12299000, loss: 0.10588775288662575, gradient norm: 0.21790780633962503\n",
      "Iteration: 12300000, loss: 0.10588777917117943, gradient norm: 0.27931272433836135\n",
      "Iteration: 12301000, loss: 0.10588746574292182, gradient norm: 0.2034131292582341\n",
      "Iteration: 12302000, loss: 0.10588756366839744, gradient norm: 0.45756758043930235\n",
      "Iteration: 12303000, loss: 0.10588735457157591, gradient norm: 0.2513368201230972\n",
      "Iteration: 12304000, loss: 0.1058872442285465, gradient norm: 0.5100589526388474\n",
      "Iteration: 12305000, loss: 0.1058872468713265, gradient norm: 0.315819114292807\n",
      "Iteration: 12306000, loss: 0.10588696717191812, gradient norm: 0.4000844975029\n",
      "Iteration: 12307000, loss: 0.10588699650044674, gradient norm: 0.37024998080746\n",
      "Iteration: 12308000, loss: 0.1058869103807289, gradient norm: 0.12275449683063784\n",
      "Iteration: 12309000, loss: 0.10588681859254756, gradient norm: 0.6300378107689212\n",
      "Iteration: 12310000, loss: 0.10588661930843896, gradient norm: 0.06188528417452934\n",
      "Iteration: 12311000, loss: 0.10588664708769527, gradient norm: 0.24895527991639002\n",
      "Iteration: 12312000, loss: 0.1058866061409474, gradient norm: 0.776639667854071\n",
      "Iteration: 12313000, loss: 0.10588633468975549, gradient norm: 0.43812570876259876\n",
      "Iteration: 12314000, loss: 0.10588621610911082, gradient norm: 0.17060170858966384\n",
      "Iteration: 12315000, loss: 0.10588620627947834, gradient norm: 0.1471242521353547\n",
      "Iteration: 12316000, loss: 0.10588581975478148, gradient norm: 0.5872220958202539\n",
      "Iteration: 12317000, loss: 0.10588612825443681, gradient norm: 0.1765637531256599\n",
      "Iteration: 12318000, loss: 0.10588576417653234, gradient norm: 0.15658714943634824\n",
      "Iteration: 12319000, loss: 0.10588559813588391, gradient norm: 0.4235116326277948\n",
      "Iteration: 12320000, loss: 0.10588580401910275, gradient norm: 0.41405114705828416\n",
      "Iteration: 12321000, loss: 0.10588563573954915, gradient norm: 0.535829271184712\n",
      "Iteration: 12322000, loss: 0.10588512036297178, gradient norm: 0.5492375667892443\n",
      "Iteration: 12323000, loss: 0.10588546585173843, gradient norm: 0.028516869005374867\n",
      "Iteration: 12324000, loss: 0.10588527532895466, gradient norm: 0.1967178229136953\n",
      "Iteration: 12325000, loss: 0.10588512261852555, gradient norm: 0.3245732398309079\n",
      "Iteration: 12326000, loss: 0.10588499766170276, gradient norm: 0.4248661767910536\n",
      "Iteration: 12327000, loss: 0.10588494969487845, gradient norm: 0.26407052195853487\n",
      "Iteration: 12328000, loss: 0.10588471029458277, gradient norm: 0.3295703048893074\n",
      "Iteration: 12329000, loss: 0.10588465872807341, gradient norm: 0.25369547374616525\n",
      "Iteration: 12330000, loss: 0.10588459926335317, gradient norm: 0.15898224999993152\n",
      "Iteration: 12331000, loss: 0.10588461935243504, gradient norm: 0.05981425579714857\n",
      "Iteration: 12332000, loss: 0.10588432278029056, gradient norm: 0.09428232284007293\n",
      "Iteration: 12333000, loss: 0.10588420785463805, gradient norm: 0.5905550784759487\n",
      "Iteration: 12334000, loss: 0.10588434903097343, gradient norm: 0.8852671995521029\n",
      "Iteration: 12335000, loss: 0.10588404424027323, gradient norm: 0.2803727199993264\n",
      "Iteration: 12336000, loss: 0.1058839825384365, gradient norm: 0.6502693376106056\n",
      "Iteration: 12337000, loss: 0.10588389125160028, gradient norm: 0.9839511170825951\n",
      "Iteration: 12338000, loss: 0.10588380638234329, gradient norm: 0.5814093216126608\n",
      "Iteration: 12339000, loss: 0.10588371880681814, gradient norm: 0.5690543521477864\n",
      "Iteration: 12340000, loss: 0.1058834623354853, gradient norm: 0.37674296125633183\n",
      "Iteration: 12341000, loss: 0.10588350625545655, gradient norm: 0.2827460737691497\n",
      "Iteration: 12342000, loss: 0.10588330749595577, gradient norm: 0.5286662691429409\n",
      "Iteration: 12343000, loss: 0.10588338238044612, gradient norm: 0.40729209677350126\n",
      "Iteration: 12344000, loss: 0.10588309603766245, gradient norm: 0.1268324669231951\n",
      "Iteration: 12345000, loss: 0.10588297637437043, gradient norm: 0.06597518706007321\n",
      "Iteration: 12346000, loss: 0.10588301071007558, gradient norm: 0.15659745772434053\n",
      "Iteration: 12347000, loss: 0.10588301967727344, gradient norm: 0.24381255118876327\n",
      "Iteration: 12348000, loss: 0.10588253472002994, gradient norm: 0.5936687650381466\n",
      "Iteration: 12349000, loss: 0.10588272615679321, gradient norm: 0.41711999316255916\n",
      "Iteration: 12350000, loss: 0.10588270972934859, gradient norm: 0.3343731413441816\n",
      "Iteration: 12351000, loss: 0.10588225139206536, gradient norm: 0.2626535075791616\n",
      "Iteration: 12352000, loss: 0.105882299729124, gradient norm: 0.44388299690355565\n",
      "Iteration: 12353000, loss: 0.10588221455376268, gradient norm: 0.039646706991069175\n",
      "Iteration: 12354000, loss: 0.10588206286571498, gradient norm: 0.23923354932658794\n",
      "Iteration: 12355000, loss: 0.10588227439422095, gradient norm: 0.046758192116252796\n",
      "Iteration: 12356000, loss: 0.10588186785026044, gradient norm: 0.1892765308062736\n",
      "Iteration: 12357000, loss: 0.10588187835389437, gradient norm: 0.9102794606063731\n",
      "Iteration: 12358000, loss: 0.10588164100147898, gradient norm: 0.6824475097802635\n",
      "Iteration: 12359000, loss: 0.10588167698530353, gradient norm: 0.7018540546695404\n",
      "Iteration: 12360000, loss: 0.10588151249255574, gradient norm: 0.7091187170933694\n",
      "Iteration: 12361000, loss: 0.10588146986871712, gradient norm: 1.1702465431732372\n",
      "Iteration: 12362000, loss: 0.10588122368318757, gradient norm: 0.16721422571470823\n",
      "Iteration: 12363000, loss: 0.10588135776737036, gradient norm: 0.6606072363012426\n",
      "Iteration: 12364000, loss: 0.10588108456098981, gradient norm: 0.1959689516611571\n",
      "Iteration: 12365000, loss: 0.10588087416008954, gradient norm: 0.38923323502820545\n",
      "Iteration: 12366000, loss: 0.1058809975403074, gradient norm: 0.5205526295105334\n",
      "Iteration: 12367000, loss: 0.10588080377173707, gradient norm: 0.09211238376502513\n",
      "Iteration: 12368000, loss: 0.10588064768559728, gradient norm: 0.32639190605481405\n",
      "Iteration: 12369000, loss: 0.1058806587825859, gradient norm: 0.30181674796747526\n",
      "Iteration: 12370000, loss: 0.10588039553143831, gradient norm: 0.8658988459586138\n",
      "Iteration: 12371000, loss: 0.10588051201665397, gradient norm: 0.9092961321912618\n",
      "Iteration: 12372000, loss: 0.10588016499377305, gradient norm: 0.36451569823414387\n",
      "Iteration: 12373000, loss: 0.10588036971152635, gradient norm: 0.775860880373481\n",
      "Iteration: 12374000, loss: 0.10587980228506248, gradient norm: 0.34407946815851187\n",
      "Iteration: 12375000, loss: 0.10588020408343206, gradient norm: 0.2567399202144406\n",
      "Iteration: 12376000, loss: 0.10587969589734227, gradient norm: 0.2597221888571297\n",
      "Iteration: 12377000, loss: 0.10587973519465922, gradient norm: 0.7538300268516059\n",
      "Iteration: 12378000, loss: 0.10587977261092477, gradient norm: 0.24033947770641895\n",
      "Iteration: 12379000, loss: 0.10587950311192199, gradient norm: 0.229331566019235\n",
      "Iteration: 12380000, loss: 0.10587956311635534, gradient norm: 0.17493642818108787\n",
      "Iteration: 12381000, loss: 0.10587933829745624, gradient norm: 0.42377973269273816\n",
      "Iteration: 12382000, loss: 0.1058792638663539, gradient norm: 0.07848906151263374\n",
      "Iteration: 12383000, loss: 0.10587929433615068, gradient norm: 0.3035363410829893\n",
      "Iteration: 12384000, loss: 0.1058788374211631, gradient norm: 0.43038284936989113\n",
      "Iteration: 12385000, loss: 0.10587910350245144, gradient norm: 0.5219356884835404\n",
      "Iteration: 12386000, loss: 0.1058786925162049, gradient norm: 0.14074394160910894\n",
      "Iteration: 12387000, loss: 0.10587878487691663, gradient norm: 0.6586709134247888\n",
      "Iteration: 12388000, loss: 0.1058787760573369, gradient norm: 0.3698895916813251\n",
      "Iteration: 12389000, loss: 0.10587850792368556, gradient norm: 0.6879857216648195\n",
      "Iteration: 12390000, loss: 0.10587844595060332, gradient norm: 0.37234558007814283\n",
      "Iteration: 12391000, loss: 0.10587838212069287, gradient norm: 0.4965327977215945\n",
      "Iteration: 12392000, loss: 0.1058783389818695, gradient norm: 0.08490913704297004\n",
      "Iteration: 12393000, loss: 0.10587796487987795, gradient norm: 0.4727318780381028\n",
      "Iteration: 12394000, loss: 0.10587801272369558, gradient norm: 0.46832182465996736\n",
      "Iteration: 12395000, loss: 0.10587794870542766, gradient norm: 0.1456842306586568\n",
      "Iteration: 12396000, loss: 0.10587769456211876, gradient norm: 0.7536590788028994\n",
      "Iteration: 12397000, loss: 0.10587790436082425, gradient norm: 0.1935596064196674\n",
      "Iteration: 12398000, loss: 0.10587750950251217, gradient norm: 0.4933178627913574\n",
      "Iteration: 12399000, loss: 0.10587764028831119, gradient norm: 0.5958816979587457\n",
      "Iteration: 12400000, loss: 0.10587728404945265, gradient norm: 0.23987685883967844\n",
      "Iteration: 12401000, loss: 0.10587733211091563, gradient norm: 0.24030945848420157\n",
      "Iteration: 12402000, loss: 0.10587726875207552, gradient norm: 0.17340652418677763\n",
      "Iteration: 12403000, loss: 0.10587711362653034, gradient norm: 0.32693396658392654\n",
      "Iteration: 12404000, loss: 0.10587705533464087, gradient norm: 0.3688954261853872\n",
      "Iteration: 12405000, loss: 0.10587685782459345, gradient norm: 0.11541493286633299\n",
      "Iteration: 12406000, loss: 0.10587676084668295, gradient norm: 0.49087967229797996\n",
      "Iteration: 12407000, loss: 0.10587698338078269, gradient norm: 0.6210885140303327\n",
      "Iteration: 12408000, loss: 0.10587642142719847, gradient norm: 0.17547620227679128\n",
      "Iteration: 12409000, loss: 0.1058766068107533, gradient norm: 0.015266949084138925\n",
      "Iteration: 12410000, loss: 0.10587628899074056, gradient norm: 0.5963738823439722\n",
      "Iteration: 12411000, loss: 0.10587624691514798, gradient norm: 0.18550259684875917\n",
      "Iteration: 12412000, loss: 0.10587632806041816, gradient norm: 0.39807243411864235\n",
      "Iteration: 12413000, loss: 0.10587602490709394, gradient norm: 0.5281873103614504\n",
      "Iteration: 12414000, loss: 0.10587612252156435, gradient norm: 0.5259390943177228\n",
      "Iteration: 12415000, loss: 0.10587572321455502, gradient norm: 0.21542186512017314\n",
      "Iteration: 12416000, loss: 0.1058758368753627, gradient norm: 0.6911819027103607\n",
      "Iteration: 12417000, loss: 0.1058756472218393, gradient norm: 0.07847036566252073\n",
      "Iteration: 12418000, loss: 0.10587566842764379, gradient norm: 0.33637547983978344\n",
      "Iteration: 12419000, loss: 0.10587540760849862, gradient norm: 0.827905049139431\n",
      "Iteration: 12420000, loss: 0.10587542229911054, gradient norm: 0.051738211101763756\n",
      "Iteration: 12421000, loss: 0.10587519429936805, gradient norm: 0.08442343591454705\n",
      "Iteration: 12422000, loss: 0.10587530029576542, gradient norm: 0.4117697700202685\n",
      "Iteration: 12423000, loss: 0.10587510258741152, gradient norm: 1.1320447733682268\n",
      "Iteration: 12424000, loss: 0.1058749492491126, gradient norm: 0.2027412677912448\n",
      "Iteration: 12425000, loss: 0.10587482501709473, gradient norm: 0.6067515952249247\n",
      "Iteration: 12426000, loss: 0.10587485670529423, gradient norm: 0.571850871659463\n",
      "Iteration: 12427000, loss: 0.10587471809088819, gradient norm: 0.39864904117721023\n",
      "Iteration: 12428000, loss: 0.10587455517845477, gradient norm: 0.1820452575358956\n",
      "Iteration: 12429000, loss: 0.10587450340898957, gradient norm: 0.1069493440267793\n",
      "Iteration: 12430000, loss: 0.10587425933721954, gradient norm: 0.22841848241994042\n",
      "Iteration: 12431000, loss: 0.10587417247317871, gradient norm: 0.4065782027670741\n",
      "Iteration: 12432000, loss: 0.10587442677678775, gradient norm: 0.25414343847927756\n",
      "Iteration: 12433000, loss: 0.10587396981619279, gradient norm: 0.33145413099404264\n",
      "Iteration: 12434000, loss: 0.10587371049841918, gradient norm: 0.11666009033235522\n",
      "Iteration: 12435000, loss: 0.1058742868438131, gradient norm: 0.6651039472735717\n",
      "Iteration: 12436000, loss: 0.10587363770582635, gradient norm: 0.9078567372865148\n",
      "Iteration: 12437000, loss: 0.10587357658295285, gradient norm: 0.2175433721037076\n",
      "Iteration: 12438000, loss: 0.10587353022103135, gradient norm: 0.7182557519280753\n",
      "Iteration: 12439000, loss: 0.10587341776877583, gradient norm: 0.30185776904039346\n",
      "Iteration: 12440000, loss: 0.10587333927774017, gradient norm: 0.6822067390303861\n",
      "Iteration: 12441000, loss: 0.10587328982899911, gradient norm: 0.08149351585893708\n",
      "Iteration: 12442000, loss: 0.10587315009359254, gradient norm: 0.556955789629491\n",
      "Iteration: 12443000, loss: 0.10587309060620265, gradient norm: 0.3886324704204437\n",
      "Iteration: 12444000, loss: 0.10587283263397138, gradient norm: 1.0288385352813059\n",
      "Iteration: 12445000, loss: 0.10587283300720736, gradient norm: 0.021213694245314853\n",
      "Iteration: 12446000, loss: 0.10587278763973246, gradient norm: 0.5180894205094364\n",
      "Iteration: 12447000, loss: 0.10587252588663605, gradient norm: 0.5492707750033282\n",
      "Iteration: 12448000, loss: 0.1058726113884814, gradient norm: 0.5296255498856359\n",
      "Iteration: 12449000, loss: 0.10587241116265932, gradient norm: 0.31862270074013094\n",
      "Iteration: 12450000, loss: 0.10587226346009046, gradient norm: 0.6984133254290495\n",
      "Iteration: 12451000, loss: 0.1058722932806974, gradient norm: 0.5379067384790704\n",
      "Iteration: 12452000, loss: 0.10587196472347386, gradient norm: 0.24961815528154163\n",
      "Iteration: 12453000, loss: 0.10587204185891534, gradient norm: 0.7734826049296847\n",
      "Iteration: 12454000, loss: 0.10587193163051935, gradient norm: 0.1071656411004928\n",
      "Iteration: 12455000, loss: 0.10587177281951847, gradient norm: 0.3678212057725651\n",
      "Iteration: 12456000, loss: 0.10587178816546751, gradient norm: 0.14590230336641516\n",
      "Iteration: 12457000, loss: 0.10587167347343883, gradient norm: 0.35958353608799515\n",
      "Iteration: 12458000, loss: 0.1058713999593302, gradient norm: 0.6503529118826694\n",
      "Iteration: 12459000, loss: 0.10587155800848225, gradient norm: 0.5004513184739072\n",
      "Iteration: 12460000, loss: 0.10587122348990895, gradient norm: 0.24188576060426087\n",
      "Iteration: 12461000, loss: 0.10587114360499278, gradient norm: 0.7431396685495177\n",
      "Iteration: 12462000, loss: 0.10587109501889888, gradient norm: 0.2555781922460799\n",
      "Iteration: 12463000, loss: 0.10587116568278156, gradient norm: 0.16027210287204102\n",
      "Iteration: 12464000, loss: 0.1058707386134204, gradient norm: 0.313569933846554\n",
      "Iteration: 12465000, loss: 0.10587092239381979, gradient norm: 0.2523404242435138\n",
      "Iteration: 12466000, loss: 0.10587057092071189, gradient norm: 0.6426848199727293\n",
      "Iteration: 12467000, loss: 0.10587078795545547, gradient norm: 0.10873788813506571\n",
      "Iteration: 12468000, loss: 0.10587034842050236, gradient norm: 0.11973093974124553\n",
      "Iteration: 12469000, loss: 0.10587055496471695, gradient norm: 0.9106183909184034\n",
      "Iteration: 12470000, loss: 0.10587034360959459, gradient norm: 0.3342977858880846\n",
      "Iteration: 12471000, loss: 0.1058701507011292, gradient norm: 0.0951728273260047\n",
      "Iteration: 12472000, loss: 0.10586999389924744, gradient norm: 0.3761552525823017\n",
      "Iteration: 12473000, loss: 0.10587007631753767, gradient norm: 0.7411359426346789\n",
      "Iteration: 12474000, loss: 0.10586983889068244, gradient norm: 0.12699315107837597\n",
      "Iteration: 12475000, loss: 0.10586983155022343, gradient norm: 1.258638460141695\n",
      "Iteration: 12476000, loss: 0.10586961476848548, gradient norm: 0.16052054186054332\n",
      "Iteration: 12477000, loss: 0.10586974342565386, gradient norm: 0.896262446625703\n",
      "Iteration: 12478000, loss: 0.10586952596168225, gradient norm: 1.0736738521403744\n",
      "Iteration: 12479000, loss: 0.10586934184578045, gradient norm: 0.07268149237517507\n",
      "Iteration: 12480000, loss: 0.10586927261097853, gradient norm: 0.431894775491534\n",
      "Iteration: 12481000, loss: 0.10586909839632545, gradient norm: 0.24331148670539723\n",
      "Iteration: 12482000, loss: 0.10586908482499194, gradient norm: 0.4234749692227417\n",
      "Iteration: 12483000, loss: 0.10586896996738966, gradient norm: 0.6140459301006628\n",
      "Iteration: 12484000, loss: 0.10586899001930154, gradient norm: 0.1764392685687735\n",
      "Iteration: 12485000, loss: 0.10586864559619491, gradient norm: 0.11738238035013172\n",
      "Iteration: 12486000, loss: 0.10586865157898132, gradient norm: 0.20970784610311916\n",
      "Iteration: 12487000, loss: 0.10586861430591535, gradient norm: 0.44062900107913866\n",
      "Iteration: 12488000, loss: 0.10586853421607353, gradient norm: 0.15478392895153198\n",
      "Iteration: 12489000, loss: 0.10586827026829362, gradient norm: 0.46838050030405154\n",
      "Iteration: 12490000, loss: 0.10586840677741492, gradient norm: 0.46220322918008055\n",
      "Iteration: 12491000, loss: 0.10586809929261456, gradient norm: 0.6715158662526782\n",
      "Iteration: 12492000, loss: 0.10586804472646827, gradient norm: 0.16898256622643715\n",
      "Iteration: 12493000, loss: 0.10586801077941806, gradient norm: 0.2996221331564873\n",
      "Iteration: 12494000, loss: 0.10586787911886068, gradient norm: 0.48331275102374677\n",
      "Iteration: 12495000, loss: 0.10586769876436136, gradient norm: 0.4909145465890537\n",
      "Iteration: 12496000, loss: 0.10586769083941223, gradient norm: 0.28200326110903756\n",
      "Iteration: 12497000, loss: 0.1058675117822135, gradient norm: 0.7581417676900511\n",
      "Iteration: 12498000, loss: 0.1058675203032518, gradient norm: 0.4112255466862514\n",
      "Iteration: 12499000, loss: 0.10586735821499083, gradient norm: 0.16520946640061251\n",
      "Iteration: 12500000, loss: 0.10586721571579075, gradient norm: 0.34639144572878305\n",
      "Iteration: 12501000, loss: 0.10586725289547354, gradient norm: 0.3302333424947461\n",
      "Iteration: 12502000, loss: 0.10586691666368354, gradient norm: 0.3761129872864694\n",
      "Iteration: 12503000, loss: 0.1058668885504274, gradient norm: 0.36344384717171163\n",
      "Iteration: 12504000, loss: 0.10586691402699634, gradient norm: 0.21277864350518327\n",
      "Iteration: 12505000, loss: 0.10586694348821342, gradient norm: 0.8558361527052386\n",
      "Iteration: 12506000, loss: 0.1058665979715899, gradient norm: 1.3199541484840864\n",
      "Iteration: 12507000, loss: 0.10586659941913511, gradient norm: 0.533488836291754\n",
      "Iteration: 12508000, loss: 0.10586634301897249, gradient norm: 0.45362294181537144\n",
      "Iteration: 12509000, loss: 0.10586637328228644, gradient norm: 0.1187632561079237\n",
      "Iteration: 12510000, loss: 0.10586621867201117, gradient norm: 0.2854630076384667\n",
      "Iteration: 12511000, loss: 0.10586613695135541, gradient norm: 0.42795784136174086\n",
      "Iteration: 12512000, loss: 0.10586598910475882, gradient norm: 0.10877871283812367\n",
      "Iteration: 12513000, loss: 0.1058659534488736, gradient norm: 0.8020363808223725\n",
      "Iteration: 12514000, loss: 0.10586595134754834, gradient norm: 0.7659215789186012\n",
      "Iteration: 12515000, loss: 0.10586581140222079, gradient norm: 1.1735561949016209\n",
      "Iteration: 12516000, loss: 0.10586558770135869, gradient norm: 0.21508108253424124\n",
      "Iteration: 12517000, loss: 0.10586544268967824, gradient norm: 0.46133100325807086\n",
      "Iteration: 12518000, loss: 0.1058655171396158, gradient norm: 0.9196122438028497\n",
      "Iteration: 12519000, loss: 0.10586535363227124, gradient norm: 0.17168714759642414\n",
      "Iteration: 12520000, loss: 0.10586526756373615, gradient norm: 0.2404095381900683\n",
      "Iteration: 12521000, loss: 0.10586508598646387, gradient norm: 0.16614917245357194\n",
      "Iteration: 12522000, loss: 0.10586502685262147, gradient norm: 0.4201265838603502\n",
      "Iteration: 12523000, loss: 0.1058650242013176, gradient norm: 1.1932698334574827\n",
      "Iteration: 12524000, loss: 0.1058648732893668, gradient norm: 0.7439858957216907\n",
      "Iteration: 12525000, loss: 0.10586477969242093, gradient norm: 0.4523471303465211\n",
      "Iteration: 12526000, loss: 0.10586452469697699, gradient norm: 0.6987958230082694\n",
      "Iteration: 12527000, loss: 0.10586462520558924, gradient norm: 0.14151305632133868\n",
      "Iteration: 12528000, loss: 0.10586438563412588, gradient norm: 0.10370721485688299\n",
      "Iteration: 12529000, loss: 0.10586432429373029, gradient norm: 0.4279904402527125\n",
      "Iteration: 12530000, loss: 0.10586415874367631, gradient norm: 0.613720036363414\n",
      "Iteration: 12531000, loss: 0.10586413799724116, gradient norm: 1.0823140195817542\n",
      "Iteration: 12532000, loss: 0.10586402556873341, gradient norm: 0.33204381747696843\n",
      "Iteration: 12533000, loss: 0.10586391284225743, gradient norm: 0.45042806858429224\n",
      "Iteration: 12534000, loss: 0.1058639814215652, gradient norm: 0.28686971206333556\n",
      "Iteration: 12535000, loss: 0.10586363518124611, gradient norm: 0.7388966072044367\n",
      "Iteration: 12536000, loss: 0.10586368621127776, gradient norm: 0.37085212001219925\n",
      "Iteration: 12537000, loss: 0.10586338212339938, gradient norm: 0.9599621370973792\n",
      "Iteration: 12538000, loss: 0.10586353412690738, gradient norm: 0.3902132725943525\n",
      "Iteration: 12539000, loss: 0.10586336348491047, gradient norm: 0.9779756073311379\n",
      "Iteration: 12540000, loss: 0.10586335345574452, gradient norm: 0.5080974009017238\n",
      "Iteration: 12541000, loss: 0.10586303877215635, gradient norm: 0.13378574100853963\n",
      "Iteration: 12542000, loss: 0.1058630149749542, gradient norm: 0.15378724731503113\n",
      "Iteration: 12543000, loss: 0.1058628996176645, gradient norm: 1.0810547592270532\n",
      "Iteration: 12544000, loss: 0.1058628618056876, gradient norm: 0.41576836594890915\n",
      "Iteration: 12545000, loss: 0.10586266954334173, gradient norm: 0.8793298153498436\n",
      "Iteration: 12546000, loss: 0.10586269396838906, gradient norm: 1.1682217544921378\n",
      "Iteration: 12547000, loss: 0.10586241216201864, gradient norm: 0.1543353883194978\n",
      "Iteration: 12548000, loss: 0.10586242670257738, gradient norm: 0.4505307331570597\n",
      "Iteration: 12549000, loss: 0.10586225342792954, gradient norm: 0.08450412100040666\n",
      "Iteration: 12550000, loss: 0.10586229620572335, gradient norm: 0.07336778692943074\n",
      "Iteration: 12551000, loss: 0.10586206758091761, gradient norm: 0.4413664286357116\n",
      "Iteration: 12552000, loss: 0.1058620141748122, gradient norm: 0.6290998131966591\n",
      "Iteration: 12553000, loss: 0.10586190683958452, gradient norm: 0.37337261678573963\n",
      "Iteration: 12554000, loss: 0.10586190092524009, gradient norm: 0.5305228029310828\n",
      "Iteration: 12555000, loss: 0.10586162866782456, gradient norm: 0.24490479807209217\n",
      "Iteration: 12556000, loss: 0.10586171040293371, gradient norm: 0.7727355826454815\n",
      "Iteration: 12557000, loss: 0.10586152210431884, gradient norm: 0.3392982757208556\n",
      "Iteration: 12558000, loss: 0.10586129815454805, gradient norm: 0.6246258485386416\n",
      "Iteration: 12559000, loss: 0.10586138315777956, gradient norm: 0.3559619590298843\n",
      "Iteration: 12560000, loss: 0.10586123072862248, gradient norm: 0.34201034057980406\n",
      "Iteration: 12561000, loss: 0.10586116445327677, gradient norm: 0.34444134864181136\n",
      "Iteration: 12562000, loss: 0.10586101450196872, gradient norm: 0.3354077528677862\n",
      "Iteration: 12563000, loss: 0.10586091201189436, gradient norm: 0.28188200853717676\n",
      "Iteration: 12564000, loss: 0.10586069869116294, gradient norm: 0.07259180247964685\n",
      "Iteration: 12565000, loss: 0.1058607227082492, gradient norm: 0.12606374981302246\n",
      "Iteration: 12566000, loss: 0.10586080392793999, gradient norm: 0.312936190054921\n",
      "Iteration: 12567000, loss: 0.10586039929211007, gradient norm: 0.4877169279596612\n",
      "Iteration: 12568000, loss: 0.10586043699864417, gradient norm: 0.3216242123883655\n",
      "Iteration: 12569000, loss: 0.10586031353676846, gradient norm: 0.7861373678900718\n",
      "Iteration: 12570000, loss: 0.10586033225278471, gradient norm: 0.30521609663614624\n",
      "Iteration: 12571000, loss: 0.1058599538456576, gradient norm: 0.137126183802192\n",
      "Iteration: 12572000, loss: 0.10586012486397536, gradient norm: 0.1998864697944208\n",
      "Iteration: 12573000, loss: 0.10585977243689403, gradient norm: 0.6735951893887752\n",
      "Iteration: 12574000, loss: 0.10585982586006312, gradient norm: 0.5013333470808633\n",
      "Iteration: 12575000, loss: 0.10585971547938937, gradient norm: 0.5078962897673847\n",
      "Iteration: 12576000, loss: 0.10585974717869655, gradient norm: 0.11648197456438436\n",
      "Iteration: 12577000, loss: 0.10585938758825826, gradient norm: 0.2531723595233958\n",
      "Iteration: 12578000, loss: 0.10585947151894945, gradient norm: 0.3155436223445291\n",
      "Iteration: 12579000, loss: 0.10585932126602225, gradient norm: 0.9744051080059191\n",
      "Iteration: 12580000, loss: 0.10585922647570696, gradient norm: 0.1969593147430568\n",
      "Iteration: 12581000, loss: 0.10585899578509082, gradient norm: 0.5080039323493833\n",
      "Iteration: 12582000, loss: 0.10585893762849205, gradient norm: 0.04578697108773456\n",
      "Iteration: 12583000, loss: 0.10585898598340408, gradient norm: 0.17199476162342126\n",
      "Iteration: 12584000, loss: 0.10585892829833952, gradient norm: 0.2563064561843261\n",
      "Iteration: 12585000, loss: 0.10585876797702683, gradient norm: 0.48054908531049567\n",
      "Iteration: 12586000, loss: 0.10585858014831326, gradient norm: 1.1662465095743026\n",
      "Iteration: 12587000, loss: 0.10585852799697738, gradient norm: 0.4120309334599788\n",
      "Iteration: 12588000, loss: 0.10585830638540397, gradient norm: 0.645411633469752\n",
      "Iteration: 12589000, loss: 0.10585826175594115, gradient norm: 0.43883351830942047\n",
      "Iteration: 12590000, loss: 0.10585843804656864, gradient norm: 0.09199077974830402\n",
      "Iteration: 12591000, loss: 0.10585796779421267, gradient norm: 0.22847258840787582\n",
      "Iteration: 12592000, loss: 0.10585796550775539, gradient norm: 0.3102472566045145\n",
      "Iteration: 12593000, loss: 0.10585800467429549, gradient norm: 0.3034566326239104\n",
      "Iteration: 12594000, loss: 0.10585783079756368, gradient norm: 0.9977941498738033\n",
      "Iteration: 12595000, loss: 0.10585760562749111, gradient norm: 0.14647897945413982\n",
      "Iteration: 12596000, loss: 0.1058575615407095, gradient norm: 0.1273807754785598\n",
      "Iteration: 12597000, loss: 0.10585755386972738, gradient norm: 0.6899532153591676\n",
      "Iteration: 12598000, loss: 0.10585737947813592, gradient norm: 0.37981277223077403\n",
      "Iteration: 12599000, loss: 0.10585731914409598, gradient norm: 0.5672158764199103\n",
      "Iteration: 12600000, loss: 0.10585715162652364, gradient norm: 0.2555511828394064\n",
      "Iteration: 12601000, loss: 0.10585714290527605, gradient norm: 0.3146393791342264\n",
      "Iteration: 12602000, loss: 0.10585701523539032, gradient norm: 0.6899725969924024\n",
      "Iteration: 12603000, loss: 0.10585691789056184, gradient norm: 0.23252126670284465\n",
      "Iteration: 12604000, loss: 0.10585675514298261, gradient norm: 0.41915867088322417\n",
      "Iteration: 12605000, loss: 0.10585682305869484, gradient norm: 0.5168641596797007\n",
      "Iteration: 12606000, loss: 0.10585655612710039, gradient norm: 1.0143683547403568\n",
      "Iteration: 12607000, loss: 0.10585651819727687, gradient norm: 0.15368013500227423\n",
      "Iteration: 12608000, loss: 0.10585638733770712, gradient norm: 0.20459012972065352\n",
      "Iteration: 12609000, loss: 0.10585626534759128, gradient norm: 0.38496494302017864\n",
      "Iteration: 12610000, loss: 0.10585632311828061, gradient norm: 0.18234646461069864\n",
      "Iteration: 12611000, loss: 0.10585603557672735, gradient norm: 0.3221463698543668\n",
      "Iteration: 12612000, loss: 0.10585613211359242, gradient norm: 0.4195069988032711\n",
      "Iteration: 12613000, loss: 0.1058557341336371, gradient norm: 0.0230693037871734\n",
      "Iteration: 12614000, loss: 0.10585590983025142, gradient norm: 0.6846304362646776\n",
      "Iteration: 12615000, loss: 0.105855771842622, gradient norm: 0.23773044610827448\n",
      "Iteration: 12616000, loss: 0.10585570362299701, gradient norm: 0.2889585836819879\n",
      "Iteration: 12617000, loss: 0.10585545697025518, gradient norm: 0.10101831287728603\n",
      "Iteration: 12618000, loss: 0.10585539747806834, gradient norm: 0.8165725100051519\n",
      "Iteration: 12619000, loss: 0.10585527791447756, gradient norm: 0.7342388552802452\n",
      "Iteration: 12620000, loss: 0.1058550945116034, gradient norm: 0.556165147862669\n",
      "Iteration: 12621000, loss: 0.10585517623056664, gradient norm: 0.2921549911792666\n",
      "Iteration: 12622000, loss: 0.10585514331977666, gradient norm: 0.40981902696752254\n",
      "Iteration: 12623000, loss: 0.10585489022194744, gradient norm: 0.7987868577244017\n",
      "Iteration: 12624000, loss: 0.10585487470561204, gradient norm: 0.31224113427837824\n",
      "Iteration: 12625000, loss: 0.10585466543085739, gradient norm: 0.6099872410387481\n",
      "Iteration: 12626000, loss: 0.10585455135205586, gradient norm: 0.13293583449614751\n",
      "Iteration: 12627000, loss: 0.10585454075351067, gradient norm: 0.37047589238005896\n",
      "Iteration: 12628000, loss: 0.10585436043475425, gradient norm: 0.24164345038607293\n",
      "Iteration: 12629000, loss: 0.1058544813189396, gradient norm: 0.1813111452783686\n",
      "Iteration: 12630000, loss: 0.10585407598566653, gradient norm: 0.5740529984749584\n",
      "Iteration: 12631000, loss: 0.10585412653640237, gradient norm: 0.1854393653511355\n",
      "Iteration: 12632000, loss: 0.10585401449045442, gradient norm: 0.2275201986482501\n",
      "Iteration: 12633000, loss: 0.10585395279784533, gradient norm: 0.09287209986221293\n",
      "Iteration: 12634000, loss: 0.10585386080161868, gradient norm: 0.8180323001790489\n",
      "Iteration: 12635000, loss: 0.10585371951985749, gradient norm: 0.6159865283784837\n",
      "Iteration: 12636000, loss: 0.10585374071660236, gradient norm: 0.8378113769832654\n",
      "Iteration: 12637000, loss: 0.10585342748908996, gradient norm: 0.22118124561440367\n",
      "Iteration: 12638000, loss: 0.10585329966371274, gradient norm: 0.10654088471021016\n",
      "Iteration: 12639000, loss: 0.10585333431702976, gradient norm: 0.5409044746133985\n",
      "Iteration: 12640000, loss: 0.10585323612217644, gradient norm: 0.23800863729739744\n",
      "Iteration: 12641000, loss: 0.10585320209696517, gradient norm: 0.4124120848675795\n",
      "Iteration: 12642000, loss: 0.10585297118474177, gradient norm: 0.4734737511478032\n",
      "Iteration: 12643000, loss: 0.10585292338585771, gradient norm: 0.3781668732299832\n",
      "Iteration: 12644000, loss: 0.10585288507749731, gradient norm: 0.14382635621198084\n",
      "Iteration: 12645000, loss: 0.10585266009530211, gradient norm: 0.2383169782854524\n",
      "Iteration: 12646000, loss: 0.10585267454021459, gradient norm: 0.3077240457040861\n",
      "Iteration: 12647000, loss: 0.10585259428339795, gradient norm: 0.601577100137295\n",
      "Iteration: 12648000, loss: 0.10585240442370221, gradient norm: 0.09742008933839598\n",
      "Iteration: 12649000, loss: 0.10585227153602238, gradient norm: 1.2529337932290074\n",
      "Iteration: 12650000, loss: 0.10585225851106941, gradient norm: 0.7336184755979198\n",
      "Iteration: 12651000, loss: 0.10585223893911816, gradient norm: 0.08594168125418068\n",
      "Iteration: 12652000, loss: 0.10585193942443907, gradient norm: 0.09480054835146859\n",
      "Iteration: 12653000, loss: 0.105852138688138, gradient norm: 0.3013372939530261\n",
      "Iteration: 12654000, loss: 0.10585160054907035, gradient norm: 0.5442048554832638\n",
      "Iteration: 12655000, loss: 0.10585181968781557, gradient norm: 0.7734249306311548\n",
      "Iteration: 12656000, loss: 0.10585164600587886, gradient norm: 0.16720319019690824\n",
      "Iteration: 12657000, loss: 0.1058515219939612, gradient norm: 0.47194640795191933\n",
      "Iteration: 12658000, loss: 0.1058514522611032, gradient norm: 0.6523855289473977\n",
      "Iteration: 12659000, loss: 0.10585132312662578, gradient norm: 1.0071395025538588\n",
      "Iteration: 12660000, loss: 0.10585122268670835, gradient norm: 0.11556721517857475\n",
      "Iteration: 12661000, loss: 0.10585119007659037, gradient norm: 0.4596682477597587\n",
      "Iteration: 12662000, loss: 0.10585092418826704, gradient norm: 0.054057170956576424\n",
      "Iteration: 12663000, loss: 0.10585097415095496, gradient norm: 0.09169901983112837\n",
      "Iteration: 12664000, loss: 0.1058509439902812, gradient norm: 0.13106882525759073\n",
      "Iteration: 12665000, loss: 0.10585067164120865, gradient norm: 0.6276752142325152\n",
      "Iteration: 12666000, loss: 0.10585064370862798, gradient norm: 0.39862270866554056\n",
      "Iteration: 12667000, loss: 0.1058505756594861, gradient norm: 0.5117736467779985\n",
      "Iteration: 12668000, loss: 0.10585032049533893, gradient norm: 0.3156540125960578\n",
      "Iteration: 12669000, loss: 0.10585037720088653, gradient norm: 0.26308317272189424\n",
      "Iteration: 12670000, loss: 0.10585023934874004, gradient norm: 0.42762195322439395\n",
      "Iteration: 12671000, loss: 0.10585002003977269, gradient norm: 0.2118343041272385\n",
      "Iteration: 12672000, loss: 0.10585020452509107, gradient norm: 0.4863018009757242\n",
      "Iteration: 12673000, loss: 0.10584995119452201, gradient norm: 0.27716615638951814\n",
      "Iteration: 12674000, loss: 0.10584982312164785, gradient norm: 0.2430883928547554\n",
      "Iteration: 12675000, loss: 0.10584970059899837, gradient norm: 0.9671154545487799\n",
      "Iteration: 12676000, loss: 0.10584973876901796, gradient norm: 0.7256032455438364\n",
      "Iteration: 12677000, loss: 0.1058495055002719, gradient norm: 0.38013965051163956\n",
      "Iteration: 12678000, loss: 0.10584937508984357, gradient norm: 0.6420017419923022\n",
      "Iteration: 12679000, loss: 0.10584930626751705, gradient norm: 0.31711879962124034\n",
      "Iteration: 12680000, loss: 0.10584936436735186, gradient norm: 0.7959220708329442\n",
      "Iteration: 12681000, loss: 0.1058491322257787, gradient norm: 0.31972263138679935\n",
      "Iteration: 12682000, loss: 0.10584909094033554, gradient norm: 0.20442675809755978\n",
      "Iteration: 12683000, loss: 0.10584907373027835, gradient norm: 0.15641632369831904\n",
      "Iteration: 12684000, loss: 0.1058487147690419, gradient norm: 0.2212875926086724\n",
      "Iteration: 12685000, loss: 0.10584876808651204, gradient norm: 0.35320975358192136\n",
      "Iteration: 12686000, loss: 0.10584882812365387, gradient norm: 0.4024692082501131\n",
      "Iteration: 12687000, loss: 0.10584844743744821, gradient norm: 0.551073553297292\n",
      "Iteration: 12688000, loss: 0.10584851406878795, gradient norm: 0.37706366361642607\n",
      "Iteration: 12689000, loss: 0.10584851370116276, gradient norm: 0.33470773617092137\n",
      "Iteration: 12690000, loss: 0.10584805893482392, gradient norm: 0.12641866546254688\n",
      "Iteration: 12691000, loss: 0.10584824722372838, gradient norm: 0.49462450820783077\n",
      "Iteration: 12692000, loss: 0.1058480486285884, gradient norm: 0.4992459056861783\n",
      "Iteration: 12693000, loss: 0.10584785528016667, gradient norm: 0.1359970910559979\n",
      "Iteration: 12694000, loss: 0.1058478956022822, gradient norm: 1.0627547323391104\n",
      "Iteration: 12695000, loss: 0.10584777805263033, gradient norm: 0.3262349723111083\n",
      "Iteration: 12696000, loss: 0.1058476482234862, gradient norm: 0.937852840881871\n",
      "Iteration: 12697000, loss: 0.10584763896085589, gradient norm: 0.21747834468086266\n",
      "Iteration: 12698000, loss: 0.10584747560588936, gradient norm: 0.2754054956171076\n",
      "Iteration: 12699000, loss: 0.1058473031472307, gradient norm: 0.6688491283190596\n",
      "Iteration: 12700000, loss: 0.10584733887679221, gradient norm: 0.38588047170047773\n",
      "Iteration: 12701000, loss: 0.10584719270378778, gradient norm: 0.2673443914797106\n",
      "Iteration: 12702000, loss: 0.10584703456285421, gradient norm: 0.5474220571939622\n",
      "Iteration: 12703000, loss: 0.105846990118294, gradient norm: 0.6024146061123603\n",
      "Iteration: 12704000, loss: 0.10584689583547395, gradient norm: 0.07888221533254117\n",
      "Iteration: 12705000, loss: 0.10584669081686675, gradient norm: 0.25629634812905594\n",
      "Iteration: 12706000, loss: 0.10584686626388745, gradient norm: 0.23761862245647117\n",
      "Iteration: 12707000, loss: 0.10584642760898054, gradient norm: 0.5636436381434915\n",
      "Iteration: 12708000, loss: 0.1058464662090409, gradient norm: 0.1463137554971228\n",
      "Iteration: 12709000, loss: 0.10584640939407189, gradient norm: 0.3861843520285666\n",
      "Iteration: 12710000, loss: 0.10584630991364449, gradient norm: 0.4794045574261726\n",
      "Iteration: 12711000, loss: 0.10584622649129569, gradient norm: 0.6649496348974971\n",
      "Iteration: 12712000, loss: 0.10584598337495231, gradient norm: 0.5575874830742751\n",
      "Iteration: 12713000, loss: 0.10584601118860697, gradient norm: 0.32466806376487356\n",
      "Iteration: 12714000, loss: 0.1058458828522753, gradient norm: 0.786686787059891\n",
      "Iteration: 12715000, loss: 0.10584574542066207, gradient norm: 0.7713077337087828\n",
      "Iteration: 12716000, loss: 0.10584566549870662, gradient norm: 0.09223693331126025\n",
      "Iteration: 12717000, loss: 0.10584584666945267, gradient norm: 0.8724872174022715\n",
      "Iteration: 12718000, loss: 0.10584547679368997, gradient norm: 0.0791038388956221\n",
      "Iteration: 12719000, loss: 0.10584532649375956, gradient norm: 0.23943447862296455\n",
      "Iteration: 12720000, loss: 0.10584532386969688, gradient norm: 0.2694154528495047\n",
      "Iteration: 12721000, loss: 0.1058452724644905, gradient norm: 0.35712106710248387\n",
      "Iteration: 12722000, loss: 0.10584495600521932, gradient norm: 0.21430826166943723\n",
      "Iteration: 12723000, loss: 0.10584508235443506, gradient norm: 0.17479436912766189\n",
      "Iteration: 12724000, loss: 0.10584490742691602, gradient norm: 0.4587210392622191\n",
      "Iteration: 12725000, loss: 0.10584478341897022, gradient norm: 0.36607536796508555\n",
      "Iteration: 12726000, loss: 0.10584466362922745, gradient norm: 0.18071868450387424\n",
      "Iteration: 12727000, loss: 0.10584463637279577, gradient norm: 0.13355859429670955\n",
      "Iteration: 12728000, loss: 0.1058444780327242, gradient norm: 0.19910181114506928\n",
      "Iteration: 12729000, loss: 0.10584441473506791, gradient norm: 0.9103620058505585\n",
      "Iteration: 12730000, loss: 0.10584437758277966, gradient norm: 0.22322984678578445\n",
      "Iteration: 12731000, loss: 0.10584410838782676, gradient norm: 0.7523657633409888\n",
      "Iteration: 12732000, loss: 0.10584415672480205, gradient norm: 0.3576978469826713\n",
      "Iteration: 12733000, loss: 0.1058439850359668, gradient norm: 0.15656271174992192\n",
      "Iteration: 12734000, loss: 0.10584397084655639, gradient norm: 0.616222751340066\n",
      "Iteration: 12735000, loss: 0.10584388184854862, gradient norm: 0.3859266776364502\n",
      "Iteration: 12736000, loss: 0.10584363266707518, gradient norm: 0.6551791705830079\n",
      "Iteration: 12737000, loss: 0.10584357590377004, gradient norm: 0.6989608391716222\n",
      "Iteration: 12738000, loss: 0.105843618565663, gradient norm: 1.1185889632773065\n",
      "Iteration: 12739000, loss: 0.10584347939431774, gradient norm: 0.49326443606049714\n",
      "Iteration: 12740000, loss: 0.1058431731878967, gradient norm: 0.5664960289771066\n",
      "Iteration: 12741000, loss: 0.10584329461691629, gradient norm: 0.1188650018273416\n",
      "Iteration: 12742000, loss: 0.10584301459252322, gradient norm: 0.2588835380929368\n",
      "Iteration: 12743000, loss: 0.10584309927763254, gradient norm: 0.21875203167395726\n",
      "Iteration: 12744000, loss: 0.10584297642250164, gradient norm: 0.3557759614191178\n",
      "Iteration: 12745000, loss: 0.10584279058942452, gradient norm: 0.2705526831471072\n",
      "Iteration: 12746000, loss: 0.10584278706879013, gradient norm: 1.0564893416080778\n",
      "Iteration: 12747000, loss: 0.10584257768051028, gradient norm: 0.21046835765211452\n",
      "Iteration: 12748000, loss: 0.1058425682901212, gradient norm: 0.3035866760332495\n",
      "Iteration: 12749000, loss: 0.10584246415712267, gradient norm: 0.1960408991856939\n",
      "Iteration: 12750000, loss: 0.10584240015186269, gradient norm: 0.18619217143203415\n",
      "Iteration: 12751000, loss: 0.10584229273133329, gradient norm: 0.4979385696298369\n",
      "Iteration: 12752000, loss: 0.10584208233153887, gradient norm: 0.28603652589827283\n",
      "Iteration: 12753000, loss: 0.10584201858565279, gradient norm: 0.1662451730139083\n",
      "Iteration: 12754000, loss: 0.10584214805202691, gradient norm: 0.43062976945364456\n",
      "Iteration: 12755000, loss: 0.10584178572292778, gradient norm: 0.38644841733558927\n",
      "Iteration: 12756000, loss: 0.10584168932497577, gradient norm: 0.26142457479762315\n",
      "Iteration: 12757000, loss: 0.10584172429814012, gradient norm: 0.2912322477885726\n",
      "Iteration: 12758000, loss: 0.10584163095662229, gradient norm: 0.7057752038939423\n",
      "Iteration: 12759000, loss: 0.10584133704569618, gradient norm: 0.9262103230698457\n",
      "Iteration: 12760000, loss: 0.1058413742350969, gradient norm: 0.32996958031530416\n",
      "Iteration: 12761000, loss: 0.10584126796443229, gradient norm: 0.430479820909565\n",
      "Iteration: 12762000, loss: 0.10584137107413553, gradient norm: 0.593317885522258\n",
      "Iteration: 12763000, loss: 0.1058409869809941, gradient norm: 0.07026437337662797\n",
      "Iteration: 12764000, loss: 0.10584098226399948, gradient norm: 0.34822985524563627\n",
      "Iteration: 12765000, loss: 0.10584079211998809, gradient norm: 0.4951387579293415\n",
      "Iteration: 12766000, loss: 0.1058408594328767, gradient norm: 0.03071545629018515\n",
      "Iteration: 12767000, loss: 0.10584065917587157, gradient norm: 0.3183433469126623\n",
      "Iteration: 12768000, loss: 0.1058406961203533, gradient norm: 0.6180941133188391\n",
      "Iteration: 12769000, loss: 0.10584044434359492, gradient norm: 0.29323813419854583\n",
      "Iteration: 12770000, loss: 0.10584034571886883, gradient norm: 0.5038910963101337\n",
      "Iteration: 12771000, loss: 0.10584042068801602, gradient norm: 0.12534149783726922\n",
      "Iteration: 12772000, loss: 0.10584007634549789, gradient norm: 0.47040647126389495\n",
      "Iteration: 12773000, loss: 0.10584002133147057, gradient norm: 0.2437820571391135\n",
      "Iteration: 12774000, loss: 0.10584011807031835, gradient norm: 0.71776125179332\n",
      "Iteration: 12775000, loss: 0.1058399578076033, gradient norm: 0.31223586679534177\n",
      "Iteration: 12776000, loss: 0.10583971764459515, gradient norm: 0.17569192983072515\n",
      "Iteration: 12777000, loss: 0.105839816288039, gradient norm: 0.17531992393275822\n",
      "Iteration: 12778000, loss: 0.10583946073922167, gradient norm: 0.7885349213466719\n",
      "Iteration: 12779000, loss: 0.10583948959687288, gradient norm: 0.20460786828411878\n",
      "Iteration: 12780000, loss: 0.10583928321618447, gradient norm: 0.821971977386455\n",
      "Iteration: 12781000, loss: 0.1058395460038519, gradient norm: 0.9424228467004064\n",
      "Iteration: 12782000, loss: 0.10583906158159746, gradient norm: 0.14729343768386763\n",
      "Iteration: 12783000, loss: 0.10583916233684804, gradient norm: 0.15844064640322117\n",
      "Iteration: 12784000, loss: 0.10583905643028177, gradient norm: 0.3407219957154645\n",
      "Iteration: 12785000, loss: 0.10583900775164627, gradient norm: 0.41877888349407183\n",
      "Iteration: 12786000, loss: 0.10583872844549327, gradient norm: 0.4836283845464828\n",
      "Iteration: 12787000, loss: 0.10583881834195591, gradient norm: 0.6090635228832522\n",
      "Iteration: 12788000, loss: 0.10583870935319722, gradient norm: 0.5126973985922442\n",
      "Iteration: 12789000, loss: 0.10583843266044281, gradient norm: 0.610054931488109\n",
      "Iteration: 12790000, loss: 0.1058382762105041, gradient norm: 0.11689805913855517\n",
      "Iteration: 12791000, loss: 0.1058384482878948, gradient norm: 0.27627867336478457\n",
      "Iteration: 12792000, loss: 0.10583824023963177, gradient norm: 0.17556584017131824\n",
      "Iteration: 12793000, loss: 0.10583823183833559, gradient norm: 0.8385838979862658\n",
      "Iteration: 12794000, loss: 0.10583786467748958, gradient norm: 0.35242304289394855\n",
      "Iteration: 12795000, loss: 0.10583784987516483, gradient norm: 0.6995545231626751\n",
      "Iteration: 12796000, loss: 0.10583798952642923, gradient norm: 0.9336623113204185\n",
      "Iteration: 12797000, loss: 0.10583778676231037, gradient norm: 0.1653266404155331\n",
      "Iteration: 12798000, loss: 0.10583763171171333, gradient norm: 0.18481110150935115\n",
      "Iteration: 12799000, loss: 0.10583751789914655, gradient norm: 0.824596520821566\n",
      "Iteration: 12800000, loss: 0.1058373357421882, gradient norm: 0.7218620736933554\n",
      "Iteration: 12801000, loss: 0.10583738736258792, gradient norm: 0.32050809731118857\n",
      "Iteration: 12802000, loss: 0.10583723876333144, gradient norm: 0.5807739018773146\n",
      "Iteration: 12803000, loss: 0.10583725790640956, gradient norm: 0.11710478576022867\n",
      "Iteration: 12804000, loss: 0.1058371623128835, gradient norm: 0.04349007747026237\n",
      "Iteration: 12805000, loss: 0.10583679118121753, gradient norm: 0.7734890933944466\n",
      "Iteration: 12806000, loss: 0.10583692774379208, gradient norm: 0.1606332744555427\n",
      "Iteration: 12807000, loss: 0.1058366608663304, gradient norm: 0.7181922446751767\n",
      "Iteration: 12808000, loss: 0.1058367598673745, gradient norm: 0.2564477540404041\n",
      "Iteration: 12809000, loss: 0.10583652329056059, gradient norm: 0.5853227947628499\n",
      "Iteration: 12810000, loss: 0.10583644724545332, gradient norm: 0.18255005676844047\n",
      "Iteration: 12811000, loss: 0.10583645164504045, gradient norm: 0.28427655232738974\n",
      "Iteration: 12812000, loss: 0.1058362474250582, gradient norm: 0.09378906766183377\n",
      "Iteration: 12813000, loss: 0.10583626430565914, gradient norm: 0.1405669785656347\n",
      "Iteration: 12814000, loss: 0.10583595801374253, gradient norm: 0.49492003096452303\n",
      "Iteration: 12815000, loss: 0.10583597238635031, gradient norm: 0.10595179388190037\n",
      "Iteration: 12816000, loss: 0.10583605477069565, gradient norm: 0.4351707760523904\n",
      "Iteration: 12817000, loss: 0.10583563145073745, gradient norm: 0.4834000036238281\n",
      "Iteration: 12818000, loss: 0.1058356866588605, gradient norm: 0.45691208996076976\n",
      "Iteration: 12819000, loss: 0.10583566418174802, gradient norm: 0.36561236528169533\n",
      "Iteration: 12820000, loss: 0.10583552524913722, gradient norm: 0.3398951233008218\n",
      "Iteration: 12821000, loss: 0.10583529889267297, gradient norm: 0.09310191868015753\n",
      "Iteration: 12822000, loss: 0.10583533800207709, gradient norm: 0.5244323493009543\n",
      "Iteration: 12823000, loss: 0.10583515469917402, gradient norm: 0.28277431466819675\n",
      "Iteration: 12824000, loss: 0.10583508943334105, gradient norm: 0.2951393525755544\n",
      "Iteration: 12825000, loss: 0.10583522872148163, gradient norm: 1.4624502859466313\n",
      "Iteration: 12826000, loss: 0.10583488600536058, gradient norm: 0.10329486026870613\n",
      "Iteration: 12827000, loss: 0.10583477726828071, gradient norm: 0.34622553311928916\n",
      "Iteration: 12828000, loss: 0.10583470299865161, gradient norm: 0.6347095291980774\n",
      "Iteration: 12829000, loss: 0.10583464622829007, gradient norm: 0.4295563558485974\n",
      "Iteration: 12830000, loss: 0.10583452763100927, gradient norm: 0.3197687150054416\n",
      "Iteration: 12831000, loss: 0.10583442035092798, gradient norm: 0.4115043599075164\n",
      "Iteration: 12832000, loss: 0.10583421857976906, gradient norm: 0.4330816948498408\n",
      "Iteration: 12833000, loss: 0.10583421426775251, gradient norm: 0.293246884489247\n",
      "Iteration: 12834000, loss: 0.10583434342291467, gradient norm: 0.20639370934405102\n",
      "Iteration: 12835000, loss: 0.1058339568202043, gradient norm: 0.17357127563842473\n",
      "Iteration: 12836000, loss: 0.1058339089490328, gradient norm: 1.0173842687121766\n",
      "Iteration: 12837000, loss: 0.10583393663518315, gradient norm: 0.7237207397421345\n",
      "Iteration: 12838000, loss: 0.10583364331211291, gradient norm: 0.7262805465422746\n",
      "Iteration: 12839000, loss: 0.10583380139376737, gradient norm: 0.3433932706902748\n",
      "Iteration: 12840000, loss: 0.10583349645924486, gradient norm: 0.10941193255893236\n",
      "Iteration: 12841000, loss: 0.1058333955512845, gradient norm: 0.14086603089482663\n",
      "Iteration: 12842000, loss: 0.10583326529529316, gradient norm: 0.7464574406941484\n",
      "Iteration: 12843000, loss: 0.10583335509162983, gradient norm: 0.1486399807639307\n",
      "Iteration: 12844000, loss: 0.10583308399750084, gradient norm: 0.3844060414591828\n",
      "Iteration: 12845000, loss: 0.10583309173084075, gradient norm: 0.07548077554834227\n",
      "Iteration: 12846000, loss: 0.1058329414143246, gradient norm: 0.33773935217505585\n",
      "Iteration: 12847000, loss: 0.10583294664140651, gradient norm: 0.28394639504463526\n",
      "Iteration: 12848000, loss: 0.10583277439472889, gradient norm: 0.47182997037475155\n",
      "Iteration: 12849000, loss: 0.10583256055096874, gradient norm: 0.43903364081551294\n",
      "Iteration: 12850000, loss: 0.10583259033040925, gradient norm: 0.18604021165262208\n",
      "Iteration: 12851000, loss: 0.10583255199468249, gradient norm: 0.03721627466244813\n",
      "Iteration: 12852000, loss: 0.10583240328852424, gradient norm: 0.22254790342464795\n",
      "Iteration: 12853000, loss: 0.10583228095098468, gradient norm: 0.5303317728677477\n",
      "Iteration: 12854000, loss: 0.1058321617910032, gradient norm: 0.8083032715659904\n",
      "Iteration: 12855000, loss: 0.10583205950207232, gradient norm: 0.777413939466567\n",
      "Iteration: 12856000, loss: 0.10583201005475464, gradient norm: 0.6861805756366159\n",
      "Iteration: 12857000, loss: 0.10583202813983988, gradient norm: 0.7037267902851244\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "loss_vals, trace = train(student_model,\n",
    "                          torch_dataset_inputs,\n",
    "                         torch_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 0\n",
    "PATH = \"model.pt\"\n",
    "LOSS = 0.4\n",
    "\n",
    "torch.save({\n",
    "        'epoch': EPOCH,\n",
    "        'model_state_dict': student_model.state_dict(),\n",
    "        'loss': LOSS,\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.append(\n",
    "#     np.append(\n",
    "#         np.append(trace[-1][0].reshape(H_student * D_in),\n",
    "#                   trace[-1][1].reshape(H_student * D_in)),\n",
    "#         trace[-1][2].reshape(H_student * D_in)), \n",
    "#     trace[-1][3][0])\n",
    "# print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_layer1 = weights[0 : 100].reshape(H_student, D_in)\n",
    "# w_layer2 = weights[100 : 200].reshape(H_student, H_student)\n",
    "# w_layer3 = weights[200 : 300].reshape(H_student, H_student)\n",
    "# w_out = weights[300 : ].reshape(D_out, H_student)\n",
    "\n",
    "# print(jax_loss(weights), jnp.linalg.norm(jax_grad(jax_loss)(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_weights = second_order_opt(weights, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(jax_loss(final_weights), jnp.linalg.norm(jax_grad(jax_loss)(final_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
