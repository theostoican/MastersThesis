{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theostoican/MastersThesis/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cKo7_tWleLC"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Various modelling parameters"
      ],
      "metadata": {
        "id": "w8SjHkIMoI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#N is batch size; D_in is input dimension;\n",
        "#H is the dimension of the hidden layer; D_out is output dimension.\n",
        "N, D_in, H_teacher, H_student, D_out = 1, 2, 4, 5, 1\n",
        "num_experiments = 1000"
      ],
      "metadata": {
        "id": "F8ktizbMoKWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ptEq_k5KeK6"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7WSK5OS-_Jb",
        "outputId": "f70fbeef-b791-4d7d-f2ae-f67a7ce5e90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1681\n"
          ]
        }
      ],
      "source": [
        "def construct_dataset():\n",
        "  data = []\n",
        "  for y in np.arange(-5, 5.1, .25):\n",
        "    for x in np.arange(-5, 5.1, .25):\n",
        "      data.append([x, y])\n",
        "  return data\n",
        "\n",
        "data = torch.Tensor(construct_dataset()) \n",
        "print(len(construct_dataset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZEwEx3Kg9_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLlxqBcnzo2e"
      },
      "outputs": [],
      "source": [
        "class TeacherNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(TeacherNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "    self.linear1.weight = torch.nn.Parameter(torch.transpose(torch.Tensor([[0.6, -0.5, -0.2, 0.1], [0.5, 0.5, -0.6, -0.6]]), 0, 1))\n",
        "    self.linear2.weight = torch.nn.Parameter(torch.transpose(torch.Tensor([[1], [-1], [1], [-1]]), 0, 1))\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COkLdjEOmBJV"
      },
      "outputs": [],
      "source": [
        "class StudentNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(StudentNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "    nn.init.xavier_uniform_(self.linear1.weight)\n",
        "    nn.init.xavier_uniform_(self.linear2.weight)\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation of the labels based on the teacher model."
      ],
      "metadata": {
        "id": "Xh4A8WS3HzeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILSXsIRu3U18"
      },
      "outputs": [],
      "source": [
        "teacher_model = TeacherNetwork(D_in, H_teacher, D_out)\n",
        "y_labels = teacher_model(data).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZrYxLLwKk6Q"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS2Ri_ya9Npv"
      },
      "outputs": [],
      "source": [
        "def train(model, x, y_labels, N = 1000, Ninner = 10**3, Nstart = 10,\n",
        "          maxtime = 10 ** 3, nlopt_threshold = 1e-7,\n",
        "          collect_history = True):\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_vals = []\n",
        "  trace = []\n",
        "  if collect_history:\n",
        "    trace.append((copy.deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                  copy.deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "  for i in range(N):\n",
        "    loss_tmp = []\n",
        "    for j in range(Ninner):\n",
        "      y = model(x)\n",
        "      loss = loss_fn(y, y_labels)\n",
        "      loss_grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
        "      loss_tmp.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(retain_graph=True)\n",
        "      optimizer.step()\n",
        "      if i == 0 and (j % Nstart == 0) and j > 0:\n",
        "        loss_vals.append(np.mean(loss_tmp[j - Nstart : j]))\n",
        "        if collect_history:\n",
        "          trace.append((copy.deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                        copy.deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    loss_vals.append(np.mean(loss_tmp))\n",
        "    if collect_history:\n",
        "      trace.append((copy.deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                    copy.deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    # stopping criterion\n",
        "    cnt = 0\n",
        "    for g in loss_grad:\n",
        "        g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "        cnt = 1\n",
        "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i, np.mean(loss_tmp), torch.norm(g_vector)))\n",
        "    if torch.norm(g_vector) <= 2e-6:\n",
        "      break\n",
        "  return loss_vals, trace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hessian evaluation"
      ],
      "metadata": {
        "id": "HlZpml_4MOQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# eval Hessian matrix\n",
        "def eval_hessian(loss_grad, model):\n",
        "    cnt = 0\n",
        "    for g in loss_grad:\n",
        "        g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "        cnt = 1\n",
        "    grad_norm = torch.norm(g_vector)\n",
        "    l = g_vector.size(0)\n",
        "    hessian = torch.zeros(l, l)\n",
        "    for idx in range(l):\n",
        "        grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "        cnt = 0\n",
        "        for g in grad2rd: \n",
        "            g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "            cnt = 1\n",
        "        hessian[idx] = g2\n",
        "    hessian = (hessian + hessian.T) / 2\n",
        "    return grad_norm.cpu().data.numpy(), hessian.cpu().data.numpy()"
      ],
      "metadata": {
        "id": "mXEHxyklMQgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c-4dScUFCP-2",
        "outputId": "ea6b0f68-71ea-4921-a3bb-e0bb1b8b91da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 5000, loss: 0.9713216862678528, gradient norm: tensor(1.9700)\n",
            "Iteration: 6000, loss: 0.6166947178542614, gradient norm: tensor(1.5331)\n",
            "Iteration: 7000, loss: 0.35503180128335954, gradient norm: tensor(1.1229)\n",
            "Iteration: 8000, loss: 0.17708319865912198, gradient norm: tensor(0.7507)\n",
            "Iteration: 9000, loss: 0.07077028541639448, gradient norm: tensor(0.4335)\n",
            "Iteration: 10000, loss: 0.019779361163266004, gradient norm: tensor(0.1944)\n",
            "Iteration: 11000, loss: 0.0030475669621955605, gradient norm: tensor(0.0554)\n",
            "Iteration: 12000, loss: 0.00018881847929151264, gradient norm: tensor(0.0071)\n",
            "Iteration: 13000, loss: 2.071104802598711e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 14000, loss: 1.7513733353553106e-05, gradient norm: tensor(6.4096e-05)\n",
            "Iteration: 15000, loss: 1.6439860997707002e-05, gradient norm: tensor(6.4526e-05)\n",
            "Iteration: 16000, loss: 1.5796905536262784e-05, gradient norm: tensor(6.3936e-05)\n",
            "Iteration: 17000, loss: 1.5336408459006635e-05, gradient norm: tensor(6.1786e-05)\n",
            "Iteration: 18000, loss: 1.4890156414367084e-05, gradient norm: tensor(5.8047e-05)\n",
            "Iteration: 19000, loss: 1.4473856569566123e-05, gradient norm: tensor(5.3295e-05)\n",
            "Iteration: 20000, loss: 1.4158446870169428e-05, gradient norm: tensor(4.5561e-05)\n",
            "Iteration: 21000, loss: 1.3953550642327172e-05, gradient norm: tensor(4.0876e-05)\n",
            "Iteration: 22000, loss: 1.3788886712973181e-05, gradient norm: tensor(3.7122e-05)\n",
            "Iteration: 23000, loss: 1.3572322084655753e-05, gradient norm: tensor(3.7851e-05)\n",
            "Iteration: 24000, loss: 1.324096690677834e-05, gradient norm: tensor(3.4288e-05)\n",
            "Iteration: 25000, loss: 1.2738224181703118e-05, gradient norm: tensor(3.2699e-05)\n",
            "Iteration: 26000, loss: 1.196118235202448e-05, gradient norm: tensor(3.4293e-05)\n",
            "Iteration: 27000, loss: 1.0482028724254633e-05, gradient norm: tensor(4.7477e-05)\n",
            "Iteration: 28000, loss: 6.396794219199364e-06, gradient norm: tensor(2.8651e-05)\n",
            "Iteration: 29000, loss: 1.3818151966233927e-06, gradient norm: tensor(1.1139e-05)\n",
            "Iteration: 30000, loss: 5.415526490253342e-07, gradient norm: tensor(1.1226e-05)\n",
            "Iteration: 31000, loss: 3.355269150802087e-07, gradient norm: tensor(1.1674e-05)\n",
            "Iteration: 32000, loss: 2.381498531036641e-07, gradient norm: tensor(5.4215e-06)\n",
            "Iteration: 33000, loss: 1.9001585555145085e-07, gradient norm: tensor(1.1104e-05)\n",
            "Iteration: 34000, loss: 1.5874844385166397e-07, gradient norm: tensor(5.8766e-06)\n",
            "Iteration: 35000, loss: 1.3364384089697978e-07, gradient norm: tensor(2.0520e-05)\n",
            "Iteration: 36000, loss: 1.1185194976093271e-07, gradient norm: tensor(1.3469e-06)\n",
            "Iteration: 0, loss: 2.840218955039978, gradient norm: tensor(3.3978)\n",
            "Iteration: 1000, loss: 2.1094101991653442, gradient norm: tensor(2.8984)\n",
            "Iteration: 2000, loss: 1.5106900107860566, gradient norm: tensor(2.4375)\n",
            "Iteration: 3000, loss: 1.029558665037155, gradient norm: tensor(1.9904)\n",
            "Iteration: 4000, loss: 0.6571527675390244, gradient norm: tensor(1.5568)\n",
            "Iteration: 5000, loss: 0.3826367016732693, gradient norm: tensor(1.1470)\n",
            "Iteration: 6000, loss: 0.19483800016343594, gradient norm: tensor(0.7728)\n",
            "Iteration: 7000, loss: 0.0814259808845818, gradient norm: tensor(0.4511)\n",
            "Iteration: 8000, loss: 0.02576436398923397, gradient norm: tensor(0.2062)\n",
            "Iteration: 9000, loss: 0.0062504653390496965, gradient norm: tensor(0.0616)\n",
            "Iteration: 10000, loss: 0.0016230673724785447, gradient norm: tensor(0.0113)\n",
            "Iteration: 11000, loss: 0.00045529101812280716, gradient norm: tensor(0.0029)\n",
            "Iteration: 12000, loss: 0.0001868729175912449, gradient norm: tensor(0.0012)\n",
            "Iteration: 13000, loss: 0.00013070790636265884, gradient norm: tensor(0.0007)\n",
            "Iteration: 14000, loss: 0.00011179760487721068, gradient norm: tensor(0.0006)\n",
            "Iteration: 15000, loss: 0.00010371505643706768, gradient norm: tensor(0.0006)\n",
            "Iteration: 16000, loss: 9.623168905091006e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 8.758398627105635e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 18000, loss: 7.783371365076164e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 6.714767447556368e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 5.58559707897075e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 4.425232637004228e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 3.2920307845415666e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 2.287047710706247e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.5063803805787757e-05, gradient norm: tensor(7.8833e-05)\n",
            "Iteration: 25000, loss: 9.883298866043333e-06, gradient norm: tensor(6.6880e-05)\n",
            "Iteration: 26000, loss: 6.972692734507291e-06, gradient norm: tensor(2.5763e-05)\n",
            "Iteration: 27000, loss: 5.5137309018391534e-06, gradient norm: tensor(1.9495e-05)\n",
            "Iteration: 28000, loss: 4.676520801695005e-06, gradient norm: tensor(1.4809e-05)\n",
            "Iteration: 29000, loss: 4.001143345931268e-06, gradient norm: tensor(8.5277e-06)\n",
            "Iteration: 30000, loss: 3.4188123966032436e-06, gradient norm: tensor(6.2476e-06)\n",
            "Iteration: 31000, loss: 2.975998344254549e-06, gradient norm: tensor(5.1863e-06)\n",
            "Iteration: 32000, loss: 2.690085142603493e-06, gradient norm: tensor(1.3681e-05)\n",
            "Iteration: 33000, loss: 2.523009434753476e-06, gradient norm: tensor(7.0088e-06)\n",
            "Iteration: 34000, loss: 2.4241238622835224e-06, gradient norm: tensor(4.1742e-06)\n",
            "Iteration: 35000, loss: 2.362868505088045e-06, gradient norm: tensor(7.8407e-06)\n",
            "Iteration: 36000, loss: 2.322424253407007e-06, gradient norm: tensor(5.9731e-06)\n",
            "Iteration: 37000, loss: 2.2940153119179742e-06, gradient norm: tensor(2.2604e-05)\n",
            "Iteration: 38000, loss: 2.2730544280875618e-06, gradient norm: tensor(1.5216e-06)\n",
            "Iteration: 0, loss: 2.1051049565076827, gradient norm: tensor(2.8688)\n",
            "Iteration: 1000, loss: 1.4622156291007995, gradient norm: tensor(2.3589)\n",
            "Iteration: 2000, loss: 0.9712918413877487, gradient norm: tensor(1.9046)\n",
            "Iteration: 3000, loss: 0.6014981022179127, gradient norm: tensor(1.4675)\n",
            "Iteration: 4000, loss: 0.33924278219044207, gradient norm: tensor(1.0608)\n",
            "Iteration: 5000, loss: 0.16535585188865662, gradient norm: tensor(0.6947)\n",
            "Iteration: 6000, loss: 0.06436115608364344, gradient norm: tensor(0.3865)\n",
            "Iteration: 7000, loss: 0.017685103935655207, gradient norm: tensor(0.1621)\n",
            "Iteration: 8000, loss: 0.002959595858119428, gradient norm: tensor(0.0409)\n",
            "Iteration: 9000, loss: 0.00047626061757910065, gradient norm: tensor(0.0049)\n",
            "Iteration: 10000, loss: 0.00021851233679626603, gradient norm: tensor(0.0015)\n",
            "Iteration: 11000, loss: 0.00013910985479742522, gradient norm: tensor(0.0011)\n",
            "Iteration: 12000, loss: 9.701677157136146e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 13000, loss: 7.284755042928737e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 14000, loss: 5.7491077368467816e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 15000, loss: 5.088841504766606e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 4.823858293457306e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 4.5513677599956284e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 4.176175755856093e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 3.691332378730294e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 3.1185807172732896e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 2.5003871904118567e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.8855440775951137e-05, gradient norm: tensor(9.4435e-05)\n",
            "Iteration: 23000, loss: 1.3392240238317754e-05, gradient norm: tensor(6.4382e-05)\n",
            "Iteration: 24000, loss: 9.33694336163171e-06, gradient norm: tensor(4.5806e-05)\n",
            "Iteration: 25000, loss: 6.694551226246404e-06, gradient norm: tensor(2.6868e-05)\n",
            "Iteration: 26000, loss: 5.0888202149508285e-06, gradient norm: tensor(1.6330e-05)\n",
            "Iteration: 27000, loss: 4.180894106866617e-06, gradient norm: tensor(1.3064e-05)\n",
            "Iteration: 28000, loss: 3.702992966736929e-06, gradient norm: tensor(9.3212e-06)\n",
            "Iteration: 29000, loss: 3.4550157772628156e-06, gradient norm: tensor(4.3433e-06)\n",
            "Iteration: 30000, loss: 3.316853321621238e-06, gradient norm: tensor(5.7847e-06)\n",
            "Iteration: 31000, loss: 3.238968350160576e-06, gradient norm: tensor(3.1680e-06)\n",
            "Iteration: 32000, loss: 3.1932102469909297e-06, gradient norm: tensor(3.0330e-06)\n",
            "Iteration: 33000, loss: 3.164821539712648e-06, gradient norm: tensor(2.0882e-05)\n",
            "Iteration: 34000, loss: 3.146576527342404e-06, gradient norm: tensor(9.0867e-06)\n",
            "Iteration: 35000, loss: 3.1345492338914484e-06, gradient norm: tensor(6.8490e-06)\n",
            "Iteration: 36000, loss: 3.1264333354101835e-06, gradient norm: tensor(9.7689e-06)\n",
            "Iteration: 37000, loss: 3.1208832510856155e-06, gradient norm: tensor(3.3712e-06)\n",
            "Iteration: 38000, loss: 3.117002876024344e-06, gradient norm: tensor(2.5995e-05)\n",
            "Iteration: 39000, loss: 3.114238068292252e-06, gradient norm: tensor(1.3026e-05)\n",
            "Iteration: 40000, loss: 3.112237321374778e-06, gradient norm: tensor(4.7592e-05)\n",
            "Iteration: 41000, loss: 3.1107615643577446e-06, gradient norm: tensor(2.5086e-05)\n",
            "Iteration: 42000, loss: 3.109683482989567e-06, gradient norm: tensor(3.8862e-06)\n",
            "Iteration: 43000, loss: 3.108863533270778e-06, gradient norm: tensor(2.3372e-06)\n",
            "Iteration: 44000, loss: 3.108248460193863e-06, gradient norm: tensor(4.0991e-05)\n",
            "Iteration: 45000, loss: 3.1077806545454223e-06, gradient norm: tensor(5.8234e-05)\n",
            "Iteration: 46000, loss: 3.1074122096015342e-06, gradient norm: tensor(1.1125e-05)\n",
            "Iteration: 47000, loss: 3.107139722487773e-06, gradient norm: tensor(3.3769e-06)\n",
            "Iteration: 48000, loss: 3.10692519406075e-06, gradient norm: tensor(2.6718e-05)\n",
            "Iteration: 49000, loss: 3.106748919435631e-06, gradient norm: tensor(5.7635e-06)\n",
            "Iteration: 50000, loss: 3.1066240892414498e-06, gradient norm: tensor(3.5180e-06)\n",
            "Iteration: 51000, loss: 3.1065127639067213e-06, gradient norm: tensor(9.6347e-06)\n",
            "Iteration: 52000, loss: 3.1064261577284923e-06, gradient norm: tensor(6.7394e-06)\n",
            "Iteration: 53000, loss: 3.106362475136848e-06, gradient norm: tensor(1.0437e-05)\n",
            "Iteration: 54000, loss: 3.106312697127578e-06, gradient norm: tensor(4.0890e-06)\n",
            "Iteration: 55000, loss: 3.106257339140939e-06, gradient norm: tensor(2.0339e-05)\n",
            "Iteration: 56000, loss: 3.106227203261369e-06, gradient norm: tensor(3.3254e-05)\n",
            "Iteration: 57000, loss: 3.106189671825632e-06, gradient norm: tensor(2.1315e-05)\n",
            "Iteration: 58000, loss: 3.106175015091139e-06, gradient norm: tensor(7.2582e-06)\n",
            "Iteration: 59000, loss: 3.106155467321514e-06, gradient norm: tensor(2.8171e-05)\n",
            "Iteration: 60000, loss: 3.106129376192257e-06, gradient norm: tensor(1.4434e-05)\n",
            "Iteration: 61000, loss: 3.1061233223681484e-06, gradient norm: tensor(6.7168e-06)\n",
            "Iteration: 62000, loss: 3.106105189772279e-06, gradient norm: tensor(8.9526e-06)\n",
            "Iteration: 63000, loss: 3.1060935286859603e-06, gradient norm: tensor(7.7106e-06)\n",
            "Iteration: 64000, loss: 3.1060840940426713e-06, gradient norm: tensor(1.5701e-05)\n",
            "Iteration: 65000, loss: 3.106076140966252e-06, gradient norm: tensor(1.4841e-05)\n",
            "Iteration: 66000, loss: 3.1060644444096395e-06, gradient norm: tensor(1.5526e-05)\n",
            "Iteration: 67000, loss: 3.106063251834712e-06, gradient norm: tensor(1.5550e-05)\n",
            "Iteration: 68000, loss: 3.1060490678100907e-06, gradient norm: tensor(1.0448e-05)\n",
            "Iteration: 69000, loss: 3.1060449512096967e-06, gradient norm: tensor(7.9498e-06)\n",
            "Iteration: 70000, loss: 3.106041139972149e-06, gradient norm: tensor(5.6357e-06)\n",
            "Iteration: 71000, loss: 3.1060349879226124e-06, gradient norm: tensor(3.3238e-05)\n",
            "Iteration: 72000, loss: 3.1060339854320774e-06, gradient norm: tensor(8.6641e-06)\n",
            "Iteration: 73000, loss: 3.1060261419497694e-06, gradient norm: tensor(3.5012e-05)\n",
            "Iteration: 74000, loss: 3.106027182639082e-06, gradient norm: tensor(3.0326e-05)\n",
            "Iteration: 75000, loss: 3.106011582985957e-06, gradient norm: tensor(9.1529e-06)\n",
            "Iteration: 76000, loss: 3.1060124931627797e-06, gradient norm: tensor(2.8167e-06)\n",
            "Iteration: 77000, loss: 3.1060163687470776e-06, gradient norm: tensor(1.5532e-05)\n",
            "Iteration: 78000, loss: 3.10600057900956e-06, gradient norm: tensor(6.9172e-06)\n",
            "Iteration: 79000, loss: 3.106002335243829e-06, gradient norm: tensor(4.3685e-06)\n",
            "Iteration: 80000, loss: 3.1060053709097703e-06, gradient norm: tensor(3.4360e-05)\n",
            "Iteration: 81000, loss: 3.1059898831244938e-06, gradient norm: tensor(2.1042e-06)\n",
            "Iteration: 82000, loss: 3.1059906566497374e-06, gradient norm: tensor(1.5363e-05)\n",
            "Iteration: 83000, loss: 3.1059934958648226e-06, gradient norm: tensor(2.3579e-05)\n",
            "Iteration: 84000, loss: 3.1059826910677656e-06, gradient norm: tensor(8.5285e-06)\n",
            "Iteration: 85000, loss: 3.1059796585850564e-06, gradient norm: tensor(2.1522e-06)\n",
            "Iteration: 86000, loss: 3.1059785524121253e-06, gradient norm: tensor(5.0703e-06)\n",
            "Iteration: 87000, loss: 3.1059804935011926e-06, gradient norm: tensor(4.9046e-05)\n",
            "Iteration: 88000, loss: 3.105972193452544e-06, gradient norm: tensor(1.3084e-05)\n",
            "Iteration: 89000, loss: 3.105971219156345e-06, gradient norm: tensor(1.7670e-05)\n",
            "Iteration: 90000, loss: 3.1059718639880886e-06, gradient norm: tensor(6.9808e-06)\n",
            "Iteration: 91000, loss: 3.105962010749863e-06, gradient norm: tensor(1.3296e-05)\n",
            "Iteration: 92000, loss: 3.1059619775533065e-06, gradient norm: tensor(3.1262e-05)\n",
            "Iteration: 93000, loss: 3.105961553274028e-06, gradient norm: tensor(8.0336e-06)\n",
            "Iteration: 94000, loss: 3.105957679963467e-06, gradient norm: tensor(6.5103e-07)\n",
            "Iteration: 0, loss: 2.84819952583313, gradient norm: tensor(3.4422)\n",
            "Iteration: 1000, loss: 2.0552381665706636, gradient norm: tensor(2.8490)\n",
            "Iteration: 2000, loss: 1.463944549560547, gradient norm: tensor(2.3767)\n",
            "Iteration: 3000, loss: 0.9895987793207168, gradient norm: tensor(1.9393)\n",
            "Iteration: 4000, loss: 0.6220313042402268, gradient norm: tensor(1.5116)\n",
            "Iteration: 5000, loss: 0.35443177437782286, gradient norm: tensor(1.1040)\n",
            "Iteration: 6000, loss: 0.17515334343910216, gradient norm: tensor(0.7334)\n",
            "Iteration: 7000, loss: 0.06988186110183596, gradient norm: tensor(0.4184)\n",
            "Iteration: 8000, loss: 0.020413963849656283, gradient norm: tensor(0.1834)\n",
            "Iteration: 9000, loss: 0.0046293134232982995, gradient norm: tensor(0.0504)\n",
            "Iteration: 10000, loss: 0.0018470422131940723, gradient norm: tensor(0.0072)\n",
            "Iteration: 11000, loss: 0.0013267690748907626, gradient norm: tensor(0.0036)\n",
            "Iteration: 12000, loss: 0.0009267880843835883, gradient norm: tensor(0.0032)\n",
            "Iteration: 13000, loss: 0.0006281758920522407, gradient norm: tensor(0.0026)\n",
            "Iteration: 14000, loss: 0.00047593974872143005, gradient norm: tensor(0.0019)\n",
            "Iteration: 15000, loss: 0.00041837124392623084, gradient norm: tensor(0.0016)\n",
            "Iteration: 16000, loss: 0.000378395629406441, gradient norm: tensor(0.0015)\n",
            "Iteration: 17000, loss: 0.0003307595723308623, gradient norm: tensor(0.0013)\n",
            "Iteration: 18000, loss: 0.00027215229450666814, gradient norm: tensor(0.0011)\n",
            "Iteration: 19000, loss: 0.00020504959949175827, gradient norm: tensor(0.0009)\n",
            "Iteration: 20000, loss: 0.00013434487910126336, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 7.103405179805122e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 3.1374772983326694e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 6.241269241400005e-06, gradient norm: tensor(6.2410e-05)\n",
            "Iteration: 24000, loss: 4.4958288890484255e-07, gradient norm: tensor(6.0429e-06)\n",
            "Iteration: 25000, loss: 3.15913156583747e-07, gradient norm: tensor(4.3326e-06)\n",
            "Iteration: 26000, loss: 3.103471605072627e-07, gradient norm: tensor(4.7683e-06)\n",
            "Iteration: 27000, loss: 3.0336251185758554e-07, gradient norm: tensor(2.7680e-05)\n",
            "Iteration: 28000, loss: 2.920293952684005e-07, gradient norm: tensor(1.0041e-05)\n",
            "Iteration: 29000, loss: 2.7413950948584895e-07, gradient norm: tensor(3.3594e-05)\n",
            "Iteration: 30000, loss: 2.485545740427142e-07, gradient norm: tensor(6.2353e-06)\n",
            "Iteration: 31000, loss: 2.1881799098366628e-07, gradient norm: tensor(1.4610e-05)\n",
            "Iteration: 32000, loss: 1.9090135187127543e-07, gradient norm: tensor(1.5613e-05)\n",
            "Iteration: 33000, loss: 1.676398083816366e-07, gradient norm: tensor(1.5526e-05)\n",
            "Iteration: 34000, loss: 1.4932711293624834e-07, gradient norm: tensor(3.5181e-05)\n",
            "Iteration: 35000, loss: 1.3560473163920506e-07, gradient norm: tensor(1.4357e-05)\n",
            "Iteration: 36000, loss: 1.2582504950842123e-07, gradient norm: tensor(7.8772e-07)\n",
            "Iteration: 0, loss: 6.783554402828217, gradient norm: tensor(5.5765)\n",
            "Iteration: 1000, loss: 5.572606272697449, gradient norm: tensor(4.9819)\n",
            "Iteration: 2000, loss: 4.534748125553131, gradient norm: tensor(4.4588)\n",
            "Iteration: 3000, loss: 3.6213608858585355, gradient norm: tensor(3.9681)\n",
            "Iteration: 4000, loss: 2.8231661069393157, gradient norm: tensor(3.4832)\n",
            "Iteration: 5000, loss: 2.1376061993837356, gradient norm: tensor(3.0014)\n",
            "Iteration: 6000, loss: 1.5603529053926468, gradient norm: tensor(2.5286)\n",
            "Iteration: 7000, loss: 1.0851798350214958, gradient norm: tensor(2.0723)\n",
            "Iteration: 8000, loss: 0.7073311342000961, gradient norm: tensor(1.6335)\n",
            "Iteration: 9000, loss: 0.42280935680866244, gradient norm: tensor(1.2172)\n",
            "Iteration: 10000, loss: 0.22447716331481934, gradient norm: tensor(0.8358)\n",
            "Iteration: 11000, loss: 0.1004066846370697, gradient norm: tensor(0.5064)\n",
            "Iteration: 12000, loss: 0.03510885603912175, gradient norm: tensor(0.2481)\n",
            "Iteration: 13000, loss: 0.009623204893898219, gradient norm: tensor(0.0838)\n",
            "Iteration: 14000, loss: 0.0031441881705541164, gradient norm: tensor(0.0177)\n",
            "Iteration: 15000, loss: 0.0016993626449257136, gradient norm: tensor(0.0085)\n",
            "Iteration: 16000, loss: 0.0009694513798458502, gradient norm: tensor(0.0066)\n",
            "Iteration: 17000, loss: 0.0004799311481474433, gradient norm: tensor(0.0042)\n",
            "Iteration: 18000, loss: 0.00022322954160335938, gradient norm: tensor(0.0021)\n",
            "Iteration: 19000, loss: 0.0001260146244749194, gradient norm: tensor(0.0010)\n",
            "Iteration: 20000, loss: 8.702342943433905e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 21000, loss: 6.22458368743537e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 4.2513409767707346e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 2.787374781109975e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 1.8958916434712592e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.4069613659557944e-05, gradient norm: tensor(9.4460e-05)\n",
            "Iteration: 26000, loss: 1.1110663737781578e-05, gradient norm: tensor(6.4580e-05)\n",
            "Iteration: 27000, loss: 8.915174249523261e-06, gradient norm: tensor(6.4016e-05)\n",
            "Iteration: 28000, loss: 7.0327837356671805e-06, gradient norm: tensor(3.6624e-05)\n",
            "Iteration: 29000, loss: 5.389142195326713e-06, gradient norm: tensor(2.4841e-05)\n",
            "Iteration: 30000, loss: 4.029617298783706e-06, gradient norm: tensor(1.7738e-05)\n",
            "Iteration: 31000, loss: 2.9796949677347584e-06, gradient norm: tensor(1.4184e-05)\n",
            "Iteration: 32000, loss: 2.272002443760357e-06, gradient norm: tensor(1.8492e-05)\n",
            "Iteration: 33000, loss: 1.969618497923875e-06, gradient norm: tensor(2.4338e-06)\n",
            "Iteration: 34000, loss: 1.909963506591339e-06, gradient norm: tensor(1.5596e-05)\n",
            "Iteration: 35000, loss: 1.892038017331288e-06, gradient norm: tensor(1.0878e-05)\n",
            "Iteration: 36000, loss: 1.8788010335129002e-06, gradient norm: tensor(1.6425e-05)\n",
            "Iteration: 37000, loss: 1.8675902532550027e-06, gradient norm: tensor(1.0605e-06)\n",
            "Iteration: 0, loss: 5.826144880771637, gradient norm: tensor(5.1201)\n",
            "Iteration: 1000, loss: 4.720410532474518, gradient norm: tensor(4.5671)\n",
            "Iteration: 2000, loss: 3.7684798266887665, gradient norm: tensor(4.0579)\n",
            "Iteration: 3000, loss: 2.9443260781764984, gradient norm: tensor(3.5620)\n",
            "Iteration: 4000, loss: 2.2386892014741897, gradient norm: tensor(3.0748)\n",
            "Iteration: 5000, loss: 1.6437690625190735, gradient norm: tensor(2.6003)\n",
            "Iteration: 6000, loss: 1.1534929144978523, gradient norm: tensor(2.1417)\n",
            "Iteration: 7000, loss: 0.7621990711092949, gradient norm: tensor(1.7016)\n",
            "Iteration: 8000, loss: 0.4636470333337784, gradient norm: tensor(1.2850)\n",
            "Iteration: 9000, loss: 0.2510644593834877, gradient norm: tensor(0.9005)\n",
            "Iteration: 10000, loss: 0.11468778172135353, gradient norm: tensor(0.5607)\n",
            "Iteration: 11000, loss: 0.04084975093603134, gradient norm: tensor(0.2855)\n",
            "Iteration: 12000, loss: 0.011125048703048378, gradient norm: tensor(0.1018)\n",
            "Iteration: 13000, loss: 0.0038940072041004897, gradient norm: tensor(0.0194)\n",
            "Iteration: 14000, loss: 0.00302298412239179, gradient norm: tensor(0.0023)\n",
            "Iteration: 15000, loss: 0.002913103281520307, gradient norm: tensor(0.0017)\n",
            "Iteration: 16000, loss: 0.0028687042114324867, gradient norm: tensor(0.0016)\n",
            "Iteration: 17000, loss: 0.0028174193382728844, gradient norm: tensor(0.0014)\n",
            "Iteration: 18000, loss: 0.0027359603960067035, gradient norm: tensor(0.0013)\n",
            "Iteration: 19000, loss: 0.0026558312070555984, gradient norm: tensor(0.0012)\n",
            "Iteration: 20000, loss: 0.002585449041100219, gradient norm: tensor(0.0012)\n",
            "Iteration: 21000, loss: 0.0025182839054614307, gradient norm: tensor(0.0012)\n",
            "Iteration: 22000, loss: 0.002439946533180773, gradient norm: tensor(0.0013)\n",
            "Iteration: 23000, loss: 0.0023225591350346806, gradient norm: tensor(0.0017)\n",
            "Iteration: 24000, loss: 0.0020644353721290826, gradient norm: tensor(0.0061)\n",
            "Iteration: 25000, loss: 0.0010276154251769186, gradient norm: tensor(0.0036)\n",
            "Iteration: 26000, loss: 0.0005187004935287405, gradient norm: tensor(0.0023)\n",
            "Iteration: 27000, loss: 0.00025127498517394997, gradient norm: tensor(0.0016)\n",
            "Iteration: 28000, loss: 6.430088228444219e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 29000, loss: 1.683465640326176e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 30000, loss: 1.0333999466638488e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 31000, loss: 8.00002374307951e-06, gradient norm: tensor(8.1070e-05)\n",
            "Iteration: 32000, loss: 5.927473506744718e-06, gradient norm: tensor(5.8957e-05)\n",
            "Iteration: 33000, loss: 4.138999973974933e-06, gradient norm: tensor(4.1538e-05)\n",
            "Iteration: 34000, loss: 2.773224181510159e-06, gradient norm: tensor(2.7940e-05)\n",
            "Iteration: 35000, loss: 1.816032438455295e-06, gradient norm: tensor(1.9478e-05)\n",
            "Iteration: 36000, loss: 1.1622106700883705e-06, gradient norm: tensor(1.3181e-05)\n",
            "Iteration: 37000, loss: 6.885814312909133e-07, gradient norm: tensor(1.0197e-05)\n",
            "Iteration: 38000, loss: 3.426353549684791e-07, gradient norm: tensor(7.7857e-06)\n",
            "Iteration: 39000, loss: 1.542857535667963e-07, gradient norm: tensor(7.9202e-05)\n",
            "Iteration: 40000, loss: 7.099746687444508e-08, gradient norm: tensor(1.3372e-05)\n",
            "Iteration: 41000, loss: 3.307832517762677e-08, gradient norm: tensor(2.2158e-06)\n",
            "Iteration: 42000, loss: 1.5845030655547988e-08, gradient norm: tensor(2.6713e-05)\n",
            "Iteration: 43000, loss: 8.280459299125908e-09, gradient norm: tensor(2.3972e-05)\n",
            "Iteration: 44000, loss: 5.023783288926609e-09, gradient norm: tensor(1.5020e-06)\n",
            "Iteration: 0, loss: 1.4288389053344726, gradient norm: tensor(2.3400)\n",
            "Iteration: 1000, loss: 0.910197101354599, gradient norm: tensor(1.8452)\n",
            "Iteration: 2000, loss: 0.5461597716212273, gradient norm: tensor(1.3883)\n",
            "Iteration: 3000, loss: 0.2990663947314024, gradient norm: tensor(0.9825)\n",
            "Iteration: 4000, loss: 0.14015073794126512, gradient norm: tensor(0.6239)\n",
            "Iteration: 5000, loss: 0.051836881851777435, gradient norm: tensor(0.3293)\n",
            "Iteration: 6000, loss: 0.014279736445751041, gradient norm: tensor(0.1252)\n",
            "Iteration: 7000, loss: 0.004240641404176131, gradient norm: tensor(0.0269)\n",
            "Iteration: 8000, loss: 0.0027834291683975607, gradient norm: tensor(0.0055)\n",
            "Iteration: 9000, loss: 0.0023385793801862747, gradient norm: tensor(0.0050)\n",
            "Iteration: 10000, loss: 0.00182477311079856, gradient norm: tensor(0.0052)\n",
            "Iteration: 11000, loss: 0.0012199382063117809, gradient norm: tensor(0.0051)\n",
            "Iteration: 12000, loss: 0.000610874565289123, gradient norm: tensor(0.0037)\n",
            "Iteration: 13000, loss: 0.00020412483638210688, gradient norm: tensor(0.0018)\n",
            "Iteration: 14000, loss: 5.286474203785474e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 15000, loss: 1.8181217789788205e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 1.0007436912928824e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 17000, loss: 7.3240029687440255e-06, gradient norm: tensor(9.4785e-05)\n",
            "Iteration: 18000, loss: 5.890593762160279e-06, gradient norm: tensor(7.5525e-05)\n",
            "Iteration: 19000, loss: 4.534184789008577e-06, gradient norm: tensor(6.0738e-05)\n",
            "Iteration: 20000, loss: 2.9964234302042314e-06, gradient norm: tensor(4.3539e-05)\n",
            "Iteration: 21000, loss: 1.5434896824899624e-06, gradient norm: tensor(2.4276e-05)\n",
            "Iteration: 22000, loss: 6.271651882912011e-07, gradient norm: tensor(1.0583e-05)\n",
            "Iteration: 23000, loss: 2.655462910041706e-07, gradient norm: tensor(4.4169e-06)\n",
            "Iteration: 24000, loss: 1.5536607322985673e-07, gradient norm: tensor(2.7941e-06)\n",
            "Iteration: 25000, loss: 1.1216352081078185e-07, gradient norm: tensor(2.5745e-06)\n",
            "Iteration: 26000, loss: 8.442395258612123e-08, gradient norm: tensor(4.4788e-06)\n",
            "Iteration: 27000, loss: 6.490477938214667e-08, gradient norm: tensor(6.7453e-06)\n",
            "Iteration: 28000, loss: 5.3308729157208747e-08, gradient norm: tensor(4.9457e-06)\n",
            "Iteration: 29000, loss: 4.752168722887973e-08, gradient norm: tensor(2.7046e-05)\n",
            "Iteration: 30000, loss: 4.448694107139772e-08, gradient norm: tensor(2.4529e-06)\n",
            "Iteration: 31000, loss: 4.260352818974411e-08, gradient norm: tensor(1.9132e-05)\n",
            "Iteration: 32000, loss: 4.116179583846247e-08, gradient norm: tensor(3.9876e-06)\n",
            "Iteration: 33000, loss: 3.995514216725837e-08, gradient norm: tensor(7.1772e-05)\n",
            "Iteration: 34000, loss: 3.885120792901375e-08, gradient norm: tensor(2.7671e-05)\n",
            "Iteration: 35000, loss: 3.780948294007658e-08, gradient norm: tensor(2.4875e-06)\n",
            "Iteration: 36000, loss: 3.681348732342826e-08, gradient norm: tensor(8.0984e-07)\n",
            "Iteration: 0, loss: 7.413078990936279, gradient norm: tensor(5.8152)\n",
            "Iteration: 1000, loss: 6.152329176425934, gradient norm: tensor(5.2698)\n",
            "Iteration: 2000, loss: 5.049810002803802, gradient norm: tensor(4.7466)\n",
            "Iteration: 3000, loss: 4.0793031477928166, gradient norm: tensor(4.2422)\n",
            "Iteration: 4000, loss: 3.2259831981658937, gradient norm: tensor(3.7516)\n",
            "Iteration: 5000, loss: 2.482412457704544, gradient norm: tensor(3.2672)\n",
            "Iteration: 6000, loss: 1.8468505786657334, gradient norm: tensor(2.7894)\n",
            "Iteration: 7000, loss: 1.31690311896801, gradient norm: tensor(2.3226)\n",
            "Iteration: 8000, loss: 0.8885244998931885, gradient norm: tensor(1.8716)\n",
            "Iteration: 9000, loss: 0.5564459017515182, gradient norm: tensor(1.4418)\n",
            "Iteration: 10000, loss: 0.3139724928587675, gradient norm: tensor(1.0405)\n",
            "Iteration: 11000, loss: 0.1522035382464528, gradient norm: tensor(0.6797)\n",
            "Iteration: 12000, loss: 0.0585924874637276, gradient norm: tensor(0.3765)\n",
            "Iteration: 13000, loss: 0.01622891985718161, gradient norm: tensor(0.1568)\n",
            "Iteration: 14000, loss: 0.0032099506345111876, gradient norm: tensor(0.0399)\n",
            "Iteration: 15000, loss: 0.0006798720713122748, gradient norm: tensor(0.0052)\n",
            "Iteration: 16000, loss: 0.00036887236073380336, gradient norm: tensor(0.0025)\n",
            "Iteration: 17000, loss: 0.00029080712507129644, gradient norm: tensor(0.0020)\n",
            "Iteration: 18000, loss: 0.00024265838992141653, gradient norm: tensor(0.0016)\n",
            "Iteration: 19000, loss: 0.00020987161183438728, gradient norm: tensor(0.0013)\n",
            "Iteration: 20000, loss: 0.00018835026743181516, gradient norm: tensor(0.0010)\n",
            "Iteration: 21000, loss: 0.00017353269581508356, gradient norm: tensor(0.0008)\n",
            "Iteration: 22000, loss: 0.00015997934997722042, gradient norm: tensor(0.0006)\n",
            "Iteration: 23000, loss: 0.00014442796056391672, gradient norm: tensor(0.0005)\n",
            "Iteration: 24000, loss: 0.00012632402961753543, gradient norm: tensor(0.0004)\n",
            "Iteration: 25000, loss: 0.000106242400877818, gradient norm: tensor(0.0004)\n",
            "Iteration: 26000, loss: 8.56225543320761e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 27000, loss: 6.628852846188238e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 4.965939765315852e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 29000, loss: 3.6364216386573386e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 2.6304899303795536e-05, gradient norm: tensor(8.1392e-05)\n",
            "Iteration: 31000, loss: 1.8871898077122752e-05, gradient norm: tensor(5.9861e-05)\n",
            "Iteration: 32000, loss: 1.2534535872873675e-05, gradient norm: tensor(5.4096e-05)\n",
            "Iteration: 33000, loss: 4.341348047091742e-06, gradient norm: tensor(2.7597e-05)\n",
            "Iteration: 34000, loss: 5.827604566945866e-07, gradient norm: tensor(2.5221e-05)\n",
            "Iteration: 35000, loss: 1.9102764220235713e-07, gradient norm: tensor(7.0586e-06)\n",
            "Iteration: 36000, loss: 1.2235812906880029e-07, gradient norm: tensor(6.2920e-06)\n",
            "Iteration: 37000, loss: 1.0033282463695059e-07, gradient norm: tensor(1.0106e-05)\n",
            "Iteration: 38000, loss: 9.1581671014751e-08, gradient norm: tensor(2.4599e-05)\n",
            "Iteration: 39000, loss: 8.686278694369776e-08, gradient norm: tensor(5.2904e-06)\n",
            "Iteration: 40000, loss: 8.354067569626977e-08, gradient norm: tensor(3.8711e-05)\n",
            "Iteration: 41000, loss: 8.081100052237388e-08, gradient norm: tensor(4.6148e-05)\n",
            "Iteration: 42000, loss: 7.837088968898343e-08, gradient norm: tensor(1.1432e-05)\n",
            "Iteration: 43000, loss: 7.612135181034319e-08, gradient norm: tensor(5.9849e-06)\n",
            "Iteration: 44000, loss: 7.398520677526221e-08, gradient norm: tensor(1.9143e-06)\n",
            "Iteration: 0, loss: 2.4780359649658203, gradient norm: tensor(3.1982)\n",
            "Iteration: 1000, loss: 1.7599305036067963, gradient norm: tensor(2.6925)\n",
            "Iteration: 2000, loss: 1.1956621084809302, gradient norm: tensor(2.1885)\n",
            "Iteration: 3000, loss: 0.7742896460294724, gradient norm: tensor(1.7161)\n",
            "Iteration: 4000, loss: 0.46670308661460874, gradient norm: tensor(1.2909)\n",
            "Iteration: 5000, loss: 0.24974125945568085, gradient norm: tensor(0.9021)\n",
            "Iteration: 6000, loss: 0.11187887156754732, gradient norm: tensor(0.5584)\n",
            "Iteration: 7000, loss: 0.03863600947149098, gradient norm: tensor(0.2823)\n",
            "Iteration: 8000, loss: 0.009770655518397689, gradient norm: tensor(0.0998)\n",
            "Iteration: 9000, loss: 0.002752336074016057, gradient norm: tensor(0.0192)\n",
            "Iteration: 10000, loss: 0.001536247177515179, gradient norm: tensor(0.0045)\n",
            "Iteration: 11000, loss: 0.0009047786095179618, gradient norm: tensor(0.0034)\n",
            "Iteration: 12000, loss: 0.00046116319653810936, gradient norm: tensor(0.0023)\n",
            "Iteration: 13000, loss: 0.0002548029370373115, gradient norm: tensor(0.0016)\n",
            "Iteration: 14000, loss: 0.00016166449957381702, gradient norm: tensor(0.0012)\n",
            "Iteration: 15000, loss: 7.899820569946314e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 16000, loss: 3.992661034499179e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 3.1724836904686526e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 2.5726367661263793e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 2.0353712201540476e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.624247678955726e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.298929952645267e-05, gradient norm: tensor(8.1209e-05)\n",
            "Iteration: 22000, loss: 1.0543162448811927e-05, gradient norm: tensor(5.9522e-05)\n",
            "Iteration: 23000, loss: 8.449170333733492e-06, gradient norm: tensor(4.6826e-05)\n",
            "Iteration: 24000, loss: 6.131985604042711e-06, gradient norm: tensor(3.7942e-05)\n",
            "Iteration: 25000, loss: 3.7700078917168866e-06, gradient norm: tensor(2.2879e-05)\n",
            "Iteration: 26000, loss: 1.8767006838515953e-06, gradient norm: tensor(1.1572e-05)\n",
            "Iteration: 27000, loss: 7.388339062117666e-07, gradient norm: tensor(6.5817e-06)\n",
            "Iteration: 28000, loss: 3.355078024753766e-07, gradient norm: tensor(2.0708e-06)\n",
            "Iteration: 29000, loss: 2.517307155187609e-07, gradient norm: tensor(8.8474e-06)\n",
            "Iteration: 30000, loss: 2.3818805100006558e-07, gradient norm: tensor(4.5031e-06)\n",
            "Iteration: 31000, loss: 2.265698744565725e-07, gradient norm: tensor(1.3782e-06)\n",
            "Iteration: 0, loss: 7.876211236000061, gradient norm: tensor(6.0159)\n",
            "Iteration: 1000, loss: 6.41583881187439, gradient norm: tensor(5.3991)\n",
            "Iteration: 2000, loss: 5.195477326393127, gradient norm: tensor(4.8427)\n",
            "Iteration: 3000, loss: 4.164771374225617, gradient norm: tensor(4.3129)\n",
            "Iteration: 4000, loss: 3.2850535662174223, gradient norm: tensor(3.8005)\n",
            "Iteration: 5000, loss: 2.532046544790268, gradient norm: tensor(3.3036)\n",
            "Iteration: 6000, loss: 1.892584291100502, gradient norm: tensor(2.8217)\n",
            "Iteration: 7000, loss: 1.3585076603889465, gradient norm: tensor(2.3550)\n",
            "Iteration: 8000, loss: 0.9244718382954598, gradient norm: tensor(1.9042)\n",
            "Iteration: 9000, loss: 0.585621774405241, gradient norm: tensor(1.4741)\n",
            "Iteration: 10000, loss: 0.3355436466038227, gradient norm: tensor(1.0717)\n",
            "Iteration: 11000, loss: 0.16630562248080968, gradient norm: tensor(0.7073)\n",
            "Iteration: 12000, loss: 0.06658975587040186, gradient norm: tensor(0.3988)\n",
            "Iteration: 13000, loss: 0.019577524648979307, gradient norm: tensor(0.1713)\n",
            "Iteration: 14000, loss: 0.004324789097998292, gradient norm: tensor(0.0452)\n",
            "Iteration: 15000, loss: 0.0012902575225452893, gradient norm: tensor(0.0065)\n",
            "Iteration: 16000, loss: 0.0006042060236504767, gradient norm: tensor(0.0029)\n",
            "Iteration: 17000, loss: 0.00029604744745302013, gradient norm: tensor(0.0016)\n",
            "Iteration: 18000, loss: 0.00018080960970837622, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 0.00015443313236755785, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 0.00014986245545151177, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 0.0001467614845460048, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 0.00014289575176371728, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 0.0001381010369223077, gradient norm: tensor(0.0003)\n",
            "Iteration: 24000, loss: 0.00013200210203649477, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 0.00012401603603939292, gradient norm: tensor(0.0003)\n",
            "Iteration: 26000, loss: 0.00011379060464969371, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 0.00010160838572483045, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 8.831860222562682e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 29000, loss: 7.4993804199039e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 6.259995304208133e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 31000, loss: 5.169256456065341e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 32000, loss: 4.239608963325736e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 33000, loss: 3.459709443632164e-05, gradient norm: tensor(7.6525e-05)\n",
            "Iteration: 34000, loss: 2.8106475236199912e-05, gradient norm: tensor(6.4718e-05)\n",
            "Iteration: 35000, loss: 2.2735182155884104e-05, gradient norm: tensor(5.2443e-05)\n",
            "Iteration: 36000, loss: 1.8316022695216815e-05, gradient norm: tensor(4.3281e-05)\n",
            "Iteration: 37000, loss: 1.4705736606629216e-05, gradient norm: tensor(3.5694e-05)\n",
            "Iteration: 38000, loss: 1.177911167997081e-05, gradient norm: tensor(2.8813e-05)\n",
            "Iteration: 39000, loss: 9.43333521081513e-06, gradient norm: tensor(2.7235e-05)\n",
            "Iteration: 40000, loss: 7.582187816751684e-06, gradient norm: tensor(1.9025e-05)\n",
            "Iteration: 41000, loss: 6.142297771930316e-06, gradient norm: tensor(1.9810e-05)\n",
            "Iteration: 42000, loss: 4.667527200581389e-06, gradient norm: tensor(1.3584e-05)\n",
            "Iteration: 43000, loss: 3.4117540960778568e-06, gradient norm: tensor(1.5794e-05)\n",
            "Iteration: 44000, loss: 2.545549812793979e-06, gradient norm: tensor(4.0284e-05)\n",
            "Iteration: 45000, loss: 1.8907818955540278e-06, gradient norm: tensor(7.6938e-06)\n",
            "Iteration: 46000, loss: 1.3979016534904077e-06, gradient norm: tensor(5.7317e-06)\n",
            "Iteration: 47000, loss: 1.0324105819563557e-06, gradient norm: tensor(1.8933e-05)\n",
            "Iteration: 48000, loss: 7.650884683698678e-07, gradient norm: tensor(4.5590e-06)\n",
            "Iteration: 49000, loss: 5.710112383212617e-07, gradient norm: tensor(3.4570e-06)\n",
            "Iteration: 50000, loss: 4.3094954193634294e-07, gradient norm: tensor(1.4140e-05)\n",
            "Iteration: 51000, loss: 3.2887132931591625e-07, gradient norm: tensor(1.0389e-05)\n",
            "Iteration: 52000, loss: 2.5375652329273634e-07, gradient norm: tensor(1.1199e-05)\n",
            "Iteration: 53000, loss: 1.9822908063815702e-07, gradient norm: tensor(3.8906e-06)\n",
            "Iteration: 54000, loss: 1.5645236035766174e-07, gradient norm: tensor(3.8908e-05)\n",
            "Iteration: 55000, loss: 1.2457361032147675e-07, gradient norm: tensor(4.2228e-06)\n",
            "Iteration: 56000, loss: 1.0006338425228023e-07, gradient norm: tensor(4.0657e-06)\n",
            "Iteration: 57000, loss: 8.112545611993483e-08, gradient norm: tensor(6.4378e-05)\n",
            "Iteration: 58000, loss: 6.626357988182007e-08, gradient norm: tensor(2.5150e-06)\n",
            "Iteration: 59000, loss: 5.4563347525515835e-08, gradient norm: tensor(3.2926e-05)\n",
            "Iteration: 60000, loss: 4.5296367961356056e-08, gradient norm: tensor(3.8583e-05)\n",
            "Iteration: 61000, loss: 3.7910347725045315e-08, gradient norm: tensor(2.6595e-05)\n",
            "Iteration: 62000, loss: 3.198441020835219e-08, gradient norm: tensor(9.3476e-07)\n",
            "Iteration: 0, loss: 0.805637600183487, gradient norm: tensor(1.6263)\n",
            "Iteration: 1000, loss: 0.368667539447546, gradient norm: tensor(1.0729)\n",
            "Iteration: 2000, loss: 0.14247645432502032, gradient norm: tensor(0.6137)\n",
            "Iteration: 3000, loss: 0.050799176931381224, gradient norm: tensor(0.3069)\n",
            "Iteration: 4000, loss: 0.017232613444328308, gradient norm: tensor(0.1121)\n",
            "Iteration: 5000, loss: 0.008728302827570587, gradient norm: tensor(0.0236)\n",
            "Iteration: 6000, loss: 0.007471492308191955, gradient norm: tensor(0.0072)\n",
            "Iteration: 7000, loss: 0.007115586716216058, gradient norm: tensor(0.0075)\n",
            "Iteration: 8000, loss: 0.00660828230343759, gradient norm: tensor(0.0101)\n",
            "Iteration: 9000, loss: 0.005424874734133482, gradient norm: tensor(0.0127)\n",
            "Iteration: 10000, loss: 0.0034085230103228243, gradient norm: tensor(0.0115)\n",
            "Iteration: 11000, loss: 0.0014885125746950507, gradient norm: tensor(0.0074)\n",
            "Iteration: 12000, loss: 0.00045628989623219243, gradient norm: tensor(0.0034)\n",
            "Iteration: 13000, loss: 0.00011772642617142992, gradient norm: tensor(0.0011)\n",
            "Iteration: 14000, loss: 4.4164232251205246e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 15000, loss: 3.1639535438444e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 16000, loss: 2.938934223857359e-05, gradient norm: tensor(8.5362e-05)\n",
            "Iteration: 17000, loss: 2.755314716887369e-05, gradient norm: tensor(8.0753e-05)\n",
            "Iteration: 18000, loss: 2.5027807876540465e-05, gradient norm: tensor(7.6498e-05)\n",
            "Iteration: 19000, loss: 2.1626505887979876e-05, gradient norm: tensor(7.1434e-05)\n",
            "Iteration: 20000, loss: 1.7256768244806152e-05, gradient norm: tensor(6.3706e-05)\n",
            "Iteration: 21000, loss: 1.2284085119063094e-05, gradient norm: tensor(4.9914e-05)\n",
            "Iteration: 22000, loss: 7.944870532355708e-06, gradient norm: tensor(3.1416e-05)\n",
            "Iteration: 23000, loss: 5.299234252561291e-06, gradient norm: tensor(1.8243e-05)\n",
            "Iteration: 24000, loss: 4.020935366497725e-06, gradient norm: tensor(1.0998e-05)\n",
            "Iteration: 25000, loss: 3.2961353219889133e-06, gradient norm: tensor(6.6394e-05)\n",
            "Iteration: 26000, loss: 2.8080634704110707e-06, gradient norm: tensor(1.0659e-05)\n",
            "Iteration: 27000, loss: 2.5212587922851527e-06, gradient norm: tensor(1.2673e-05)\n",
            "Iteration: 28000, loss: 2.373336134951387e-06, gradient norm: tensor(3.1232e-06)\n",
            "Iteration: 29000, loss: 2.2974701494149484e-06, gradient norm: tensor(4.6841e-05)\n",
            "Iteration: 30000, loss: 2.256300633234787e-06, gradient norm: tensor(1.1347e-05)\n",
            "Iteration: 31000, loss: 2.2319820543543756e-06, gradient norm: tensor(3.6020e-06)\n",
            "Iteration: 32000, loss: 2.2161582085118426e-06, gradient norm: tensor(6.7530e-05)\n",
            "Iteration: 33000, loss: 2.204962547921241e-06, gradient norm: tensor(1.3550e-05)\n",
            "Iteration: 34000, loss: 2.1965044463740924e-06, gradient norm: tensor(7.2906e-06)\n",
            "Iteration: 35000, loss: 2.189769383448947e-06, gradient norm: tensor(1.8006e-06)\n",
            "Iteration: 0, loss: 2.6328428897857665, gradient norm: tensor(3.1537)\n",
            "Iteration: 1000, loss: 1.9214795106649398, gradient norm: tensor(2.6811)\n",
            "Iteration: 2000, loss: 1.3441328266859054, gradient norm: tensor(2.2541)\n",
            "Iteration: 3000, loss: 0.8858640119433403, gradient norm: tensor(1.8204)\n",
            "Iteration: 4000, loss: 0.543594799041748, gradient norm: tensor(1.3956)\n",
            "Iteration: 5000, loss: 0.3026035172641277, gradient norm: tensor(0.9955)\n",
            "Iteration: 6000, loss: 0.14640204918384553, gradient norm: tensor(0.6379)\n",
            "Iteration: 7000, loss: 0.05875763066858053, gradient norm: tensor(0.3436)\n",
            "Iteration: 8000, loss: 0.019772888477891683, gradient norm: tensor(0.1378)\n",
            "Iteration: 9000, loss: 0.007135429451242089, gradient norm: tensor(0.0352)\n",
            "Iteration: 10000, loss: 0.00388745541498065, gradient norm: tensor(0.0116)\n",
            "Iteration: 11000, loss: 0.002620968934381381, gradient norm: tensor(0.0078)\n",
            "Iteration: 12000, loss: 0.0018939886188600213, gradient norm: tensor(0.0052)\n",
            "Iteration: 13000, loss: 0.001478371519013308, gradient norm: tensor(0.0037)\n",
            "Iteration: 14000, loss: 0.0012182554138125852, gradient norm: tensor(0.0034)\n",
            "Iteration: 15000, loss: 0.0009799583865096793, gradient norm: tensor(0.0037)\n",
            "Iteration: 16000, loss: 0.0006809695994015783, gradient norm: tensor(0.0034)\n",
            "Iteration: 17000, loss: 0.00037673768188687973, gradient norm: tensor(0.0023)\n",
            "Iteration: 18000, loss: 0.00019416988619195763, gradient norm: tensor(0.0011)\n",
            "Iteration: 19000, loss: 0.00013273045780078975, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 0.00010926388533698627, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 8.66811735322699e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 6.424730024082237e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 4.539062016192474e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 3.1920554913085655e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 2.3368235866655597e-05, gradient norm: tensor(7.1838e-05)\n",
            "Iteration: 26000, loss: 1.795235913596116e-05, gradient norm: tensor(4.7543e-05)\n",
            "Iteration: 27000, loss: 1.4124620501206664e-05, gradient norm: tensor(3.4284e-05)\n",
            "Iteration: 28000, loss: 1.1167703258252004e-05, gradient norm: tensor(2.6410e-05)\n",
            "Iteration: 29000, loss: 8.843248385346669e-06, gradient norm: tensor(3.0927e-05)\n",
            "Iteration: 30000, loss: 7.041147655399982e-06, gradient norm: tensor(1.7344e-05)\n",
            "Iteration: 31000, loss: 5.428370328445453e-06, gradient norm: tensor(1.6042e-05)\n",
            "Iteration: 32000, loss: 4.005127922482643e-06, gradient norm: tensor(1.2783e-05)\n",
            "Iteration: 33000, loss: 2.991537616480855e-06, gradient norm: tensor(1.0519e-05)\n",
            "Iteration: 34000, loss: 2.2303213693248837e-06, gradient norm: tensor(1.0248e-05)\n",
            "Iteration: 35000, loss: 1.6570002526350435e-06, gradient norm: tensor(6.5950e-06)\n",
            "Iteration: 36000, loss: 1.2286940417425284e-06, gradient norm: tensor(7.2028e-06)\n",
            "Iteration: 37000, loss: 9.145378483594868e-07, gradient norm: tensor(4.5451e-05)\n",
            "Iteration: 38000, loss: 6.861199818217756e-07, gradient norm: tensor(4.4846e-06)\n",
            "Iteration: 39000, loss: 5.213723781878343e-07, gradient norm: tensor(6.3626e-06)\n",
            "Iteration: 40000, loss: 4.023792656653313e-07, gradient norm: tensor(5.3201e-06)\n",
            "Iteration: 41000, loss: 3.1524498643875633e-07, gradient norm: tensor(2.4248e-06)\n",
            "Iteration: 42000, loss: 2.501806271055784e-07, gradient norm: tensor(1.3181e-05)\n",
            "Iteration: 43000, loss: 2.015369858128224e-07, gradient norm: tensor(5.7098e-05)\n",
            "Iteration: 44000, loss: 1.64541153793607e-07, gradient norm: tensor(1.8233e-06)\n",
            "Iteration: 0, loss: 7.698305583000183, gradient norm: tensor(5.8695)\n",
            "Iteration: 1000, loss: 6.393770805358887, gradient norm: tensor(5.2712)\n",
            "Iteration: 2000, loss: 5.264108806133271, gradient norm: tensor(4.7077)\n",
            "Iteration: 3000, loss: 4.271900501966477, gradient norm: tensor(4.1642)\n",
            "Iteration: 4000, loss: 3.4020833916664124, gradient norm: tensor(3.6895)\n",
            "Iteration: 5000, loss: 2.631017384290695, gradient norm: tensor(3.2613)\n",
            "Iteration: 6000, loss: 1.9607612233161926, gradient norm: tensor(2.8225)\n",
            "Iteration: 7000, loss: 1.402237038731575, gradient norm: tensor(2.3744)\n",
            "Iteration: 8000, loss: 0.9523045026063919, gradient norm: tensor(1.9284)\n",
            "Iteration: 9000, loss: 0.6032023466527462, gradient norm: tensor(1.4974)\n",
            "Iteration: 10000, loss: 0.3457041413336992, gradient norm: tensor(1.0931)\n",
            "Iteration: 11000, loss: 0.17058607812970877, gradient norm: tensor(0.7257)\n",
            "Iteration: 12000, loss: 0.06698232264071703, gradient norm: tensor(0.4126)\n",
            "Iteration: 13000, loss: 0.018194483292754738, gradient norm: tensor(0.1792)\n",
            "Iteration: 14000, loss: 0.002724894491082523, gradient norm: tensor(0.0479)\n",
            "Iteration: 15000, loss: 0.0002249572533328319, gradient norm: tensor(0.0054)\n",
            "Iteration: 16000, loss: 7.368965869682143e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 6.0696118449413915e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 5.035154510915163e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 4.0333181819733e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 3.142522413509141e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 2.4441030642265105e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.9675718946018604e-05, gradient norm: tensor(8.4273e-05)\n",
            "Iteration: 23000, loss: 1.677584548269806e-05, gradient norm: tensor(6.6072e-05)\n",
            "Iteration: 24000, loss: 1.5111034055735218e-05, gradient norm: tensor(5.6728e-05)\n",
            "Iteration: 25000, loss: 1.413215308366489e-05, gradient norm: tensor(4.9161e-05)\n",
            "Iteration: 26000, loss: 1.3467011586726585e-05, gradient norm: tensor(4.3039e-05)\n",
            "Iteration: 27000, loss: 1.2877143721198082e-05, gradient norm: tensor(3.7741e-05)\n",
            "Iteration: 28000, loss: 1.2216712022564025e-05, gradient norm: tensor(3.3427e-05)\n",
            "Iteration: 29000, loss: 1.1428157697991992e-05, gradient norm: tensor(2.8251e-05)\n",
            "Iteration: 30000, loss: 1.0507199335734186e-05, gradient norm: tensor(2.4338e-05)\n",
            "Iteration: 31000, loss: 9.44514296406851e-06, gradient norm: tensor(2.0926e-05)\n",
            "Iteration: 32000, loss: 8.244728187492e-06, gradient norm: tensor(1.9205e-05)\n",
            "Iteration: 33000, loss: 6.985300879478018e-06, gradient norm: tensor(1.9088e-05)\n",
            "Iteration: 34000, loss: 5.8104691620428635e-06, gradient norm: tensor(1.1403e-05)\n",
            "Iteration: 35000, loss: 4.841149005187617e-06, gradient norm: tensor(8.8462e-06)\n",
            "Iteration: 36000, loss: 4.131642546781222e-06, gradient norm: tensor(6.1468e-06)\n",
            "Iteration: 37000, loss: 3.674877976436619e-06, gradient norm: tensor(6.7444e-06)\n",
            "Iteration: 38000, loss: 3.4155183009261234e-06, gradient norm: tensor(3.5376e-05)\n",
            "Iteration: 39000, loss: 3.2613354819659435e-06, gradient norm: tensor(4.1010e-06)\n",
            "Iteration: 40000, loss: 3.0507085466524586e-06, gradient norm: tensor(3.9058e-06)\n",
            "Iteration: 41000, loss: 2.821833152893305e-06, gradient norm: tensor(2.5271e-05)\n",
            "Iteration: 42000, loss: 2.6721947954229107e-06, gradient norm: tensor(2.7270e-06)\n",
            "Iteration: 43000, loss: 2.5685602818157347e-06, gradient norm: tensor(5.8148e-05)\n",
            "Iteration: 44000, loss: 2.4897926227822608e-06, gradient norm: tensor(1.2021e-05)\n",
            "Iteration: 45000, loss: 2.4253940305243306e-06, gradient norm: tensor(6.9809e-06)\n",
            "Iteration: 46000, loss: 2.368160358628302e-06, gradient norm: tensor(3.2455e-06)\n",
            "Iteration: 47000, loss: 2.311958626705746e-06, gradient norm: tensor(8.6059e-06)\n",
            "Iteration: 48000, loss: 2.249173469181187e-06, gradient norm: tensor(1.0025e-05)\n",
            "Iteration: 49000, loss: 2.1751547392341307e-06, gradient norm: tensor(3.2694e-05)\n",
            "Iteration: 50000, loss: 2.1100081992244666e-06, gradient norm: tensor(4.7344e-06)\n",
            "Iteration: 51000, loss: 2.071034172558939e-06, gradient norm: tensor(6.0613e-06)\n",
            "Iteration: 52000, loss: 2.0444387378120154e-06, gradient norm: tensor(8.3070e-06)\n",
            "Iteration: 53000, loss: 2.022815600184913e-06, gradient norm: tensor(3.9368e-05)\n",
            "Iteration: 54000, loss: 2.0043133260969627e-06, gradient norm: tensor(6.7000e-06)\n",
            "Iteration: 55000, loss: 1.9880152933637873e-06, gradient norm: tensor(3.1299e-05)\n",
            "Iteration: 56000, loss: 1.9733109020307894e-06, gradient norm: tensor(2.3414e-06)\n",
            "Iteration: 57000, loss: 1.959883863264622e-06, gradient norm: tensor(1.4741e-05)\n",
            "Iteration: 58000, loss: 1.9474864980111308e-06, gradient norm: tensor(4.9088e-06)\n",
            "Iteration: 59000, loss: 1.9360288761163246e-06, gradient norm: tensor(5.3997e-06)\n",
            "Iteration: 60000, loss: 1.925332438759142e-06, gradient norm: tensor(6.1776e-06)\n",
            "Iteration: 61000, loss: 1.91536925467517e-06, gradient norm: tensor(1.9173e-06)\n",
            "Iteration: 0, loss: 3.498402309179306, gradient norm: tensor(4.0029)\n",
            "Iteration: 1000, loss: 2.5555703637599945, gradient norm: tensor(3.3550)\n",
            "Iteration: 2000, loss: 1.8588838374614716, gradient norm: tensor(2.7926)\n",
            "Iteration: 3000, loss: 1.3225079865455627, gradient norm: tensor(2.3080)\n",
            "Iteration: 4000, loss: 0.8938111048340798, gradient norm: tensor(1.8628)\n",
            "Iteration: 5000, loss: 0.5601715268790722, gradient norm: tensor(1.4351)\n",
            "Iteration: 6000, loss: 0.31696530440449716, gradient norm: tensor(1.0332)\n",
            "Iteration: 7000, loss: 0.15557118482142687, gradient norm: tensor(0.6714)\n",
            "Iteration: 8000, loss: 0.06278745893761516, gradient norm: tensor(0.3702)\n",
            "Iteration: 9000, loss: 0.019440101275220514, gradient norm: tensor(0.1541)\n",
            "Iteration: 10000, loss: 0.004591137364506721, gradient norm: tensor(0.0395)\n",
            "Iteration: 11000, loss: 0.001282740113849286, gradient norm: tensor(0.0076)\n",
            "Iteration: 12000, loss: 0.0005756336171471048, gradient norm: tensor(0.0038)\n",
            "Iteration: 13000, loss: 0.0003105609212798299, gradient norm: tensor(0.0024)\n",
            "Iteration: 14000, loss: 0.00016755989787634463, gradient norm: tensor(0.0015)\n",
            "Iteration: 15000, loss: 9.3290526077908e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 16000, loss: 6.499840358810616e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 5.71377899905201e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 18000, loss: 5.282211277881288e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 19000, loss: 4.723354770248989e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 3.9585519221873255e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 2.9535227757151007e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 1.7116126765358786e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 5.978945112019574e-06, gradient norm: tensor(5.0803e-05)\n",
            "Iteration: 24000, loss: 3.5037729346640845e-06, gradient norm: tensor(3.3814e-05)\n",
            "Iteration: 25000, loss: 2.5871648422253202e-06, gradient norm: tensor(2.0914e-05)\n",
            "Iteration: 26000, loss: 1.8786661848935182e-06, gradient norm: tensor(1.6140e-05)\n",
            "Iteration: 27000, loss: 1.4900419339483052e-06, gradient norm: tensor(7.8467e-05)\n",
            "Iteration: 28000, loss: 1.3600120186083586e-06, gradient norm: tensor(1.1523e-05)\n",
            "Iteration: 29000, loss: 1.3034267406055732e-06, gradient norm: tensor(4.6990e-06)\n",
            "Iteration: 30000, loss: 1.255321724102032e-06, gradient norm: tensor(3.2921e-06)\n",
            "Iteration: 31000, loss: 1.2016700342201149e-06, gradient norm: tensor(2.0449e-05)\n",
            "Iteration: 32000, loss: 1.1484772812764277e-06, gradient norm: tensor(1.8010e-05)\n",
            "Iteration: 33000, loss: 1.099137581945797e-06, gradient norm: tensor(1.1606e-05)\n",
            "Iteration: 34000, loss: 1.0535551047041736e-06, gradient norm: tensor(1.9631e-06)\n",
            "Iteration: 0, loss: 4.026511906385422, gradient norm: tensor(4.2078)\n",
            "Iteration: 1000, loss: 3.029503653526306, gradient norm: tensor(3.6420)\n",
            "Iteration: 2000, loss: 2.2508457008600233, gradient norm: tensor(3.1027)\n",
            "Iteration: 3000, loss: 1.6378181565999985, gradient norm: tensor(2.6064)\n",
            "Iteration: 4000, loss: 1.1442315286397935, gradient norm: tensor(2.1419)\n",
            "Iteration: 5000, loss: 0.7526630690097809, gradient norm: tensor(1.6969)\n",
            "Iteration: 6000, loss: 0.4565123092532158, gradient norm: tensor(1.2757)\n",
            "Iteration: 7000, loss: 0.24760284923017026, gradient norm: tensor(0.8879)\n",
            "Iteration: 8000, loss: 0.11442339679598808, gradient norm: tensor(0.5487)\n",
            "Iteration: 9000, loss: 0.04136349404789507, gradient norm: tensor(0.2787)\n",
            "Iteration: 10000, loss: 0.010533281770534813, gradient norm: tensor(0.0994)\n",
            "Iteration: 11000, loss: 0.0021896897745318712, gradient norm: tensor(0.0199)\n",
            "Iteration: 12000, loss: 0.0008055846754577942, gradient norm: tensor(0.0042)\n",
            "Iteration: 13000, loss: 0.0005352013352385256, gradient norm: tensor(0.0019)\n",
            "Iteration: 14000, loss: 0.00045560923212906347, gradient norm: tensor(0.0012)\n",
            "Iteration: 15000, loss: 0.00042495253580273126, gradient norm: tensor(0.0011)\n",
            "Iteration: 16000, loss: 0.00040335113939363507, gradient norm: tensor(0.0011)\n",
            "Iteration: 17000, loss: 0.00038391575988498515, gradient norm: tensor(0.0010)\n",
            "Iteration: 18000, loss: 0.00036284266426810064, gradient norm: tensor(0.0010)\n",
            "Iteration: 19000, loss: 0.00033501237467862665, gradient norm: tensor(0.0009)\n",
            "Iteration: 20000, loss: 0.00029605560234631414, gradient norm: tensor(0.0009)\n",
            "Iteration: 21000, loss: 0.000242036193900276, gradient norm: tensor(0.0008)\n",
            "Iteration: 22000, loss: 0.00017018159668077715, gradient norm: tensor(0.0007)\n",
            "Iteration: 23000, loss: 8.944842697746935e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 24000, loss: 3.822665349616727e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.5952701530750347e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 2.807743359539927e-06, gradient norm: tensor(2.2677e-05)\n",
            "Iteration: 27000, loss: 1.527801963220554e-06, gradient norm: tensor(1.5901e-05)\n",
            "Iteration: 28000, loss: 1.2866767081050057e-06, gradient norm: tensor(1.5441e-05)\n",
            "Iteration: 29000, loss: 1.1425153778645835e-06, gradient norm: tensor(1.5408e-05)\n",
            "Iteration: 30000, loss: 1.034178122836238e-06, gradient norm: tensor(1.7086e-05)\n",
            "Iteration: 31000, loss: 8.996058996899592e-07, gradient norm: tensor(4.6518e-06)\n",
            "Iteration: 32000, loss: 7.135287260098267e-07, gradient norm: tensor(3.8696e-06)\n",
            "Iteration: 33000, loss: 5.083161704533268e-07, gradient norm: tensor(5.1724e-06)\n",
            "Iteration: 34000, loss: 3.4916187260591867e-07, gradient norm: tensor(1.7864e-05)\n",
            "Iteration: 35000, loss: 2.587034871339711e-07, gradient norm: tensor(6.6816e-06)\n",
            "Iteration: 36000, loss: 2.1575119026806532e-07, gradient norm: tensor(4.2918e-06)\n",
            "Iteration: 37000, loss: 1.943698413100492e-07, gradient norm: tensor(1.2709e-05)\n",
            "Iteration: 38000, loss: 1.8071661150997898e-07, gradient norm: tensor(1.1008e-05)\n",
            "Iteration: 39000, loss: 1.699180386367516e-07, gradient norm: tensor(3.6970e-06)\n",
            "Iteration: 40000, loss: 1.6055578787188552e-07, gradient norm: tensor(1.6981e-06)\n",
            "Iteration: 0, loss: 4.398268371582032, gradient norm: tensor(4.4325)\n",
            "Iteration: 1000, loss: 3.3396117866039274, gradient norm: tensor(3.8490)\n",
            "Iteration: 2000, loss: 2.4485782763957977, gradient norm: tensor(3.2341)\n",
            "Iteration: 3000, loss: 1.7495220605134965, gradient norm: tensor(2.6666)\n",
            "Iteration: 4000, loss: 1.2166034358739852, gradient norm: tensor(2.1770)\n",
            "Iteration: 5000, loss: 0.8107966052889823, gradient norm: tensor(1.7264)\n",
            "Iteration: 6000, loss: 0.5048686159253121, gradient norm: tensor(1.3119)\n",
            "Iteration: 7000, loss: 0.2834675078392029, gradient norm: tensor(0.9268)\n",
            "Iteration: 8000, loss: 0.13896167363226414, gradient norm: tensor(0.5832)\n",
            "Iteration: 9000, loss: 0.05877088015154004, gradient norm: tensor(0.3052)\n",
            "Iteration: 10000, loss: 0.023204011844471097, gradient norm: tensor(0.1197)\n",
            "Iteration: 11000, loss: 0.010202601280063391, gradient norm: tensor(0.0350)\n",
            "Iteration: 12000, loss: 0.00524096058239229, gradient norm: tensor(0.0172)\n",
            "Iteration: 13000, loss: 0.002820572573458776, gradient norm: tensor(0.0111)\n",
            "Iteration: 14000, loss: 0.001642573615303263, gradient norm: tensor(0.0065)\n",
            "Iteration: 15000, loss: 0.0011004551167134195, gradient norm: tensor(0.0035)\n",
            "Iteration: 16000, loss: 0.0008224712145747617, gradient norm: tensor(0.0024)\n",
            "Iteration: 17000, loss: 0.0006068395592155867, gradient norm: tensor(0.0024)\n",
            "Iteration: 18000, loss: 0.0003848672686726786, gradient norm: tensor(0.0020)\n",
            "Iteration: 19000, loss: 0.00020623866582172924, gradient norm: tensor(0.0013)\n",
            "Iteration: 20000, loss: 0.0001059093927469803, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 6.569509407563601e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 5.2022700918314516e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 4.405911135472707e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 3.572298543076613e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 2.749796868192789e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 2.038758119306294e-05, gradient norm: tensor(7.7035e-05)\n",
            "Iteration: 27000, loss: 1.4831330131528376e-05, gradient norm: tensor(5.1773e-05)\n",
            "Iteration: 28000, loss: 1.0727380599746539e-05, gradient norm: tensor(6.1374e-05)\n",
            "Iteration: 29000, loss: 7.647475045359898e-06, gradient norm: tensor(2.3659e-05)\n",
            "Iteration: 30000, loss: 5.0205340280626845e-06, gradient norm: tensor(8.0211e-05)\n",
            "Iteration: 31000, loss: 3.2559773696902995e-06, gradient norm: tensor(1.2689e-05)\n",
            "Iteration: 32000, loss: 1.8719055533438223e-06, gradient norm: tensor(1.0478e-05)\n",
            "Iteration: 33000, loss: 7.276923601295948e-07, gradient norm: tensor(7.7302e-06)\n",
            "Iteration: 34000, loss: 2.9904613589337717e-07, gradient norm: tensor(3.1231e-06)\n",
            "Iteration: 35000, loss: 1.6619348420476854e-07, gradient norm: tensor(7.1898e-06)\n",
            "Iteration: 36000, loss: 1.0655353509747556e-07, gradient norm: tensor(3.5760e-05)\n",
            "Iteration: 37000, loss: 7.479065369864202e-08, gradient norm: tensor(9.2528e-05)\n",
            "Iteration: 38000, loss: 5.880195122642817e-08, gradient norm: tensor(1.2351e-06)\n",
            "Iteration: 0, loss: 3.1274954154491423, gradient norm: tensor(3.5881)\n",
            "Iteration: 1000, loss: 2.3259825958013534, gradient norm: tensor(3.0924)\n",
            "Iteration: 2000, loss: 1.6748882167339325, gradient norm: tensor(2.6184)\n",
            "Iteration: 3000, loss: 1.1541730188131332, gradient norm: tensor(2.1531)\n",
            "Iteration: 4000, loss: 0.7510072251558304, gradient norm: tensor(1.7032)\n",
            "Iteration: 5000, loss: 0.45044488698244095, gradient norm: tensor(1.2793)\n",
            "Iteration: 6000, loss: 0.23956686928868293, gradient norm: tensor(0.8900)\n",
            "Iteration: 7000, loss: 0.10629526475444435, gradient norm: tensor(0.5487)\n",
            "Iteration: 8000, loss: 0.03562621316313744, gradient norm: tensor(0.2762)\n",
            "Iteration: 9000, loss: 0.007862568525830284, gradient norm: tensor(0.0970)\n",
            "Iteration: 10000, loss: 0.0012784067316679284, gradient norm: tensor(0.0180)\n",
            "Iteration: 11000, loss: 0.0004326792125066277, gradient norm: tensor(0.0024)\n",
            "Iteration: 12000, loss: 0.00019100180341774831, gradient norm: tensor(0.0013)\n",
            "Iteration: 13000, loss: 5.4740411233069605e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 14000, loss: 3.000714757035894e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 15000, loss: 2.897246966495004e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 16000, loss: 2.8711298837151846e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 17000, loss: 2.8310244233580305e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 2.7709084235539193e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 2.68567889197584e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 2.571572431770619e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 2.4220390112532185e-05, gradient norm: tensor(9.6265e-05)\n",
            "Iteration: 22000, loss: 2.2268034845183136e-05, gradient norm: tensor(8.4139e-05)\n",
            "Iteration: 23000, loss: 1.983854834725207e-05, gradient norm: tensor(7.1169e-05)\n",
            "Iteration: 24000, loss: 1.693205714400392e-05, gradient norm: tensor(6.2077e-05)\n",
            "Iteration: 25000, loss: 1.3511322347767419e-05, gradient norm: tensor(5.3030e-05)\n",
            "Iteration: 26000, loss: 9.687495787147782e-06, gradient norm: tensor(4.3062e-05)\n",
            "Iteration: 27000, loss: 6.013979827002913e-06, gradient norm: tensor(3.2879e-05)\n",
            "Iteration: 28000, loss: 3.2937303765265826e-06, gradient norm: tensor(1.9340e-05)\n",
            "Iteration: 29000, loss: 1.716500836209889e-06, gradient norm: tensor(3.9592e-05)\n",
            "Iteration: 30000, loss: 9.189899905095444e-07, gradient norm: tensor(1.9855e-05)\n",
            "Iteration: 31000, loss: 5.460290310281834e-07, gradient norm: tensor(2.5724e-05)\n",
            "Iteration: 32000, loss: 3.86488442359223e-07, gradient norm: tensor(4.0627e-05)\n",
            "Iteration: 33000, loss: 3.192879786126923e-07, gradient norm: tensor(1.8208e-06)\n",
            "Iteration: 0, loss: 4.312649813175201, gradient norm: tensor(3.8689)\n",
            "Iteration: 1000, loss: 3.3721797201633454, gradient norm: tensor(3.3600)\n",
            "Iteration: 2000, loss: 2.5766406807899473, gradient norm: tensor(2.9237)\n",
            "Iteration: 3000, loss: 1.9052946940660476, gradient norm: tensor(2.4937)\n",
            "Iteration: 4000, loss: 1.3589644533395768, gradient norm: tensor(2.0745)\n",
            "Iteration: 5000, loss: 0.922165435552597, gradient norm: tensor(1.6824)\n",
            "Iteration: 6000, loss: 0.5810713689029217, gradient norm: tensor(1.3145)\n",
            "Iteration: 7000, loss: 0.3268131344765425, gradient norm: tensor(0.9544)\n",
            "Iteration: 8000, loss: 0.15621788915246726, gradient norm: tensor(0.6120)\n",
            "Iteration: 9000, loss: 0.06129946583881974, gradient norm: tensor(0.3232)\n",
            "Iteration: 10000, loss: 0.020180669935420156, gradient norm: tensor(0.1224)\n",
            "Iteration: 11000, loss: 0.008000097952317447, gradient norm: tensor(0.0290)\n",
            "Iteration: 12000, loss: 0.005368610543198884, gradient norm: tensor(0.0121)\n",
            "Iteration: 13000, loss: 0.004348568486515433, gradient norm: tensor(0.0106)\n",
            "Iteration: 14000, loss: 0.003414763202192262, gradient norm: tensor(0.0119)\n",
            "Iteration: 15000, loss: 0.002131727443658747, gradient norm: tensor(0.0111)\n",
            "Iteration: 16000, loss: 0.0009496264502522535, gradient norm: tensor(0.0071)\n",
            "Iteration: 17000, loss: 0.0003553840657987166, gradient norm: tensor(0.0031)\n",
            "Iteration: 18000, loss: 0.00017544554681808223, gradient norm: tensor(0.0010)\n",
            "Iteration: 19000, loss: 0.00013749939067929517, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 0.0001191249070034246, gradient norm: tensor(0.0005)\n",
            "Iteration: 21000, loss: 9.896504673815798e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 7.744764978269814e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 5.6938583049486623e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 3.9552225214720235e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 2.642464405289502e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 1.7567300209520908e-05, gradient norm: tensor(7.3900e-05)\n",
            "Iteration: 27000, loss: 1.2146168282924918e-05, gradient norm: tensor(4.5384e-05)\n",
            "Iteration: 28000, loss: 9.014780179313676e-06, gradient norm: tensor(2.8454e-05)\n",
            "Iteration: 29000, loss: 7.19082498244461e-06, gradient norm: tensor(1.8414e-05)\n",
            "Iteration: 30000, loss: 6.019104263032205e-06, gradient norm: tensor(1.3102e-05)\n",
            "Iteration: 31000, loss: 5.136665150985209e-06, gradient norm: tensor(9.8934e-06)\n",
            "Iteration: 32000, loss: 4.42212578445833e-06, gradient norm: tensor(3.7094e-05)\n",
            "Iteration: 33000, loss: 3.886765704464779e-06, gradient norm: tensor(2.7969e-05)\n",
            "Iteration: 34000, loss: 3.543360295907405e-06, gradient norm: tensor(1.3639e-05)\n",
            "Iteration: 35000, loss: 3.3506853326343846e-06, gradient norm: tensor(9.0225e-06)\n",
            "Iteration: 36000, loss: 3.2467121320678416e-06, gradient norm: tensor(8.1683e-06)\n",
            "Iteration: 37000, loss: 3.1627303314962773e-06, gradient norm: tensor(3.2263e-06)\n",
            "Iteration: 38000, loss: 2.9636445726737294e-06, gradient norm: tensor(3.5087e-05)\n",
            "Iteration: 39000, loss: 2.741579130770333e-06, gradient norm: tensor(4.2109e-06)\n",
            "Iteration: 40000, loss: 2.6239200715281187e-06, gradient norm: tensor(2.4957e-05)\n",
            "Iteration: 41000, loss: 2.555750954115865e-06, gradient norm: tensor(6.9214e-06)\n",
            "Iteration: 42000, loss: 2.5088156221499957e-06, gradient norm: tensor(6.2631e-06)\n",
            "Iteration: 43000, loss: 2.4736258701523185e-06, gradient norm: tensor(3.5628e-06)\n",
            "Iteration: 44000, loss: 2.445446975343657e-06, gradient norm: tensor(1.1940e-05)\n",
            "Iteration: 45000, loss: 2.4214599645802083e-06, gradient norm: tensor(2.4964e-06)\n",
            "Iteration: 46000, loss: 2.3998118588224316e-06, gradient norm: tensor(5.0087e-06)\n",
            "Iteration: 47000, loss: 2.3785960058830824e-06, gradient norm: tensor(5.6387e-06)\n",
            "Iteration: 48000, loss: 2.3555653042421907e-06, gradient norm: tensor(3.1952e-06)\n",
            "Iteration: 49000, loss: 2.327261348227694e-06, gradient norm: tensor(2.4761e-05)\n",
            "Iteration: 50000, loss: 2.2888371468070546e-06, gradient norm: tensor(7.1144e-06)\n",
            "Iteration: 51000, loss: 2.24043877301483e-06, gradient norm: tensor(3.9744e-06)\n",
            "Iteration: 52000, loss: 2.1955673662432672e-06, gradient norm: tensor(1.6309e-06)\n",
            "Iteration: 0, loss: 2.8033952927589416, gradient norm: tensor(3.3616)\n",
            "Iteration: 1000, loss: 2.009902604341507, gradient norm: tensor(2.8510)\n",
            "Iteration: 2000, loss: 1.3968429157733917, gradient norm: tensor(2.3685)\n",
            "Iteration: 3000, loss: 0.9281062130331993, gradient norm: tensor(1.9089)\n",
            "Iteration: 4000, loss: 0.5764331338405609, gradient norm: tensor(1.4720)\n",
            "Iteration: 5000, loss: 0.3239334906935692, gradient norm: tensor(1.0639)\n",
            "Iteration: 6000, loss: 0.15679977534711362, gradient norm: tensor(0.6972)\n",
            "Iteration: 7000, loss: 0.06025693421624601, gradient norm: tensor(0.3898)\n",
            "Iteration: 8000, loss: 0.01604466271819547, gradient norm: tensor(0.1653)\n",
            "Iteration: 9000, loss: 0.002543298181204591, gradient norm: tensor(0.0428)\n",
            "Iteration: 10000, loss: 0.0004134677485853899, gradient norm: tensor(0.0050)\n",
            "Iteration: 11000, loss: 0.00022638108939281664, gradient norm: tensor(0.0013)\n",
            "Iteration: 12000, loss: 0.00017174636796698906, gradient norm: tensor(0.0011)\n",
            "Iteration: 13000, loss: 0.00013737652689451351, gradient norm: tensor(0.0010)\n",
            "Iteration: 14000, loss: 0.0001128776574623771, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 9.355511889589252e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 16000, loss: 7.455905892857117e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 17000, loss: 5.4466256933665136e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 18000, loss: 3.633705687934707e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 2.2663148854917382e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 1.4043599596334388e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 9.367264258798968e-06, gradient norm: tensor(8.0372e-05)\n",
            "Iteration: 22000, loss: 5.5314583116796715e-06, gradient norm: tensor(6.6594e-05)\n",
            "Iteration: 23000, loss: 2.1183368623951538e-06, gradient norm: tensor(2.7001e-05)\n",
            "Iteration: 24000, loss: 8.559330388493436e-07, gradient norm: tensor(1.1388e-05)\n",
            "Iteration: 25000, loss: 6.909964105830113e-07, gradient norm: tensor(2.7377e-05)\n",
            "Iteration: 26000, loss: 6.61102371736888e-07, gradient norm: tensor(4.8750e-06)\n",
            "Iteration: 27000, loss: 6.358199072451498e-07, gradient norm: tensor(2.5519e-06)\n",
            "Iteration: 28000, loss: 6.040830437541444e-07, gradient norm: tensor(3.0334e-06)\n",
            "Iteration: 29000, loss: 5.627045137543974e-07, gradient norm: tensor(2.5892e-06)\n",
            "Iteration: 30000, loss: 5.153956505523638e-07, gradient norm: tensor(1.0262e-05)\n",
            "Iteration: 31000, loss: 4.71792540054139e-07, gradient norm: tensor(2.1831e-06)\n",
            "Iteration: 32000, loss: 4.3488882923270466e-07, gradient norm: tensor(6.7040e-06)\n",
            "Iteration: 33000, loss: 4.0326722924532985e-07, gradient norm: tensor(2.1243e-05)\n",
            "Iteration: 34000, loss: 3.759423627798242e-07, gradient norm: tensor(6.1856e-05)\n",
            "Iteration: 35000, loss: 3.5195446585589706e-07, gradient norm: tensor(2.3035e-05)\n",
            "Iteration: 36000, loss: 3.3078310383416465e-07, gradient norm: tensor(2.4093e-05)\n",
            "Iteration: 37000, loss: 3.1190647183620967e-07, gradient norm: tensor(2.3604e-05)\n",
            "Iteration: 38000, loss: 2.9496178876797784e-07, gradient norm: tensor(2.0881e-06)\n",
            "Iteration: 39000, loss: 2.796513237228737e-07, gradient norm: tensor(5.9593e-05)\n",
            "Iteration: 40000, loss: 2.656797183533399e-07, gradient norm: tensor(6.9540e-06)\n",
            "Iteration: 41000, loss: 2.5289489875035543e-07, gradient norm: tensor(4.9210e-06)\n",
            "Iteration: 42000, loss: 2.4125061489144174e-07, gradient norm: tensor(6.4690e-06)\n",
            "Iteration: 43000, loss: 2.3040762508230727e-07, gradient norm: tensor(4.8827e-06)\n",
            "Iteration: 44000, loss: 2.2036299903049895e-07, gradient norm: tensor(6.6495e-05)\n",
            "Iteration: 45000, loss: 2.1100344356739242e-07, gradient norm: tensor(5.6925e-06)\n",
            "Iteration: 46000, loss: 2.0227580516518629e-07, gradient norm: tensor(1.9652e-06)\n",
            "Iteration: 0, loss: 3.0548363292217253, gradient norm: tensor(3.5534)\n",
            "Iteration: 1000, loss: 2.2682760598659515, gradient norm: tensor(3.0491)\n",
            "Iteration: 2000, loss: 1.6365141438245774, gradient norm: tensor(2.5675)\n",
            "Iteration: 3000, loss: 1.1319020627140999, gradient norm: tensor(2.1058)\n",
            "Iteration: 4000, loss: 0.7381560809612274, gradient norm: tensor(1.6640)\n",
            "Iteration: 5000, loss: 0.4438410005271435, gradient norm: tensor(1.2470)\n",
            "Iteration: 6000, loss: 0.23756857374310494, gradient norm: tensor(0.8643)\n",
            "Iteration: 7000, loss: 0.10693395777791738, gradient norm: tensor(0.5295)\n",
            "Iteration: 8000, loss: 0.03744444471970201, gradient norm: tensor(0.2630)\n",
            "Iteration: 9000, loss: 0.010152218591887504, gradient norm: tensor(0.0904)\n",
            "Iteration: 10000, loss: 0.003442319804104045, gradient norm: tensor(0.0187)\n",
            "Iteration: 11000, loss: 0.0021784890735289083, gradient norm: tensor(0.0089)\n",
            "Iteration: 12000, loss: 0.0016289712879806757, gradient norm: tensor(0.0082)\n",
            "Iteration: 13000, loss: 0.0012243868974037469, gradient norm: tensor(0.0075)\n",
            "Iteration: 14000, loss: 0.0008593305830145255, gradient norm: tensor(0.0061)\n",
            "Iteration: 15000, loss: 0.0005292560294619761, gradient norm: tensor(0.0041)\n",
            "Iteration: 16000, loss: 0.00029641922605514994, gradient norm: tensor(0.0024)\n",
            "Iteration: 17000, loss: 0.00017964525426214094, gradient norm: tensor(0.0014)\n",
            "Iteration: 18000, loss: 0.00012305511909653432, gradient norm: tensor(0.0009)\n",
            "Iteration: 19000, loss: 8.587580288440222e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 5.755769959068857e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 3.6781389979296366e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 2.2646938461548414e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 1.3549319988669595e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 7.931289314910827e-06, gradient norm: tensor(7.1877e-05)\n",
            "Iteration: 25000, loss: 4.564731777918496e-06, gradient norm: tensor(5.1961e-05)\n",
            "Iteration: 26000, loss: 2.568282853303572e-06, gradient norm: tensor(2.9779e-05)\n",
            "Iteration: 27000, loss: 1.3948404074426434e-06, gradient norm: tensor(1.6224e-05)\n",
            "Iteration: 28000, loss: 7.339105336541252e-07, gradient norm: tensor(9.6455e-06)\n",
            "Iteration: 29000, loss: 3.7900801223145206e-07, gradient norm: tensor(7.2287e-06)\n",
            "Iteration: 30000, loss: 1.9597580775609914e-07, gradient norm: tensor(6.6980e-06)\n",
            "Iteration: 31000, loss: 1.0781175378582475e-07, gradient norm: tensor(6.5974e-06)\n",
            "Iteration: 32000, loss: 6.864649881066498e-08, gradient norm: tensor(7.7767e-06)\n",
            "Iteration: 33000, loss: 5.2224675396672635e-08, gradient norm: tensor(5.6617e-06)\n",
            "Iteration: 34000, loss: 4.527194948167335e-08, gradient norm: tensor(1.8136e-05)\n",
            "Iteration: 35000, loss: 4.188683185546438e-08, gradient norm: tensor(2.1261e-05)\n",
            "Iteration: 36000, loss: 3.988084643680168e-08, gradient norm: tensor(1.5854e-05)\n",
            "Iteration: 37000, loss: 3.845632043564251e-08, gradient norm: tensor(7.0955e-07)\n",
            "Iteration: 0, loss: 1.628252524137497, gradient norm: tensor(1.9222)\n",
            "Iteration: 1000, loss: 1.1714704595804215, gradient norm: tensor(1.5524)\n",
            "Iteration: 2000, loss: 0.7725860521197319, gradient norm: tensor(1.3153)\n",
            "Iteration: 3000, loss: 0.42559130212664603, gradient norm: tensor(1.0231)\n",
            "Iteration: 4000, loss: 0.1871125443801284, gradient norm: tensor(0.6577)\n",
            "Iteration: 5000, loss: 0.0696719886995852, gradient norm: tensor(0.3373)\n",
            "Iteration: 6000, loss: 0.026003837995231153, gradient norm: tensor(0.1273)\n",
            "Iteration: 7000, loss: 0.013504588316194712, gradient norm: tensor(0.0341)\n",
            "Iteration: 8000, loss: 0.009869500604458153, gradient norm: tensor(0.0220)\n",
            "Iteration: 9000, loss: 0.00699069716827944, gradient norm: tensor(0.0210)\n",
            "Iteration: 10000, loss: 0.003982971163699403, gradient norm: tensor(0.0164)\n",
            "Iteration: 11000, loss: 0.0016991331533063204, gradient norm: tensor(0.0099)\n",
            "Iteration: 12000, loss: 0.0005355015083332546, gradient norm: tensor(0.0045)\n",
            "Iteration: 13000, loss: 0.00014772711554542184, gradient norm: tensor(0.0015)\n",
            "Iteration: 14000, loss: 6.003409613913391e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 15000, loss: 3.3210388113730006e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 16000, loss: 2.1176844125875504e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 17000, loss: 1.6864675555552823e-05, gradient norm: tensor(7.4564e-05)\n",
            "Iteration: 18000, loss: 1.4691770068566257e-05, gradient norm: tensor(6.8851e-05)\n",
            "Iteration: 19000, loss: 1.21186474316346e-05, gradient norm: tensor(7.4304e-05)\n",
            "Iteration: 20000, loss: 7.80392995557122e-06, gradient norm: tensor(6.5725e-05)\n",
            "Iteration: 21000, loss: 2.9101780763767236e-06, gradient norm: tensor(2.8837e-05)\n",
            "Iteration: 22000, loss: 6.721428076730263e-07, gradient norm: tensor(7.9454e-06)\n",
            "Iteration: 23000, loss: 2.3101402246084035e-07, gradient norm: tensor(3.0933e-06)\n",
            "Iteration: 24000, loss: 1.3876941464729952e-07, gradient norm: tensor(2.2687e-06)\n",
            "Iteration: 25000, loss: 9.447104887527758e-08, gradient norm: tensor(2.0079e-06)\n",
            "Iteration: 26000, loss: 5.017729911926949e-08, gradient norm: tensor(3.5410e-05)\n",
            "Iteration: 27000, loss: 1.814732631189031e-08, gradient norm: tensor(1.9776e-05)\n",
            "Iteration: 28000, loss: 7.994025796431714e-09, gradient norm: tensor(2.5989e-06)\n",
            "Iteration: 29000, loss: 4.727451592856369e-09, gradient norm: tensor(6.4243e-06)\n",
            "Iteration: 30000, loss: 3.2970384964414735e-09, gradient norm: tensor(1.3502e-06)\n",
            "Iteration: 0, loss: 2.3900816802978517, gradient norm: tensor(3.1281)\n",
            "Iteration: 1000, loss: 1.6805231815576553, gradient norm: tensor(2.6190)\n",
            "Iteration: 2000, loss: 1.134400744318962, gradient norm: tensor(2.1141)\n",
            "Iteration: 3000, loss: 0.7325923634171486, gradient norm: tensor(1.6468)\n",
            "Iteration: 4000, loss: 0.4414721178114414, gradient norm: tensor(1.2299)\n",
            "Iteration: 5000, loss: 0.23666114147007467, gradient norm: tensor(0.8518)\n",
            "Iteration: 6000, loss: 0.10590304529294371, gradient norm: tensor(0.5194)\n",
            "Iteration: 7000, loss: 0.03597870305925608, gradient norm: tensor(0.2544)\n",
            "Iteration: 8000, loss: 0.008479083761805669, gradient norm: tensor(0.0844)\n",
            "Iteration: 9000, loss: 0.001980435683275573, gradient norm: tensor(0.0150)\n",
            "Iteration: 10000, loss: 0.0010430839787586592, gradient norm: tensor(0.0043)\n",
            "Iteration: 11000, loss: 0.0007469595550210215, gradient norm: tensor(0.0032)\n",
            "Iteration: 12000, loss: 0.0005261000922764652, gradient norm: tensor(0.0026)\n",
            "Iteration: 13000, loss: 0.0003323040234390646, gradient norm: tensor(0.0021)\n",
            "Iteration: 14000, loss: 0.00018106041305873078, gradient norm: tensor(0.0014)\n",
            "Iteration: 15000, loss: 9.805627882451518e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 16000, loss: 6.676280617466546e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 17000, loss: 5.640582946580253e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 4.989362035485101e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 4.325174179393798e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 3.613091180159245e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 2.8731916561810068e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 2.171809868195851e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.588721206462651e-05, gradient norm: tensor(7.1891e-05)\n",
            "Iteration: 24000, loss: 1.1579018768316019e-05, gradient norm: tensor(4.9393e-05)\n",
            "Iteration: 25000, loss: 8.444591407169355e-06, gradient norm: tensor(3.3601e-05)\n",
            "Iteration: 26000, loss: 6.1767602269355845e-06, gradient norm: tensor(2.1603e-05)\n",
            "Iteration: 27000, loss: 4.6741288265366166e-06, gradient norm: tensor(2.1199e-05)\n",
            "Iteration: 28000, loss: 3.6018293053530216e-06, gradient norm: tensor(1.1516e-05)\n",
            "Iteration: 29000, loss: 2.7543392038751337e-06, gradient norm: tensor(1.0745e-05)\n",
            "Iteration: 30000, loss: 2.0894305411047754e-06, gradient norm: tensor(8.0875e-06)\n",
            "Iteration: 31000, loss: 1.599089559931599e-06, gradient norm: tensor(1.3803e-05)\n",
            "Iteration: 32000, loss: 1.2572650008451092e-06, gradient norm: tensor(2.4116e-05)\n",
            "Iteration: 33000, loss: 1.0243264883911251e-06, gradient norm: tensor(4.4831e-05)\n",
            "Iteration: 34000, loss: 8.643369184824223e-07, gradient norm: tensor(4.7393e-06)\n",
            "Iteration: 35000, loss: 7.529766648985969e-07, gradient norm: tensor(2.0669e-05)\n",
            "Iteration: 36000, loss: 6.730438577733366e-07, gradient norm: tensor(4.0278e-05)\n",
            "Iteration: 37000, loss: 6.134355064659758e-07, gradient norm: tensor(3.8530e-06)\n",
            "Iteration: 38000, loss: 5.673172693150264e-07, gradient norm: tensor(7.2814e-06)\n",
            "Iteration: 39000, loss: 5.300133989862843e-07, gradient norm: tensor(1.1663e-05)\n",
            "Iteration: 40000, loss: 4.989479203345582e-07, gradient norm: tensor(6.4018e-06)\n",
            "Iteration: 41000, loss: 4.7227699232621487e-07, gradient norm: tensor(2.0299e-06)\n",
            "Iteration: 42000, loss: 4.487709336444823e-07, gradient norm: tensor(6.6140e-06)\n",
            "Iteration: 43000, loss: 4.2761280042213914e-07, gradient norm: tensor(2.3984e-05)\n",
            "Iteration: 44000, loss: 4.0842784940764434e-07, gradient norm: tensor(1.2039e-05)\n",
            "Iteration: 45000, loss: 3.9075122771237147e-07, gradient norm: tensor(7.7583e-06)\n",
            "Iteration: 46000, loss: 3.7431845069590964e-07, gradient norm: tensor(3.8805e-05)\n",
            "Iteration: 47000, loss: 3.589170880218262e-07, gradient norm: tensor(2.6224e-06)\n",
            "Iteration: 48000, loss: 3.4443348627632985e-07, gradient norm: tensor(1.6957e-05)\n",
            "Iteration: 49000, loss: 3.307870905189247e-07, gradient norm: tensor(6.0171e-06)\n",
            "Iteration: 50000, loss: 3.17883165649846e-07, gradient norm: tensor(4.1284e-06)\n",
            "Iteration: 51000, loss: 3.056595591317546e-07, gradient norm: tensor(1.0128e-05)\n",
            "Iteration: 52000, loss: 2.9395721182368106e-07, gradient norm: tensor(3.5594e-06)\n",
            "Iteration: 53000, loss: 2.828512323844734e-07, gradient norm: tensor(5.4459e-06)\n",
            "Iteration: 54000, loss: 2.7227342661717555e-07, gradient norm: tensor(1.9567e-06)\n",
            "Iteration: 0, loss: 7.13843649482727, gradient norm: tensor(5.7117)\n",
            "Iteration: 1000, loss: 5.87388531589508, gradient norm: tensor(5.1541)\n",
            "Iteration: 2000, loss: 4.7841928625106815, gradient norm: tensor(4.6240)\n",
            "Iteration: 3000, loss: 3.8366666564941405, gradient norm: tensor(4.1114)\n",
            "Iteration: 4000, loss: 3.013050673007965, gradient norm: tensor(3.6141)\n",
            "Iteration: 5000, loss: 2.302275240778923, gradient norm: tensor(3.1287)\n",
            "Iteration: 6000, loss: 1.698545461177826, gradient norm: tensor(2.6570)\n",
            "Iteration: 7000, loss: 1.1961889321208, gradient norm: tensor(2.2020)\n",
            "Iteration: 8000, loss: 0.7902390726804733, gradient norm: tensor(1.7580)\n",
            "Iteration: 9000, loss: 0.48019209522008893, gradient norm: tensor(1.3310)\n",
            "Iteration: 10000, loss: 0.26014068219065667, gradient norm: tensor(0.9374)\n",
            "Iteration: 11000, loss: 0.11839649692922831, gradient norm: tensor(0.5894)\n",
            "Iteration: 12000, loss: 0.0408890547119081, gradient norm: tensor(0.3064)\n",
            "Iteration: 13000, loss: 0.00898921824619174, gradient norm: tensor(0.1140)\n",
            "Iteration: 14000, loss: 0.0009525279441295424, gradient norm: tensor(0.0233)\n",
            "Iteration: 15000, loss: 5.235376093878585e-05, gradient norm: tensor(0.0017)\n",
            "Iteration: 16000, loss: 1.7052408270501474e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 1.3626942090922966e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 1.1789849470915215e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 1.0575480948318727e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 9.882243895845023e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 9.515506642856054e-06, gradient norm: tensor(9.8795e-05)\n",
            "Iteration: 22000, loss: 9.212743476382456e-06, gradient norm: tensor(9.6911e-05)\n",
            "Iteration: 23000, loss: 8.791508492322463e-06, gradient norm: tensor(9.5432e-05)\n",
            "Iteration: 24000, loss: 8.124343486088036e-06, gradient norm: tensor(9.2988e-05)\n",
            "Iteration: 25000, loss: 7.069592547850334e-06, gradient norm: tensor(9.1548e-05)\n",
            "Iteration: 26000, loss: 5.447708917472482e-06, gradient norm: tensor(8.5976e-05)\n",
            "Iteration: 27000, loss: 3.141720948860893e-06, gradient norm: tensor(7.0352e-05)\n",
            "Iteration: 28000, loss: 9.345069260575656e-07, gradient norm: tensor(1.9031e-05)\n",
            "Iteration: 29000, loss: 2.0378541753984792e-07, gradient norm: tensor(2.9817e-06)\n",
            "Iteration: 30000, loss: 1.367389856028467e-07, gradient norm: tensor(2.4211e-05)\n",
            "Iteration: 31000, loss: 1.32104448056225e-07, gradient norm: tensor(2.0636e-06)\n",
            "Iteration: 32000, loss: 1.2993575435871206e-07, gradient norm: tensor(4.8020e-06)\n",
            "Iteration: 33000, loss: 1.2705759429820772e-07, gradient norm: tensor(3.6968e-06)\n",
            "Iteration: 34000, loss: 1.2320450488800816e-07, gradient norm: tensor(3.0420e-06)\n",
            "Iteration: 35000, loss: 1.1876755890227741e-07, gradient norm: tensor(2.6890e-05)\n",
            "Iteration: 36000, loss: 1.1460010454555914e-07, gradient norm: tensor(4.4307e-05)\n",
            "Iteration: 37000, loss: 1.1083397298250475e-07, gradient norm: tensor(8.4425e-05)\n",
            "Iteration: 38000, loss: 1.0734317273630722e-07, gradient norm: tensor(2.3235e-05)\n",
            "Iteration: 39000, loss: 1.0405743242358767e-07, gradient norm: tensor(3.1874e-06)\n",
            "Iteration: 40000, loss: 1.009438081780445e-07, gradient norm: tensor(4.7437e-06)\n",
            "Iteration: 41000, loss: 9.7965905517583e-08, gradient norm: tensor(3.6606e-05)\n",
            "Iteration: 42000, loss: 9.510980056859352e-08, gradient norm: tensor(1.4467e-06)\n",
            "Iteration: 0, loss: 6.7622520270347595, gradient norm: tensor(5.5292)\n",
            "Iteration: 1000, loss: 5.555014009475708, gradient norm: tensor(4.9903)\n",
            "Iteration: 2000, loss: 4.508288246154785, gradient norm: tensor(4.4788)\n",
            "Iteration: 3000, loss: 3.5910826954841615, gradient norm: tensor(3.9771)\n",
            "Iteration: 4000, loss: 2.7944254567623137, gradient norm: tensor(3.4832)\n",
            "Iteration: 5000, loss: 2.111806766271591, gradient norm: tensor(2.9986)\n",
            "Iteration: 6000, loss: 1.537339828491211, gradient norm: tensor(2.5255)\n",
            "Iteration: 7000, loss: 1.0661157948374749, gradient norm: tensor(2.0664)\n",
            "Iteration: 8000, loss: 0.6934677346348762, gradient norm: tensor(1.6257)\n",
            "Iteration: 9000, loss: 0.41338259157538415, gradient norm: tensor(1.2115)\n",
            "Iteration: 10000, loss: 0.21730240051448346, gradient norm: tensor(0.8327)\n",
            "Iteration: 11000, loss: 0.09473847191408277, gradient norm: tensor(0.5030)\n",
            "Iteration: 12000, loss: 0.031127169980667533, gradient norm: tensor(0.2437)\n",
            "Iteration: 13000, loss: 0.007084853564854711, gradient norm: tensor(0.0801)\n",
            "Iteration: 14000, loss: 0.0016005928710801527, gradient norm: tensor(0.0147)\n",
            "Iteration: 15000, loss: 0.0007731401950004511, gradient norm: tensor(0.0054)\n",
            "Iteration: 16000, loss: 0.00048671548929996787, gradient norm: tensor(0.0039)\n",
            "Iteration: 17000, loss: 0.00030792611253855284, gradient norm: tensor(0.0028)\n",
            "Iteration: 18000, loss: 0.00019762413304124494, gradient norm: tensor(0.0018)\n",
            "Iteration: 19000, loss: 0.00013502794485975757, gradient norm: tensor(0.0012)\n",
            "Iteration: 20000, loss: 0.00010049672522291076, gradient norm: tensor(0.0008)\n",
            "Iteration: 21000, loss: 7.875022073130822e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 22000, loss: 6.157751055434346e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 23000, loss: 4.062111630628351e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 24000, loss: 1.9511832604621306e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 8.712165183169418e-06, gradient norm: tensor(9.7606e-05)\n",
            "Iteration: 26000, loss: 4.921256972920673e-06, gradient norm: tensor(5.0606e-05)\n",
            "Iteration: 27000, loss: 3.511047804067857e-06, gradient norm: tensor(4.2164e-05)\n",
            "Iteration: 28000, loss: 2.6971132097060036e-06, gradient norm: tensor(2.2203e-05)\n",
            "Iteration: 29000, loss: 2.1499004301404055e-06, gradient norm: tensor(1.3035e-05)\n",
            "Iteration: 30000, loss: 1.8510233953747956e-06, gradient norm: tensor(6.9620e-06)\n",
            "Iteration: 31000, loss: 1.7068682628860189e-06, gradient norm: tensor(5.0888e-06)\n",
            "Iteration: 32000, loss: 1.5916029874460946e-06, gradient norm: tensor(6.3980e-06)\n",
            "Iteration: 33000, loss: 1.4531180009953459e-06, gradient norm: tensor(3.9607e-06)\n",
            "Iteration: 34000, loss: 1.2929407068895671e-06, gradient norm: tensor(3.0163e-05)\n",
            "Iteration: 35000, loss: 1.1260628921263561e-06, gradient norm: tensor(1.0239e-05)\n",
            "Iteration: 36000, loss: 9.6644881358543e-07, gradient norm: tensor(1.1346e-05)\n",
            "Iteration: 37000, loss: 8.078329216232305e-07, gradient norm: tensor(1.2034e-05)\n",
            "Iteration: 38000, loss: 6.541001937421243e-07, gradient norm: tensor(5.7398e-06)\n",
            "Iteration: 39000, loss: 5.300193179209601e-07, gradient norm: tensor(5.0723e-06)\n",
            "Iteration: 40000, loss: 4.4784897170302427e-07, gradient norm: tensor(5.8073e-06)\n",
            "Iteration: 41000, loss: 3.969802473875461e-07, gradient norm: tensor(8.8853e-06)\n",
            "Iteration: 42000, loss: 3.622216669896261e-07, gradient norm: tensor(1.4194e-05)\n",
            "Iteration: 43000, loss: 3.353089620645733e-07, gradient norm: tensor(8.3590e-06)\n",
            "Iteration: 44000, loss: 3.1295967329469933e-07, gradient norm: tensor(1.0491e-05)\n",
            "Iteration: 45000, loss: 2.936961857074039e-07, gradient norm: tensor(4.9967e-06)\n",
            "Iteration: 46000, loss: 2.7692387283195786e-07, gradient norm: tensor(3.6515e-05)\n",
            "Iteration: 47000, loss: 2.6199464448950494e-07, gradient norm: tensor(3.4255e-05)\n",
            "Iteration: 48000, loss: 2.4859871737703543e-07, gradient norm: tensor(1.5626e-05)\n",
            "Iteration: 49000, loss: 2.365511071218407e-07, gradient norm: tensor(1.1757e-05)\n",
            "Iteration: 50000, loss: 2.2561769155515777e-07, gradient norm: tensor(3.7848e-06)\n",
            "Iteration: 51000, loss: 2.1565333605622073e-07, gradient norm: tensor(5.6163e-06)\n",
            "Iteration: 52000, loss: 2.0648464206374228e-07, gradient norm: tensor(1.8755e-05)\n",
            "Iteration: 53000, loss: 1.980454347574323e-07, gradient norm: tensor(6.3526e-06)\n",
            "Iteration: 54000, loss: 1.902529299542266e-07, gradient norm: tensor(1.1876e-05)\n",
            "Iteration: 55000, loss: 1.8297369267372689e-07, gradient norm: tensor(1.1786e-06)\n",
            "Iteration: 0, loss: 2.1819216129779817, gradient norm: tensor(2.9090)\n",
            "Iteration: 1000, loss: 1.560380893945694, gradient norm: tensor(2.3994)\n",
            "Iteration: 2000, loss: 1.0759982254505158, gradient norm: tensor(1.9440)\n",
            "Iteration: 3000, loss: 0.6990764783024788, gradient norm: tensor(1.5384)\n",
            "Iteration: 4000, loss: 0.4143624328672886, gradient norm: tensor(1.1426)\n",
            "Iteration: 5000, loss: 0.21845328642427922, gradient norm: tensor(0.7729)\n",
            "Iteration: 6000, loss: 0.09868457682430744, gradient norm: tensor(0.4570)\n",
            "Iteration: 7000, loss: 0.03591088753566146, gradient norm: tensor(0.2147)\n",
            "Iteration: 8000, loss: 0.010795506506692618, gradient norm: tensor(0.0669)\n",
            "Iteration: 9000, loss: 0.003866534006781876, gradient norm: tensor(0.0159)\n",
            "Iteration: 10000, loss: 0.0018581977366702632, gradient norm: tensor(0.0096)\n",
            "Iteration: 11000, loss: 0.0008962331132497638, gradient norm: tensor(0.0061)\n",
            "Iteration: 12000, loss: 0.00042088916344800966, gradient norm: tensor(0.0036)\n",
            "Iteration: 13000, loss: 0.00019766916282242165, gradient norm: tensor(0.0019)\n",
            "Iteration: 14000, loss: 9.418062985787401e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 15000, loss: 5.6971157377120105e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 4.898609824522282e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 17000, loss: 4.7084710458875634e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 4.5295695865206656e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 4.313019798428286e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 4.057633614866063e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 3.7627988731401275e-05, gradient norm: tensor(9.0716e-05)\n",
            "Iteration: 22000, loss: 3.420926656326628e-05, gradient norm: tensor(7.8454e-05)\n",
            "Iteration: 23000, loss: 3.0222381255953223e-05, gradient norm: tensor(6.7518e-05)\n",
            "Iteration: 24000, loss: 2.560908464693057e-05, gradient norm: tensor(5.7087e-05)\n",
            "Iteration: 25000, loss: 2.0586829446983755e-05, gradient norm: tensor(4.7598e-05)\n",
            "Iteration: 26000, loss: 1.610032000007777e-05, gradient norm: tensor(3.6089e-05)\n",
            "Iteration: 27000, loss: 1.2742736707878066e-05, gradient norm: tensor(2.7191e-05)\n",
            "Iteration: 28000, loss: 1.012075642029231e-05, gradient norm: tensor(2.1367e-05)\n",
            "Iteration: 29000, loss: 8.003689009456138e-06, gradient norm: tensor(2.0484e-05)\n",
            "Iteration: 30000, loss: 6.358947755416011e-06, gradient norm: tensor(1.5157e-05)\n",
            "Iteration: 31000, loss: 5.14628171640652e-06, gradient norm: tensor(1.0381e-05)\n",
            "Iteration: 32000, loss: 4.29870994457815e-06, gradient norm: tensor(1.1800e-05)\n",
            "Iteration: 33000, loss: 3.726336116415041e-06, gradient norm: tensor(5.1447e-06)\n",
            "Iteration: 34000, loss: 3.341596256404955e-06, gradient norm: tensor(6.3471e-05)\n",
            "Iteration: 35000, loss: 3.0919064697627617e-06, gradient norm: tensor(8.0925e-06)\n",
            "Iteration: 36000, loss: 2.928575901933073e-06, gradient norm: tensor(1.2916e-05)\n",
            "Iteration: 37000, loss: 2.8159341502487223e-06, gradient norm: tensor(2.3156e-06)\n",
            "Iteration: 38000, loss: 2.7339788543940814e-06, gradient norm: tensor(3.4293e-06)\n",
            "Iteration: 39000, loss: 2.6704575834628485e-06, gradient norm: tensor(2.4372e-05)\n",
            "Iteration: 40000, loss: 2.617865105776218e-06, gradient norm: tensor(1.3415e-05)\n",
            "Iteration: 41000, loss: 2.571454173676102e-06, gradient norm: tensor(2.9202e-06)\n",
            "Iteration: 42000, loss: 2.527731934833355e-06, gradient norm: tensor(1.0237e-05)\n",
            "Iteration: 43000, loss: 2.482790326439499e-06, gradient norm: tensor(6.8914e-06)\n",
            "Iteration: 44000, loss: 2.4309750638167316e-06, gradient norm: tensor(3.6003e-05)\n",
            "Iteration: 45000, loss: 2.363950008430038e-06, gradient norm: tensor(7.3220e-06)\n",
            "Iteration: 46000, loss: 2.283022600295226e-06, gradient norm: tensor(2.9565e-06)\n",
            "Iteration: 47000, loss: 2.217562706618992e-06, gradient norm: tensor(6.1693e-06)\n",
            "Iteration: 48000, loss: 2.1752021743850493e-06, gradient norm: tensor(8.7706e-06)\n",
            "Iteration: 49000, loss: 2.142571446711372e-06, gradient norm: tensor(3.3715e-06)\n",
            "Iteration: 50000, loss: 2.1150672439489426e-06, gradient norm: tensor(8.4163e-06)\n",
            "Iteration: 51000, loss: 2.0911218141463905e-06, gradient norm: tensor(1.3806e-06)\n",
            "Iteration: 0, loss: 0.7832329865694047, gradient norm: tensor(1.6938)\n",
            "Iteration: 1000, loss: 0.4379620343744755, gradient norm: tensor(1.2306)\n",
            "Iteration: 2000, loss: 0.21624811680614947, gradient norm: tensor(0.8237)\n",
            "Iteration: 3000, loss: 0.08708565121144056, gradient norm: tensor(0.4805)\n",
            "Iteration: 4000, loss: 0.024981688939966263, gradient norm: tensor(0.2204)\n",
            "Iteration: 5000, loss: 0.004099389186361805, gradient norm: tensor(0.0658)\n",
            "Iteration: 6000, loss: 0.0002939593005467032, gradient norm: tensor(0.0092)\n",
            "Iteration: 7000, loss: 2.5223777160135798e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 8000, loss: 1.3413826336545753e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 9000, loss: 1.0025823192336247e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 10000, loss: 7.879365030930786e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 11000, loss: 6.628016744343768e-06, gradient norm: tensor(6.1543e-05)\n",
            "Iteration: 12000, loss: 6.092220055506914e-06, gradient norm: tensor(5.3105e-05)\n",
            "Iteration: 13000, loss: 5.877941797280073e-06, gradient norm: tensor(5.4252e-05)\n",
            "Iteration: 14000, loss: 5.699783986074181e-06, gradient norm: tensor(5.4139e-05)\n",
            "Iteration: 15000, loss: 5.470933183914895e-06, gradient norm: tensor(4.9423e-05)\n",
            "Iteration: 16000, loss: 5.178787549084518e-06, gradient norm: tensor(4.6685e-05)\n",
            "Iteration: 17000, loss: 4.802268286766776e-06, gradient norm: tensor(4.5415e-05)\n",
            "Iteration: 18000, loss: 4.304727061025914e-06, gradient norm: tensor(3.8920e-05)\n",
            "Iteration: 19000, loss: 3.668073129574623e-06, gradient norm: tensor(3.4554e-05)\n",
            "Iteration: 20000, loss: 2.9255605702473987e-06, gradient norm: tensor(2.6809e-05)\n",
            "Iteration: 21000, loss: 2.166079867379267e-06, gradient norm: tensor(2.1366e-05)\n",
            "Iteration: 22000, loss: 1.4908765223253794e-06, gradient norm: tensor(1.5024e-05)\n",
            "Iteration: 23000, loss: 9.586754632096018e-07, gradient norm: tensor(9.6220e-06)\n",
            "Iteration: 24000, loss: 5.77391267938765e-07, gradient norm: tensor(1.0633e-05)\n",
            "Iteration: 25000, loss: 3.242142518615765e-07, gradient norm: tensor(8.1644e-06)\n",
            "Iteration: 26000, loss: 1.6934004486302e-07, gradient norm: tensor(4.8829e-06)\n",
            "Iteration: 27000, loss: 8.750583383942967e-08, gradient norm: tensor(6.1171e-06)\n",
            "Iteration: 28000, loss: 5.306756210998742e-08, gradient norm: tensor(9.0475e-06)\n",
            "Iteration: 29000, loss: 4.0624783942178056e-08, gradient norm: tensor(1.1173e-05)\n",
            "Iteration: 30000, loss: 3.5993306788384414e-08, gradient norm: tensor(1.2982e-05)\n",
            "Iteration: 31000, loss: 3.386526649151733e-08, gradient norm: tensor(2.2332e-05)\n",
            "Iteration: 32000, loss: 3.252197496195208e-08, gradient norm: tensor(6.1355e-06)\n",
            "Iteration: 33000, loss: 3.144666787946448e-08, gradient norm: tensor(2.5490e-06)\n",
            "Iteration: 34000, loss: 3.0467358708818894e-08, gradient norm: tensor(2.7663e-06)\n",
            "Iteration: 35000, loss: 2.9553524386827233e-08, gradient norm: tensor(2.5304e-05)\n",
            "Iteration: 36000, loss: 2.866166632919942e-08, gradient norm: tensor(1.9621e-06)\n",
            "Iteration: 0, loss: 2.650443558216095, gradient norm: tensor(3.2676)\n",
            "Iteration: 1000, loss: 1.9220670105218887, gradient norm: tensor(2.7807)\n",
            "Iteration: 2000, loss: 1.3408623050451278, gradient norm: tensor(2.3094)\n",
            "Iteration: 3000, loss: 0.8870758191943169, gradient norm: tensor(1.8548)\n",
            "Iteration: 4000, loss: 0.5454959320425987, gradient norm: tensor(1.4228)\n",
            "Iteration: 5000, loss: 0.301666621953249, gradient norm: tensor(1.0202)\n",
            "Iteration: 6000, loss: 0.14220230270177125, gradient norm: tensor(0.6590)\n",
            "Iteration: 7000, loss: 0.052261605989187956, gradient norm: tensor(0.3589)\n",
            "Iteration: 8000, loss: 0.012814669710118324, gradient norm: tensor(0.1455)\n",
            "Iteration: 9000, loss: 0.0016501575865841005, gradient norm: tensor(0.0347)\n",
            "Iteration: 10000, loss: 0.00014228122503118356, gradient norm: tensor(0.0033)\n",
            "Iteration: 11000, loss: 6.357008705163026e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 12000, loss: 4.7767190353624754e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 13000, loss: 3.8404077662562484e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 14000, loss: 3.414838746539317e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 15000, loss: 3.152052148288931e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 2.8923108404342202e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 2.6430232745042304e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 2.3870245184298256e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 2.064639224408893e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.621068702024786e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.0708659079682548e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 5.526107290506843e-06, gradient norm: tensor(7.2138e-05)\n",
            "Iteration: 23000, loss: 2.6233646965465594e-06, gradient norm: tensor(2.6603e-05)\n",
            "Iteration: 24000, loss: 1.3517100529725213e-06, gradient norm: tensor(1.0644e-05)\n",
            "Iteration: 25000, loss: 8.108609779924336e-07, gradient norm: tensor(3.6857e-05)\n",
            "Iteration: 26000, loss: 5.857094548105124e-07, gradient norm: tensor(6.7685e-06)\n",
            "Iteration: 27000, loss: 5.037251837620715e-07, gradient norm: tensor(6.3638e-06)\n",
            "Iteration: 28000, loss: 4.610403031222177e-07, gradient norm: tensor(2.0203e-05)\n",
            "Iteration: 29000, loss: 4.255103304444674e-07, gradient norm: tensor(2.0999e-06)\n",
            "Iteration: 30000, loss: 3.9055303608392933e-07, gradient norm: tensor(3.2550e-06)\n",
            "Iteration: 31000, loss: 3.587049995985581e-07, gradient norm: tensor(7.4557e-06)\n",
            "Iteration: 32000, loss: 3.321819270922788e-07, gradient norm: tensor(2.7772e-06)\n",
            "Iteration: 33000, loss: 3.0959679173747646e-07, gradient norm: tensor(2.7812e-05)\n",
            "Iteration: 34000, loss: 2.899356068439829e-07, gradient norm: tensor(2.5071e-05)\n",
            "Iteration: 35000, loss: 2.726911383206243e-07, gradient norm: tensor(3.2165e-05)\n",
            "Iteration: 36000, loss: 2.5753369786229994e-07, gradient norm: tensor(1.8948e-06)\n",
            "Iteration: 0, loss: 1.738138561964035, gradient norm: tensor(2.6439)\n",
            "Iteration: 1000, loss: 1.1726727174520493, gradient norm: tensor(2.1292)\n",
            "Iteration: 2000, loss: 0.7536613040566444, gradient norm: tensor(1.6729)\n",
            "Iteration: 3000, loss: 0.4456143180727959, gradient norm: tensor(1.2523)\n",
            "Iteration: 4000, loss: 0.23232691748440265, gradient norm: tensor(0.8651)\n",
            "Iteration: 5000, loss: 0.10014492024108768, gradient norm: tensor(0.5265)\n",
            "Iteration: 6000, loss: 0.0318777053905651, gradient norm: tensor(0.2586)\n",
            "Iteration: 7000, loss: 0.006204235982033424, gradient norm: tensor(0.0868)\n",
            "Iteration: 8000, loss: 0.0006291192917124136, gradient norm: tensor(0.0148)\n",
            "Iteration: 9000, loss: 9.938141183374682e-05, gradient norm: tensor(0.0011)\n",
            "Iteration: 10000, loss: 4.930623244945309e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 11000, loss: 2.5166393561448787e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 12000, loss: 1.4295295208285097e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 13000, loss: 1.0819593869200617e-05, gradient norm: tensor(4.9055e-05)\n",
            "Iteration: 14000, loss: 9.407584942891845e-06, gradient norm: tensor(3.4441e-05)\n",
            "Iteration: 15000, loss: 8.444291647720093e-06, gradient norm: tensor(3.0015e-05)\n",
            "Iteration: 16000, loss: 7.716913654803647e-06, gradient norm: tensor(2.9162e-05)\n",
            "Iteration: 17000, loss: 7.2444539496245856e-06, gradient norm: tensor(3.0067e-05)\n",
            "Iteration: 18000, loss: 6.994196243795159e-06, gradient norm: tensor(3.0888e-05)\n",
            "Iteration: 19000, loss: 6.823568906384025e-06, gradient norm: tensor(3.0688e-05)\n",
            "Iteration: 20000, loss: 6.610158396597399e-06, gradient norm: tensor(3.6907e-05)\n",
            "Iteration: 21000, loss: 6.296362690136448e-06, gradient norm: tensor(2.7240e-05)\n",
            "Iteration: 22000, loss: 5.859816435531684e-06, gradient norm: tensor(2.9908e-05)\n",
            "Iteration: 23000, loss: 5.312170707384212e-06, gradient norm: tensor(2.1784e-05)\n",
            "Iteration: 24000, loss: 4.704086974925303e-06, gradient norm: tensor(1.5196e-05)\n",
            "Iteration: 25000, loss: 4.0796043779209865e-06, gradient norm: tensor(1.2967e-05)\n",
            "Iteration: 26000, loss: 3.4329141708440147e-06, gradient norm: tensor(2.2796e-05)\n",
            "Iteration: 27000, loss: 2.766531216821022e-06, gradient norm: tensor(2.6107e-05)\n",
            "Iteration: 28000, loss: 2.1379790126729857e-06, gradient norm: tensor(4.0932e-05)\n",
            "Iteration: 29000, loss: 1.6184014327791375e-06, gradient norm: tensor(2.9786e-05)\n",
            "Iteration: 30000, loss: 1.2442472291240846e-06, gradient norm: tensor(2.2082e-05)\n",
            "Iteration: 31000, loss: 9.841169042488218e-07, gradient norm: tensor(1.1852e-05)\n",
            "Iteration: 32000, loss: 8.020722913784085e-07, gradient norm: tensor(7.5165e-06)\n",
            "Iteration: 33000, loss: 6.732228926580319e-07, gradient norm: tensor(6.4487e-06)\n",
            "Iteration: 34000, loss: 5.804446237220873e-07, gradient norm: tensor(1.4664e-05)\n",
            "Iteration: 35000, loss: 5.116333709338506e-07, gradient norm: tensor(2.9534e-06)\n",
            "Iteration: 36000, loss: 4.594413107099626e-07, gradient norm: tensor(2.1580e-06)\n",
            "Iteration: 37000, loss: 4.188450123479015e-07, gradient norm: tensor(9.5878e-06)\n",
            "Iteration: 38000, loss: 3.8616033961602623e-07, gradient norm: tensor(2.5736e-06)\n",
            "Iteration: 39000, loss: 3.592373667800075e-07, gradient norm: tensor(1.6778e-05)\n",
            "Iteration: 40000, loss: 3.3645086827505113e-07, gradient norm: tensor(8.5438e-06)\n",
            "Iteration: 41000, loss: 3.168316385426806e-07, gradient norm: tensor(3.8465e-06)\n",
            "Iteration: 42000, loss: 2.9961404885625596e-07, gradient norm: tensor(1.2526e-05)\n",
            "Iteration: 43000, loss: 2.8425196433090607e-07, gradient norm: tensor(7.4637e-06)\n",
            "Iteration: 44000, loss: 2.7038743235152654e-07, gradient norm: tensor(3.1716e-05)\n",
            "Iteration: 45000, loss: 2.5772304044835436e-07, gradient norm: tensor(6.8473e-06)\n",
            "Iteration: 46000, loss: 2.4604286613794103e-07, gradient norm: tensor(3.0168e-05)\n",
            "Iteration: 47000, loss: 2.3522748132620564e-07, gradient norm: tensor(7.7312e-06)\n",
            "Iteration: 48000, loss: 2.2514373783621977e-07, gradient norm: tensor(1.5678e-05)\n",
            "Iteration: 49000, loss: 2.1569464668402817e-07, gradient norm: tensor(1.2042e-05)\n",
            "Iteration: 50000, loss: 2.068323796322602e-07, gradient norm: tensor(9.0804e-06)\n",
            "Iteration: 51000, loss: 1.9845289592979042e-07, gradient norm: tensor(1.4677e-05)\n",
            "Iteration: 52000, loss: 1.905043048537891e-07, gradient norm: tensor(4.8518e-06)\n",
            "Iteration: 53000, loss: 1.8297522352384022e-07, gradient norm: tensor(1.2670e-05)\n",
            "Iteration: 54000, loss: 1.7584124282166158e-07, gradient norm: tensor(9.0779e-06)\n",
            "Iteration: 55000, loss: 1.690608642661573e-07, gradient norm: tensor(1.6124e-05)\n",
            "Iteration: 56000, loss: 1.6262007417822133e-07, gradient norm: tensor(6.9121e-06)\n",
            "Iteration: 57000, loss: 1.5648637497633898e-07, gradient norm: tensor(9.3437e-06)\n",
            "Iteration: 58000, loss: 1.5064786232699134e-07, gradient norm: tensor(1.2277e-05)\n",
            "Iteration: 59000, loss: 1.4507524383589044e-07, gradient norm: tensor(8.0250e-06)\n",
            "Iteration: 60000, loss: 1.397358364130241e-07, gradient norm: tensor(3.7395e-05)\n",
            "Iteration: 61000, loss: 1.3463293107918162e-07, gradient norm: tensor(6.2229e-06)\n",
            "Iteration: 62000, loss: 1.2974999185644264e-07, gradient norm: tensor(3.6463e-06)\n",
            "Iteration: 63000, loss: 1.250781497077469e-07, gradient norm: tensor(4.5081e-06)\n",
            "Iteration: 64000, loss: 1.206029271614284e-07, gradient norm: tensor(2.0464e-05)\n",
            "Iteration: 65000, loss: 1.1633460232474136e-07, gradient norm: tensor(8.2017e-06)\n",
            "Iteration: 66000, loss: 1.122445022190277e-07, gradient norm: tensor(1.3556e-05)\n",
            "Iteration: 67000, loss: 1.0831878715578114e-07, gradient norm: tensor(1.4749e-06)\n",
            "Iteration: 0, loss: 0.595207649409771, gradient norm: tensor(1.1107)\n",
            "Iteration: 1000, loss: 0.3165505019724369, gradient norm: tensor(0.7675)\n",
            "Iteration: 2000, loss: 0.14147628754377364, gradient norm: tensor(0.4565)\n",
            "Iteration: 3000, loss: 0.05338373702764511, gradient norm: tensor(0.1987)\n",
            "Iteration: 4000, loss: 0.02305209594219923, gradient norm: tensor(0.0622)\n",
            "Iteration: 5000, loss: 0.014619870544411242, gradient norm: tensor(0.0362)\n",
            "Iteration: 6000, loss: 0.010107957394793629, gradient norm: tensor(0.0274)\n",
            "Iteration: 7000, loss: 0.006954406935721636, gradient norm: tensor(0.0200)\n",
            "Iteration: 8000, loss: 0.004847076566889882, gradient norm: tensor(0.0155)\n",
            "Iteration: 9000, loss: 0.0030873966163489966, gradient norm: tensor(0.0126)\n",
            "Iteration: 10000, loss: 0.001438735957024619, gradient norm: tensor(0.0079)\n",
            "Iteration: 11000, loss: 0.0004959398249629885, gradient norm: tensor(0.0038)\n",
            "Iteration: 12000, loss: 0.0001470360109306057, gradient norm: tensor(0.0015)\n",
            "Iteration: 13000, loss: 5.361459106643451e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 14000, loss: 3.362409056717297e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 15000, loss: 2.7686638526574824e-05, gradient norm: tensor(8.4791e-05)\n",
            "Iteration: 16000, loss: 2.3966079897945746e-05, gradient norm: tensor(8.3748e-05)\n",
            "Iteration: 17000, loss: 1.963763953790476e-05, gradient norm: tensor(9.8771e-05)\n",
            "Iteration: 18000, loss: 1.2432120964149363e-05, gradient norm: tensor(9.6395e-05)\n",
            "Iteration: 19000, loss: 3.911918285211868e-06, gradient norm: tensor(2.9396e-05)\n",
            "Iteration: 20000, loss: 9.842497169074704e-07, gradient norm: tensor(1.1364e-05)\n",
            "Iteration: 21000, loss: 3.2634866960279396e-07, gradient norm: tensor(5.5561e-06)\n",
            "Iteration: 22000, loss: 8.129350118224466e-08, gradient norm: tensor(2.2973e-06)\n",
            "Iteration: 23000, loss: 1.9722570329783195e-08, gradient norm: tensor(2.7670e-06)\n",
            "Iteration: 24000, loss: 7.007457486718493e-09, gradient norm: tensor(2.9378e-06)\n",
            "Iteration: 25000, loss: 4.94824202990074e-09, gradient norm: tensor(3.2454e-05)\n",
            "Iteration: 26000, loss: 4.787770859771001e-09, gradient norm: tensor(7.2908e-06)\n",
            "Iteration: 27000, loss: 4.7796144309053544e-09, gradient norm: tensor(1.0691e-05)\n",
            "Iteration: 28000, loss: 4.762217438614158e-09, gradient norm: tensor(7.3421e-06)\n",
            "Iteration: 29000, loss: 4.761395453911632e-09, gradient norm: tensor(9.2983e-06)\n",
            "Iteration: 30000, loss: 4.757276580669156e-09, gradient norm: tensor(2.6433e-05)\n",
            "Iteration: 31000, loss: 4.754075420443371e-09, gradient norm: tensor(5.4145e-05)\n",
            "Iteration: 32000, loss: 4.742306999094837e-09, gradient norm: tensor(5.6834e-06)\n",
            "Iteration: 33000, loss: 4.748785694896896e-09, gradient norm: tensor(2.8283e-05)\n",
            "Iteration: 34000, loss: 4.736946246808316e-09, gradient norm: tensor(1.2744e-05)\n",
            "Iteration: 35000, loss: 4.733470296613973e-09, gradient norm: tensor(3.8946e-06)\n",
            "Iteration: 36000, loss: 4.742926500433953e-09, gradient norm: tensor(4.2883e-05)\n",
            "Iteration: 37000, loss: 4.722247976207683e-09, gradient norm: tensor(6.7123e-05)\n",
            "Iteration: 38000, loss: 4.730972184230353e-09, gradient norm: tensor(3.0359e-05)\n",
            "Iteration: 39000, loss: 4.718335115949657e-09, gradient norm: tensor(4.0078e-06)\n",
            "Iteration: 40000, loss: 4.714880004197397e-09, gradient norm: tensor(1.5596e-05)\n",
            "Iteration: 41000, loss: 4.717445203805682e-09, gradient norm: tensor(2.5247e-05)\n",
            "Iteration: 42000, loss: 4.715549950962128e-09, gradient norm: tensor(5.8580e-05)\n",
            "Iteration: 43000, loss: 4.705646337477276e-09, gradient norm: tensor(5.4925e-05)\n",
            "Iteration: 44000, loss: 4.706349293392975e-09, gradient norm: tensor(6.0933e-06)\n",
            "Iteration: 45000, loss: 4.703572030084757e-09, gradient norm: tensor(8.9073e-06)\n",
            "Iteration: 46000, loss: 4.697257292907153e-09, gradient norm: tensor(7.1595e-07)\n",
            "Iteration: 0, loss: 5.055300028800964, gradient norm: tensor(4.6489)\n",
            "Iteration: 1000, loss: 3.9956127090454103, gradient norm: tensor(4.1327)\n",
            "Iteration: 2000, loss: 3.1067369134426115, gradient norm: tensor(3.6478)\n",
            "Iteration: 3000, loss: 2.359252768754959, gradient norm: tensor(3.1672)\n",
            "Iteration: 4000, loss: 1.734743930697441, gradient norm: tensor(2.6897)\n",
            "Iteration: 5000, loss: 1.221009365081787, gradient norm: tensor(2.2255)\n",
            "Iteration: 6000, loss: 0.810364266872406, gradient norm: tensor(1.7784)\n",
            "Iteration: 7000, loss: 0.49631222784519197, gradient norm: tensor(1.3537)\n",
            "Iteration: 8000, loss: 0.27125760638713836, gradient norm: tensor(0.9594)\n",
            "Iteration: 9000, loss: 0.1250547027811408, gradient norm: tensor(0.6085)\n",
            "Iteration: 10000, loss: 0.04408660747297108, gradient norm: tensor(0.3212)\n",
            "Iteration: 11000, loss: 0.009985257836757229, gradient norm: tensor(0.1228)\n",
            "Iteration: 12000, loss: 0.001094672991297557, gradient norm: tensor(0.0263)\n",
            "Iteration: 13000, loss: 6.530277759884484e-05, gradient norm: tensor(0.0021)\n",
            "Iteration: 14000, loss: 3.2020244867453586e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 15000, loss: 3.099894784827484e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 16000, loss: 3.049145581644552e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 3.0129111602946068e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 2.9625122986544738e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 2.8838719103077893e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 2.7643801271551638e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 2.5906397859216667e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 2.3524980038928334e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 2.050803515157895e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.7092233178118478e-05, gradient norm: tensor(9.9236e-05)\n",
            "Iteration: 25000, loss: 1.3793834310490638e-05, gradient norm: tensor(7.0407e-05)\n",
            "Iteration: 26000, loss: 7.609325454723148e-06, gradient norm: tensor(5.8985e-05)\n",
            "Iteration: 27000, loss: 2.7735796654724253e-06, gradient norm: tensor(1.4976e-05)\n",
            "Iteration: 28000, loss: 2.318793303857092e-06, gradient norm: tensor(1.0891e-05)\n",
            "Iteration: 29000, loss: 2.0340156268048304e-06, gradient norm: tensor(8.2443e-06)\n",
            "Iteration: 30000, loss: 1.7591178437896815e-06, gradient norm: tensor(8.4363e-06)\n",
            "Iteration: 31000, loss: 1.4936444578097508e-06, gradient norm: tensor(9.8915e-06)\n",
            "Iteration: 32000, loss: 1.2620474225286672e-06, gradient norm: tensor(1.1945e-05)\n",
            "Iteration: 33000, loss: 1.0830282359393094e-06, gradient norm: tensor(1.0824e-05)\n",
            "Iteration: 34000, loss: 9.507059389193273e-07, gradient norm: tensor(1.8523e-05)\n",
            "Iteration: 35000, loss: 8.49441660022876e-07, gradient norm: tensor(8.7384e-05)\n",
            "Iteration: 36000, loss: 7.681770518388475e-07, gradient norm: tensor(3.0935e-06)\n",
            "Iteration: 37000, loss: 7.006911810094607e-07, gradient norm: tensor(7.8165e-06)\n",
            "Iteration: 38000, loss: 6.437307766304912e-07, gradient norm: tensor(6.6572e-06)\n",
            "Iteration: 39000, loss: 5.948551203118768e-07, gradient norm: tensor(1.3495e-05)\n",
            "Iteration: 40000, loss: 5.523189956306851e-07, gradient norm: tensor(4.3860e-05)\n",
            "Iteration: 41000, loss: 5.150835561380518e-07, gradient norm: tensor(7.7854e-05)\n",
            "Iteration: 42000, loss: 4.82185637196153e-07, gradient norm: tensor(1.0587e-05)\n",
            "Iteration: 43000, loss: 4.527628058781374e-07, gradient norm: tensor(7.2519e-06)\n",
            "Iteration: 44000, loss: 4.2631278282101447e-07, gradient norm: tensor(2.3854e-06)\n",
            "Iteration: 45000, loss: 4.0268326293357857e-07, gradient norm: tensor(1.8859e-06)\n",
            "Iteration: 0, loss: 6.559189249992371, gradient norm: tensor(5.3666)\n",
            "Iteration: 1000, loss: 5.324680567264557, gradient norm: tensor(4.7909)\n",
            "Iteration: 2000, loss: 4.266248839855194, gradient norm: tensor(4.2765)\n",
            "Iteration: 3000, loss: 3.3532091748714445, gradient norm: tensor(3.7829)\n",
            "Iteration: 4000, loss: 2.573842445850372, gradient norm: tensor(3.3016)\n",
            "Iteration: 5000, loss: 1.915982785820961, gradient norm: tensor(2.8287)\n",
            "Iteration: 6000, loss: 1.3711831055879593, gradient norm: tensor(2.3636)\n",
            "Iteration: 7000, loss: 0.9324103853702546, gradient norm: tensor(1.9114)\n",
            "Iteration: 8000, loss: 0.5910194282829762, gradient norm: tensor(1.4809)\n",
            "Iteration: 9000, loss: 0.33838404466211797, gradient norm: tensor(1.0784)\n",
            "Iteration: 10000, loss: 0.16648204710334538, gradient norm: tensor(0.7125)\n",
            "Iteration: 11000, loss: 0.06492528150975704, gradient norm: tensor(0.4015)\n",
            "Iteration: 12000, loss: 0.017758831058163196, gradient norm: tensor(0.1721)\n",
            "Iteration: 13000, loss: 0.0030973275492433457, gradient norm: tensor(0.0455)\n",
            "Iteration: 14000, loss: 0.0005992284621461295, gradient norm: tensor(0.0060)\n",
            "Iteration: 15000, loss: 0.0002588155076664407, gradient norm: tensor(0.0022)\n",
            "Iteration: 16000, loss: 0.000122615843283711, gradient norm: tensor(0.0013)\n",
            "Iteration: 17000, loss: 5.4810039273434086e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 18000, loss: 2.5180164724588393e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 19000, loss: 1.6670169142344094e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.4252799855057674e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.2920799646053637e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 1.1771182950724324e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.0698373527702643e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 9.573841515702952e-06, gradient norm: tensor(9.1543e-05)\n",
            "Iteration: 25000, loss: 8.200481649510038e-06, gradient norm: tensor(7.5926e-05)\n",
            "Iteration: 26000, loss: 6.5829068885250305e-06, gradient norm: tensor(6.1824e-05)\n",
            "Iteration: 27000, loss: 4.910563032808568e-06, gradient norm: tensor(4.4775e-05)\n",
            "Iteration: 28000, loss: 3.424688698032696e-06, gradient norm: tensor(3.4725e-05)\n",
            "Iteration: 29000, loss: 2.2895601665595675e-06, gradient norm: tensor(2.4052e-05)\n",
            "Iteration: 30000, loss: 1.5302891877126967e-06, gradient norm: tensor(1.4571e-05)\n",
            "Iteration: 31000, loss: 1.0722207205162705e-06, gradient norm: tensor(1.1670e-05)\n",
            "Iteration: 32000, loss: 8.136768768736147e-07, gradient norm: tensor(5.1865e-06)\n",
            "Iteration: 33000, loss: 6.700486705426556e-07, gradient norm: tensor(2.3912e-05)\n",
            "Iteration: 34000, loss: 5.849748498576446e-07, gradient norm: tensor(3.7326e-05)\n",
            "Iteration: 35000, loss: 5.306075069597682e-07, gradient norm: tensor(3.1681e-06)\n",
            "Iteration: 36000, loss: 4.916071581533287e-07, gradient norm: tensor(3.4496e-05)\n",
            "Iteration: 37000, loss: 4.5969133162770956e-07, gradient norm: tensor(1.6726e-05)\n",
            "Iteration: 38000, loss: 4.317694905182634e-07, gradient norm: tensor(2.6971e-06)\n",
            "Iteration: 39000, loss: 4.066506656670299e-07, gradient norm: tensor(3.7774e-06)\n",
            "Iteration: 40000, loss: 3.8377813919510116e-07, gradient norm: tensor(1.0929e-05)\n",
            "Iteration: 41000, loss: 3.6300943676792487e-07, gradient norm: tensor(4.6437e-06)\n",
            "Iteration: 42000, loss: 3.4408475843861197e-07, gradient norm: tensor(2.4561e-06)\n",
            "Iteration: 43000, loss: 3.2657189007068153e-07, gradient norm: tensor(7.4734e-06)\n",
            "Iteration: 44000, loss: 3.103709799461285e-07, gradient norm: tensor(4.2408e-06)\n",
            "Iteration: 45000, loss: 2.954041010241326e-07, gradient norm: tensor(5.9863e-05)\n",
            "Iteration: 46000, loss: 2.8157757884628154e-07, gradient norm: tensor(1.8421e-05)\n",
            "Iteration: 47000, loss: 2.6864980137020214e-07, gradient norm: tensor(2.3726e-05)\n",
            "Iteration: 48000, loss: 2.5666442553529124e-07, gradient norm: tensor(1.8772e-05)\n",
            "Iteration: 49000, loss: 2.45418407928355e-07, gradient norm: tensor(1.3841e-05)\n",
            "Iteration: 50000, loss: 2.348583468574361e-07, gradient norm: tensor(6.8952e-06)\n",
            "Iteration: 51000, loss: 2.2500255798263424e-07, gradient norm: tensor(1.3518e-06)\n",
            "Iteration: 0, loss: 5.11487232208252, gradient norm: tensor(4.6687)\n",
            "Iteration: 1000, loss: 4.036094381093979, gradient norm: tensor(4.1803)\n",
            "Iteration: 2000, loss: 3.109040402650833, gradient norm: tensor(3.6821)\n",
            "Iteration: 3000, loss: 2.3362381846904756, gradient norm: tensor(3.1732)\n",
            "Iteration: 4000, loss: 1.7070620206594467, gradient norm: tensor(2.6749)\n",
            "Iteration: 5000, loss: 1.2004837958812713, gradient norm: tensor(2.2054)\n",
            "Iteration: 6000, loss: 0.7971801038980484, gradient norm: tensor(1.7601)\n",
            "Iteration: 7000, loss: 0.4873036699295044, gradient norm: tensor(1.3371)\n",
            "Iteration: 8000, loss: 0.2641656425893307, gradient norm: tensor(0.9440)\n",
            "Iteration: 9000, loss: 0.11983619437366724, gradient norm: tensor(0.5942)\n",
            "Iteration: 10000, loss: 0.041189704049378634, gradient norm: tensor(0.3093)\n",
            "Iteration: 11000, loss: 0.008992402372416108, gradient norm: tensor(0.1155)\n",
            "Iteration: 12000, loss: 0.0009473144215444336, gradient norm: tensor(0.0238)\n",
            "Iteration: 13000, loss: 6.57300315797329e-05, gradient norm: tensor(0.0018)\n",
            "Iteration: 14000, loss: 3.608071448616101e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 15000, loss: 3.197823704067559e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 16000, loss: 2.812584923412942e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 2.4825404938383145e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 2.2160660293593536e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 19000, loss: 1.957486877836345e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.687492432029103e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.4321622490570008e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 1.152631799050141e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 8.169999618075963e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 4.42424258653773e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.2594379996926365e-06, gradient norm: tensor(2.5266e-05)\n",
            "Iteration: 26000, loss: 4.810585376731069e-07, gradient norm: tensor(4.1283e-06)\n",
            "Iteration: 27000, loss: 4.1643071497787786e-07, gradient norm: tensor(1.6342e-05)\n",
            "Iteration: 28000, loss: 4.0579652431915745e-07, gradient norm: tensor(2.8247e-06)\n",
            "Iteration: 29000, loss: 3.9359764897994864e-07, gradient norm: tensor(1.5175e-05)\n",
            "Iteration: 30000, loss: 3.769695044013588e-07, gradient norm: tensor(2.2140e-06)\n",
            "Iteration: 31000, loss: 3.558335142486158e-07, gradient norm: tensor(4.0145e-05)\n",
            "Iteration: 32000, loss: 3.3119647667945174e-07, gradient norm: tensor(3.1865e-05)\n",
            "Iteration: 33000, loss: 3.06509500063612e-07, gradient norm: tensor(6.7485e-06)\n",
            "Iteration: 34000, loss: 2.858339748570415e-07, gradient norm: tensor(6.4260e-06)\n",
            "Iteration: 35000, loss: 2.689220304716855e-07, gradient norm: tensor(1.1730e-05)\n",
            "Iteration: 36000, loss: 2.545442205530435e-07, gradient norm: tensor(2.0261e-06)\n",
            "Iteration: 37000, loss: 2.419950843659535e-07, gradient norm: tensor(5.0394e-06)\n",
            "Iteration: 38000, loss: 2.3081067620012162e-07, gradient norm: tensor(1.4878e-05)\n",
            "Iteration: 39000, loss: 2.2061147201668518e-07, gradient norm: tensor(3.4429e-05)\n",
            "Iteration: 40000, loss: 2.1122059897038525e-07, gradient norm: tensor(1.5157e-06)\n",
            "Iteration: 0, loss: 2.810602955579758, gradient norm: tensor(2.5709)\n",
            "Iteration: 1000, loss: 2.0595811821222307, gradient norm: tensor(2.2225)\n",
            "Iteration: 2000, loss: 1.3921916671991348, gradient norm: tensor(1.8764)\n",
            "Iteration: 3000, loss: 0.8544159338474274, gradient norm: tensor(1.4843)\n",
            "Iteration: 4000, loss: 0.4736016463935375, gradient norm: tensor(1.0892)\n",
            "Iteration: 5000, loss: 0.23009703136980533, gradient norm: tensor(0.7279)\n",
            "Iteration: 6000, loss: 0.09134531723335386, gradient norm: tensor(0.4197)\n",
            "Iteration: 7000, loss: 0.026702899971976876, gradient norm: tensor(0.1871)\n",
            "Iteration: 8000, loss: 0.00593807472079061, gradient norm: tensor(0.0529)\n",
            "Iteration: 9000, loss: 0.0024106221559923144, gradient norm: tensor(0.0098)\n",
            "Iteration: 10000, loss: 0.0020794015426654367, gradient norm: tensor(0.0063)\n",
            "Iteration: 11000, loss: 0.001891940729925409, gradient norm: tensor(0.0053)\n",
            "Iteration: 12000, loss: 0.0016554710315540434, gradient norm: tensor(0.0045)\n",
            "Iteration: 13000, loss: 0.0013793990404810756, gradient norm: tensor(0.0036)\n",
            "Iteration: 14000, loss: 0.0011121925534680486, gradient norm: tensor(0.0026)\n",
            "Iteration: 15000, loss: 0.0009101287844823674, gradient norm: tensor(0.0018)\n",
            "Iteration: 16000, loss: 0.0007774816990131512, gradient norm: tensor(0.0014)\n",
            "Iteration: 17000, loss: 0.0006734758560778574, gradient norm: tensor(0.0012)\n",
            "Iteration: 18000, loss: 0.0005773742750170641, gradient norm: tensor(0.0009)\n",
            "Iteration: 19000, loss: 0.000492340316879563, gradient norm: tensor(0.0007)\n",
            "Iteration: 20000, loss: 0.0004198219677491579, gradient norm: tensor(0.0006)\n",
            "Iteration: 21000, loss: 0.00035737277162843384, gradient norm: tensor(0.0005)\n",
            "Iteration: 22000, loss: 0.0003010137234814465, gradient norm: tensor(0.0005)\n",
            "Iteration: 23000, loss: 0.00024766337592154743, gradient norm: tensor(0.0005)\n",
            "Iteration: 24000, loss: 0.00019660553176072427, gradient norm: tensor(0.0004)\n",
            "Iteration: 25000, loss: 0.00014960749207239133, gradient norm: tensor(0.0004)\n",
            "Iteration: 26000, loss: 0.0001099894227081677, gradient norm: tensor(0.0003)\n",
            "Iteration: 27000, loss: 8.055805777985369e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 5.9054797147837235e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 29000, loss: 3.0101833086519036e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 1.2623353371964185e-05, gradient norm: tensor(4.2943e-05)\n",
            "Iteration: 31000, loss: 7.977216643666907e-06, gradient norm: tensor(1.6812e-05)\n",
            "Iteration: 32000, loss: 6.328537214812968e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 33000, loss: 5.396755561378086e-06, gradient norm: tensor(2.6712e-05)\n",
            "Iteration: 34000, loss: 4.6114868819131515e-06, gradient norm: tensor(3.0664e-05)\n",
            "Iteration: 35000, loss: 3.911508363671601e-06, gradient norm: tensor(1.4116e-05)\n",
            "Iteration: 36000, loss: 3.306049659158816e-06, gradient norm: tensor(8.4046e-06)\n",
            "Iteration: 37000, loss: 2.7925812048579247e-06, gradient norm: tensor(7.5309e-06)\n",
            "Iteration: 38000, loss: 2.364007385722289e-06, gradient norm: tensor(2.7761e-05)\n",
            "Iteration: 39000, loss: 2.009108901461332e-06, gradient norm: tensor(6.7832e-06)\n",
            "Iteration: 40000, loss: 1.7135093305569171e-06, gradient norm: tensor(5.0325e-06)\n",
            "Iteration: 41000, loss: 1.4683163515201158e-06, gradient norm: tensor(1.0873e-05)\n",
            "Iteration: 42000, loss: 1.264334158577185e-06, gradient norm: tensor(4.3211e-06)\n",
            "Iteration: 43000, loss: 1.094910239771707e-06, gradient norm: tensor(4.5656e-06)\n",
            "Iteration: 44000, loss: 9.536559786624821e-07, gradient norm: tensor(0.0002)\n",
            "Iteration: 45000, loss: 8.361939225665082e-07, gradient norm: tensor(9.6489e-06)\n",
            "Iteration: 46000, loss: 7.382679481793275e-07, gradient norm: tensor(0.0001)\n",
            "Iteration: 47000, loss: 6.561019120567835e-07, gradient norm: tensor(1.7253e-05)\n",
            "Iteration: 48000, loss: 5.872554218626647e-07, gradient norm: tensor(7.3099e-05)\n",
            "Iteration: 49000, loss: 5.289616273103092e-07, gradient norm: tensor(3.2685e-06)\n",
            "Iteration: 50000, loss: 4.793999085563882e-07, gradient norm: tensor(1.0527e-05)\n",
            "Iteration: 51000, loss: 4.3680741720208973e-07, gradient norm: tensor(7.9944e-06)\n",
            "Iteration: 52000, loss: 4.0021611738438876e-07, gradient norm: tensor(4.6634e-05)\n",
            "Iteration: 53000, loss: 3.6859607723727094e-07, gradient norm: tensor(2.9869e-05)\n",
            "Iteration: 54000, loss: 3.4101453724133536e-07, gradient norm: tensor(1.8646e-06)\n",
            "Iteration: 0, loss: 2.9376570508480073, gradient norm: tensor(3.5596)\n",
            "Iteration: 1000, loss: 2.179061836242676, gradient norm: tensor(3.0383)\n",
            "Iteration: 2000, loss: 1.567598029255867, gradient norm: tensor(2.5478)\n",
            "Iteration: 3000, loss: 1.0781082179546355, gradient norm: tensor(2.0820)\n",
            "Iteration: 4000, loss: 0.6954951577186584, gradient norm: tensor(1.6378)\n",
            "Iteration: 5000, loss: 0.40998628789186475, gradient norm: tensor(1.2192)\n",
            "Iteration: 6000, loss: 0.21222784163057803, gradient norm: tensor(0.8358)\n",
            "Iteration: 7000, loss: 0.09016864839568735, gradient norm: tensor(0.5030)\n",
            "Iteration: 8000, loss: 0.027891922685317695, gradient norm: tensor(0.2426)\n",
            "Iteration: 9000, loss: 0.005087626792839728, gradient norm: tensor(0.0789)\n",
            "Iteration: 10000, loss: 0.00039623798855245693, gradient norm: tensor(0.0127)\n",
            "Iteration: 11000, loss: 2.661433165667404e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 12000, loss: 2.0255615900168778e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 13000, loss: 2.0105304047319804e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 14000, loss: 1.9881854810591904e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 15000, loss: 1.9529724148014792e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 16000, loss: 1.8981015507961275e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 1.8146467520637088e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 1.693339201665367e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 1.5289726861738017e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.3274638436087116e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.1114281967820716e-05, gradient norm: tensor(8.8486e-05)\n",
            "Iteration: 22000, loss: 9.154490376204194e-06, gradient norm: tensor(6.0279e-05)\n",
            "Iteration: 23000, loss: 7.6772503130087e-06, gradient norm: tensor(4.2041e-05)\n",
            "Iteration: 24000, loss: 6.712817782954516e-06, gradient norm: tensor(2.8082e-05)\n",
            "Iteration: 25000, loss: 6.0458501397988584e-06, gradient norm: tensor(2.3580e-05)\n",
            "Iteration: 26000, loss: 5.42864184217251e-06, gradient norm: tensor(1.9994e-05)\n",
            "Iteration: 27000, loss: 4.713142062428233e-06, gradient norm: tensor(1.3278e-05)\n",
            "Iteration: 28000, loss: 3.85063055387036e-06, gradient norm: tensor(1.5631e-05)\n",
            "Iteration: 29000, loss: 3.2187187155159337e-06, gradient norm: tensor(2.0782e-05)\n",
            "Iteration: 30000, loss: 2.840960232560974e-06, gradient norm: tensor(5.3088e-06)\n",
            "Iteration: 31000, loss: 2.6241284447223733e-06, gradient norm: tensor(5.3964e-06)\n",
            "Iteration: 32000, loss: 2.5130329711373635e-06, gradient norm: tensor(3.0728e-06)\n",
            "Iteration: 33000, loss: 2.4565648373027217e-06, gradient norm: tensor(5.4862e-05)\n",
            "Iteration: 34000, loss: 2.427322342100524e-06, gradient norm: tensor(3.6253e-05)\n",
            "Iteration: 35000, loss: 2.4116442261856718e-06, gradient norm: tensor(4.0704e-06)\n",
            "Iteration: 36000, loss: 2.4028800951327866e-06, gradient norm: tensor(8.5129e-06)\n",
            "Iteration: 37000, loss: 2.3977965097401464e-06, gradient norm: tensor(1.0514e-05)\n",
            "Iteration: 38000, loss: 2.394769805505348e-06, gradient norm: tensor(6.8211e-06)\n",
            "Iteration: 39000, loss: 2.3929376015985325e-06, gradient norm: tensor(1.6838e-05)\n",
            "Iteration: 40000, loss: 2.3918128031255037e-06, gradient norm: tensor(6.0970e-05)\n",
            "Iteration: 41000, loss: 2.391098252019219e-06, gradient norm: tensor(6.9648e-06)\n",
            "Iteration: 42000, loss: 2.3906524886569967e-06, gradient norm: tensor(1.0472e-05)\n",
            "Iteration: 43000, loss: 2.390353690998381e-06, gradient norm: tensor(5.7873e-06)\n",
            "Iteration: 44000, loss: 2.3901792478682183e-06, gradient norm: tensor(3.8340e-06)\n",
            "Iteration: 45000, loss: 2.390051473867061e-06, gradient norm: tensor(8.5061e-06)\n",
            "Iteration: 46000, loss: 2.3899709676697966e-06, gradient norm: tensor(9.9924e-06)\n",
            "Iteration: 47000, loss: 2.3899197333321353e-06, gradient norm: tensor(6.0639e-06)\n",
            "Iteration: 48000, loss: 2.3898678396108155e-06, gradient norm: tensor(2.0547e-06)\n",
            "Iteration: 49000, loss: 2.3898475494661396e-06, gradient norm: tensor(3.6295e-06)\n",
            "Iteration: 50000, loss: 2.3898310346339712e-06, gradient norm: tensor(1.5867e-05)\n",
            "Iteration: 51000, loss: 2.3898017318515485e-06, gradient norm: tensor(2.0131e-05)\n",
            "Iteration: 52000, loss: 2.389796522720644e-06, gradient norm: tensor(1.7606e-05)\n",
            "Iteration: 53000, loss: 2.3897843220765934e-06, gradient norm: tensor(6.7491e-06)\n",
            "Iteration: 54000, loss: 2.389765643101782e-06, gradient norm: tensor(1.0407e-05)\n",
            "Iteration: 55000, loss: 2.389760283222131e-06, gradient norm: tensor(1.1490e-05)\n",
            "Iteration: 56000, loss: 2.3897495118490043e-06, gradient norm: tensor(2.8150e-06)\n",
            "Iteration: 57000, loss: 2.3897401210888346e-06, gradient norm: tensor(5.2324e-05)\n",
            "Iteration: 58000, loss: 2.3897436617517088e-06, gradient norm: tensor(4.4845e-05)\n",
            "Iteration: 59000, loss: 2.3897203013802937e-06, gradient norm: tensor(2.2511e-05)\n",
            "Iteration: 60000, loss: 2.3897197654605405e-06, gradient norm: tensor(4.0545e-06)\n",
            "Iteration: 61000, loss: 2.3897068422229495e-06, gradient norm: tensor(1.0852e-06)\n",
            "Iteration: 0, loss: 5.46020094871521, gradient norm: tensor(4.9332)\n",
            "Iteration: 1000, loss: 4.366588633298874, gradient norm: tensor(4.3784)\n",
            "Iteration: 2000, loss: 3.4524130945205687, gradient norm: tensor(3.8687)\n",
            "Iteration: 3000, loss: 2.6711998171806335, gradient norm: tensor(3.3771)\n",
            "Iteration: 4000, loss: 2.0050201451778413, gradient norm: tensor(2.8966)\n",
            "Iteration: 5000, loss: 1.4469164440631865, gradient norm: tensor(2.4276)\n",
            "Iteration: 6000, loss: 0.9922000153660774, gradient norm: tensor(1.9734)\n",
            "Iteration: 7000, loss: 0.6358481187820435, gradient norm: tensor(1.5389)\n",
            "Iteration: 8000, loss: 0.3711951556801796, gradient norm: tensor(1.1314)\n",
            "Iteration: 9000, loss: 0.18954644954204558, gradient norm: tensor(0.7614)\n",
            "Iteration: 10000, loss: 0.0793525318801403, gradient norm: tensor(0.4438)\n",
            "Iteration: 11000, loss: 0.025070683846250177, gradient norm: tensor(0.2016)\n",
            "Iteration: 12000, loss: 0.006240760221378878, gradient norm: tensor(0.0586)\n",
            "Iteration: 13000, loss: 0.002355377395171672, gradient norm: tensor(0.0090)\n",
            "Iteration: 14000, loss: 0.0016433904629666357, gradient norm: tensor(0.0049)\n",
            "Iteration: 15000, loss: 0.0012984465366462246, gradient norm: tensor(0.0049)\n",
            "Iteration: 16000, loss: 0.0010533749348251148, gradient norm: tensor(0.0047)\n",
            "Iteration: 17000, loss: 0.0008261532955802977, gradient norm: tensor(0.0042)\n",
            "Iteration: 18000, loss: 0.0005974343590205535, gradient norm: tensor(0.0034)\n",
            "Iteration: 19000, loss: 0.00039837524117319846, gradient norm: tensor(0.0023)\n",
            "Iteration: 20000, loss: 0.00026911042707797604, gradient norm: tensor(0.0013)\n",
            "Iteration: 21000, loss: 0.00021296236288617366, gradient norm: tensor(0.0006)\n",
            "Iteration: 22000, loss: 0.00019005694959196262, gradient norm: tensor(0.0005)\n",
            "Iteration: 23000, loss: 0.0001688770153123187, gradient norm: tensor(0.0004)\n",
            "Iteration: 24000, loss: 0.0001440177971671801, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 0.00011766280375741189, gradient norm: tensor(0.0003)\n",
            "Iteration: 26000, loss: 9.269929613219574e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 7.128975708474173e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 5.447295970225241e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 29000, loss: 4.224746125692036e-05, gradient norm: tensor(8.7734e-05)\n",
            "Iteration: 30000, loss: 3.366891349287471e-05, gradient norm: tensor(7.2528e-05)\n",
            "Iteration: 31000, loss: 2.7252475812929333e-05, gradient norm: tensor(6.0029e-05)\n",
            "Iteration: 32000, loss: 2.2020544642145978e-05, gradient norm: tensor(5.1596e-05)\n",
            "Iteration: 33000, loss: 1.769124776183162e-05, gradient norm: tensor(6.9752e-05)\n",
            "Iteration: 34000, loss: 1.4152497434224643e-05, gradient norm: tensor(3.4661e-05)\n",
            "Iteration: 35000, loss: 1.1300177120574518e-05, gradient norm: tensor(2.8579e-05)\n",
            "Iteration: 36000, loss: 9.034204324962047e-06, gradient norm: tensor(2.3918e-05)\n",
            "Iteration: 37000, loss: 7.263836641413945e-06, gradient norm: tensor(1.8745e-05)\n",
            "Iteration: 38000, loss: 5.805420722026611e-06, gradient norm: tensor(1.7901e-05)\n",
            "Iteration: 39000, loss: 4.24147018225085e-06, gradient norm: tensor(1.5845e-05)\n",
            "Iteration: 40000, loss: 3.1488182419252554e-06, gradient norm: tensor(1.0840e-05)\n",
            "Iteration: 41000, loss: 2.3458988719085026e-06, gradient norm: tensor(1.0744e-05)\n",
            "Iteration: 42000, loss: 1.7348097788953964e-06, gradient norm: tensor(8.9029e-06)\n",
            "Iteration: 43000, loss: 1.2776380453942694e-06, gradient norm: tensor(1.9744e-05)\n",
            "Iteration: 44000, loss: 9.407594608887849e-07, gradient norm: tensor(2.7279e-05)\n",
            "Iteration: 45000, loss: 6.967057548195043e-07, gradient norm: tensor(7.1486e-06)\n",
            "Iteration: 46000, loss: 5.210635056016599e-07, gradient norm: tensor(1.9262e-05)\n",
            "Iteration: 47000, loss: 3.940372357362776e-07, gradient norm: tensor(4.7311e-06)\n",
            "Iteration: 48000, loss: 3.0196802310911153e-07, gradient norm: tensor(3.0827e-06)\n",
            "Iteration: 49000, loss: 2.34196303225076e-07, gradient norm: tensor(8.8836e-06)\n",
            "Iteration: 50000, loss: 1.8379328204787271e-07, gradient norm: tensor(2.0443e-05)\n",
            "Iteration: 51000, loss: 1.4588308708596287e-07, gradient norm: tensor(1.5432e-06)\n",
            "Iteration: 0, loss: 4.011236627340317, gradient norm: tensor(4.1975)\n",
            "Iteration: 1000, loss: 3.1079403812885285, gradient norm: tensor(3.6660)\n",
            "Iteration: 2000, loss: 2.3581442174911498, gradient norm: tensor(3.1670)\n",
            "Iteration: 3000, loss: 1.7321770404577255, gradient norm: tensor(2.6861)\n",
            "Iteration: 4000, loss: 1.216904531955719, gradient norm: tensor(2.2207)\n",
            "Iteration: 5000, loss: 0.806233128786087, gradient norm: tensor(1.7723)\n",
            "Iteration: 6000, loss: 0.4933913314342499, gradient norm: tensor(1.3467)\n",
            "Iteration: 7000, loss: 0.2699729642421007, gradient norm: tensor(0.9526)\n",
            "Iteration: 8000, loss: 0.12534559945017099, gradient norm: tensor(0.6029)\n",
            "Iteration: 9000, loss: 0.04524923696741462, gradient norm: tensor(0.3172)\n",
            "Iteration: 10000, loss: 0.01133720441441983, gradient norm: tensor(0.1207)\n",
            "Iteration: 11000, loss: 0.0022388713146792726, gradient norm: tensor(0.0260)\n",
            "Iteration: 12000, loss: 0.0009055010318406858, gradient norm: tensor(0.0044)\n",
            "Iteration: 13000, loss: 0.0006307230449747294, gradient norm: tensor(0.0036)\n",
            "Iteration: 14000, loss: 0.00045300533031695524, gradient norm: tensor(0.0031)\n",
            "Iteration: 15000, loss: 0.00032060743065085264, gradient norm: tensor(0.0025)\n",
            "Iteration: 16000, loss: 0.00022409568264265546, gradient norm: tensor(0.0019)\n",
            "Iteration: 17000, loss: 0.00016080640735162887, gradient norm: tensor(0.0012)\n",
            "Iteration: 18000, loss: 0.0001276180885542999, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 0.0001132556563388789, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 0.00010307292980724015, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 9.109183967666468e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 7.772653419669951e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 6.45997718675062e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 5.29270635888679e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 4.286708046493004e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 3.4097362355169025e-05, gradient norm: tensor(8.3858e-05)\n",
            "Iteration: 27000, loss: 2.6662468431823072e-05, gradient norm: tensor(6.2883e-05)\n",
            "Iteration: 28000, loss: 2.0609240789781325e-05, gradient norm: tensor(4.7342e-05)\n",
            "Iteration: 29000, loss: 1.5973149621459016e-05, gradient norm: tensor(4.2063e-05)\n",
            "Iteration: 30000, loss: 1.2508406320193899e-05, gradient norm: tensor(3.0660e-05)\n",
            "Iteration: 31000, loss: 9.794921886168594e-06, gradient norm: tensor(2.5403e-05)\n",
            "Iteration: 32000, loss: 7.598911191053049e-06, gradient norm: tensor(2.0767e-05)\n",
            "Iteration: 33000, loss: 5.8329390067228816e-06, gradient norm: tensor(1.6716e-05)\n",
            "Iteration: 34000, loss: 4.4361934751577795e-06, gradient norm: tensor(3.2852e-05)\n",
            "Iteration: 35000, loss: 3.348217614529858e-06, gradient norm: tensor(1.3241e-05)\n",
            "Iteration: 36000, loss: 2.508427208113062e-06, gradient norm: tensor(1.8244e-05)\n",
            "Iteration: 37000, loss: 1.8698391546649874e-06, gradient norm: tensor(7.7925e-06)\n",
            "Iteration: 38000, loss: 1.3946212648079382e-06, gradient norm: tensor(1.2754e-05)\n",
            "Iteration: 39000, loss: 1.0484958864935834e-06, gradient norm: tensor(1.0501e-05)\n",
            "Iteration: 40000, loss: 8.015549541937617e-07, gradient norm: tensor(7.2235e-06)\n",
            "Iteration: 41000, loss: 6.26947267221567e-07, gradient norm: tensor(2.4499e-05)\n",
            "Iteration: 42000, loss: 5.028303665142175e-07, gradient norm: tensor(8.2668e-06)\n",
            "Iteration: 43000, loss: 4.1376336827170234e-07, gradient norm: tensor(9.1239e-06)\n",
            "Iteration: 44000, loss: 3.4914046153744494e-07, gradient norm: tensor(4.0187e-06)\n",
            "Iteration: 45000, loss: 3.0116312990458026e-07, gradient norm: tensor(3.0482e-06)\n",
            "Iteration: 46000, loss: 2.649107501326853e-07, gradient norm: tensor(3.4097e-05)\n",
            "Iteration: 47000, loss: 2.3676324806842786e-07, gradient norm: tensor(3.1813e-06)\n",
            "Iteration: 48000, loss: 2.146392335191649e-07, gradient norm: tensor(7.3247e-06)\n",
            "Iteration: 49000, loss: 1.9684310954914963e-07, gradient norm: tensor(2.6900e-05)\n",
            "Iteration: 50000, loss: 1.8225463868759561e-07, gradient norm: tensor(3.6065e-05)\n",
            "Iteration: 51000, loss: 1.7010887363255734e-07, gradient norm: tensor(8.3708e-05)\n",
            "Iteration: 52000, loss: 1.598919099166096e-07, gradient norm: tensor(3.0362e-06)\n",
            "Iteration: 53000, loss: 1.5100266459455725e-07, gradient norm: tensor(3.1764e-05)\n",
            "Iteration: 54000, loss: 1.4323646965408443e-07, gradient norm: tensor(4.4935e-05)\n",
            "Iteration: 55000, loss: 1.3638597062026747e-07, gradient norm: tensor(2.8929e-05)\n",
            "Iteration: 56000, loss: 1.3023116497379305e-07, gradient norm: tensor(6.7280e-06)\n",
            "Iteration: 57000, loss: 1.246745260488069e-07, gradient norm: tensor(4.0362e-06)\n",
            "Iteration: 58000, loss: 1.1958596321903769e-07, gradient norm: tensor(8.1681e-06)\n",
            "Iteration: 59000, loss: 1.1492475142915737e-07, gradient norm: tensor(3.4228e-05)\n",
            "Iteration: 60000, loss: 1.1059031913873696e-07, gradient norm: tensor(6.1281e-07)\n",
            "Iteration: 0, loss: 5.767491598129272, gradient norm: tensor(5.3524)\n",
            "Iteration: 1000, loss: 4.588527436733246, gradient norm: tensor(4.6467)\n",
            "Iteration: 2000, loss: 3.622914092063904, gradient norm: tensor(3.9939)\n",
            "Iteration: 3000, loss: 2.8319450578689573, gradient norm: tensor(3.3980)\n",
            "Iteration: 4000, loss: 2.1737178320884705, gradient norm: tensor(2.8854)\n",
            "Iteration: 5000, loss: 1.612069672703743, gradient norm: tensor(2.4485)\n",
            "Iteration: 6000, loss: 1.1328848364949227, gradient norm: tensor(2.0374)\n",
            "Iteration: 7000, loss: 0.7369257270097732, gradient norm: tensor(1.6396)\n",
            "Iteration: 8000, loss: 0.4374042757749558, gradient norm: tensor(1.2339)\n",
            "Iteration: 9000, loss: 0.23219821189343928, gradient norm: tensor(0.8496)\n",
            "Iteration: 10000, loss: 0.10480268926918507, gradient norm: tensor(0.5156)\n",
            "Iteration: 11000, loss: 0.03658402870409191, gradient norm: tensor(0.2529)\n",
            "Iteration: 12000, loss: 0.008289158426225185, gradient norm: tensor(0.0833)\n",
            "Iteration: 13000, loss: 0.0012336086757131852, gradient norm: tensor(0.0133)\n",
            "Iteration: 14000, loss: 0.0004072670867026318, gradient norm: tensor(0.0016)\n",
            "Iteration: 15000, loss: 0.00032192326022777704, gradient norm: tensor(0.0012)\n",
            "Iteration: 16000, loss: 0.000259196767656249, gradient norm: tensor(0.0012)\n",
            "Iteration: 17000, loss: 0.0001755051797372289, gradient norm: tensor(0.0009)\n",
            "Iteration: 18000, loss: 9.462562909175176e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 19000, loss: 4.861189808434574e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 3.439226967020659e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 3.162147040347918e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 3.0209165255655536e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 2.856796443120402e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 2.6620703747539663e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 2.4302125071699266e-05, gradient norm: tensor(8.8208e-05)\n",
            "Iteration: 26000, loss: 2.1520862568650045e-05, gradient norm: tensor(7.4736e-05)\n",
            "Iteration: 27000, loss: 1.831452242913656e-05, gradient norm: tensor(6.0959e-05)\n",
            "Iteration: 28000, loss: 1.496136734931497e-05, gradient norm: tensor(5.0687e-05)\n",
            "Iteration: 29000, loss: 1.1745876512577524e-05, gradient norm: tensor(3.8737e-05)\n",
            "Iteration: 30000, loss: 8.799692389857227e-06, gradient norm: tensor(3.0191e-05)\n",
            "Iteration: 31000, loss: 6.287763481850561e-06, gradient norm: tensor(2.3904e-05)\n",
            "Iteration: 32000, loss: 4.242948726641771e-06, gradient norm: tensor(1.6454e-05)\n",
            "Iteration: 33000, loss: 2.7073994140209834e-06, gradient norm: tensor(1.2188e-05)\n",
            "Iteration: 34000, loss: 1.6211428945780426e-06, gradient norm: tensor(1.4764e-05)\n",
            "Iteration: 35000, loss: 9.344274199065695e-07, gradient norm: tensor(2.5698e-05)\n",
            "Iteration: 36000, loss: 5.514881982833231e-07, gradient norm: tensor(2.0233e-05)\n",
            "Iteration: 37000, loss: 3.4134602364588316e-07, gradient norm: tensor(2.4715e-05)\n",
            "Iteration: 38000, loss: 1.9735344395144238e-07, gradient norm: tensor(7.6104e-06)\n",
            "Iteration: 39000, loss: 1.0793933917341292e-07, gradient norm: tensor(1.1819e-05)\n",
            "Iteration: 40000, loss: 7.852712055722577e-08, gradient norm: tensor(6.0830e-05)\n",
            "Iteration: 41000, loss: 6.887225236340555e-08, gradient norm: tensor(1.6832e-06)\n",
            "Iteration: 0, loss: 0.23436024242639542, gradient norm: tensor(0.6627)\n",
            "Iteration: 1000, loss: 0.0651808826290071, gradient norm: tensor(0.2776)\n",
            "Iteration: 2000, loss: 0.01483115245681256, gradient norm: tensor(0.0699)\n",
            "Iteration: 3000, loss: 0.007352817972656339, gradient norm: tensor(0.0365)\n",
            "Iteration: 4000, loss: 0.005124002636875957, gradient norm: tensor(0.0293)\n",
            "Iteration: 5000, loss: 0.003053704690886661, gradient norm: tensor(0.0212)\n",
            "Iteration: 6000, loss: 0.0014395613772794605, gradient norm: tensor(0.0133)\n",
            "Iteration: 7000, loss: 0.0004926579632883658, gradient norm: tensor(0.0066)\n",
            "Iteration: 8000, loss: 0.00011414759508261341, gradient norm: tensor(0.0023)\n",
            "Iteration: 9000, loss: 2.761785142683948e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 10000, loss: 1.3502517720553442e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 11000, loss: 7.988323834069888e-06, gradient norm: tensor(0.0002)\n",
            "Iteration: 12000, loss: 5.535263499041321e-06, gradient norm: tensor(8.1669e-05)\n",
            "Iteration: 13000, loss: 4.602687731676269e-06, gradient norm: tensor(5.8215e-05)\n",
            "Iteration: 14000, loss: 3.813361053744302e-06, gradient norm: tensor(5.2224e-05)\n",
            "Iteration: 15000, loss: 2.8169566846827364e-06, gradient norm: tensor(4.3384e-05)\n",
            "Iteration: 16000, loss: 1.7671840339517076e-06, gradient norm: tensor(3.0141e-05)\n",
            "Iteration: 17000, loss: 9.622938501934187e-07, gradient norm: tensor(1.6893e-05)\n",
            "Iteration: 18000, loss: 4.967486064231252e-07, gradient norm: tensor(8.4084e-06)\n",
            "Iteration: 19000, loss: 2.521781609630125e-07, gradient norm: tensor(4.4824e-06)\n",
            "Iteration: 20000, loss: 1.3798723970381844e-07, gradient norm: tensor(2.9797e-06)\n",
            "Iteration: 21000, loss: 9.403336303392962e-08, gradient norm: tensor(3.2305e-05)\n",
            "Iteration: 22000, loss: 7.623739551121388e-08, gradient norm: tensor(3.3081e-06)\n",
            "Iteration: 23000, loss: 6.392324432624719e-08, gradient norm: tensor(1.4302e-05)\n",
            "Iteration: 24000, loss: 5.4180028886463564e-08, gradient norm: tensor(1.4467e-05)\n",
            "Iteration: 25000, loss: 4.753442312477319e-08, gradient norm: tensor(2.8220e-05)\n",
            "Iteration: 26000, loss: 4.321775675464323e-08, gradient norm: tensor(2.7323e-05)\n",
            "Iteration: 27000, loss: 4.049843895614913e-08, gradient norm: tensor(1.5558e-05)\n",
            "Iteration: 28000, loss: 3.879219476843332e-08, gradient norm: tensor(2.5817e-05)\n",
            "Iteration: 29000, loss: 3.766730381826733e-08, gradient norm: tensor(3.8388e-06)\n",
            "Iteration: 30000, loss: 3.687235943061751e-08, gradient norm: tensor(3.7097e-06)\n",
            "Iteration: 31000, loss: 3.6254006555935805e-08, gradient norm: tensor(2.1652e-06)\n",
            "Iteration: 32000, loss: 3.57361030545178e-08, gradient norm: tensor(3.2218e-06)\n",
            "Iteration: 33000, loss: 3.5269379306868133e-08, gradient norm: tensor(9.6037e-06)\n",
            "Iteration: 34000, loss: 3.483383212810054e-08, gradient norm: tensor(2.3121e-05)\n",
            "Iteration: 35000, loss: 3.440309847135836e-08, gradient norm: tensor(1.5704e-06)\n",
            "Iteration: 0, loss: 5.622986946582794, gradient norm: tensor(5.0401)\n",
            "Iteration: 1000, loss: 4.458936237096786, gradient norm: tensor(4.4493)\n",
            "Iteration: 2000, loss: 3.4767916152477265, gradient norm: tensor(3.8981)\n",
            "Iteration: 3000, loss: 2.648859661579132, gradient norm: tensor(3.3738)\n",
            "Iteration: 4000, loss: 1.9655211389064788, gradient norm: tensor(2.8712)\n",
            "Iteration: 5000, loss: 1.4123244739770888, gradient norm: tensor(2.3959)\n",
            "Iteration: 6000, loss: 0.9670812333226204, gradient norm: tensor(1.9457)\n",
            "Iteration: 7000, loss: 0.6179936989545822, gradient norm: tensor(1.5161)\n",
            "Iteration: 8000, loss: 0.35865957477688787, gradient norm: tensor(1.1125)\n",
            "Iteration: 9000, loss: 0.18090368092805148, gradient norm: tensor(0.7444)\n",
            "Iteration: 10000, loss: 0.07391296526789665, gradient norm: tensor(0.4286)\n",
            "Iteration: 11000, loss: 0.022159594017080963, gradient norm: tensor(0.1906)\n",
            "Iteration: 12000, loss: 0.0048355055537540464, gradient norm: tensor(0.0538)\n",
            "Iteration: 13000, loss: 0.001469045877456665, gradient norm: tensor(0.0093)\n",
            "Iteration: 14000, loss: 0.0008919843744370155, gradient norm: tensor(0.0053)\n",
            "Iteration: 15000, loss: 0.0006078738832729869, gradient norm: tensor(0.0041)\n",
            "Iteration: 16000, loss: 0.0004281262249860447, gradient norm: tensor(0.0029)\n",
            "Iteration: 17000, loss: 0.000324900596315274, gradient norm: tensor(0.0020)\n",
            "Iteration: 18000, loss: 0.00027284303816850296, gradient norm: tensor(0.0014)\n",
            "Iteration: 19000, loss: 0.00024458429247897583, gradient norm: tensor(0.0010)\n",
            "Iteration: 20000, loss: 0.000219085422824719, gradient norm: tensor(0.0009)\n",
            "Iteration: 21000, loss: 0.0001883523436554242, gradient norm: tensor(0.0008)\n",
            "Iteration: 22000, loss: 0.00015309231655555777, gradient norm: tensor(0.0006)\n",
            "Iteration: 23000, loss: 0.0001176144731871318, gradient norm: tensor(0.0005)\n",
            "Iteration: 24000, loss: 8.680642199033173e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 6.364080202547484e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 4.8210203167400324e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 27000, loss: 3.844481489795726e-05, gradient norm: tensor(9.2754e-05)\n",
            "Iteration: 28000, loss: 3.17024881587713e-05, gradient norm: tensor(7.3902e-05)\n",
            "Iteration: 29000, loss: 2.6222277607303113e-05, gradient norm: tensor(5.7380e-05)\n",
            "Iteration: 30000, loss: 2.142993067354837e-05, gradient norm: tensor(4.7871e-05)\n",
            "Iteration: 31000, loss: 1.729723012249451e-05, gradient norm: tensor(4.0111e-05)\n",
            "Iteration: 32000, loss: 1.3841479261827771e-05, gradient norm: tensor(3.3544e-05)\n",
            "Iteration: 33000, loss: 1.102805043956323e-05, gradient norm: tensor(2.7034e-05)\n",
            "Iteration: 34000, loss: 8.791865460807457e-06, gradient norm: tensor(2.8746e-05)\n",
            "Iteration: 35000, loss: 7.0459835010296954e-06, gradient norm: tensor(3.3512e-05)\n",
            "Iteration: 36000, loss: 5.328559567260527e-06, gradient norm: tensor(1.5804e-05)\n",
            "Iteration: 37000, loss: 3.843884584512125e-06, gradient norm: tensor(1.2137e-05)\n",
            "Iteration: 38000, loss: 2.869724412676078e-06, gradient norm: tensor(2.1395e-05)\n",
            "Iteration: 39000, loss: 2.1432185382082026e-06, gradient norm: tensor(8.0324e-06)\n",
            "Iteration: 40000, loss: 1.602025208512714e-06, gradient norm: tensor(2.1791e-05)\n",
            "Iteration: 41000, loss: 1.206326121746315e-06, gradient norm: tensor(1.0527e-05)\n",
            "Iteration: 42000, loss: 9.235911091991511e-07, gradient norm: tensor(9.1766e-06)\n",
            "Iteration: 43000, loss: 7.244094275051793e-07, gradient norm: tensor(1.5924e-05)\n",
            "Iteration: 44000, loss: 5.836569527559731e-07, gradient norm: tensor(6.4283e-06)\n",
            "Iteration: 45000, loss: 4.838620917269054e-07, gradient norm: tensor(3.1772e-06)\n",
            "Iteration: 46000, loss: 4.117859177199534e-07, gradient norm: tensor(8.5279e-06)\n",
            "Iteration: 47000, loss: 3.586863947191432e-07, gradient norm: tensor(2.8372e-06)\n",
            "Iteration: 48000, loss: 3.1872489165607476e-07, gradient norm: tensor(7.9598e-06)\n",
            "Iteration: 49000, loss: 2.8803687939671364e-07, gradient norm: tensor(7.2512e-05)\n",
            "Iteration: 50000, loss: 2.6365616653833966e-07, gradient norm: tensor(1.7574e-05)\n",
            "Iteration: 51000, loss: 2.438146849073064e-07, gradient norm: tensor(2.5382e-06)\n",
            "Iteration: 52000, loss: 2.2744300855492838e-07, gradient norm: tensor(1.2538e-05)\n",
            "Iteration: 53000, loss: 2.1366229513830604e-07, gradient norm: tensor(3.3626e-05)\n",
            "Iteration: 54000, loss: 2.0183295885090046e-07, gradient norm: tensor(8.1827e-06)\n",
            "Iteration: 55000, loss: 1.9152881597506167e-07, gradient norm: tensor(5.2220e-06)\n",
            "Iteration: 56000, loss: 1.823814675532276e-07, gradient norm: tensor(8.8021e-06)\n",
            "Iteration: 57000, loss: 1.7417660740193242e-07, gradient norm: tensor(2.8189e-05)\n",
            "Iteration: 58000, loss: 1.6674444307795966e-07, gradient norm: tensor(3.8099e-06)\n",
            "Iteration: 59000, loss: 1.5991406381488103e-07, gradient norm: tensor(8.5807e-06)\n",
            "Iteration: 60000, loss: 1.5362509712701922e-07, gradient norm: tensor(5.3641e-06)\n",
            "Iteration: 61000, loss: 1.4774750047763518e-07, gradient norm: tensor(2.5269e-06)\n",
            "Iteration: 62000, loss: 1.4228585570208452e-07, gradient norm: tensor(1.0549e-05)\n",
            "Iteration: 63000, loss: 1.371436107859836e-07, gradient norm: tensor(5.2214e-06)\n",
            "Iteration: 64000, loss: 1.3229653453095126e-07, gradient norm: tensor(1.1114e-05)\n",
            "Iteration: 65000, loss: 1.2770510365101017e-07, gradient norm: tensor(4.1328e-05)\n",
            "Iteration: 66000, loss: 1.2336230686571525e-07, gradient norm: tensor(1.9347e-06)\n",
            "Iteration: 0, loss: 5.4981139073371885, gradient norm: tensor(4.7790)\n",
            "Iteration: 1000, loss: 4.446981777667999, gradient norm: tensor(4.1888)\n",
            "Iteration: 2000, loss: 3.5634041633605955, gradient norm: tensor(3.6478)\n",
            "Iteration: 3000, loss: 2.8084860849380493, gradient norm: tensor(3.1776)\n",
            "Iteration: 4000, loss: 2.155889242887497, gradient norm: tensor(2.7542)\n",
            "Iteration: 5000, loss: 1.5843028899431228, gradient norm: tensor(2.3847)\n",
            "Iteration: 6000, loss: 1.0785512437820435, gradient norm: tensor(1.9949)\n",
            "Iteration: 7000, loss: 0.6676980255246162, gradient norm: tensor(1.5682)\n",
            "Iteration: 8000, loss: 0.37094736099243164, gradient norm: tensor(1.1422)\n",
            "Iteration: 9000, loss: 0.18089197370409965, gradient norm: tensor(0.7587)\n",
            "Iteration: 10000, loss: 0.07194699510931969, gradient norm: tensor(0.4363)\n",
            "Iteration: 11000, loss: 0.02031130396388471, gradient norm: tensor(0.1949)\n",
            "Iteration: 12000, loss: 0.0033934898063889703, gradient norm: tensor(0.0554)\n",
            "Iteration: 13000, loss: 0.0004393508192442823, gradient norm: tensor(0.0072)\n",
            "Iteration: 14000, loss: 0.00021683712452067992, gradient norm: tensor(0.0007)\n",
            "Iteration: 15000, loss: 0.00019337987367180177, gradient norm: tensor(0.0005)\n",
            "Iteration: 16000, loss: 0.00018480824578728062, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 0.0001799140131479362, gradient norm: tensor(0.0005)\n",
            "Iteration: 18000, loss: 0.000174602879138547, gradient norm: tensor(0.0005)\n",
            "Iteration: 19000, loss: 0.00016647213135729543, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 0.00015412678009306546, gradient norm: tensor(0.0005)\n",
            "Iteration: 21000, loss: 0.00013805597966711503, gradient norm: tensor(0.0005)\n",
            "Iteration: 22000, loss: 0.00011470891780481907, gradient norm: tensor(0.0004)\n",
            "Iteration: 23000, loss: 7.452701749571133e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 24000, loss: 4.5798769326211187e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 2.36248225937743e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 1.151118543202756e-05, gradient norm: tensor(8.8032e-05)\n",
            "Iteration: 27000, loss: 6.504722679437691e-06, gradient norm: tensor(6.6978e-05)\n",
            "Iteration: 28000, loss: 3.808344056324131e-06, gradient norm: tensor(3.2687e-05)\n",
            "Iteration: 29000, loss: 2.196238194073885e-06, gradient norm: tensor(2.1346e-05)\n",
            "Iteration: 30000, loss: 1.2502271252401442e-06, gradient norm: tensor(1.2610e-05)\n",
            "Iteration: 31000, loss: 7.154582533530629e-07, gradient norm: tensor(7.8013e-06)\n",
            "Iteration: 32000, loss: 4.210846488774678e-07, gradient norm: tensor(5.5411e-06)\n",
            "Iteration: 33000, loss: 2.6293934482168877e-07, gradient norm: tensor(4.6766e-06)\n",
            "Iteration: 34000, loss: 1.8100484810190664e-07, gradient norm: tensor(1.8605e-05)\n",
            "Iteration: 35000, loss: 1.397225655210832e-07, gradient norm: tensor(1.2097e-06)\n",
            "Iteration: 0, loss: 8.46866114616394, gradient norm: tensor(6.2432)\n",
            "Iteration: 1000, loss: 7.115265445709229, gradient norm: tensor(5.6916)\n",
            "Iteration: 2000, loss: 5.916933398246766, gradient norm: tensor(5.1636)\n",
            "Iteration: 3000, loss: 4.851614583015442, gradient norm: tensor(4.6504)\n",
            "Iteration: 4000, loss: 3.9071818861961365, gradient norm: tensor(4.1493)\n",
            "Iteration: 5000, loss: 3.076432430028915, gradient norm: tensor(3.6560)\n",
            "Iteration: 6000, loss: 2.3561854803562166, gradient norm: tensor(3.1710)\n",
            "Iteration: 7000, loss: 1.743270116329193, gradient norm: tensor(2.6959)\n",
            "Iteration: 8000, loss: 1.2341063007116317, gradient norm: tensor(2.2342)\n",
            "Iteration: 9000, loss: 0.8241947047114372, gradient norm: tensor(1.7911)\n",
            "Iteration: 10000, loss: 0.5078255304396152, gradient norm: tensor(1.3684)\n",
            "Iteration: 11000, loss: 0.27906665500998495, gradient norm: tensor(0.9726)\n",
            "Iteration: 12000, loss: 0.12970065809041262, gradient norm: tensor(0.6189)\n",
            "Iteration: 13000, loss: 0.04641759774647653, gradient norm: tensor(0.3285)\n",
            "Iteration: 14000, loss: 0.01086775564146228, gradient norm: tensor(0.1270)\n",
            "Iteration: 15000, loss: 0.0012993716791970655, gradient norm: tensor(0.0278)\n",
            "Iteration: 16000, loss: 7.613195134945271e-05, gradient norm: tensor(0.0023)\n",
            "Iteration: 17000, loss: 1.3995164120387926e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 1.13821662735063e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 1.1032964557671221e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.0612761932861758e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 9.915036063830484e-06, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 8.08798048637982e-06, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 1.9631924943723788e-06, gradient norm: tensor(6.4286e-05)\n",
            "Iteration: 24000, loss: 4.0019686184677994e-07, gradient norm: tensor(3.6806e-05)\n",
            "Iteration: 25000, loss: 1.913806868287793e-07, gradient norm: tensor(1.7459e-05)\n",
            "Iteration: 26000, loss: 1.0211764173817527e-07, gradient norm: tensor(8.7211e-06)\n",
            "Iteration: 27000, loss: 6.888753410549952e-08, gradient norm: tensor(9.1426e-06)\n",
            "Iteration: 28000, loss: 5.9364823016494487e-08, gradient norm: tensor(4.9747e-06)\n",
            "Iteration: 29000, loss: 5.6822222898489374e-08, gradient norm: tensor(6.2008e-06)\n",
            "Iteration: 30000, loss: 5.4133856291826985e-08, gradient norm: tensor(1.3801e-05)\n",
            "Iteration: 31000, loss: 5.0105855880389074e-08, gradient norm: tensor(1.8110e-06)\n",
            "Iteration: 0, loss: 2.7632458345890045, gradient norm: tensor(3.2599)\n",
            "Iteration: 1000, loss: 2.031628973484039, gradient norm: tensor(2.8254)\n",
            "Iteration: 2000, loss: 1.422319004535675, gradient norm: tensor(2.3740)\n",
            "Iteration: 3000, loss: 0.9427791971564293, gradient norm: tensor(1.9165)\n",
            "Iteration: 4000, loss: 0.5857907066345215, gradient norm: tensor(1.4776)\n",
            "Iteration: 5000, loss: 0.3311974920630455, gradient norm: tensor(1.0695)\n",
            "Iteration: 6000, loss: 0.16210124275833368, gradient norm: tensor(0.7024)\n",
            "Iteration: 7000, loss: 0.0640342849008739, gradient norm: tensor(0.3932)\n",
            "Iteration: 8000, loss: 0.01914999850373715, gradient norm: tensor(0.1676)\n",
            "Iteration: 9000, loss: 0.00542730616312474, gradient norm: tensor(0.0467)\n",
            "Iteration: 10000, loss: 0.0029844497772864996, gradient norm: tensor(0.0123)\n",
            "Iteration: 11000, loss: 0.002300597597612068, gradient norm: tensor(0.0067)\n",
            "Iteration: 12000, loss: 0.001712584130349569, gradient norm: tensor(0.0062)\n",
            "Iteration: 13000, loss: 0.001288652611314319, gradient norm: tensor(0.0056)\n",
            "Iteration: 14000, loss: 0.0010015353061025962, gradient norm: tensor(0.0045)\n",
            "Iteration: 15000, loss: 0.0007845791788422502, gradient norm: tensor(0.0034)\n",
            "Iteration: 16000, loss: 0.0006104980355012231, gradient norm: tensor(0.0025)\n",
            "Iteration: 17000, loss: 0.0004590877251466736, gradient norm: tensor(0.0019)\n",
            "Iteration: 18000, loss: 0.0003156267506419681, gradient norm: tensor(0.0015)\n",
            "Iteration: 19000, loss: 0.00019025315446197054, gradient norm: tensor(0.0011)\n",
            "Iteration: 20000, loss: 9.317229585940368e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 3.053136050493777e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 5.972983667447806e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 3.9444229893348394e-07, gradient norm: tensor(8.9199e-06)\n",
            "Iteration: 24000, loss: 8.905650007307032e-08, gradient norm: tensor(1.5361e-05)\n",
            "Iteration: 25000, loss: 7.177637677813209e-08, gradient norm: tensor(1.7609e-05)\n",
            "Iteration: 26000, loss: 5.8159956445535954e-08, gradient norm: tensor(2.2273e-06)\n",
            "Iteration: 27000, loss: 4.376327799349155e-08, gradient norm: tensor(2.9782e-06)\n",
            "Iteration: 28000, loss: 2.8589301686920975e-08, gradient norm: tensor(4.3390e-06)\n",
            "Iteration: 29000, loss: 1.544915002682501e-08, gradient norm: tensor(2.2442e-06)\n",
            "Iteration: 30000, loss: 7.031656299449907e-09, gradient norm: tensor(1.3779e-05)\n",
            "Iteration: 31000, loss: 3.2386730770550542e-09, gradient norm: tensor(5.1549e-06)\n",
            "Iteration: 32000, loss: 1.9071979113771674e-09, gradient norm: tensor(3.8944e-06)\n",
            "Iteration: 33000, loss: 1.385002174947303e-09, gradient norm: tensor(2.3703e-06)\n",
            "Iteration: 34000, loss: 1.1249923063116895e-09, gradient norm: tensor(8.4546e-07)\n",
            "Iteration: 0, loss: 3.355111891746521, gradient norm: tensor(3.6707)\n",
            "Iteration: 1000, loss: 2.5588027341365813, gradient norm: tensor(3.1612)\n",
            "Iteration: 2000, loss: 1.8939779601097106, gradient norm: tensor(2.7234)\n",
            "Iteration: 3000, loss: 1.3302844095230102, gradient norm: tensor(2.2931)\n",
            "Iteration: 4000, loss: 0.8782253211736679, gradient norm: tensor(1.8439)\n",
            "Iteration: 5000, loss: 0.5415391052365303, gradient norm: tensor(1.4120)\n",
            "Iteration: 6000, loss: 0.30034974579513074, gradient norm: tensor(1.0114)\n",
            "Iteration: 7000, loss: 0.14118217351287604, gradient norm: tensor(0.6510)\n",
            "Iteration: 8000, loss: 0.05159333741106093, gradient norm: tensor(0.3516)\n",
            "Iteration: 9000, loss: 0.012911511609330774, gradient norm: tensor(0.1405)\n",
            "Iteration: 10000, loss: 0.0021246732936706396, gradient norm: tensor(0.0330)\n",
            "Iteration: 11000, loss: 0.0005855971729906742, gradient norm: tensor(0.0035)\n",
            "Iteration: 12000, loss: 0.0004044400404964108, gradient norm: tensor(0.0018)\n",
            "Iteration: 13000, loss: 0.00023329722581547684, gradient norm: tensor(0.0015)\n",
            "Iteration: 14000, loss: 8.449749819556018e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 15000, loss: 3.504174996669462e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 2.8932598628671258e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 2.7779666597780307e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 2.6263412846674327e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 2.4099388383547193e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 2.140626542677637e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.8643476705619834e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.565166141153895e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.1864751804750995e-05, gradient norm: tensor(8.3869e-05)\n",
            "Iteration: 24000, loss: 7.696435260186264e-06, gradient norm: tensor(4.7331e-05)\n",
            "Iteration: 25000, loss: 4.813768254734896e-06, gradient norm: tensor(2.5266e-05)\n",
            "Iteration: 26000, loss: 3.6410364148196095e-06, gradient norm: tensor(1.2069e-05)\n",
            "Iteration: 27000, loss: 3.3083681594234802e-06, gradient norm: tensor(1.1962e-05)\n",
            "Iteration: 28000, loss: 3.1395036639878526e-06, gradient norm: tensor(1.6977e-05)\n",
            "Iteration: 29000, loss: 3.0077536973749373e-06, gradient norm: tensor(2.3513e-05)\n",
            "Iteration: 30000, loss: 2.918217591741268e-06, gradient norm: tensor(3.8462e-06)\n",
            "Iteration: 31000, loss: 2.8609871058051793e-06, gradient norm: tensor(1.6023e-05)\n",
            "Iteration: 32000, loss: 2.824826027335803e-06, gradient norm: tensor(1.0178e-05)\n",
            "Iteration: 33000, loss: 2.7997957581646915e-06, gradient norm: tensor(3.0079e-05)\n",
            "Iteration: 34000, loss: 2.7800730319995636e-06, gradient norm: tensor(4.4344e-06)\n",
            "Iteration: 35000, loss: 2.7634320131255663e-06, gradient norm: tensor(2.8161e-05)\n",
            "Iteration: 36000, loss: 2.748881386651192e-06, gradient norm: tensor(2.2893e-06)\n",
            "Iteration: 37000, loss: 2.7362019620795765e-06, gradient norm: tensor(3.3891e-06)\n",
            "Iteration: 38000, loss: 2.7251093952145313e-06, gradient norm: tensor(1.9168e-05)\n",
            "Iteration: 39000, loss: 2.7154521428656155e-06, gradient norm: tensor(2.9443e-05)\n",
            "Iteration: 40000, loss: 2.707332478621538e-06, gradient norm: tensor(2.8225e-05)\n",
            "Iteration: 41000, loss: 2.700530028050707e-06, gradient norm: tensor(3.9254e-05)\n",
            "Iteration: 42000, loss: 2.694925498644807e-06, gradient norm: tensor(1.8396e-05)\n",
            "Iteration: 43000, loss: 2.690416625910075e-06, gradient norm: tensor(3.9415e-06)\n",
            "Iteration: 44000, loss: 2.686891735493191e-06, gradient norm: tensor(8.4122e-06)\n",
            "Iteration: 45000, loss: 2.6841226685974106e-06, gradient norm: tensor(6.9812e-06)\n",
            "Iteration: 46000, loss: 2.681980720581123e-06, gradient norm: tensor(1.5451e-05)\n",
            "Iteration: 47000, loss: 2.680329354461719e-06, gradient norm: tensor(5.3277e-06)\n",
            "Iteration: 48000, loss: 2.679023768678235e-06, gradient norm: tensor(5.6604e-06)\n",
            "Iteration: 49000, loss: 2.6779862423609304e-06, gradient norm: tensor(1.2666e-05)\n",
            "Iteration: 50000, loss: 2.677128727100353e-06, gradient norm: tensor(4.5793e-05)\n",
            "Iteration: 51000, loss: 2.676413898143437e-06, gradient norm: tensor(6.3161e-06)\n",
            "Iteration: 52000, loss: 2.67577247564077e-06, gradient norm: tensor(8.7393e-07)\n",
            "Iteration: 0, loss: 2.799085525751114, gradient norm: tensor(3.4644)\n",
            "Iteration: 1000, loss: 2.0606840535402298, gradient norm: tensor(2.9475)\n",
            "Iteration: 2000, loss: 1.4685570850372314, gradient norm: tensor(2.4564)\n",
            "Iteration: 3000, loss: 0.9987053440213204, gradient norm: tensor(1.9933)\n",
            "Iteration: 4000, loss: 0.6352137018144131, gradient norm: tensor(1.5531)\n",
            "Iteration: 5000, loss: 0.3673795282840729, gradient norm: tensor(1.1415)\n",
            "Iteration: 6000, loss: 0.18468511935323476, gradient norm: tensor(0.7673)\n",
            "Iteration: 7000, loss: 0.07489774450659752, gradient norm: tensor(0.4467)\n",
            "Iteration: 8000, loss: 0.021576273737475275, gradient norm: tensor(0.2032)\n",
            "Iteration: 9000, loss: 0.0036744623153936117, gradient norm: tensor(0.0595)\n",
            "Iteration: 10000, loss: 0.00046306319118593817, gradient norm: tensor(0.0083)\n",
            "Iteration: 11000, loss: 0.00022309065326408018, gradient norm: tensor(0.0022)\n",
            "Iteration: 12000, loss: 0.00018056013618479482, gradient norm: tensor(0.0018)\n",
            "Iteration: 13000, loss: 0.00013002536722342485, gradient norm: tensor(0.0013)\n",
            "Iteration: 14000, loss: 8.961338825611165e-05, gradient norm: tensor(0.0011)\n",
            "Iteration: 15000, loss: 6.81529633293394e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 16000, loss: 5.337970258915448e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 17000, loss: 3.783951925834117e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 18000, loss: 2.163654036121443e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 9.782129055111e-06, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 4.548239170617308e-06, gradient norm: tensor(7.9913e-05)\n",
            "Iteration: 21000, loss: 3.20341942642699e-06, gradient norm: tensor(4.1149e-05)\n",
            "Iteration: 22000, loss: 2.73346379594841e-06, gradient norm: tensor(3.3232e-05)\n",
            "Iteration: 23000, loss: 2.2603931658977673e-06, gradient norm: tensor(3.4300e-05)\n",
            "Iteration: 24000, loss: 1.7383414125333729e-06, gradient norm: tensor(2.1445e-05)\n",
            "Iteration: 25000, loss: 1.233736172252975e-06, gradient norm: tensor(3.1217e-05)\n",
            "Iteration: 26000, loss: 8.109115116212706e-07, gradient norm: tensor(1.0935e-05)\n",
            "Iteration: 27000, loss: 5.046080947579412e-07, gradient norm: tensor(1.5203e-05)\n",
            "Iteration: 28000, loss: 3.13033813426955e-07, gradient norm: tensor(2.4289e-05)\n",
            "Iteration: 29000, loss: 2.0879397058592986e-07, gradient norm: tensor(2.0363e-05)\n",
            "Iteration: 30000, loss: 1.5821905992652318e-07, gradient norm: tensor(5.6300e-06)\n",
            "Iteration: 31000, loss: 1.3534349838550953e-07, gradient norm: tensor(1.5543e-05)\n",
            "Iteration: 32000, loss: 1.2433558877944506e-07, gradient norm: tensor(2.9038e-06)\n",
            "Iteration: 33000, loss: 1.1777565919857125e-07, gradient norm: tensor(2.5236e-05)\n",
            "Iteration: 34000, loss: 1.1293435617432124e-07, gradient norm: tensor(1.4044e-05)\n",
            "Iteration: 35000, loss: 1.0887952915084043e-07, gradient norm: tensor(7.7377e-06)\n",
            "Iteration: 36000, loss: 1.0525708288611213e-07, gradient norm: tensor(1.5353e-05)\n",
            "Iteration: 37000, loss: 1.0187905299119393e-07, gradient norm: tensor(6.9414e-06)\n",
            "Iteration: 38000, loss: 9.86744030839759e-08, gradient norm: tensor(5.4423e-06)\n",
            "Iteration: 39000, loss: 9.56207832203404e-08, gradient norm: tensor(4.6348e-05)\n",
            "Iteration: 40000, loss: 9.269286372415308e-08, gradient norm: tensor(5.4558e-05)\n",
            "Iteration: 41000, loss: 8.988554396438531e-08, gradient norm: tensor(3.3937e-06)\n",
            "Iteration: 42000, loss: 8.717486802112262e-08, gradient norm: tensor(2.6554e-06)\n",
            "Iteration: 43000, loss: 8.45697530209577e-08, gradient norm: tensor(5.1466e-06)\n",
            "Iteration: 44000, loss: 8.205737542255065e-08, gradient norm: tensor(2.7669e-06)\n",
            "Iteration: 45000, loss: 7.964932285631221e-08, gradient norm: tensor(3.3349e-05)\n",
            "Iteration: 46000, loss: 7.733956507394169e-08, gradient norm: tensor(1.8464e-05)\n",
            "Iteration: 47000, loss: 7.509208501232933e-08, gradient norm: tensor(3.4887e-06)\n",
            "Iteration: 48000, loss: 7.294454223227831e-08, gradient norm: tensor(2.8077e-06)\n",
            "Iteration: 49000, loss: 7.084810308555234e-08, gradient norm: tensor(1.0186e-05)\n",
            "Iteration: 50000, loss: 6.884219273928239e-08, gradient norm: tensor(2.2763e-06)\n",
            "Iteration: 51000, loss: 6.688703535928652e-08, gradient norm: tensor(8.2342e-06)\n",
            "Iteration: 52000, loss: 6.500783479168604e-08, gradient norm: tensor(3.5613e-05)\n",
            "Iteration: 53000, loss: 6.318193312182529e-08, gradient norm: tensor(5.9844e-06)\n",
            "Iteration: 54000, loss: 6.1421747936663e-08, gradient norm: tensor(5.0770e-07)\n",
            "Iteration: 0, loss: 1.954918354153633, gradient norm: tensor(2.7025)\n",
            "Iteration: 1000, loss: 1.314232600092888, gradient norm: tensor(2.2303)\n",
            "Iteration: 2000, loss: 0.8420208399295807, gradient norm: tensor(1.7722)\n",
            "Iteration: 3000, loss: 0.5046029717326165, gradient norm: tensor(1.3381)\n",
            "Iteration: 4000, loss: 0.272150759935379, gradient norm: tensor(0.9388)\n",
            "Iteration: 5000, loss: 0.12454097954183817, gradient norm: tensor(0.5878)\n",
            "Iteration: 6000, loss: 0.04450047047995031, gradient norm: tensor(0.3043)\n",
            "Iteration: 7000, loss: 0.011623456613626331, gradient norm: tensor(0.1129)\n",
            "Iteration: 8000, loss: 0.003043297978350893, gradient norm: tensor(0.0238)\n",
            "Iteration: 9000, loss: 0.001617101180832833, gradient norm: tensor(0.0046)\n",
            "Iteration: 10000, loss: 0.0012258958786260338, gradient norm: tensor(0.0039)\n",
            "Iteration: 11000, loss: 0.0009967560788500124, gradient norm: tensor(0.0034)\n",
            "Iteration: 12000, loss: 0.0008129708889173344, gradient norm: tensor(0.0029)\n",
            "Iteration: 13000, loss: 0.0006708470030571335, gradient norm: tensor(0.0024)\n",
            "Iteration: 14000, loss: 0.0005658714337041602, gradient norm: tensor(0.0019)\n",
            "Iteration: 15000, loss: 0.00048286014358745887, gradient norm: tensor(0.0015)\n",
            "Iteration: 16000, loss: 0.0004124669941375032, gradient norm: tensor(0.0011)\n",
            "Iteration: 17000, loss: 0.0003527262392162811, gradient norm: tensor(0.0009)\n",
            "Iteration: 18000, loss: 0.00029672551492694767, gradient norm: tensor(0.0008)\n",
            "Iteration: 19000, loss: 0.00023019033804303035, gradient norm: tensor(0.0007)\n",
            "Iteration: 20000, loss: 0.00015786265053611715, gradient norm: tensor(0.0006)\n",
            "Iteration: 21000, loss: 9.519646307307994e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 22000, loss: 5.0653349888307276e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 2.9049305490843836e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 2.1430316113765003e-05, gradient norm: tensor(7.3957e-05)\n",
            "Iteration: 25000, loss: 1.481693875757628e-05, gradient norm: tensor(3.5618e-05)\n",
            "Iteration: 26000, loss: 1.1344568428285129e-05, gradient norm: tensor(1.5862e-05)\n",
            "Iteration: 27000, loss: 1.0229803614492994e-05, gradient norm: tensor(1.4142e-05)\n",
            "Iteration: 28000, loss: 9.983784625546832e-06, gradient norm: tensor(2.9156e-05)\n",
            "Iteration: 29000, loss: 9.902475803755805e-06, gradient norm: tensor(4.2298e-06)\n",
            "Iteration: 30000, loss: 9.818783005357546e-06, gradient norm: tensor(3.6717e-06)\n",
            "Iteration: 31000, loss: 9.719654169202841e-06, gradient norm: tensor(1.2111e-05)\n",
            "Iteration: 32000, loss: 9.613028909370768e-06, gradient norm: tensor(3.6338e-06)\n",
            "Iteration: 33000, loss: 9.514888669400534e-06, gradient norm: tensor(1.1697e-05)\n",
            "Iteration: 34000, loss: 9.43986056427093e-06, gradient norm: tensor(2.2367e-06)\n",
            "Iteration: 35000, loss: 9.3933364159966e-06, gradient norm: tensor(6.4294e-06)\n",
            "Iteration: 36000, loss: 9.368340172841272e-06, gradient norm: tensor(3.3323e-06)\n",
            "Iteration: 37000, loss: 9.355225274703117e-06, gradient norm: tensor(4.5864e-05)\n",
            "Iteration: 38000, loss: 9.348364528705133e-06, gradient norm: tensor(2.0664e-06)\n",
            "Iteration: 39000, loss: 9.344875303213484e-06, gradient norm: tensor(1.2133e-06)\n",
            "Iteration: 0, loss: 2.243005380988121, gradient norm: tensor(3.0811)\n",
            "Iteration: 1000, loss: 1.5846463865041733, gradient norm: tensor(2.5544)\n",
            "Iteration: 2000, loss: 1.0790477879047393, gradient norm: tensor(2.0741)\n",
            "Iteration: 3000, loss: 0.6918684188723564, gradient norm: tensor(1.6242)\n",
            "Iteration: 4000, loss: 0.40644378054142, gradient norm: tensor(1.2038)\n",
            "Iteration: 5000, loss: 0.21061882446706295, gradient norm: tensor(0.8212)\n",
            "Iteration: 6000, loss: 0.09069498436152935, gradient norm: tensor(0.4910)\n",
            "Iteration: 7000, loss: 0.029324852729216217, gradient norm: tensor(0.2353)\n",
            "Iteration: 8000, loss: 0.0060826470092870295, gradient norm: tensor(0.0755)\n",
            "Iteration: 9000, loss: 0.0009836516865179873, gradient norm: tensor(0.0122)\n",
            "Iteration: 10000, loss: 0.00040408521791687236, gradient norm: tensor(0.0019)\n",
            "Iteration: 11000, loss: 0.00027544054595637137, gradient norm: tensor(0.0016)\n",
            "Iteration: 12000, loss: 0.00015116002636932535, gradient norm: tensor(0.0011)\n",
            "Iteration: 13000, loss: 4.55268144942238e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 14000, loss: 1.655215688606404e-05, gradient norm: tensor(7.0861e-05)\n",
            "Iteration: 15000, loss: 1.4788057966143243e-05, gradient norm: tensor(4.3749e-05)\n",
            "Iteration: 16000, loss: 1.4488046316728287e-05, gradient norm: tensor(3.8021e-05)\n",
            "Iteration: 17000, loss: 1.4167233086482156e-05, gradient norm: tensor(3.6416e-05)\n",
            "Iteration: 18000, loss: 1.3812159987537598e-05, gradient norm: tensor(3.9645e-05)\n",
            "Iteration: 19000, loss: 1.3365812582378566e-05, gradient norm: tensor(4.7109e-05)\n",
            "Iteration: 20000, loss: 1.2673783438003738e-05, gradient norm: tensor(5.6477e-05)\n",
            "Iteration: 21000, loss: 1.1419453491726017e-05, gradient norm: tensor(8.2300e-05)\n",
            "Iteration: 22000, loss: 9.391435977704532e-06, gradient norm: tensor(5.3077e-05)\n",
            "Iteration: 23000, loss: 7.492709973121237e-06, gradient norm: tensor(4.1722e-05)\n",
            "Iteration: 24000, loss: 5.879688715594967e-06, gradient norm: tensor(3.0524e-05)\n",
            "Iteration: 25000, loss: 4.166432756846916e-06, gradient norm: tensor(2.4405e-05)\n",
            "Iteration: 26000, loss: 2.7064810938099983e-06, gradient norm: tensor(1.5726e-05)\n",
            "Iteration: 27000, loss: 1.7177249275164286e-06, gradient norm: tensor(1.1824e-05)\n",
            "Iteration: 28000, loss: 1.1146687051564186e-06, gradient norm: tensor(8.4737e-06)\n",
            "Iteration: 29000, loss: 7.813738814093085e-07, gradient norm: tensor(7.9228e-06)\n",
            "Iteration: 30000, loss: 6.236841859958985e-07, gradient norm: tensor(9.0752e-06)\n",
            "Iteration: 31000, loss: 5.542282743817851e-07, gradient norm: tensor(2.3856e-05)\n",
            "Iteration: 32000, loss: 5.191485797126916e-07, gradient norm: tensor(1.8468e-05)\n",
            "Iteration: 33000, loss: 4.96940771881782e-07, gradient norm: tensor(6.7361e-05)\n",
            "Iteration: 34000, loss: 4.80336788882596e-07, gradient norm: tensor(2.9425e-06)\n",
            "Iteration: 35000, loss: 4.663396480850679e-07, gradient norm: tensor(8.0832e-06)\n",
            "Iteration: 36000, loss: 4.537329471929752e-07, gradient norm: tensor(4.1889e-06)\n",
            "Iteration: 37000, loss: 4.4200581518794024e-07, gradient norm: tensor(4.2683e-06)\n",
            "Iteration: 38000, loss: 4.3083115309627827e-07, gradient norm: tensor(8.4391e-05)\n",
            "Iteration: 39000, loss: 4.2018406193733424e-07, gradient norm: tensor(7.8596e-06)\n",
            "Iteration: 40000, loss: 4.099022972070543e-07, gradient norm: tensor(1.7138e-05)\n",
            "Iteration: 41000, loss: 4.000183722894235e-07, gradient norm: tensor(5.5308e-06)\n",
            "Iteration: 42000, loss: 3.905185922974397e-07, gradient norm: tensor(6.7250e-06)\n",
            "Iteration: 43000, loss: 3.8132689311964896e-07, gradient norm: tensor(6.0622e-06)\n",
            "Iteration: 44000, loss: 3.7241786171193783e-07, gradient norm: tensor(8.6433e-05)\n",
            "Iteration: 45000, loss: 3.637896906241167e-07, gradient norm: tensor(1.0043e-05)\n",
            "Iteration: 46000, loss: 3.5545742554177197e-07, gradient norm: tensor(6.3182e-06)\n",
            "Iteration: 47000, loss: 3.473831072255962e-07, gradient norm: tensor(3.2960e-06)\n",
            "Iteration: 48000, loss: 3.3958817516577254e-07, gradient norm: tensor(7.2170e-05)\n",
            "Iteration: 49000, loss: 3.320030584177403e-07, gradient norm: tensor(1.9580e-05)\n",
            "Iteration: 50000, loss: 3.246626808959263e-07, gradient norm: tensor(8.1907e-05)\n",
            "Iteration: 51000, loss: 3.1753857362559755e-07, gradient norm: tensor(1.0129e-05)\n",
            "Iteration: 52000, loss: 3.106031891775274e-07, gradient norm: tensor(3.1581e-05)\n",
            "Iteration: 53000, loss: 3.0386873311272213e-07, gradient norm: tensor(1.6384e-05)\n",
            "Iteration: 54000, loss: 2.9734175330986545e-07, gradient norm: tensor(3.8649e-05)\n",
            "Iteration: 55000, loss: 2.9103387333861974e-07, gradient norm: tensor(3.1449e-06)\n",
            "Iteration: 56000, loss: 2.8489036594692154e-07, gradient norm: tensor(6.7860e-05)\n",
            "Iteration: 57000, loss: 2.7889961452842724e-07, gradient norm: tensor(6.6318e-05)\n",
            "Iteration: 58000, loss: 2.7313889410152117e-07, gradient norm: tensor(6.5521e-06)\n",
            "Iteration: 59000, loss: 2.6755701156844223e-07, gradient norm: tensor(1.1166e-05)\n",
            "Iteration: 60000, loss: 2.6209808370936117e-07, gradient norm: tensor(1.9951e-05)\n",
            "Iteration: 61000, loss: 2.567931618955299e-07, gradient norm: tensor(6.6958e-06)\n",
            "Iteration: 62000, loss: 2.515922117254377e-07, gradient norm: tensor(3.8326e-06)\n",
            "Iteration: 63000, loss: 2.4653780479866325e-07, gradient norm: tensor(8.4597e-06)\n",
            "Iteration: 64000, loss: 2.416285332174084e-07, gradient norm: tensor(8.6656e-05)\n",
            "Iteration: 65000, loss: 2.3684619984010169e-07, gradient norm: tensor(1.6656e-05)\n",
            "Iteration: 66000, loss: 2.322075030463111e-07, gradient norm: tensor(1.2294e-06)\n",
            "Iteration: 0, loss: 5.118059138298035, gradient norm: tensor(4.8091)\n",
            "Iteration: 1000, loss: 4.064113403320312, gradient norm: tensor(4.2489)\n",
            "Iteration: 2000, loss: 3.1817178177833556, gradient norm: tensor(3.7289)\n",
            "Iteration: 3000, loss: 2.4341828646659853, gradient norm: tensor(3.2343)\n",
            "Iteration: 4000, loss: 1.8026655557155609, gradient norm: tensor(2.7537)\n",
            "Iteration: 5000, loss: 1.2787385729551315, gradient norm: tensor(2.2898)\n",
            "Iteration: 6000, loss: 0.8558766422867775, gradient norm: tensor(1.8411)\n",
            "Iteration: 7000, loss: 0.52938396063447, gradient norm: tensor(1.4092)\n",
            "Iteration: 8000, loss: 0.2934746124148369, gradient norm: tensor(1.0069)\n",
            "Iteration: 9000, loss: 0.1385595117136836, gradient norm: tensor(0.6491)\n",
            "Iteration: 10000, loss: 0.0508187419809401, gradient norm: tensor(0.3530)\n",
            "Iteration: 11000, loss: 0.012255781644955277, gradient norm: tensor(0.1419)\n",
            "Iteration: 12000, loss: 0.0014652760896715336, gradient norm: tensor(0.0333)\n",
            "Iteration: 13000, loss: 6.0650287257885797e-05, gradient norm: tensor(0.0030)\n",
            "Iteration: 14000, loss: 6.678772694158397e-06, gradient norm: tensor(6.7358e-05)\n",
            "Iteration: 15000, loss: 5.726452636736212e-06, gradient norm: tensor(3.2524e-05)\n",
            "Iteration: 16000, loss: 5.117145782605803e-06, gradient norm: tensor(2.5711e-05)\n",
            "Iteration: 17000, loss: 4.645172004984488e-06, gradient norm: tensor(2.0477e-05)\n",
            "Iteration: 18000, loss: 4.339578155850177e-06, gradient norm: tensor(1.7028e-05)\n",
            "Iteration: 19000, loss: 4.174859216618642e-06, gradient norm: tensor(1.5099e-05)\n",
            "Iteration: 20000, loss: 4.100236153135483e-06, gradient norm: tensor(1.4139e-05)\n",
            "Iteration: 21000, loss: 4.06892165165118e-06, gradient norm: tensor(1.4038e-05)\n",
            "Iteration: 22000, loss: 4.051016186167544e-06, gradient norm: tensor(1.3335e-05)\n",
            "Iteration: 23000, loss: 4.0310284712177235e-06, gradient norm: tensor(1.4248e-05)\n",
            "Iteration: 24000, loss: 4.001353114745143e-06, gradient norm: tensor(1.7649e-05)\n",
            "Iteration: 25000, loss: 3.9572391710862575e-06, gradient norm: tensor(1.2895e-05)\n",
            "Iteration: 26000, loss: 3.895184935572615e-06, gradient norm: tensor(1.1299e-05)\n",
            "Iteration: 27000, loss: 3.8142907860674313e-06, gradient norm: tensor(1.1091e-05)\n",
            "Iteration: 28000, loss: 3.716919016369502e-06, gradient norm: tensor(8.5707e-06)\n",
            "Iteration: 29000, loss: 3.6040823447365255e-06, gradient norm: tensor(7.6387e-06)\n",
            "Iteration: 30000, loss: 3.4684008817293943e-06, gradient norm: tensor(3.5891e-05)\n",
            "Iteration: 31000, loss: 3.300281016208828e-06, gradient norm: tensor(3.0473e-05)\n",
            "Iteration: 32000, loss: 3.10530615342941e-06, gradient norm: tensor(4.4281e-05)\n",
            "Iteration: 33000, loss: 2.914038374001393e-06, gradient norm: tensor(3.2489e-06)\n",
            "Iteration: 34000, loss: 2.76561193049929e-06, gradient norm: tensor(3.3057e-05)\n",
            "Iteration: 35000, loss: 2.6600977641919598e-06, gradient norm: tensor(6.7754e-06)\n",
            "Iteration: 36000, loss: 2.581056042572527e-06, gradient norm: tensor(2.0612e-06)\n",
            "Iteration: 37000, loss: 2.5176389415264568e-06, gradient norm: tensor(6.3728e-05)\n",
            "Iteration: 38000, loss: 2.4630161990444323e-06, gradient norm: tensor(3.7081e-06)\n",
            "Iteration: 39000, loss: 2.41119301335857e-06, gradient norm: tensor(1.0513e-05)\n",
            "Iteration: 40000, loss: 2.356578919261665e-06, gradient norm: tensor(6.7491e-06)\n",
            "Iteration: 41000, loss: 2.290983843522554e-06, gradient norm: tensor(2.1464e-05)\n",
            "Iteration: 42000, loss: 2.213040400420141e-06, gradient norm: tensor(2.6274e-06)\n",
            "Iteration: 43000, loss: 2.149764640535068e-06, gradient norm: tensor(8.7243e-06)\n",
            "Iteration: 44000, loss: 2.111377500796152e-06, gradient norm: tensor(4.2609e-05)\n",
            "Iteration: 45000, loss: 2.0829847990171404e-06, gradient norm: tensor(1.7971e-06)\n",
            "Iteration: 0, loss: 2.229956448674202, gradient norm: tensor(2.5043)\n",
            "Iteration: 1000, loss: 1.5722701934576035, gradient norm: tensor(2.1735)\n",
            "Iteration: 2000, loss: 1.0119621072411538, gradient norm: tensor(1.8224)\n",
            "Iteration: 3000, loss: 0.5902301107048988, gradient norm: tensor(1.4086)\n",
            "Iteration: 4000, loss: 0.317816245675087, gradient norm: tensor(1.0006)\n",
            "Iteration: 5000, loss: 0.15429615335166455, gradient norm: tensor(0.6362)\n",
            "Iteration: 6000, loss: 0.06551386231556534, gradient norm: tensor(0.3397)\n",
            "Iteration: 7000, loss: 0.02669933070987463, gradient norm: tensor(0.1344)\n",
            "Iteration: 8000, loss: 0.014676171571947633, gradient norm: tensor(0.0335)\n",
            "Iteration: 9000, loss: 0.011885291700251401, gradient norm: tensor(0.0132)\n",
            "Iteration: 10000, loss: 0.010537444696761667, gradient norm: tensor(0.0141)\n",
            "Iteration: 11000, loss: 0.008728124527726323, gradient norm: tensor(0.0155)\n",
            "Iteration: 12000, loss: 0.006486019506119192, gradient norm: tensor(0.0145)\n",
            "Iteration: 13000, loss: 0.004408754775067791, gradient norm: tensor(0.0122)\n",
            "Iteration: 14000, loss: 0.0026431852384703235, gradient norm: tensor(0.0089)\n",
            "Iteration: 15000, loss: 0.0013721800640341827, gradient norm: tensor(0.0056)\n",
            "Iteration: 16000, loss: 0.0006691080701129976, gradient norm: tensor(0.0030)\n",
            "Iteration: 17000, loss: 0.00037369399864110164, gradient norm: tensor(0.0015)\n",
            "Iteration: 18000, loss: 0.0002649174528341973, gradient norm: tensor(0.0008)\n",
            "Iteration: 19000, loss: 0.00020987268719181883, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 0.000160210639674915, gradient norm: tensor(0.0005)\n",
            "Iteration: 21000, loss: 0.00011229576269397512, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 7.25717335990339e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 4.4216563001100436e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 2.6574005254587974e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.7053098690666958e-05, gradient norm: tensor(5.8728e-05)\n",
            "Iteration: 26000, loss: 1.246409955638228e-05, gradient norm: tensor(3.3183e-05)\n",
            "Iteration: 27000, loss: 9.926903578161728e-06, gradient norm: tensor(2.4050e-05)\n",
            "Iteration: 28000, loss: 7.965094017890806e-06, gradient norm: tensor(2.4353e-05)\n",
            "Iteration: 29000, loss: 6.147872895780893e-06, gradient norm: tensor(1.8536e-05)\n",
            "Iteration: 30000, loss: 4.655310612633912e-06, gradient norm: tensor(2.0396e-05)\n",
            "Iteration: 31000, loss: 3.492010839863724e-06, gradient norm: tensor(1.3648e-05)\n",
            "Iteration: 32000, loss: 2.5944751764654938e-06, gradient norm: tensor(1.2242e-05)\n",
            "Iteration: 33000, loss: 1.914528250836156e-06, gradient norm: tensor(1.6162e-05)\n",
            "Iteration: 34000, loss: 1.408308169857264e-06, gradient norm: tensor(8.7539e-06)\n",
            "Iteration: 35000, loss: 1.0352938114692734e-06, gradient norm: tensor(3.0375e-05)\n",
            "Iteration: 36000, loss: 7.634115049768297e-07, gradient norm: tensor(2.2718e-05)\n",
            "Iteration: 37000, loss: 5.673296905115421e-07, gradient norm: tensor(5.6173e-06)\n",
            "Iteration: 38000, loss: 4.2498473871432906e-07, gradient norm: tensor(4.8669e-05)\n",
            "Iteration: 39000, loss: 3.2149618385801657e-07, gradient norm: tensor(3.5985e-06)\n",
            "Iteration: 40000, loss: 2.4540397197370113e-07, gradient norm: tensor(1.0216e-05)\n",
            "Iteration: 41000, loss: 1.8924189352276243e-07, gradient norm: tensor(5.6305e-06)\n",
            "Iteration: 42000, loss: 1.473488733125805e-07, gradient norm: tensor(7.6395e-06)\n",
            "Iteration: 43000, loss: 1.1560907372398787e-07, gradient norm: tensor(1.5902e-06)\n",
            "Iteration: 0, loss: 1.8835437543392182, gradient norm: tensor(2.5079)\n",
            "Iteration: 1000, loss: 1.2860547761917114, gradient norm: tensor(2.0524)\n",
            "Iteration: 2000, loss: 0.7991741641163826, gradient norm: tensor(1.6120)\n",
            "Iteration: 3000, loss: 0.42549595057964323, gradient norm: tensor(1.1386)\n",
            "Iteration: 4000, loss: 0.18834368520975112, gradient norm: tensor(0.6958)\n",
            "Iteration: 5000, loss: 0.06716050995141268, gradient norm: tensor(0.3585)\n",
            "Iteration: 6000, loss: 0.01809472715202719, gradient norm: tensor(0.1379)\n",
            "Iteration: 7000, loss: 0.004529211347224191, gradient norm: tensor(0.0313)\n",
            "Iteration: 8000, loss: 0.002369228382129222, gradient norm: tensor(0.0060)\n",
            "Iteration: 9000, loss: 0.0019797304733656347, gradient norm: tensor(0.0039)\n",
            "Iteration: 10000, loss: 0.0017582089360803365, gradient norm: tensor(0.0035)\n",
            "Iteration: 11000, loss: 0.001526849181158468, gradient norm: tensor(0.0035)\n",
            "Iteration: 12000, loss: 0.0012194167017005385, gradient norm: tensor(0.0034)\n",
            "Iteration: 13000, loss: 0.0008513520275009796, gradient norm: tensor(0.0031)\n",
            "Iteration: 14000, loss: 0.0005103558873524889, gradient norm: tensor(0.0024)\n",
            "Iteration: 15000, loss: 0.0002589453740947647, gradient norm: tensor(0.0017)\n",
            "Iteration: 16000, loss: 0.00010974346568400505, gradient norm: tensor(0.0008)\n",
            "Iteration: 17000, loss: 5.0572620551974975e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 3.62016721337568e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 3.090451839671005e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 2.6877733473156696e-05, gradient norm: tensor(9.3316e-05)\n",
            "Iteration: 21000, loss: 2.2136615585623077e-05, gradient norm: tensor(8.8680e-05)\n",
            "Iteration: 22000, loss: 1.5706764116657723e-05, gradient norm: tensor(7.0746e-05)\n",
            "Iteration: 23000, loss: 9.484080902439019e-06, gradient norm: tensor(6.1562e-05)\n",
            "Iteration: 24000, loss: 5.324027987171576e-06, gradient norm: tensor(2.5746e-05)\n",
            "Iteration: 25000, loss: 3.1984621243736912e-06, gradient norm: tensor(1.7255e-05)\n",
            "Iteration: 26000, loss: 1.973052841435674e-06, gradient norm: tensor(2.9887e-05)\n",
            "Iteration: 27000, loss: 1.2166259455170803e-06, gradient norm: tensor(2.5778e-05)\n",
            "Iteration: 28000, loss: 9.853453908590382e-07, gradient norm: tensor(2.9285e-06)\n",
            "Iteration: 29000, loss: 9.177232120691769e-07, gradient norm: tensor(4.9787e-05)\n",
            "Iteration: 30000, loss: 8.734248999644478e-07, gradient norm: tensor(5.1350e-06)\n",
            "Iteration: 31000, loss: 8.276565087612653e-07, gradient norm: tensor(3.4399e-06)\n",
            "Iteration: 32000, loss: 7.744209178213168e-07, gradient norm: tensor(5.5248e-05)\n",
            "Iteration: 33000, loss: 7.099416980622664e-07, gradient norm: tensor(7.6631e-06)\n",
            "Iteration: 34000, loss: 6.365046955352227e-07, gradient norm: tensor(3.1841e-05)\n",
            "Iteration: 35000, loss: 5.658853579575407e-07, gradient norm: tensor(7.4910e-06)\n",
            "Iteration: 36000, loss: 5.117136428793856e-07, gradient norm: tensor(7.8081e-06)\n",
            "Iteration: 37000, loss: 4.7669134616512566e-07, gradient norm: tensor(1.5488e-06)\n",
            "Iteration: 0, loss: 2.524345993757248, gradient norm: tensor(3.3638)\n",
            "Iteration: 1000, loss: 1.8059274889230728, gradient norm: tensor(2.7903)\n",
            "Iteration: 2000, loss: 1.252765274167061, gradient norm: tensor(2.2647)\n",
            "Iteration: 3000, loss: 0.8289634723067284, gradient norm: tensor(1.7957)\n",
            "Iteration: 4000, loss: 0.5086749846339226, gradient norm: tensor(1.3658)\n",
            "Iteration: 5000, loss: 0.2785525118410587, gradient norm: tensor(0.9709)\n",
            "Iteration: 6000, loss: 0.128470724388957, gradient norm: tensor(0.6182)\n",
            "Iteration: 7000, loss: 0.045407562201842666, gradient norm: tensor(0.3279)\n",
            "Iteration: 8000, loss: 0.01042261075018905, gradient norm: tensor(0.1264)\n",
            "Iteration: 9000, loss: 0.001192993441974977, gradient norm: tensor(0.0275)\n",
            "Iteration: 10000, loss: 7.152510521154909e-05, gradient norm: tensor(0.0022)\n",
            "Iteration: 11000, loss: 1.961468225272256e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 12000, loss: 1.3623649694636697e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 13000, loss: 1.1291150735814881e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 14000, loss: 1.0185712219026755e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 15000, loss: 9.586685177055187e-06, gradient norm: tensor(9.9841e-05)\n",
            "Iteration: 16000, loss: 9.171909997348848e-06, gradient norm: tensor(9.5761e-05)\n",
            "Iteration: 17000, loss: 8.812264003609016e-06, gradient norm: tensor(8.9247e-05)\n",
            "Iteration: 18000, loss: 8.44418270389724e-06, gradient norm: tensor(8.1150e-05)\n",
            "Iteration: 19000, loss: 7.988323577592382e-06, gradient norm: tensor(7.2942e-05)\n",
            "Iteration: 20000, loss: 7.385251482901367e-06, gradient norm: tensor(6.5946e-05)\n",
            "Iteration: 21000, loss: 6.590138536921586e-06, gradient norm: tensor(5.7890e-05)\n",
            "Iteration: 22000, loss: 5.59736843479186e-06, gradient norm: tensor(4.6849e-05)\n",
            "Iteration: 23000, loss: 4.4770210106435115e-06, gradient norm: tensor(5.0953e-05)\n",
            "Iteration: 24000, loss: 3.3607023083277456e-06, gradient norm: tensor(2.8244e-05)\n",
            "Iteration: 25000, loss: 2.367047597772398e-06, gradient norm: tensor(3.0682e-05)\n",
            "Iteration: 26000, loss: 1.5408213091632205e-06, gradient norm: tensor(1.2924e-05)\n",
            "Iteration: 27000, loss: 9.040199385594861e-07, gradient norm: tensor(8.6347e-06)\n",
            "Iteration: 28000, loss: 4.773248303138189e-07, gradient norm: tensor(1.3202e-05)\n",
            "Iteration: 29000, loss: 2.2710291059979682e-07, gradient norm: tensor(2.2067e-05)\n",
            "Iteration: 30000, loss: 9.711026338266038e-08, gradient norm: tensor(4.0802e-06)\n",
            "Iteration: 31000, loss: 3.936607018673044e-08, gradient norm: tensor(1.4779e-05)\n",
            "Iteration: 32000, loss: 1.714251458118099e-08, gradient norm: tensor(2.5248e-05)\n",
            "Iteration: 33000, loss: 8.356009039367507e-09, gradient norm: tensor(2.9075e-05)\n",
            "Iteration: 34000, loss: 4.398393237847742e-09, gradient norm: tensor(6.0218e-06)\n",
            "Iteration: 35000, loss: 2.478391298410543e-09, gradient norm: tensor(1.0846e-05)\n",
            "Iteration: 36000, loss: 1.54483937730987e-09, gradient norm: tensor(2.7186e-06)\n",
            "Iteration: 37000, loss: 1.0870475150026593e-09, gradient norm: tensor(6.6984e-06)\n",
            "Iteration: 38000, loss: 8.842839559486393e-10, gradient norm: tensor(1.6622e-05)\n",
            "Iteration: 39000, loss: 7.771880073947913e-10, gradient norm: tensor(5.6743e-06)\n",
            "Iteration: 40000, loss: 7.327284012870194e-10, gradient norm: tensor(9.4204e-06)\n",
            "Iteration: 41000, loss: 7.111580956342678e-10, gradient norm: tensor(2.5147e-05)\n",
            "Iteration: 42000, loss: 6.97156106232999e-10, gradient norm: tensor(4.5720e-06)\n",
            "Iteration: 43000, loss: 6.958928004952014e-10, gradient norm: tensor(4.8477e-06)\n",
            "Iteration: 44000, loss: 6.997020330912207e-10, gradient norm: tensor(2.7337e-06)\n",
            "Iteration: 45000, loss: 6.979659119554782e-10, gradient norm: tensor(1.7051e-06)\n",
            "Iteration: 0, loss: 1.5763481942415238, gradient norm: tensor(2.2915)\n",
            "Iteration: 1000, loss: 0.9092611753344536, gradient norm: tensor(1.7562)\n",
            "Iteration: 2000, loss: 0.45695962169766424, gradient norm: tensor(1.1775)\n",
            "Iteration: 3000, loss: 0.21054559840261935, gradient norm: tensor(0.7133)\n",
            "Iteration: 4000, loss: 0.09239962568879127, gradient norm: tensor(0.3857)\n",
            "Iteration: 5000, loss: 0.04103369089961052, gradient norm: tensor(0.1701)\n",
            "Iteration: 6000, loss: 0.021088143041357397, gradient norm: tensor(0.0669)\n",
            "Iteration: 7000, loss: 0.012764308337122202, gradient norm: tensor(0.0413)\n",
            "Iteration: 8000, loss: 0.007656191279180348, gradient norm: tensor(0.0291)\n",
            "Iteration: 9000, loss: 0.004055922769475728, gradient norm: tensor(0.0187)\n",
            "Iteration: 10000, loss: 0.0017941874130629003, gradient norm: tensor(0.0101)\n",
            "Iteration: 11000, loss: 0.0007357283448800445, gradient norm: tensor(0.0045)\n",
            "Iteration: 12000, loss: 0.0003647010027198121, gradient norm: tensor(0.0023)\n",
            "Iteration: 13000, loss: 0.00021358455436711664, gradient norm: tensor(0.0016)\n",
            "Iteration: 14000, loss: 0.00011668274449039017, gradient norm: tensor(0.0011)\n",
            "Iteration: 15000, loss: 5.4550609569560035e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 16000, loss: 2.8135393293268864e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 2.1524736206629313e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 1.8480396005543298e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 1.5255113466992044e-05, gradient norm: tensor(8.8813e-05)\n",
            "Iteration: 20000, loss: 1.1824777127912967e-05, gradient norm: tensor(6.8597e-05)\n",
            "Iteration: 21000, loss: 8.648814676234906e-06, gradient norm: tensor(4.8793e-05)\n",
            "Iteration: 22000, loss: 6.162618487906002e-06, gradient norm: tensor(3.1685e-05)\n",
            "Iteration: 23000, loss: 4.55983591336917e-06, gradient norm: tensor(1.7967e-05)\n",
            "Iteration: 24000, loss: 3.7471780603937076e-06, gradient norm: tensor(4.9776e-05)\n",
            "Iteration: 25000, loss: 3.444820862114284e-06, gradient norm: tensor(5.7057e-06)\n",
            "Iteration: 26000, loss: 3.3577069809780367e-06, gradient norm: tensor(5.4334e-06)\n",
            "Iteration: 27000, loss: 3.2993573586281853e-06, gradient norm: tensor(8.5645e-06)\n",
            "Iteration: 28000, loss: 3.1400957732330425e-06, gradient norm: tensor(1.1487e-05)\n",
            "Iteration: 29000, loss: 2.3369737596112825e-06, gradient norm: tensor(5.5013e-05)\n",
            "Iteration: 30000, loss: 8.439288294823655e-07, gradient norm: tensor(2.9727e-05)\n",
            "Iteration: 31000, loss: 2.921601037542132e-07, gradient norm: tensor(3.6546e-05)\n",
            "Iteration: 32000, loss: 1.7577588886297235e-07, gradient norm: tensor(7.6503e-05)\n",
            "Iteration: 33000, loss: 1.4774209964230068e-07, gradient norm: tensor(8.8325e-06)\n",
            "Iteration: 34000, loss: 1.3782290346853188e-07, gradient norm: tensor(2.5069e-05)\n",
            "Iteration: 35000, loss: 1.3270161521461433e-07, gradient norm: tensor(9.8846e-06)\n",
            "Iteration: 36000, loss: 1.2924522117430114e-07, gradient norm: tensor(3.8790e-05)\n",
            "Iteration: 37000, loss: 1.2649936950026585e-07, gradient norm: tensor(8.2260e-06)\n",
            "Iteration: 38000, loss: 1.240206453871906e-07, gradient norm: tensor(9.6677e-05)\n",
            "Iteration: 39000, loss: 1.2167344523561495e-07, gradient norm: tensor(4.8185e-05)\n",
            "Iteration: 40000, loss: 1.1939788082315772e-07, gradient norm: tensor(9.8615e-05)\n",
            "Iteration: 41000, loss: 1.171431052000571e-07, gradient norm: tensor(3.6660e-05)\n",
            "Iteration: 42000, loss: 1.1492462529361092e-07, gradient norm: tensor(0.0001)\n",
            "Iteration: 43000, loss: 1.1277095546091686e-07, gradient norm: tensor(2.5185e-05)\n",
            "Iteration: 44000, loss: 1.105964252872127e-07, gradient norm: tensor(1.4239e-06)\n",
            "Iteration: 0, loss: 2.6570483438968657, gradient norm: tensor(2.9665)\n",
            "Iteration: 1000, loss: 1.9296996743679047, gradient norm: tensor(2.5659)\n",
            "Iteration: 2000, loss: 1.2992982884645463, gradient norm: tensor(2.1850)\n",
            "Iteration: 3000, loss: 0.7988538264632224, gradient norm: tensor(1.7377)\n",
            "Iteration: 4000, loss: 0.45439709109067916, gradient norm: tensor(1.2790)\n",
            "Iteration: 5000, loss: 0.23566493740677832, gradient norm: tensor(0.8732)\n",
            "Iteration: 6000, loss: 0.10433340097591282, gradient norm: tensor(0.5330)\n",
            "Iteration: 7000, loss: 0.03573921414837241, gradient norm: tensor(0.2647)\n",
            "Iteration: 8000, loss: 0.009174625606276094, gradient norm: tensor(0.0912)\n",
            "Iteration: 9000, loss: 0.0027593037753831594, gradient norm: tensor(0.0184)\n",
            "Iteration: 10000, loss: 0.001478443989297375, gradient norm: tensor(0.0073)\n",
            "Iteration: 11000, loss: 0.0007656427290057764, gradient norm: tensor(0.0047)\n",
            "Iteration: 12000, loss: 0.0003242008365486981, gradient norm: tensor(0.0024)\n",
            "Iteration: 13000, loss: 0.00012809421370911877, gradient norm: tensor(0.0012)\n",
            "Iteration: 14000, loss: 5.550066436990164e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 15000, loss: 2.9127389261702774e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 2.167209022809402e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 1.9983212794613793e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 1.8759419323032488e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 1.711687896386138e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 1.5214749228107393e-05, gradient norm: tensor(8.8245e-05)\n",
            "Iteration: 21000, loss: 1.3447376243675535e-05, gradient norm: tensor(6.6034e-05)\n",
            "Iteration: 22000, loss: 1.1888334079230844e-05, gradient norm: tensor(4.9841e-05)\n",
            "Iteration: 23000, loss: 1.043267836121231e-05, gradient norm: tensor(3.6048e-05)\n",
            "Iteration: 24000, loss: 9.159695960988756e-06, gradient norm: tensor(2.7139e-05)\n",
            "Iteration: 25000, loss: 8.00063501128534e-06, gradient norm: tensor(2.3915e-05)\n",
            "Iteration: 26000, loss: 6.093413584949303e-06, gradient norm: tensor(3.5481e-05)\n",
            "Iteration: 27000, loss: 2.6701955976022873e-06, gradient norm: tensor(1.6659e-05)\n",
            "Iteration: 28000, loss: 8.317008430083206e-07, gradient norm: tensor(8.2704e-06)\n",
            "Iteration: 29000, loss: 3.077006966805129e-07, gradient norm: tensor(6.4424e-06)\n",
            "Iteration: 30000, loss: 1.582187899487053e-07, gradient norm: tensor(5.3268e-06)\n",
            "Iteration: 31000, loss: 1.155608076217618e-07, gradient norm: tensor(2.8585e-06)\n",
            "Iteration: 32000, loss: 1.0103273679362701e-07, gradient norm: tensor(1.2325e-05)\n",
            "Iteration: 33000, loss: 9.23588781347462e-08, gradient norm: tensor(3.6862e-06)\n",
            "Iteration: 34000, loss: 8.438075983008275e-08, gradient norm: tensor(5.3527e-05)\n",
            "Iteration: 35000, loss: 7.586206624665692e-08, gradient norm: tensor(8.3156e-07)\n",
            "Iteration: 0, loss: 3.418179632902145, gradient norm: tensor(3.7809)\n",
            "Iteration: 1000, loss: 2.507339975833893, gradient norm: tensor(3.1898)\n",
            "Iteration: 2000, loss: 1.7847983568906785, gradient norm: tensor(2.6967)\n",
            "Iteration: 3000, loss: 1.213215841293335, gradient norm: tensor(2.1997)\n",
            "Iteration: 4000, loss: 0.7853864153623581, gradient norm: tensor(1.7233)\n",
            "Iteration: 5000, loss: 0.47767713898420333, gradient norm: tensor(1.2982)\n",
            "Iteration: 6000, loss: 0.26086327715218066, gradient norm: tensor(0.9129)\n",
            "Iteration: 7000, loss: 0.120704808101058, gradient norm: tensor(0.5703)\n",
            "Iteration: 8000, loss: 0.04451527552120388, gradient norm: tensor(0.2926)\n",
            "Iteration: 9000, loss: 0.013353434381540864, gradient norm: tensor(0.1071)\n",
            "Iteration: 10000, loss: 0.0051638665357604625, gradient norm: tensor(0.0239)\n",
            "Iteration: 11000, loss: 0.0036504842108115556, gradient norm: tensor(0.0089)\n",
            "Iteration: 12000, loss: 0.0030424202885478736, gradient norm: tensor(0.0084)\n",
            "Iteration: 13000, loss: 0.0025012426695320755, gradient norm: tensor(0.0091)\n",
            "Iteration: 14000, loss: 0.0018321622860385104, gradient norm: tensor(0.0090)\n",
            "Iteration: 15000, loss: 0.0010998552585951985, gradient norm: tensor(0.0063)\n",
            "Iteration: 16000, loss: 0.0005969353530090303, gradient norm: tensor(0.0041)\n",
            "Iteration: 17000, loss: 0.0003000581259111641, gradient norm: tensor(0.0026)\n",
            "Iteration: 18000, loss: 0.00013996983440301845, gradient norm: tensor(0.0013)\n",
            "Iteration: 19000, loss: 7.764668816525955e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 5.681435911537847e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 4.218917014804902e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 2.8698315401925357e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 1.7909763399984514e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.0419586714306206e-05, gradient norm: tensor(9.1487e-05)\n",
            "Iteration: 25000, loss: 5.771251961505186e-06, gradient norm: tensor(5.5188e-05)\n",
            "Iteration: 26000, loss: 3.135798460789374e-06, gradient norm: tensor(3.1595e-05)\n",
            "Iteration: 27000, loss: 1.7311197851768157e-06, gradient norm: tensor(1.9013e-05)\n",
            "Iteration: 28000, loss: 9.983115962768351e-07, gradient norm: tensor(1.6108e-05)\n",
            "Iteration: 29000, loss: 6.083243675334415e-07, gradient norm: tensor(6.5771e-06)\n",
            "Iteration: 30000, loss: 3.89369385544569e-07, gradient norm: tensor(6.7394e-06)\n",
            "Iteration: 31000, loss: 2.5807446932901713e-07, gradient norm: tensor(1.0954e-05)\n",
            "Iteration: 32000, loss: 1.749054250979043e-07, gradient norm: tensor(1.2571e-05)\n",
            "Iteration: 33000, loss: 1.207990928833169e-07, gradient norm: tensor(5.5783e-06)\n",
            "Iteration: 34000, loss: 8.770738295993397e-08, gradient norm: tensor(7.3487e-06)\n",
            "Iteration: 35000, loss: 6.926897908954288e-08, gradient norm: tensor(1.2153e-05)\n",
            "Iteration: 36000, loss: 5.945141678509458e-08, gradient norm: tensor(4.7309e-06)\n",
            "Iteration: 37000, loss: 5.42396979028581e-08, gradient norm: tensor(9.3169e-07)\n",
            "Iteration: 0, loss: 9.745858386993408, gradient norm: tensor(6.8805)\n",
            "Iteration: 1000, loss: 8.198850854873657, gradient norm: tensor(6.2152)\n",
            "Iteration: 2000, loss: 6.8797856693267825, gradient norm: tensor(5.6148)\n",
            "Iteration: 3000, loss: 5.724392090797425, gradient norm: tensor(5.0701)\n",
            "Iteration: 4000, loss: 4.693989515781403, gradient norm: tensor(4.5531)\n",
            "Iteration: 5000, loss: 3.7770750753879545, gradient norm: tensor(4.0492)\n",
            "Iteration: 6000, loss: 2.9702850801944733, gradient norm: tensor(3.5608)\n",
            "Iteration: 7000, loss: 2.2690511536598206, gradient norm: tensor(3.0834)\n",
            "Iteration: 8000, loss: 1.6709495896100999, gradient norm: tensor(2.6088)\n",
            "Iteration: 9000, loss: 1.1763982917070388, gradient norm: tensor(2.1501)\n",
            "Iteration: 10000, loss: 0.7802760918736458, gradient norm: tensor(1.7148)\n",
            "Iteration: 11000, loss: 0.4758784835040569, gradient norm: tensor(1.3041)\n",
            "Iteration: 12000, loss: 0.255838698938489, gradient norm: tensor(0.9213)\n",
            "Iteration: 13000, loss: 0.11344777742028236, gradient norm: tensor(0.5740)\n",
            "Iteration: 14000, loss: 0.037916331443935636, gradient norm: tensor(0.2929)\n",
            "Iteration: 15000, loss: 0.008045560294063761, gradient norm: tensor(0.1059)\n",
            "Iteration: 16000, loss: 0.0008755469154712045, gradient norm: tensor(0.0206)\n",
            "Iteration: 17000, loss: 0.00011806787436944433, gradient norm: tensor(0.0015)\n",
            "Iteration: 18000, loss: 6.568106900158454e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 19000, loss: 3.618581836781232e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 1.982676046100096e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.4847314531834855e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.3163097797587397e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.2283212586226e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.1683443285619432e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.1051519166358048e-05, gradient norm: tensor(9.2959e-05)\n",
            "Iteration: 26000, loss: 1.0285073442901194e-05, gradient norm: tensor(8.5278e-05)\n",
            "Iteration: 27000, loss: 9.35517589277879e-06, gradient norm: tensor(7.1321e-05)\n",
            "Iteration: 28000, loss: 8.2875001407956e-06, gradient norm: tensor(5.7563e-05)\n",
            "Iteration: 29000, loss: 7.155096175665676e-06, gradient norm: tensor(4.4765e-05)\n",
            "Iteration: 30000, loss: 6.035245335169748e-06, gradient norm: tensor(3.0533e-05)\n",
            "Iteration: 31000, loss: 5.025910429139913e-06, gradient norm: tensor(2.1198e-05)\n",
            "Iteration: 32000, loss: 3.958263725280631e-06, gradient norm: tensor(2.6356e-05)\n",
            "Iteration: 33000, loss: 2.9132276870313945e-06, gradient norm: tensor(6.7594e-06)\n",
            "Iteration: 34000, loss: 2.347822102592545e-06, gradient norm: tensor(8.5797e-06)\n",
            "Iteration: 35000, loss: 2.189959702491251e-06, gradient norm: tensor(7.6542e-06)\n",
            "Iteration: 36000, loss: 2.1280866985762257e-06, gradient norm: tensor(1.7878e-06)\n",
            "Iteration: 0, loss: 3.93963760638237, gradient norm: tensor(3.9607)\n",
            "Iteration: 1000, loss: 3.0174531388282775, gradient norm: tensor(3.5068)\n",
            "Iteration: 2000, loss: 2.233253282189369, gradient norm: tensor(3.0383)\n",
            "Iteration: 3000, loss: 1.6079209059476853, gradient norm: tensor(2.5656)\n",
            "Iteration: 4000, loss: 1.1117249813079835, gradient norm: tensor(2.1049)\n",
            "Iteration: 5000, loss: 0.7230779542326927, gradient norm: tensor(1.6603)\n",
            "Iteration: 6000, loss: 0.4316170972883701, gradient norm: tensor(1.2412)\n",
            "Iteration: 7000, loss: 0.227201325237751, gradient norm: tensor(0.8565)\n",
            "Iteration: 8000, loss: 0.09871375282108784, gradient norm: tensor(0.5202)\n",
            "Iteration: 9000, loss: 0.031735558507964015, gradient norm: tensor(0.2545)\n",
            "Iteration: 10000, loss: 0.006477659762138501, gradient norm: tensor(0.0848)\n",
            "Iteration: 11000, loss: 0.0009672486144700087, gradient norm: tensor(0.0143)\n",
            "Iteration: 12000, loss: 0.0004231553881254513, gradient norm: tensor(0.0014)\n",
            "Iteration: 13000, loss: 0.0003637166480475571, gradient norm: tensor(0.0009)\n",
            "Iteration: 14000, loss: 0.0003260015262349043, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 0.00029411765732220376, gradient norm: tensor(0.0009)\n",
            "Iteration: 16000, loss: 0.00026911815101630053, gradient norm: tensor(0.0010)\n",
            "Iteration: 17000, loss: 0.00024426841505919585, gradient norm: tensor(0.0009)\n",
            "Iteration: 18000, loss: 0.00021513897918339352, gradient norm: tensor(0.0009)\n",
            "Iteration: 19000, loss: 0.00018206113800988533, gradient norm: tensor(0.0008)\n",
            "Iteration: 20000, loss: 0.00014879274787381292, gradient norm: tensor(0.0006)\n",
            "Iteration: 21000, loss: 0.00012004577805782901, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 9.639580862858566e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 7.500680210068822e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 24000, loss: 5.2810500736086395e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 2.7751749697927154e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 9.339560140688263e-06, gradient norm: tensor(5.2987e-05)\n",
            "Iteration: 27000, loss: 2.7810973974737863e-06, gradient norm: tensor(1.5630e-05)\n",
            "Iteration: 28000, loss: 9.223477081832243e-07, gradient norm: tensor(3.5293e-05)\n",
            "Iteration: 29000, loss: 6.964344996163163e-07, gradient norm: tensor(9.7393e-06)\n",
            "Iteration: 30000, loss: 5.759898911605887e-07, gradient norm: tensor(3.6137e-06)\n",
            "Iteration: 31000, loss: 4.456915598893829e-07, gradient norm: tensor(3.1204e-06)\n",
            "Iteration: 32000, loss: 3.216192534694073e-07, gradient norm: tensor(8.2404e-06)\n",
            "Iteration: 33000, loss: 2.4160988886023917e-07, gradient norm: tensor(3.9608e-05)\n",
            "Iteration: 34000, loss: 2.0744913140902098e-07, gradient norm: tensor(3.6970e-06)\n",
            "Iteration: 35000, loss: 1.9272670181180729e-07, gradient norm: tensor(3.2888e-06)\n",
            "Iteration: 36000, loss: 1.8352834810286823e-07, gradient norm: tensor(5.4625e-06)\n",
            "Iteration: 37000, loss: 1.7599059178508014e-07, gradient norm: tensor(3.0630e-05)\n",
            "Iteration: 38000, loss: 1.6916342494255332e-07, gradient norm: tensor(1.3963e-05)\n",
            "Iteration: 39000, loss: 1.6281084765523702e-07, gradient norm: tensor(1.0113e-05)\n",
            "Iteration: 40000, loss: 1.568872951196454e-07, gradient norm: tensor(1.0517e-05)\n",
            "Iteration: 41000, loss: 1.513437759399494e-07, gradient norm: tensor(3.5356e-05)\n",
            "Iteration: 42000, loss: 1.4612235966637855e-07, gradient norm: tensor(8.5842e-06)\n",
            "Iteration: 43000, loss: 1.41177701962647e-07, gradient norm: tensor(2.8716e-05)\n",
            "Iteration: 44000, loss: 1.3652151388043875e-07, gradient norm: tensor(2.0045e-05)\n",
            "Iteration: 45000, loss: 1.3210364387816753e-07, gradient norm: tensor(5.9784e-06)\n",
            "Iteration: 46000, loss: 1.2791923855104415e-07, gradient norm: tensor(1.3735e-05)\n",
            "Iteration: 47000, loss: 1.239313750573956e-07, gradient norm: tensor(1.2926e-06)\n",
            "Iteration: 0, loss: 4.500326493263245, gradient norm: tensor(4.4758)\n",
            "Iteration: 1000, loss: 3.542553413629532, gradient norm: tensor(3.9427)\n",
            "Iteration: 2000, loss: 2.7337789578437803, gradient norm: tensor(3.4377)\n",
            "Iteration: 3000, loss: 2.050694035887718, gradient norm: tensor(2.9492)\n",
            "Iteration: 4000, loss: 1.4816453778743743, gradient norm: tensor(2.4752)\n",
            "Iteration: 5000, loss: 1.0187865725159646, gradient norm: tensor(2.0171)\n",
            "Iteration: 6000, loss: 0.6554463953971863, gradient norm: tensor(1.5790)\n",
            "Iteration: 7000, loss: 0.3846373328268528, gradient norm: tensor(1.1670)\n",
            "Iteration: 8000, loss: 0.19771322083473206, gradient norm: tensor(0.7915)\n",
            "Iteration: 9000, loss: 0.08337000906839967, gradient norm: tensor(0.4676)\n",
            "Iteration: 10000, loss: 0.026122312420979142, gradient norm: tensor(0.2183)\n",
            "Iteration: 11000, loss: 0.00567556525580585, gradient norm: tensor(0.0672)\n",
            "Iteration: 12000, loss: 0.0013568545603193343, gradient norm: tensor(0.0110)\n",
            "Iteration: 13000, loss: 0.0007008589423494414, gradient norm: tensor(0.0038)\n",
            "Iteration: 14000, loss: 0.00044643911041202953, gradient norm: tensor(0.0028)\n",
            "Iteration: 15000, loss: 0.00029505663872987496, gradient norm: tensor(0.0020)\n",
            "Iteration: 16000, loss: 0.0002093272365309531, gradient norm: tensor(0.0014)\n",
            "Iteration: 17000, loss: 0.00016476287353725638, gradient norm: tensor(0.0010)\n",
            "Iteration: 18000, loss: 0.0001422066662926227, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 0.00012679013885644964, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 0.00011086649462231434, gradient norm: tensor(0.0005)\n",
            "Iteration: 21000, loss: 9.39825797395315e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 7.785198371129809e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 6.380669271675287e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 5.250808776327176e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 4.384325593855465e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 3.709382186207222e-05, gradient norm: tensor(8.1686e-05)\n",
            "Iteration: 27000, loss: 3.141772593517089e-05, gradient norm: tensor(6.4621e-05)\n",
            "Iteration: 28000, loss: 2.6283751958544598e-05, gradient norm: tensor(5.2633e-05)\n",
            "Iteration: 29000, loss: 2.158361149668053e-05, gradient norm: tensor(4.4813e-05)\n",
            "Iteration: 30000, loss: 1.743147918386967e-05, gradient norm: tensor(5.8359e-05)\n",
            "Iteration: 31000, loss: 1.3921512096203515e-05, gradient norm: tensor(2.9739e-05)\n",
            "Iteration: 32000, loss: 1.10520335201727e-05, gradient norm: tensor(2.4668e-05)\n",
            "Iteration: 33000, loss: 8.765281229898391e-06, gradient norm: tensor(1.9289e-05)\n",
            "Iteration: 34000, loss: 6.988402935348858e-06, gradient norm: tensor(1.5380e-05)\n",
            "Iteration: 35000, loss: 5.649802536936477e-06, gradient norm: tensor(1.3192e-05)\n",
            "Iteration: 36000, loss: 4.6861428904776405e-06, gradient norm: tensor(1.0088e-05)\n",
            "Iteration: 37000, loss: 4.039514633859653e-06, gradient norm: tensor(5.9480e-06)\n",
            "Iteration: 38000, loss: 3.636617919937635e-06, gradient norm: tensor(4.9387e-06)\n",
            "Iteration: 39000, loss: 3.4029803364319378e-06, gradient norm: tensor(3.0476e-06)\n",
            "Iteration: 40000, loss: 3.275886150959195e-06, gradient norm: tensor(2.0343e-06)\n",
            "Iteration: 41000, loss: 3.1958118597685823e-06, gradient norm: tensor(2.3601e-05)\n",
            "Iteration: 42000, loss: 2.968493382240922e-06, gradient norm: tensor(5.0789e-06)\n",
            "Iteration: 43000, loss: 2.606757350577027e-06, gradient norm: tensor(1.0161e-05)\n",
            "Iteration: 44000, loss: 2.4185513457268826e-06, gradient norm: tensor(2.9671e-06)\n",
            "Iteration: 45000, loss: 2.3120081191336796e-06, gradient norm: tensor(8.7138e-06)\n",
            "Iteration: 46000, loss: 2.240255312244699e-06, gradient norm: tensor(9.7089e-06)\n",
            "Iteration: 47000, loss: 2.186811678257072e-06, gradient norm: tensor(4.1850e-05)\n",
            "Iteration: 48000, loss: 2.143294931784112e-06, gradient norm: tensor(1.7856e-05)\n",
            "Iteration: 49000, loss: 2.1040216240635344e-06, gradient norm: tensor(2.0145e-06)\n",
            "Iteration: 50000, loss: 2.064424662194142e-06, gradient norm: tensor(1.3152e-05)\n",
            "Iteration: 51000, loss: 2.0227097197675905e-06, gradient norm: tensor(1.2381e-05)\n",
            "Iteration: 52000, loss: 1.987036185710167e-06, gradient norm: tensor(7.3683e-05)\n",
            "Iteration: 53000, loss: 1.9647585702387003e-06, gradient norm: tensor(3.3167e-05)\n",
            "Iteration: 54000, loss: 1.950650030266843e-06, gradient norm: tensor(8.3610e-06)\n",
            "Iteration: 55000, loss: 1.939933481935441e-06, gradient norm: tensor(1.1157e-05)\n",
            "Iteration: 56000, loss: 1.9306740953197733e-06, gradient norm: tensor(4.5896e-06)\n",
            "Iteration: 57000, loss: 1.9219955536300403e-06, gradient norm: tensor(5.2627e-06)\n",
            "Iteration: 58000, loss: 1.9133010564473808e-06, gradient norm: tensor(2.6381e-05)\n",
            "Iteration: 59000, loss: 1.9044042340965461e-06, gradient norm: tensor(2.3609e-05)\n",
            "Iteration: 60000, loss: 1.8952337466089376e-06, gradient norm: tensor(2.0217e-05)\n",
            "Iteration: 61000, loss: 1.8859149842000988e-06, gradient norm: tensor(4.8832e-06)\n",
            "Iteration: 62000, loss: 1.876572730566295e-06, gradient norm: tensor(1.6087e-05)\n",
            "Iteration: 63000, loss: 1.8674653335892798e-06, gradient norm: tensor(6.0300e-05)\n",
            "Iteration: 64000, loss: 1.8587246507877353e-06, gradient norm: tensor(1.9374e-06)\n",
            "Iteration: 0, loss: 1.404185420036316, gradient norm: tensor(1.7115)\n",
            "Iteration: 1000, loss: 0.9486714570522309, gradient norm: tensor(1.3592)\n",
            "Iteration: 2000, loss: 0.5937343103885651, gradient norm: tensor(1.0776)\n",
            "Iteration: 3000, loss: 0.31826566986739635, gradient norm: tensor(0.7930)\n",
            "Iteration: 4000, loss: 0.13432014662027358, gradient norm: tensor(0.4835)\n",
            "Iteration: 5000, loss: 0.043162024669349194, gradient norm: tensor(0.2182)\n",
            "Iteration: 6000, loss: 0.013318489727564156, gradient norm: tensor(0.0670)\n",
            "Iteration: 7000, loss: 0.006874082775786519, gradient norm: tensor(0.0293)\n",
            "Iteration: 8000, loss: 0.004458683549193665, gradient norm: tensor(0.0211)\n",
            "Iteration: 9000, loss: 0.0027109503189567475, gradient norm: tensor(0.0154)\n",
            "Iteration: 10000, loss: 0.0014560944923432543, gradient norm: tensor(0.0100)\n",
            "Iteration: 11000, loss: 0.0006872709262534045, gradient norm: tensor(0.0054)\n",
            "Iteration: 12000, loss: 0.00033436243075993846, gradient norm: tensor(0.0024)\n",
            "Iteration: 13000, loss: 0.00021330193968606181, gradient norm: tensor(0.0013)\n",
            "Iteration: 14000, loss: 0.00015386611508438363, gradient norm: tensor(0.0010)\n",
            "Iteration: 15000, loss: 9.342964724783087e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 16000, loss: 4.089815815677866e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 17000, loss: 1.3545502956731071e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 6.362649206948845e-06, gradient norm: tensor(5.3477e-05)\n",
            "Iteration: 19000, loss: 4.109696263640217e-06, gradient norm: tensor(4.2169e-05)\n",
            "Iteration: 20000, loss: 2.0479239433370823e-06, gradient norm: tensor(2.5064e-05)\n",
            "Iteration: 21000, loss: 8.667986363661839e-07, gradient norm: tensor(8.3532e-06)\n",
            "Iteration: 22000, loss: 5.458011481209724e-07, gradient norm: tensor(3.4375e-06)\n",
            "Iteration: 23000, loss: 4.5050711759131444e-07, gradient norm: tensor(2.7292e-06)\n",
            "Iteration: 24000, loss: 3.85691830331325e-07, gradient norm: tensor(2.3081e-06)\n",
            "Iteration: 25000, loss: 3.3507277325384164e-07, gradient norm: tensor(1.9571e-06)\n",
            "Iteration: 0, loss: 4.222561242341995, gradient norm: tensor(4.3160)\n",
            "Iteration: 1000, loss: 3.2713363287448884, gradient norm: tensor(3.7621)\n",
            "Iteration: 2000, loss: 2.4996719272136687, gradient norm: tensor(3.2589)\n",
            "Iteration: 3000, loss: 1.853071790575981, gradient norm: tensor(2.7771)\n",
            "Iteration: 4000, loss: 1.3179104008674623, gradient norm: tensor(2.3093)\n",
            "Iteration: 5000, loss: 0.887681725859642, gradient norm: tensor(1.8584)\n",
            "Iteration: 6000, loss: 0.5552683739364147, gradient norm: tensor(1.4295)\n",
            "Iteration: 7000, loss: 0.31290663629770277, gradient norm: tensor(1.0298)\n",
            "Iteration: 8000, loss: 0.15165553479641675, gradient norm: tensor(0.6703)\n",
            "Iteration: 9000, loss: 0.05914628328382969, gradient norm: tensor(0.3694)\n",
            "Iteration: 10000, loss: 0.01753790452517569, gradient norm: tensor(0.1530)\n",
            "Iteration: 11000, loss: 0.0050791948628611866, gradient norm: tensor(0.0398)\n",
            "Iteration: 12000, loss: 0.002928863330045715, gradient norm: tensor(0.0124)\n",
            "Iteration: 13000, loss: 0.002413592646364123, gradient norm: tensor(0.0109)\n",
            "Iteration: 14000, loss: 0.001970432230154984, gradient norm: tensor(0.0098)\n",
            "Iteration: 15000, loss: 0.0015199325494468213, gradient norm: tensor(0.0082)\n",
            "Iteration: 16000, loss: 0.0010910871441010385, gradient norm: tensor(0.0062)\n",
            "Iteration: 17000, loss: 0.0007336932778707706, gradient norm: tensor(0.0041)\n",
            "Iteration: 18000, loss: 0.000479365766630508, gradient norm: tensor(0.0024)\n",
            "Iteration: 19000, loss: 0.0003358017829887103, gradient norm: tensor(0.0013)\n",
            "Iteration: 20000, loss: 0.00026007001656398645, gradient norm: tensor(0.0009)\n",
            "Iteration: 21000, loss: 0.00020032114844070747, gradient norm: tensor(0.0007)\n",
            "Iteration: 22000, loss: 0.00014497119624866172, gradient norm: tensor(0.0005)\n",
            "Iteration: 23000, loss: 9.805294631951255e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 24000, loss: 6.208739857174805e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 3.70774569837522e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 2.119786644834676e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 27000, loss: 1.192268654540385e-05, gradient norm: tensor(7.6660e-05)\n",
            "Iteration: 28000, loss: 6.947420898995915e-06, gradient norm: tensor(4.2609e-05)\n",
            "Iteration: 29000, loss: 4.53283810566063e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 3.4917148052500126e-06, gradient norm: tensor(1.2239e-05)\n",
            "Iteration: 31000, loss: 3.0246485046063752e-06, gradient norm: tensor(9.6322e-06)\n",
            "Iteration: 32000, loss: 2.113765262265588e-06, gradient norm: tensor(1.4463e-05)\n",
            "Iteration: 33000, loss: 7.066597482889847e-07, gradient norm: tensor(8.8903e-06)\n",
            "Iteration: 34000, loss: 2.2727527685617587e-07, gradient norm: tensor(3.8884e-06)\n",
            "Iteration: 35000, loss: 1.0526545208477955e-07, gradient norm: tensor(2.1762e-05)\n",
            "Iteration: 36000, loss: 6.499263582782078e-08, gradient norm: tensor(1.6010e-05)\n",
            "Iteration: 37000, loss: 4.713267712119773e-08, gradient norm: tensor(5.4304e-06)\n",
            "Iteration: 38000, loss: 3.782637918092746e-08, gradient norm: tensor(7.6730e-05)\n",
            "Iteration: 39000, loss: 3.2411170749924166e-08, gradient norm: tensor(3.1297e-06)\n",
            "Iteration: 40000, loss: 2.8970896176261364e-08, gradient norm: tensor(1.4345e-06)\n",
            "Iteration: 0, loss: 3.729137592792511, gradient norm: tensor(4.0423)\n",
            "Iteration: 1000, loss: 2.794820608615875, gradient norm: tensor(3.4751)\n",
            "Iteration: 2000, loss: 2.046267083287239, gradient norm: tensor(2.9374)\n",
            "Iteration: 3000, loss: 1.451557594537735, gradient norm: tensor(2.4340)\n",
            "Iteration: 4000, loss: 0.9853479705452919, gradient norm: tensor(1.9704)\n",
            "Iteration: 5000, loss: 0.6257405255138874, gradient norm: tensor(1.5349)\n",
            "Iteration: 6000, loss: 0.360579759567976, gradient norm: tensor(1.1244)\n",
            "Iteration: 7000, loss: 0.18074019341915845, gradient norm: tensor(0.7511)\n",
            "Iteration: 8000, loss: 0.07362714372575282, gradient norm: tensor(0.4333)\n",
            "Iteration: 9000, loss: 0.021668109589256346, gradient norm: tensor(0.1947)\n",
            "Iteration: 10000, loss: 0.004050057031447068, gradient norm: tensor(0.0558)\n",
            "Iteration: 11000, loss: 0.0007936883614165708, gradient norm: tensor(0.0077)\n",
            "Iteration: 12000, loss: 0.0004618572361650877, gradient norm: tensor(0.0022)\n",
            "Iteration: 13000, loss: 0.00037665134758572093, gradient norm: tensor(0.0019)\n",
            "Iteration: 14000, loss: 0.0003217392959340941, gradient norm: tensor(0.0017)\n",
            "Iteration: 15000, loss: 0.0002808315459114965, gradient norm: tensor(0.0016)\n",
            "Iteration: 16000, loss: 0.0002470435485156486, gradient norm: tensor(0.0015)\n",
            "Iteration: 17000, loss: 0.00021234419956454075, gradient norm: tensor(0.0014)\n",
            "Iteration: 18000, loss: 0.0001687570489157224, gradient norm: tensor(0.0012)\n",
            "Iteration: 19000, loss: 0.00011415022034634604, gradient norm: tensor(0.0010)\n",
            "Iteration: 20000, loss: 5.828923839726485e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 1.9546627591807918e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 5.61389584186145e-06, gradient norm: tensor(6.3003e-05)\n",
            "Iteration: 23000, loss: 2.5528536201591122e-06, gradient norm: tensor(3.8859e-05)\n",
            "Iteration: 24000, loss: 8.985810130752724e-07, gradient norm: tensor(2.1253e-05)\n",
            "Iteration: 25000, loss: 4.4378501212349877e-07, gradient norm: tensor(5.5980e-06)\n",
            "Iteration: 26000, loss: 3.3711320745055675e-07, gradient norm: tensor(2.3248e-05)\n",
            "Iteration: 27000, loss: 2.4708017400598694e-07, gradient norm: tensor(1.7869e-05)\n",
            "Iteration: 28000, loss: 1.8902349235361272e-07, gradient norm: tensor(2.8113e-05)\n",
            "Iteration: 29000, loss: 1.6820523427441003e-07, gradient norm: tensor(6.2491e-06)\n",
            "Iteration: 30000, loss: 1.6030931148236504e-07, gradient norm: tensor(2.5870e-05)\n",
            "Iteration: 31000, loss: 1.5454135346715248e-07, gradient norm: tensor(3.7024e-05)\n",
            "Iteration: 32000, loss: 1.4902031274743877e-07, gradient norm: tensor(4.0647e-05)\n",
            "Iteration: 33000, loss: 1.441268679513996e-07, gradient norm: tensor(6.2587e-06)\n",
            "Iteration: 34000, loss: 1.3986891191564155e-07, gradient norm: tensor(3.0089e-05)\n",
            "Iteration: 35000, loss: 1.3610777504879934e-07, gradient norm: tensor(1.1983e-05)\n",
            "Iteration: 36000, loss: 1.327645691304724e-07, gradient norm: tensor(2.7467e-05)\n",
            "Iteration: 37000, loss: 1.2977007489212155e-07, gradient norm: tensor(7.3678e-06)\n",
            "Iteration: 38000, loss: 1.270704428435465e-07, gradient norm: tensor(3.1838e-05)\n",
            "Iteration: 39000, loss: 1.2462667402246551e-07, gradient norm: tensor(4.1977e-06)\n",
            "Iteration: 40000, loss: 1.2240403189878179e-07, gradient norm: tensor(1.0155e-05)\n",
            "Iteration: 41000, loss: 1.203865537462434e-07, gradient norm: tensor(7.4476e-06)\n",
            "Iteration: 42000, loss: 1.1853502782344094e-07, gradient norm: tensor(8.4740e-06)\n",
            "Iteration: 43000, loss: 1.1685256119875476e-07, gradient norm: tensor(1.5496e-06)\n",
            "Iteration: 0, loss: 1.2346689000725746, gradient norm: tensor(1.6043)\n",
            "Iteration: 1000, loss: 0.7740733150243759, gradient norm: tensor(1.3318)\n",
            "Iteration: 2000, loss: 0.4182495745420456, gradient norm: tensor(0.9916)\n",
            "Iteration: 3000, loss: 0.18420393989980222, gradient norm: tensor(0.6200)\n",
            "Iteration: 4000, loss: 0.061890362810343505, gradient norm: tensor(0.2965)\n",
            "Iteration: 5000, loss: 0.018568514693528413, gradient norm: tensor(0.0978)\n",
            "Iteration: 6000, loss: 0.0099931147582829, gradient norm: tensor(0.0437)\n",
            "Iteration: 7000, loss: 0.007917899232823402, gradient norm: tensor(0.0349)\n",
            "Iteration: 8000, loss: 0.006076433707494289, gradient norm: tensor(0.0283)\n",
            "Iteration: 9000, loss: 0.004278636987553909, gradient norm: tensor(0.0211)\n",
            "Iteration: 10000, loss: 0.0028229636044707147, gradient norm: tensor(0.0144)\n",
            "Iteration: 11000, loss: 0.0018073200973449275, gradient norm: tensor(0.0097)\n",
            "Iteration: 12000, loss: 0.0010707231934648008, gradient norm: tensor(0.0064)\n",
            "Iteration: 13000, loss: 0.0005703905692207627, gradient norm: tensor(0.0035)\n",
            "Iteration: 14000, loss: 0.0003030229945288738, gradient norm: tensor(0.0015)\n",
            "Iteration: 15000, loss: 0.00018282144745171536, gradient norm: tensor(0.0007)\n",
            "Iteration: 16000, loss: 0.00012515092566172826, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 9.198976545303595e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 6.891454125798191e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 4.979515276863822e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 3.47475342459802e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 2.4060787865892054e-05, gradient norm: tensor(9.3384e-05)\n",
            "Iteration: 22000, loss: 1.6524435995052046e-05, gradient norm: tensor(6.6013e-05)\n",
            "Iteration: 23000, loss: 1.1123288245471486e-05, gradient norm: tensor(4.5464e-05)\n",
            "Iteration: 24000, loss: 7.383417766050115e-06, gradient norm: tensor(3.0318e-05)\n",
            "Iteration: 25000, loss: 4.957065892085666e-06, gradient norm: tensor(1.9642e-05)\n",
            "Iteration: 26000, loss: 3.4808128386885073e-06, gradient norm: tensor(2.1699e-05)\n",
            "Iteration: 27000, loss: 2.610421949384545e-06, gradient norm: tensor(1.8264e-05)\n",
            "Iteration: 28000, loss: 2.083197395108982e-06, gradient norm: tensor(5.7560e-06)\n",
            "Iteration: 29000, loss: 1.8005270584353638e-06, gradient norm: tensor(2.8732e-05)\n",
            "Iteration: 30000, loss: 1.7301691227658012e-06, gradient norm: tensor(6.7014e-06)\n",
            "Iteration: 31000, loss: 1.725567717926424e-06, gradient norm: tensor(1.1317e-06)\n",
            "Iteration: 0, loss: 3.5844914727211, gradient norm: tensor(3.8558)\n",
            "Iteration: 1000, loss: 2.729325845003128, gradient norm: tensor(3.3647)\n",
            "Iteration: 2000, loss: 2.005709553837776, gradient norm: tensor(2.8913)\n",
            "Iteration: 3000, loss: 1.4054981536865234, gradient norm: tensor(2.4025)\n",
            "Iteration: 4000, loss: 0.9357177317738533, gradient norm: tensor(1.9165)\n",
            "Iteration: 5000, loss: 0.5866539744436741, gradient norm: tensor(1.4666)\n",
            "Iteration: 6000, loss: 0.33516085949540136, gradient norm: tensor(1.0665)\n",
            "Iteration: 7000, loss: 0.16387960506975652, gradient norm: tensor(0.7046)\n",
            "Iteration: 8000, loss: 0.06336490105837583, gradient norm: tensor(0.3960)\n",
            "Iteration: 9000, loss: 0.017151260539423674, gradient norm: tensor(0.1691)\n",
            "Iteration: 10000, loss: 0.002837999222043436, gradient norm: tensor(0.0443)\n",
            "Iteration: 11000, loss: 0.00047933027122053315, gradient norm: tensor(0.0052)\n",
            "Iteration: 12000, loss: 0.00023903240665094927, gradient norm: tensor(0.0012)\n",
            "Iteration: 13000, loss: 0.0001565133874100866, gradient norm: tensor(0.0008)\n",
            "Iteration: 14000, loss: 0.00010550209266511956, gradient norm: tensor(0.0006)\n",
            "Iteration: 15000, loss: 7.493994795368053e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 16000, loss: 5.764655654274975e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 17000, loss: 4.7505035854555897e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 18000, loss: 4.006182165903738e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 3.2025415704993065e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 2.209922833026212e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 1.4098124476731754e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 8.446867381280753e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 4.1168697373450416e-06, gradient norm: tensor(4.8553e-05)\n",
            "Iteration: 24000, loss: 1.8955607251882612e-06, gradient norm: tensor(1.7182e-05)\n",
            "Iteration: 25000, loss: 1.3957329838376609e-06, gradient norm: tensor(1.3942e-05)\n",
            "Iteration: 26000, loss: 1.1609925639959329e-06, gradient norm: tensor(2.0108e-05)\n",
            "Iteration: 27000, loss: 9.369000597416743e-07, gradient norm: tensor(1.1448e-05)\n",
            "Iteration: 28000, loss: 7.479435123514122e-07, gradient norm: tensor(1.2254e-05)\n",
            "Iteration: 29000, loss: 6.07760986838457e-07, gradient norm: tensor(1.2436e-05)\n",
            "Iteration: 30000, loss: 5.121530950873421e-07, gradient norm: tensor(9.4478e-06)\n",
            "Iteration: 31000, loss: 4.485359537511613e-07, gradient norm: tensor(2.9805e-05)\n",
            "Iteration: 32000, loss: 4.0713661798008614e-07, gradient norm: tensor(1.5886e-05)\n",
            "Iteration: 33000, loss: 3.78337682690244e-07, gradient norm: tensor(3.0951e-06)\n",
            "Iteration: 34000, loss: 3.5560683900826007e-07, gradient norm: tensor(2.1097e-06)\n",
            "Iteration: 35000, loss: 3.362882046928917e-07, gradient norm: tensor(1.6350e-05)\n",
            "Iteration: 36000, loss: 3.190283370031466e-07, gradient norm: tensor(1.0261e-05)\n",
            "Iteration: 37000, loss: 3.03413850645029e-07, gradient norm: tensor(2.5001e-06)\n",
            "Iteration: 38000, loss: 2.89183463792142e-07, gradient norm: tensor(1.2607e-05)\n",
            "Iteration: 39000, loss: 2.760544489603944e-07, gradient norm: tensor(1.3765e-05)\n",
            "Iteration: 40000, loss: 2.639012118095252e-07, gradient norm: tensor(1.1418e-05)\n",
            "Iteration: 41000, loss: 2.525727456657023e-07, gradient norm: tensor(1.9903e-06)\n",
            "Iteration: 0, loss: 10.10046095275879, gradient norm: tensor(6.8358)\n",
            "Iteration: 1000, loss: 8.517247661113739, gradient norm: tensor(6.2632)\n",
            "Iteration: 2000, loss: 7.107681848049164, gradient norm: tensor(5.7237)\n",
            "Iteration: 3000, loss: 5.862681601047516, gradient norm: tensor(5.1806)\n",
            "Iteration: 4000, loss: 4.777121500492096, gradient norm: tensor(4.6455)\n",
            "Iteration: 5000, loss: 3.831606037855148, gradient norm: tensor(4.1243)\n",
            "Iteration: 6000, loss: 3.009308478116989, gradient norm: tensor(3.6166)\n",
            "Iteration: 7000, loss: 2.3010155489444735, gradient norm: tensor(3.1277)\n",
            "Iteration: 8000, loss: 1.6982204134464265, gradient norm: tensor(2.6579)\n",
            "Iteration: 9000, loss: 1.196419063925743, gradient norm: tensor(2.2000)\n",
            "Iteration: 10000, loss: 0.7932310814857483, gradient norm: tensor(1.7573)\n",
            "Iteration: 11000, loss: 0.4838900205492973, gradient norm: tensor(1.3356)\n",
            "Iteration: 12000, loss: 0.2620199178010225, gradient norm: tensor(0.9428)\n",
            "Iteration: 13000, loss: 0.1189530050382018, gradient norm: tensor(0.5929)\n",
            "Iteration: 14000, loss: 0.041120998814702034, gradient norm: tensor(0.3084)\n",
            "Iteration: 15000, loss: 0.009136899777688086, gradient norm: tensor(0.1151)\n",
            "Iteration: 16000, loss: 0.001027257755806204, gradient norm: tensor(0.0237)\n",
            "Iteration: 17000, loss: 8.212916112097446e-05, gradient norm: tensor(0.0019)\n",
            "Iteration: 18000, loss: 2.4769754794760957e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 1.3226645980466855e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 9.379044583511131e-06, gradient norm: tensor(6.7477e-05)\n",
            "Iteration: 21000, loss: 7.698146046550392e-06, gradient norm: tensor(4.9073e-05)\n",
            "Iteration: 22000, loss: 6.676006213183427e-06, gradient norm: tensor(4.8147e-05)\n",
            "Iteration: 23000, loss: 6.102484961957088e-06, gradient norm: tensor(4.2147e-05)\n",
            "Iteration: 24000, loss: 5.532416408186691e-06, gradient norm: tensor(4.0053e-05)\n",
            "Iteration: 25000, loss: 4.641121717213536e-06, gradient norm: tensor(3.8518e-05)\n",
            "Iteration: 26000, loss: 3.981505400133756e-06, gradient norm: tensor(4.0386e-05)\n",
            "Iteration: 27000, loss: 3.751769657128534e-06, gradient norm: tensor(3.1644e-05)\n",
            "Iteration: 28000, loss: 3.551207495092967e-06, gradient norm: tensor(3.2695e-05)\n",
            "Iteration: 29000, loss: 3.293657536232786e-06, gradient norm: tensor(2.4855e-05)\n",
            "Iteration: 30000, loss: 2.974897940930532e-06, gradient norm: tensor(2.8565e-05)\n",
            "Iteration: 31000, loss: 2.5997893369549275e-06, gradient norm: tensor(1.7457e-05)\n",
            "Iteration: 32000, loss: 2.17648339958032e-06, gradient norm: tensor(1.5223e-05)\n",
            "Iteration: 33000, loss: 1.6531059247881786e-06, gradient norm: tensor(2.3498e-05)\n",
            "Iteration: 34000, loss: 8.354491488375971e-07, gradient norm: tensor(1.0614e-05)\n",
            "Iteration: 35000, loss: 3.2486047076929483e-07, gradient norm: tensor(9.2842e-06)\n",
            "Iteration: 36000, loss: 1.4922403143913242e-07, gradient norm: tensor(6.0805e-06)\n",
            "Iteration: 37000, loss: 7.212465920858336e-08, gradient norm: tensor(7.8279e-06)\n",
            "Iteration: 38000, loss: 3.826086251912386e-08, gradient norm: tensor(3.1277e-06)\n",
            "Iteration: 39000, loss: 2.411440316052449e-08, gradient norm: tensor(2.6711e-06)\n",
            "Iteration: 40000, loss: 1.803587593052214e-08, gradient norm: tensor(3.7791e-06)\n",
            "Iteration: 41000, loss: 1.5341194048446027e-08, gradient norm: tensor(2.6804e-05)\n",
            "Iteration: 42000, loss: 1.4111242733427787e-08, gradient norm: tensor(1.3139e-05)\n",
            "Iteration: 43000, loss: 1.3517574974919455e-08, gradient norm: tensor(2.7408e-05)\n",
            "Iteration: 44000, loss: 1.3248986786784656e-08, gradient norm: tensor(1.6060e-06)\n",
            "Iteration: 0, loss: 3.5833496603965758, gradient norm: tensor(4.0164)\n",
            "Iteration: 1000, loss: 2.7091933767795564, gradient norm: tensor(3.3965)\n",
            "Iteration: 2000, loss: 2.0170294204950334, gradient norm: tensor(2.8786)\n",
            "Iteration: 3000, loss: 1.4488532528877258, gradient norm: tensor(2.4048)\n",
            "Iteration: 4000, loss: 0.989852957546711, gradient norm: tensor(1.9605)\n",
            "Iteration: 5000, loss: 0.6305213369727135, gradient norm: tensor(1.5306)\n",
            "Iteration: 6000, loss: 0.36420647126436234, gradient norm: tensor(1.1217)\n",
            "Iteration: 7000, loss: 0.18286294792592525, gradient norm: tensor(0.7498)\n",
            "Iteration: 8000, loss: 0.07461908380314708, gradient norm: tensor(0.4327)\n",
            "Iteration: 9000, loss: 0.02254608894791454, gradient norm: tensor(0.1943)\n",
            "Iteration: 10000, loss: 0.00520931313210167, gradient norm: tensor(0.0563)\n",
            "Iteration: 11000, loss: 0.001957823560689576, gradient norm: tensor(0.0111)\n",
            "Iteration: 12000, loss: 0.001413409566041082, gradient norm: tensor(0.0071)\n",
            "Iteration: 13000, loss: 0.0010064083043253048, gradient norm: tensor(0.0052)\n",
            "Iteration: 14000, loss: 0.0006394695057533682, gradient norm: tensor(0.0031)\n",
            "Iteration: 15000, loss: 0.00040332077807397584, gradient norm: tensor(0.0021)\n",
            "Iteration: 16000, loss: 0.00027667756962182466, gradient norm: tensor(0.0016)\n",
            "Iteration: 17000, loss: 0.00019245056934596506, gradient norm: tensor(0.0013)\n",
            "Iteration: 18000, loss: 0.00012641112297569635, gradient norm: tensor(0.0009)\n",
            "Iteration: 19000, loss: 7.995171036600368e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 5.6673470127861944e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 4.88102558083483e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 4.4492135926702755e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 4.019321259329445e-05, gradient norm: tensor(9.8191e-05)\n",
            "Iteration: 24000, loss: 3.5897997393476544e-05, gradient norm: tensor(8.0395e-05)\n",
            "Iteration: 25000, loss: 3.157987507256621e-05, gradient norm: tensor(6.5262e-05)\n",
            "Iteration: 26000, loss: 2.7021344772947487e-05, gradient norm: tensor(6.1643e-05)\n",
            "Iteration: 27000, loss: 2.195278315411997e-05, gradient norm: tensor(5.4122e-05)\n",
            "Iteration: 28000, loss: 1.6175770270820065e-05, gradient norm: tensor(4.9532e-05)\n",
            "Iteration: 29000, loss: 1.0373054653427971e-05, gradient norm: tensor(3.6950e-05)\n",
            "Iteration: 30000, loss: 5.9985294437865374e-06, gradient norm: tensor(2.2732e-05)\n",
            "Iteration: 31000, loss: 3.4254291783781808e-06, gradient norm: tensor(1.6879e-05)\n",
            "Iteration: 32000, loss: 1.9400675686256363e-06, gradient norm: tensor(1.0159e-05)\n",
            "Iteration: 33000, loss: 1.12593335342126e-06, gradient norm: tensor(7.0947e-06)\n",
            "Iteration: 34000, loss: 7.200729086775936e-07, gradient norm: tensor(6.7898e-06)\n",
            "Iteration: 35000, loss: 5.116866720982216e-07, gradient norm: tensor(7.0677e-06)\n",
            "Iteration: 36000, loss: 3.9542227622746395e-07, gradient norm: tensor(1.1345e-05)\n",
            "Iteration: 37000, loss: 3.311022347247672e-07, gradient norm: tensor(6.7629e-06)\n",
            "Iteration: 38000, loss: 2.9539906492459523e-07, gradient norm: tensor(4.9046e-06)\n",
            "Iteration: 39000, loss: 2.725201389921494e-07, gradient norm: tensor(7.0755e-06)\n",
            "Iteration: 40000, loss: 2.549446770672148e-07, gradient norm: tensor(1.0401e-05)\n",
            "Iteration: 41000, loss: 2.400712868677601e-07, gradient norm: tensor(2.6366e-06)\n",
            "Iteration: 42000, loss: 2.2689006810594492e-07, gradient norm: tensor(2.3192e-06)\n",
            "Iteration: 43000, loss: 2.1516464133242152e-07, gradient norm: tensor(0.0002)\n",
            "Iteration: 44000, loss: 2.0447337615792095e-07, gradient norm: tensor(6.0384e-06)\n",
            "Iteration: 45000, loss: 1.9477359107611392e-07, gradient norm: tensor(1.7822e-05)\n",
            "Iteration: 46000, loss: 1.8583313239162181e-07, gradient norm: tensor(1.3756e-06)\n",
            "Iteration: 0, loss: 1.8096857662200927, gradient norm: tensor(2.5295)\n",
            "Iteration: 1000, loss: 1.2439381768107414, gradient norm: tensor(2.1180)\n",
            "Iteration: 2000, loss: 0.7862112753987313, gradient norm: tensor(1.6875)\n",
            "Iteration: 3000, loss: 0.446994803249836, gradient norm: tensor(1.2501)\n",
            "Iteration: 4000, loss: 0.22268020851910114, gradient norm: tensor(0.8444)\n",
            "Iteration: 5000, loss: 0.09220515260472893, gradient norm: tensor(0.5015)\n",
            "Iteration: 6000, loss: 0.028217413868755103, gradient norm: tensor(0.2386)\n",
            "Iteration: 7000, loss: 0.005428543495479971, gradient norm: tensor(0.0764)\n",
            "Iteration: 8000, loss: 0.0007380150757380761, gradient norm: tensor(0.0124)\n",
            "Iteration: 9000, loss: 0.000220976082739071, gradient norm: tensor(0.0018)\n",
            "Iteration: 10000, loss: 0.00010798055915802252, gradient norm: tensor(0.0010)\n",
            "Iteration: 11000, loss: 5.63366427377332e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 12000, loss: 3.6281815155234654e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 13000, loss: 3.0322983555379325e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 14000, loss: 2.874477551995369e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 15000, loss: 2.7876878617462353e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 16000, loss: 2.6962360461766366e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 2.574677827396954e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 18000, loss: 2.3963609584825462e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 2.1400630534117227e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 1.797774408987607e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.3887794884794858e-05, gradient norm: tensor(8.5944e-05)\n",
            "Iteration: 22000, loss: 9.92976448196714e-06, gradient norm: tensor(5.9765e-05)\n",
            "Iteration: 23000, loss: 6.885844049520528e-06, gradient norm: tensor(3.9321e-05)\n",
            "Iteration: 24000, loss: 4.878453461515164e-06, gradient norm: tensor(2.4431e-05)\n",
            "Iteration: 25000, loss: 3.6049059337983635e-06, gradient norm: tensor(2.0128e-05)\n",
            "Iteration: 26000, loss: 2.7609791152372053e-06, gradient norm: tensor(1.3049e-05)\n",
            "Iteration: 27000, loss: 2.1283619913674557e-06, gradient norm: tensor(8.5277e-06)\n",
            "Iteration: 28000, loss: 1.5922630088880396e-06, gradient norm: tensor(8.5480e-06)\n",
            "Iteration: 29000, loss: 1.1520214213760481e-06, gradient norm: tensor(1.8251e-05)\n",
            "Iteration: 30000, loss: 8.318520725651979e-07, gradient norm: tensor(5.3988e-06)\n",
            "Iteration: 31000, loss: 6.123413630234609e-07, gradient norm: tensor(6.2246e-06)\n",
            "Iteration: 32000, loss: 4.5738982973375644e-07, gradient norm: tensor(3.7444e-06)\n",
            "Iteration: 33000, loss: 3.4646991153408635e-07, gradient norm: tensor(7.4479e-06)\n",
            "Iteration: 34000, loss: 2.6554554393953823e-07, gradient norm: tensor(2.8155e-05)\n",
            "Iteration: 35000, loss: 2.0569008300697078e-07, gradient norm: tensor(1.8904e-05)\n",
            "Iteration: 36000, loss: 1.6109485790138934e-07, gradient norm: tensor(4.8552e-06)\n",
            "Iteration: 37000, loss: 1.2733850519452972e-07, gradient norm: tensor(6.5938e-06)\n",
            "Iteration: 38000, loss: 1.0152934917329048e-07, gradient norm: tensor(2.2751e-05)\n",
            "Iteration: 39000, loss: 8.165926707448534e-08, gradient norm: tensor(1.5881e-05)\n",
            "Iteration: 40000, loss: 6.6232747862216e-08, gradient norm: tensor(2.2878e-05)\n",
            "Iteration: 41000, loss: 5.421128336990932e-08, gradient norm: tensor(1.3334e-05)\n",
            "Iteration: 42000, loss: 4.4635884574262265e-08, gradient norm: tensor(3.7630e-06)\n",
            "Iteration: 43000, loss: 3.707074917613795e-08, gradient norm: tensor(1.9101e-05)\n",
            "Iteration: 44000, loss: 3.1049026373963786e-08, gradient norm: tensor(1.0460e-05)\n",
            "Iteration: 45000, loss: 2.6183641246291243e-08, gradient norm: tensor(1.0453e-05)\n",
            "Iteration: 46000, loss: 2.2265862336112717e-08, gradient norm: tensor(1.8133e-06)\n",
            "Iteration: 0, loss: 1.9708798854351044, gradient norm: tensor(2.8525)\n",
            "Iteration: 1000, loss: 1.3513645008802413, gradient norm: tensor(2.3228)\n",
            "Iteration: 2000, loss: 0.8943392646312713, gradient norm: tensor(1.8497)\n",
            "Iteration: 3000, loss: 0.5520764206051827, gradient norm: tensor(1.4135)\n",
            "Iteration: 4000, loss: 0.3071048548966646, gradient norm: tensor(1.0123)\n",
            "Iteration: 5000, loss: 0.1462198613733053, gradient norm: tensor(0.6542)\n",
            "Iteration: 6000, loss: 0.05517265727557242, gradient norm: tensor(0.3560)\n",
            "Iteration: 7000, loss: 0.015289520670194179, gradient norm: tensor(0.1437)\n",
            "Iteration: 8000, loss: 0.0038702328386716546, gradient norm: tensor(0.0346)\n",
            "Iteration: 9000, loss: 0.0019511960810050368, gradient norm: tensor(0.0069)\n",
            "Iteration: 10000, loss: 0.001338133172132075, gradient norm: tensor(0.0055)\n",
            "Iteration: 11000, loss: 0.0008026008218876086, gradient norm: tensor(0.0045)\n",
            "Iteration: 12000, loss: 0.0004087084377533756, gradient norm: tensor(0.0032)\n",
            "Iteration: 13000, loss: 0.00018513920919212978, gradient norm: tensor(0.0019)\n",
            "Iteration: 14000, loss: 9.106669419270474e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 6.448552180881961e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 16000, loss: 5.727889667105046e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 5.1545937560149466e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 4.521040300096501e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 3.820662963335053e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 3.0535968320691606e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 2.1941752283964888e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.1220393845633225e-05, gradient norm: tensor(9.1598e-05)\n",
            "Iteration: 23000, loss: 2.9551765162523227e-06, gradient norm: tensor(3.0226e-05)\n",
            "Iteration: 24000, loss: 1.3968236163464098e-06, gradient norm: tensor(1.6829e-05)\n",
            "Iteration: 25000, loss: 7.861542562750401e-07, gradient norm: tensor(9.5645e-06)\n",
            "Iteration: 26000, loss: 5.496555493209599e-07, gradient norm: tensor(4.3547e-06)\n",
            "Iteration: 27000, loss: 4.776490113442833e-07, gradient norm: tensor(1.5424e-06)\n",
            "Iteration: 0, loss: 3.243392436981201, gradient norm: tensor(3.6897)\n",
            "Iteration: 1000, loss: 2.4048328273296358, gradient norm: tensor(3.1635)\n",
            "Iteration: 2000, loss: 1.7280086796283722, gradient norm: tensor(2.6593)\n",
            "Iteration: 3000, loss: 1.1936186872124672, gradient norm: tensor(2.1854)\n",
            "Iteration: 4000, loss: 0.7808302693367004, gradient norm: tensor(1.7359)\n",
            "Iteration: 5000, loss: 0.47234750816226007, gradient norm: tensor(1.3115)\n",
            "Iteration: 6000, loss: 0.2545203291773796, gradient norm: tensor(0.9194)\n",
            "Iteration: 7000, loss: 0.1156156663671136, gradient norm: tensor(0.5732)\n",
            "Iteration: 8000, loss: 0.04091582727245986, gradient norm: tensor(0.2938)\n",
            "Iteration: 9000, loss: 0.010881025344599038, gradient norm: tensor(0.1067)\n",
            "Iteration: 10000, loss: 0.0035139865197706967, gradient norm: tensor(0.0211)\n",
            "Iteration: 11000, loss: 0.0025119878861587495, gradient norm: tensor(0.0033)\n",
            "Iteration: 12000, loss: 0.0021071308087557555, gradient norm: tensor(0.0037)\n",
            "Iteration: 13000, loss: 0.0017030168351484462, gradient norm: tensor(0.0041)\n",
            "Iteration: 14000, loss: 0.0013581229615956545, gradient norm: tensor(0.0043)\n",
            "Iteration: 15000, loss: 0.0010325107029057108, gradient norm: tensor(0.0041)\n",
            "Iteration: 16000, loss: 0.0007082380703650415, gradient norm: tensor(0.0036)\n",
            "Iteration: 17000, loss: 0.0004139046122436412, gradient norm: tensor(0.0026)\n",
            "Iteration: 18000, loss: 0.0002114983820792986, gradient norm: tensor(0.0014)\n",
            "Iteration: 19000, loss: 0.00012501592504850123, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 0.0001024384969714447, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 9.116214612731711e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 7.862143241072773e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 6.534563078457722e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 5.2929438104911243e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 4.238797741345479e-05, gradient norm: tensor(9.8238e-05)\n",
            "Iteration: 26000, loss: 3.38595738830918e-05, gradient norm: tensor(7.4824e-05)\n",
            "Iteration: 27000, loss: 2.7053690368120442e-05, gradient norm: tensor(5.9328e-05)\n",
            "Iteration: 28000, loss: 2.161877314938465e-05, gradient norm: tensor(4.8585e-05)\n",
            "Iteration: 29000, loss: 1.7256528522921143e-05, gradient norm: tensor(4.0213e-05)\n",
            "Iteration: 30000, loss: 1.3747863185017195e-05, gradient norm: tensor(3.3764e-05)\n",
            "Iteration: 31000, loss: 1.094359020680713e-05, gradient norm: tensor(3.3516e-05)\n",
            "Iteration: 32000, loss: 8.728747172426665e-06, gradient norm: tensor(3.3012e-05)\n",
            "Iteration: 33000, loss: 6.817087158651703e-06, gradient norm: tensor(2.1902e-05)\n",
            "Iteration: 34000, loss: 5.079888179807313e-06, gradient norm: tensor(1.4607e-05)\n",
            "Iteration: 35000, loss: 3.833307363947825e-06, gradient norm: tensor(1.5762e-05)\n",
            "Iteration: 36000, loss: 2.8800022739687847e-06, gradient norm: tensor(1.1627e-05)\n",
            "Iteration: 37000, loss: 2.1495481204283352e-06, gradient norm: tensor(1.3139e-05)\n",
            "Iteration: 38000, loss: 1.5989912750455914e-06, gradient norm: tensor(6.1064e-06)\n",
            "Iteration: 39000, loss: 1.1928841793178435e-06, gradient norm: tensor(4.8719e-06)\n",
            "Iteration: 40000, loss: 9.002231357158052e-07, gradient norm: tensor(4.6595e-05)\n",
            "Iteration: 41000, loss: 6.91790090570521e-07, gradient norm: tensor(5.6297e-06)\n",
            "Iteration: 42000, loss: 5.439447708681655e-07, gradient norm: tensor(3.2855e-06)\n",
            "Iteration: 43000, loss: 4.390810808558854e-07, gradient norm: tensor(5.1581e-06)\n",
            "Iteration: 44000, loss: 3.62791195044565e-07, gradient norm: tensor(2.8774e-05)\n",
            "Iteration: 45000, loss: 3.0653054616891494e-07, gradient norm: tensor(8.3491e-06)\n",
            "Iteration: 46000, loss: 2.643622471225626e-07, gradient norm: tensor(5.9852e-05)\n",
            "Iteration: 47000, loss: 2.3230123269968318e-07, gradient norm: tensor(4.8218e-05)\n",
            "Iteration: 48000, loss: 2.074253064563436e-07, gradient norm: tensor(5.0508e-06)\n",
            "Iteration: 49000, loss: 1.8765054467451137e-07, gradient norm: tensor(1.8309e-06)\n",
            "Iteration: 0, loss: 4.061175259351731, gradient norm: tensor(4.1541)\n",
            "Iteration: 1000, loss: 3.1497479183673858, gradient norm: tensor(3.6358)\n",
            "Iteration: 2000, loss: 2.384433285474777, gradient norm: tensor(3.1452)\n",
            "Iteration: 3000, loss: 1.750762688755989, gradient norm: tensor(2.6723)\n",
            "Iteration: 4000, loss: 1.2332889316082, gradient norm: tensor(2.2119)\n",
            "Iteration: 5000, loss: 0.8206006756424904, gradient norm: tensor(1.7678)\n",
            "Iteration: 6000, loss: 0.5047013528048993, gradient norm: tensor(1.3476)\n",
            "Iteration: 7000, loss: 0.27651568928360937, gradient norm: tensor(0.9580)\n",
            "Iteration: 8000, loss: 0.12698548551648856, gradient norm: tensor(0.6078)\n",
            "Iteration: 9000, loss: 0.04474606672488153, gradient norm: tensor(0.3192)\n",
            "Iteration: 10000, loss: 0.010674336931668221, gradient norm: tensor(0.1210)\n",
            "Iteration: 11000, loss: 0.0017444682465866208, gradient norm: tensor(0.0259)\n",
            "Iteration: 12000, loss: 0.0005150472188252025, gradient norm: tensor(0.0044)\n",
            "Iteration: 13000, loss: 0.0003033725228451658, gradient norm: tensor(0.0030)\n",
            "Iteration: 14000, loss: 0.00018368616032239516, gradient norm: tensor(0.0021)\n",
            "Iteration: 15000, loss: 0.00011814084291108885, gradient norm: tensor(0.0013)\n",
            "Iteration: 16000, loss: 9.060743088048184e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 17000, loss: 8.111580388504081e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 18000, loss: 7.56183308258187e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 19000, loss: 6.869308125897077e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 5.939173433580436e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 4.808251760186977e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 3.6084101746382656e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 2.5153225478788955e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 1.6466745301841003e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.0288479027622089e-05, gradient norm: tensor(7.5526e-05)\n",
            "Iteration: 26000, loss: 6.253448348161328e-06, gradient norm: tensor(5.0530e-05)\n",
            "Iteration: 27000, loss: 3.7721599412634534e-06, gradient norm: tensor(3.1626e-05)\n",
            "Iteration: 28000, loss: 2.2183711977277197e-06, gradient norm: tensor(3.1975e-05)\n",
            "Iteration: 29000, loss: 1.1447854760149311e-06, gradient norm: tensor(1.1699e-05)\n",
            "Iteration: 30000, loss: 6.271545574634274e-07, gradient norm: tensor(1.6865e-05)\n",
            "Iteration: 31000, loss: 3.551947798712263e-07, gradient norm: tensor(2.1041e-05)\n",
            "Iteration: 32000, loss: 2.1272376004333182e-07, gradient norm: tensor(4.4938e-06)\n",
            "Iteration: 33000, loss: 1.402236361229825e-07, gradient norm: tensor(1.7504e-06)\n",
            "Iteration: 0, loss: 8.56959822845459, gradient norm: tensor(6.2473)\n",
            "Iteration: 1000, loss: 7.174891483783722, gradient norm: tensor(5.7031)\n",
            "Iteration: 2000, loss: 5.958166308879853, gradient norm: tensor(5.1817)\n",
            "Iteration: 3000, loss: 4.882939348697662, gradient norm: tensor(4.6715)\n",
            "Iteration: 4000, loss: 3.932570123195648, gradient norm: tensor(4.1701)\n",
            "Iteration: 5000, loss: 3.098225115776062, gradient norm: tensor(3.6776)\n",
            "Iteration: 6000, loss: 2.3744143731594085, gradient norm: tensor(3.1929)\n",
            "Iteration: 7000, loss: 1.7576497761011123, gradient norm: tensor(2.7168)\n",
            "Iteration: 8000, loss: 1.24481902217865, gradient norm: tensor(2.2531)\n",
            "Iteration: 9000, loss: 0.8317741280794144, gradient norm: tensor(1.8061)\n",
            "Iteration: 10000, loss: 0.5129955385029316, gradient norm: tensor(1.3799)\n",
            "Iteration: 11000, loss: 0.2821926969885826, gradient norm: tensor(0.9824)\n",
            "Iteration: 12000, loss: 0.13102479685097934, gradient norm: tensor(0.6260)\n",
            "Iteration: 13000, loss: 0.047056359086185696, gradient norm: tensor(0.3339)\n",
            "Iteration: 14000, loss: 0.011136374043300748, gradient norm: tensor(0.1305)\n",
            "Iteration: 15000, loss: 0.0013640300488623324, gradient norm: tensor(0.0291)\n",
            "Iteration: 16000, loss: 9.043804593056848e-05, gradient norm: tensor(0.0025)\n",
            "Iteration: 17000, loss: 1.3578956017681775e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 5.657961706674541e-06, gradient norm: tensor(3.5130e-05)\n",
            "Iteration: 19000, loss: 4.359824395578471e-06, gradient norm: tensor(2.3627e-05)\n",
            "Iteration: 20000, loss: 3.806066854394885e-06, gradient norm: tensor(1.7658e-05)\n",
            "Iteration: 21000, loss: 3.433186338270389e-06, gradient norm: tensor(1.2680e-05)\n",
            "Iteration: 22000, loss: 3.2093312622691884e-06, gradient norm: tensor(1.2278e-05)\n",
            "Iteration: 23000, loss: 3.120427519434088e-06, gradient norm: tensor(1.2972e-05)\n",
            "Iteration: 24000, loss: 3.100811739841447e-06, gradient norm: tensor(9.4862e-06)\n",
            "Iteration: 25000, loss: 3.096427196624063e-06, gradient norm: tensor(1.2224e-05)\n",
            "Iteration: 26000, loss: 3.0914043099983245e-06, gradient norm: tensor(7.1705e-06)\n",
            "Iteration: 27000, loss: 3.0833022292426903e-06, gradient norm: tensor(6.7773e-06)\n",
            "Iteration: 28000, loss: 3.070260485174003e-06, gradient norm: tensor(9.8446e-06)\n",
            "Iteration: 29000, loss: 3.049774873034039e-06, gradient norm: tensor(6.4929e-06)\n",
            "Iteration: 30000, loss: 3.0184158283645956e-06, gradient norm: tensor(5.5449e-06)\n",
            "Iteration: 31000, loss: 2.9725528609105823e-06, gradient norm: tensor(1.5839e-05)\n",
            "Iteration: 32000, loss: 2.9099922028308355e-06, gradient norm: tensor(7.3657e-06)\n",
            "Iteration: 33000, loss: 2.8326927515536225e-06, gradient norm: tensor(2.2869e-05)\n",
            "Iteration: 34000, loss: 2.74793943253826e-06, gradient norm: tensor(6.2885e-06)\n",
            "Iteration: 35000, loss: 2.6644613622011092e-06, gradient norm: tensor(3.3088e-06)\n",
            "Iteration: 36000, loss: 2.589492725974196e-06, gradient norm: tensor(2.4898e-05)\n",
            "Iteration: 37000, loss: 2.529807029532094e-06, gradient norm: tensor(6.2721e-06)\n",
            "Iteration: 38000, loss: 2.4792781000542163e-06, gradient norm: tensor(2.1010e-05)\n",
            "Iteration: 39000, loss: 2.428195773518382e-06, gradient norm: tensor(7.3111e-06)\n",
            "Iteration: 40000, loss: 2.366721926819082e-06, gradient norm: tensor(7.9448e-06)\n",
            "Iteration: 41000, loss: 2.2892425949976313e-06, gradient norm: tensor(1.9489e-05)\n",
            "Iteration: 42000, loss: 2.2165329712606762e-06, gradient norm: tensor(6.5559e-06)\n",
            "Iteration: 43000, loss: 2.1695359425848435e-06, gradient norm: tensor(1.1781e-05)\n",
            "Iteration: 44000, loss: 2.135949626790534e-06, gradient norm: tensor(1.0690e-05)\n",
            "Iteration: 45000, loss: 2.1079735251987584e-06, gradient norm: tensor(1.0528e-05)\n",
            "Iteration: 46000, loss: 2.083855729097195e-06, gradient norm: tensor(1.5437e-05)\n",
            "Iteration: 47000, loss: 2.0627124079055646e-06, gradient norm: tensor(2.6362e-05)\n",
            "Iteration: 48000, loss: 2.0438335670860396e-06, gradient norm: tensor(5.2793e-05)\n",
            "Iteration: 49000, loss: 2.026842017585295e-06, gradient norm: tensor(2.3619e-05)\n",
            "Iteration: 50000, loss: 2.0113789871629704e-06, gradient norm: tensor(2.0185e-05)\n",
            "Iteration: 51000, loss: 1.9971793635704673e-06, gradient norm: tensor(9.9261e-07)\n",
            "Iteration: 0, loss: 1.9401800882816316, gradient norm: tensor(2.2464)\n",
            "Iteration: 1000, loss: 1.3876267272233962, gradient norm: tensor(1.8725)\n",
            "Iteration: 2000, loss: 0.9302852119207382, gradient norm: tensor(1.5252)\n",
            "Iteration: 3000, loss: 0.5728566768169403, gradient norm: tensor(1.1820)\n",
            "Iteration: 4000, loss: 0.31287257924675943, gradient norm: tensor(0.8464)\n",
            "Iteration: 5000, loss: 0.14220480524748563, gradient norm: tensor(0.5290)\n",
            "Iteration: 6000, loss: 0.0502693284843117, gradient norm: tensor(0.2563)\n",
            "Iteration: 7000, loss: 0.016984954881481825, gradient norm: tensor(0.0803)\n",
            "Iteration: 8000, loss: 0.0104146545836702, gradient norm: tensor(0.0171)\n",
            "Iteration: 9000, loss: 0.008991934715770184, gradient norm: tensor(0.0116)\n",
            "Iteration: 10000, loss: 0.0076419475385919216, gradient norm: tensor(0.0138)\n",
            "Iteration: 11000, loss: 0.005802532337605954, gradient norm: tensor(0.0161)\n",
            "Iteration: 12000, loss: 0.003279606537660584, gradient norm: tensor(0.0137)\n",
            "Iteration: 13000, loss: 0.0012381163969403133, gradient norm: tensor(0.0073)\n",
            "Iteration: 14000, loss: 0.00046475215142709203, gradient norm: tensor(0.0035)\n",
            "Iteration: 15000, loss: 0.00021562581349280662, gradient norm: tensor(0.0019)\n",
            "Iteration: 16000, loss: 9.544660714163911e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 17000, loss: 4.885596242456813e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 3.7197562469373225e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 3.1535689251541044e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 2.1269890017720174e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 5.4241738075688775e-06, gradient norm: tensor(4.7144e-05)\n",
            "Iteration: 22000, loss: 1.9046913998863602e-06, gradient norm: tensor(2.2310e-05)\n",
            "Iteration: 23000, loss: 1.1100496961944373e-06, gradient norm: tensor(1.2149e-05)\n",
            "Iteration: 24000, loss: 7.616036983222329e-07, gradient norm: tensor(8.6029e-06)\n",
            "Iteration: 25000, loss: 5.182373675722829e-07, gradient norm: tensor(5.9241e-06)\n",
            "Iteration: 26000, loss: 3.2973462339214165e-07, gradient norm: tensor(4.4247e-06)\n",
            "Iteration: 27000, loss: 1.9419961468258863e-07, gradient norm: tensor(3.2970e-06)\n",
            "Iteration: 28000, loss: 1.0684656151482841e-07, gradient norm: tensor(4.6316e-05)\n",
            "Iteration: 29000, loss: 5.912573935518139e-08, gradient norm: tensor(1.1750e-05)\n",
            "Iteration: 30000, loss: 3.5423051970795425e-08, gradient norm: tensor(2.8812e-05)\n",
            "Iteration: 31000, loss: 2.366044583190785e-08, gradient norm: tensor(1.2130e-05)\n",
            "Iteration: 32000, loss: 1.76175121495703e-08, gradient norm: tensor(1.9034e-05)\n",
            "Iteration: 33000, loss: 1.4385725110699355e-08, gradient norm: tensor(6.0042e-06)\n",
            "Iteration: 34000, loss: 1.2600717203881117e-08, gradient norm: tensor(8.3017e-06)\n",
            "Iteration: 35000, loss: 1.1566762782244667e-08, gradient norm: tensor(3.6827e-06)\n",
            "Iteration: 36000, loss: 1.0928833582113384e-08, gradient norm: tensor(9.4110e-06)\n",
            "Iteration: 37000, loss: 1.0507955704674998e-08, gradient norm: tensor(1.5128e-06)\n",
            "Iteration: 0, loss: 8.20910908985138, gradient norm: tensor(6.0583)\n",
            "Iteration: 1000, loss: 6.803403010845185, gradient norm: tensor(5.4048)\n",
            "Iteration: 2000, loss: 5.57394131565094, gradient norm: tensor(4.8072)\n",
            "Iteration: 3000, loss: 4.489682460308075, gradient norm: tensor(4.2459)\n",
            "Iteration: 4000, loss: 3.5477173504829405, gradient norm: tensor(3.7264)\n",
            "Iteration: 5000, loss: 2.7337464656829833, gradient norm: tensor(3.2485)\n",
            "Iteration: 6000, loss: 2.032005092382431, gradient norm: tensor(2.8029)\n",
            "Iteration: 7000, loss: 1.442803671836853, gradient norm: tensor(2.3648)\n",
            "Iteration: 8000, loss: 0.9733941826224327, gradient norm: tensor(1.9275)\n",
            "Iteration: 9000, loss: 0.6167851202487945, gradient norm: tensor(1.4983)\n",
            "Iteration: 10000, loss: 0.3578171284198761, gradient norm: tensor(1.0923)\n",
            "Iteration: 11000, loss: 0.1830552578419447, gradient norm: tensor(0.7251)\n",
            "Iteration: 12000, loss: 0.07922363151609897, gradient norm: tensor(0.4138)\n",
            "Iteration: 13000, loss: 0.029556203860789537, gradient norm: tensor(0.1821)\n",
            "Iteration: 14000, loss: 0.012814155080355704, gradient norm: tensor(0.0519)\n",
            "Iteration: 15000, loss: 0.008519347306806594, gradient norm: tensor(0.0143)\n",
            "Iteration: 16000, loss: 0.0064477107450366025, gradient norm: tensor(0.0139)\n",
            "Iteration: 17000, loss: 0.004691305687418207, gradient norm: tensor(0.0150)\n",
            "Iteration: 18000, loss: 0.002971228652866557, gradient norm: tensor(0.0140)\n",
            "Iteration: 19000, loss: 0.0015154259625123814, gradient norm: tensor(0.0089)\n",
            "Iteration: 20000, loss: 0.0008031614302308298, gradient norm: tensor(0.0054)\n",
            "Iteration: 21000, loss: 0.00045781824688310735, gradient norm: tensor(0.0035)\n",
            "Iteration: 22000, loss: 0.0002484848753665574, gradient norm: tensor(0.0022)\n",
            "Iteration: 23000, loss: 0.00012256350306415697, gradient norm: tensor(0.0012)\n",
            "Iteration: 24000, loss: 6.237854920618702e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 25000, loss: 4.0779276187095096e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 26000, loss: 3.055987386505876e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 2.2093557765401784e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 1.4713345341078821e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 29000, loss: 9.083814873974917e-06, gradient norm: tensor(7.5837e-05)\n",
            "Iteration: 30000, loss: 5.310330829161103e-06, gradient norm: tensor(4.8838e-05)\n",
            "Iteration: 31000, loss: 2.985901160400317e-06, gradient norm: tensor(3.0559e-05)\n",
            "Iteration: 32000, loss: 1.6191071030107195e-06, gradient norm: tensor(1.8536e-05)\n",
            "Iteration: 33000, loss: 8.374207577048765e-07, gradient norm: tensor(1.1318e-05)\n",
            "Iteration: 34000, loss: 4.021035940695583e-07, gradient norm: tensor(1.4459e-05)\n",
            "Iteration: 35000, loss: 1.7010417465002091e-07, gradient norm: tensor(1.7214e-05)\n",
            "Iteration: 36000, loss: 6.230313915267516e-08, gradient norm: tensor(3.2998e-06)\n",
            "Iteration: 37000, loss: 2.2325636916953327e-08, gradient norm: tensor(8.4887e-06)\n",
            "Iteration: 38000, loss: 9.286982287637357e-09, gradient norm: tensor(1.8475e-06)\n",
            "Iteration: 0, loss: 4.861193576335907, gradient norm: tensor(4.8901)\n",
            "Iteration: 1000, loss: 3.798556003570557, gradient norm: tensor(4.1655)\n",
            "Iteration: 2000, loss: 2.9527257018089292, gradient norm: tensor(3.5309)\n",
            "Iteration: 3000, loss: 2.257640883684158, gradient norm: tensor(2.9989)\n",
            "Iteration: 4000, loss: 1.6693607872724534, gradient norm: tensor(2.5593)\n",
            "Iteration: 5000, loss: 1.1718736634850502, gradient norm: tensor(2.1369)\n",
            "Iteration: 6000, loss: 0.7708135668635369, gradient norm: tensor(1.7040)\n",
            "Iteration: 7000, loss: 0.46659348756074903, gradient norm: tensor(1.2852)\n",
            "Iteration: 8000, loss: 0.25183700385689733, gradient norm: tensor(0.8956)\n",
            "Iteration: 9000, loss: 0.11611857576668262, gradient norm: tensor(0.5521)\n",
            "Iteration: 10000, loss: 0.04401427418924868, gradient norm: tensor(0.2801)\n",
            "Iteration: 11000, loss: 0.013970929022412747, gradient norm: tensor(0.1039)\n",
            "Iteration: 12000, loss: 0.005122772638453171, gradient norm: tensor(0.0282)\n",
            "Iteration: 13000, loss: 0.0030250159432180226, gradient norm: tensor(0.0128)\n",
            "Iteration: 14000, loss: 0.0022684503234922885, gradient norm: tensor(0.0075)\n",
            "Iteration: 15000, loss: 0.0019426316929748281, gradient norm: tensor(0.0042)\n",
            "Iteration: 16000, loss: 0.0018242821496678517, gradient norm: tensor(0.0027)\n",
            "Iteration: 17000, loss: 0.0017679698794381693, gradient norm: tensor(0.0026)\n",
            "Iteration: 18000, loss: 0.0016814974881708621, gradient norm: tensor(0.0036)\n",
            "Iteration: 19000, loss: 0.0012963369155768304, gradient norm: tensor(0.0054)\n",
            "Iteration: 20000, loss: 0.00044148564014176374, gradient norm: tensor(0.0023)\n",
            "Iteration: 21000, loss: 0.0001412601875490509, gradient norm: tensor(0.0007)\n",
            "Iteration: 22000, loss: 9.34130718524102e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 7.802043897390832e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 6.51404327727505e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 5.279315001826035e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 4.052721233892953e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 27000, loss: 2.8961067529962748e-05, gradient norm: tensor(9.2293e-05)\n",
            "Iteration: 28000, loss: 1.9058044414123288e-05, gradient norm: tensor(6.8021e-05)\n",
            "Iteration: 29000, loss: 1.1523164136633568e-05, gradient norm: tensor(4.6244e-05)\n",
            "Iteration: 30000, loss: 6.7080009102937765e-06, gradient norm: tensor(2.5077e-05)\n",
            "Iteration: 31000, loss: 4.4001333608321145e-06, gradient norm: tensor(3.1675e-05)\n",
            "Iteration: 32000, loss: 3.5875585913345276e-06, gradient norm: tensor(7.2854e-05)\n",
            "Iteration: 33000, loss: 2.991250524019051e-06, gradient norm: tensor(1.4915e-05)\n",
            "Iteration: 34000, loss: 2.519593240094764e-06, gradient norm: tensor(8.4554e-06)\n",
            "Iteration: 35000, loss: 2.2983120099979716e-06, gradient norm: tensor(1.0243e-05)\n",
            "Iteration: 36000, loss: 2.1921007551100046e-06, gradient norm: tensor(3.8281e-06)\n",
            "Iteration: 37000, loss: 2.139264876404923e-06, gradient norm: tensor(1.8514e-06)\n",
            "Iteration: 0, loss: 4.16831146144867, gradient norm: tensor(4.2303)\n",
            "Iteration: 1000, loss: 3.247163962841034, gradient norm: tensor(3.7110)\n",
            "Iteration: 2000, loss: 2.472577390193939, gradient norm: tensor(3.2164)\n",
            "Iteration: 3000, loss: 1.8254792906045914, gradient norm: tensor(2.7430)\n",
            "Iteration: 4000, loss: 1.2916234052181244, gradient norm: tensor(2.2841)\n",
            "Iteration: 5000, loss: 0.86319095236063, gradient norm: tensor(1.8377)\n",
            "Iteration: 6000, loss: 0.5344875834882259, gradient norm: tensor(1.4087)\n",
            "Iteration: 7000, loss: 0.29776693104207513, gradient norm: tensor(1.0082)\n",
            "Iteration: 8000, loss: 0.14227033944427966, gradient norm: tensor(0.6500)\n",
            "Iteration: 9000, loss: 0.05417634900473058, gradient norm: tensor(0.3530)\n",
            "Iteration: 10000, loss: 0.015372886487748474, gradient norm: tensor(0.1421)\n",
            "Iteration: 11000, loss: 0.003964704002719373, gradient norm: tensor(0.0340)\n",
            "Iteration: 12000, loss: 0.0015712629444897175, gradient norm: tensor(0.0064)\n",
            "Iteration: 13000, loss: 0.0008167202248005196, gradient norm: tensor(0.0043)\n",
            "Iteration: 14000, loss: 0.00042086587849189525, gradient norm: tensor(0.0029)\n",
            "Iteration: 15000, loss: 0.0002188333544472698, gradient norm: tensor(0.0016)\n",
            "Iteration: 16000, loss: 0.00013603608868288574, gradient norm: tensor(0.0007)\n",
            "Iteration: 17000, loss: 0.00011359532827191288, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 0.00010966437325259903, gradient norm: tensor(0.0003)\n",
            "Iteration: 19000, loss: 0.00010749641832808265, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 0.00010425374862825266, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 9.93184537437628e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 9.211178521218244e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 8.244834210927366e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 7.109519022196764e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 5.92134116741363e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 4.7900772955472345e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 27000, loss: 3.793670806771843e-05, gradient norm: tensor(9.3348e-05)\n",
            "Iteration: 28000, loss: 2.9570666813015123e-05, gradient norm: tensor(7.9369e-05)\n",
            "Iteration: 29000, loss: 2.2698097109241643e-05, gradient norm: tensor(6.6363e-05)\n",
            "Iteration: 30000, loss: 1.7186241310810148e-05, gradient norm: tensor(5.5828e-05)\n",
            "Iteration: 31000, loss: 1.2924268771712377e-05, gradient norm: tensor(4.7672e-05)\n",
            "Iteration: 32000, loss: 9.714425003949145e-06, gradient norm: tensor(4.5656e-05)\n",
            "Iteration: 33000, loss: 7.340570605720132e-06, gradient norm: tensor(2.6000e-05)\n",
            "Iteration: 34000, loss: 5.623095439659665e-06, gradient norm: tensor(4.0488e-05)\n",
            "Iteration: 35000, loss: 4.413121556353872e-06, gradient norm: tensor(1.3227e-05)\n",
            "Iteration: 36000, loss: 3.592068393572845e-06, gradient norm: tensor(8.5350e-06)\n",
            "Iteration: 37000, loss: 3.0637603795184987e-06, gradient norm: tensor(1.4479e-05)\n",
            "Iteration: 38000, loss: 2.749466789964572e-06, gradient norm: tensor(1.9934e-05)\n",
            "Iteration: 39000, loss: 2.5803345060921857e-06, gradient norm: tensor(1.2664e-05)\n",
            "Iteration: 40000, loss: 2.4966000635231465e-06, gradient norm: tensor(1.2838e-05)\n",
            "Iteration: 41000, loss: 2.456064277339465e-06, gradient norm: tensor(5.6633e-05)\n",
            "Iteration: 42000, loss: 2.4361143753139913e-06, gradient norm: tensor(2.2449e-05)\n",
            "Iteration: 43000, loss: 2.4260926809347437e-06, gradient norm: tensor(2.3639e-05)\n",
            "Iteration: 44000, loss: 2.420914251842987e-06, gradient norm: tensor(5.2206e-06)\n",
            "Iteration: 45000, loss: 2.4181435192076605e-06, gradient norm: tensor(5.3408e-06)\n",
            "Iteration: 46000, loss: 2.416516006178426e-06, gradient norm: tensor(8.4612e-06)\n",
            "Iteration: 47000, loss: 2.4155192570560756e-06, gradient norm: tensor(2.3739e-05)\n",
            "Iteration: 48000, loss: 2.4148213919943374e-06, gradient norm: tensor(3.4710e-06)\n",
            "Iteration: 49000, loss: 2.414314230918535e-06, gradient norm: tensor(1.5740e-05)\n",
            "Iteration: 50000, loss: 2.4138920962286646e-06, gradient norm: tensor(2.7928e-05)\n",
            "Iteration: 51000, loss: 2.4135028374985266e-06, gradient norm: tensor(1.7016e-05)\n",
            "Iteration: 52000, loss: 2.4131527015924804e-06, gradient norm: tensor(1.7645e-05)\n",
            "Iteration: 53000, loss: 2.412842145531613e-06, gradient norm: tensor(4.0262e-06)\n",
            "Iteration: 54000, loss: 2.4125215386447964e-06, gradient norm: tensor(2.9688e-06)\n",
            "Iteration: 55000, loss: 2.4122152092331818e-06, gradient norm: tensor(6.4849e-05)\n",
            "Iteration: 56000, loss: 2.411936537100701e-06, gradient norm: tensor(1.0768e-05)\n",
            "Iteration: 57000, loss: 2.41164220005885e-06, gradient norm: tensor(4.7039e-06)\n",
            "Iteration: 58000, loss: 2.41136480917703e-06, gradient norm: tensor(2.8503e-05)\n",
            "Iteration: 59000, loss: 2.411112656545811e-06, gradient norm: tensor(1.9037e-06)\n",
            "Iteration: 0, loss: 6.617377326011658, gradient norm: tensor(5.5168)\n",
            "Iteration: 1000, loss: 5.397997269630432, gradient norm: tensor(4.9330)\n",
            "Iteration: 2000, loss: 4.36615110874176, gradient norm: tensor(4.3959)\n",
            "Iteration: 3000, loss: 3.473093668222427, gradient norm: tensor(3.8908)\n",
            "Iteration: 4000, loss: 2.69599774646759, gradient norm: tensor(3.4035)\n",
            "Iteration: 5000, loss: 2.0277503222227096, gradient norm: tensor(2.9270)\n",
            "Iteration: 6000, loss: 1.4655690937042236, gradient norm: tensor(2.4602)\n",
            "Iteration: 7000, loss: 1.0060737978816032, gradient norm: tensor(2.0064)\n",
            "Iteration: 8000, loss: 0.6447723934054375, gradient norm: tensor(1.5702)\n",
            "Iteration: 9000, loss: 0.3758685076534748, gradient norm: tensor(1.1587)\n",
            "Iteration: 10000, loss: 0.19122907948493958, gradient norm: tensor(0.7830)\n",
            "Iteration: 11000, loss: 0.0793485978320241, gradient norm: tensor(0.4599)\n",
            "Iteration: 12000, loss: 0.024172856908291577, gradient norm: tensor(0.2127)\n",
            "Iteration: 13000, loss: 0.005011217752005905, gradient norm: tensor(0.0645)\n",
            "Iteration: 14000, loss: 0.001253552660753485, gradient norm: tensor(0.0101)\n",
            "Iteration: 15000, loss: 0.0007911487045348622, gradient norm: tensor(0.0036)\n",
            "Iteration: 16000, loss: 0.0005639313284191303, gradient norm: tensor(0.0030)\n",
            "Iteration: 17000, loss: 0.0003575052670785226, gradient norm: tensor(0.0024)\n",
            "Iteration: 18000, loss: 0.00020839618197351228, gradient norm: tensor(0.0018)\n",
            "Iteration: 19000, loss: 0.0001203439046657877, gradient norm: tensor(0.0013)\n",
            "Iteration: 20000, loss: 7.0632365259371e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 21000, loss: 4.148275950137759e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 22000, loss: 2.4417872966296273e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 1.55424893637246e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.0992843544045173e-05, gradient norm: tensor(8.1710e-05)\n",
            "Iteration: 25000, loss: 7.561252662526386e-06, gradient norm: tensor(6.0982e-05)\n",
            "Iteration: 26000, loss: 4.532948936002867e-06, gradient norm: tensor(3.3718e-05)\n",
            "Iteration: 27000, loss: 2.668691696953829e-06, gradient norm: tensor(1.9057e-05)\n",
            "Iteration: 28000, loss: 1.856126248981127e-06, gradient norm: tensor(1.3140e-05)\n",
            "Iteration: 29000, loss: 1.5749001679523645e-06, gradient norm: tensor(7.3104e-06)\n",
            "Iteration: 30000, loss: 1.4909754206655635e-06, gradient norm: tensor(5.2445e-06)\n",
            "Iteration: 31000, loss: 1.4460424813478312e-06, gradient norm: tensor(3.0503e-06)\n",
            "Iteration: 32000, loss: 1.396864773823836e-06, gradient norm: tensor(3.4141e-06)\n",
            "Iteration: 33000, loss: 1.328649071183463e-06, gradient norm: tensor(7.0393e-06)\n",
            "Iteration: 34000, loss: 1.2380746768485552e-06, gradient norm: tensor(7.9435e-06)\n",
            "Iteration: 35000, loss: 1.1417620600013835e-06, gradient norm: tensor(1.2987e-05)\n",
            "Iteration: 36000, loss: 1.0563856378666969e-06, gradient norm: tensor(1.7014e-05)\n",
            "Iteration: 37000, loss: 9.806216252172817e-07, gradient norm: tensor(5.5134e-05)\n",
            "Iteration: 38000, loss: 9.128202759143278e-07, gradient norm: tensor(1.3585e-05)\n",
            "Iteration: 39000, loss: 8.515291726212127e-07, gradient norm: tensor(1.1600e-05)\n",
            "Iteration: 40000, loss: 7.967098148355945e-07, gradient norm: tensor(4.5538e-05)\n",
            "Iteration: 41000, loss: 7.479361912601235e-07, gradient norm: tensor(2.7490e-06)\n",
            "Iteration: 42000, loss: 7.04696959417106e-07, gradient norm: tensor(4.6621e-05)\n",
            "Iteration: 43000, loss: 6.660668503286615e-07, gradient norm: tensor(3.4947e-05)\n",
            "Iteration: 44000, loss: 6.313005757760947e-07, gradient norm: tensor(6.3672e-06)\n",
            "Iteration: 45000, loss: 5.997824287646836e-07, gradient norm: tensor(1.1446e-05)\n",
            "Iteration: 46000, loss: 5.710324635401775e-07, gradient norm: tensor(1.5612e-05)\n",
            "Iteration: 47000, loss: 5.444735533615131e-07, gradient norm: tensor(1.4069e-05)\n",
            "Iteration: 48000, loss: 5.199391866312908e-07, gradient norm: tensor(7.5694e-06)\n",
            "Iteration: 49000, loss: 4.969773624452501e-07, gradient norm: tensor(7.0888e-06)\n",
            "Iteration: 50000, loss: 4.755002464946756e-07, gradient norm: tensor(6.1349e-06)\n",
            "Iteration: 51000, loss: 4.553083978180439e-07, gradient norm: tensor(6.4368e-06)\n",
            "Iteration: 52000, loss: 4.3627778353538813e-07, gradient norm: tensor(2.3748e-05)\n",
            "Iteration: 53000, loss: 4.1834201121559997e-07, gradient norm: tensor(3.8337e-05)\n",
            "Iteration: 54000, loss: 4.013006860077439e-07, gradient norm: tensor(3.1129e-05)\n",
            "Iteration: 55000, loss: 3.8522695405163177e-07, gradient norm: tensor(3.3038e-06)\n",
            "Iteration: 56000, loss: 3.699517436643873e-07, gradient norm: tensor(2.9570e-06)\n",
            "Iteration: 57000, loss: 3.5545801060266057e-07, gradient norm: tensor(1.1050e-05)\n",
            "Iteration: 58000, loss: 3.4168728163308515e-07, gradient norm: tensor(3.3432e-05)\n",
            "Iteration: 59000, loss: 3.2847187594597926e-07, gradient norm: tensor(4.2204e-06)\n",
            "Iteration: 60000, loss: 3.1583854595851335e-07, gradient norm: tensor(1.7341e-06)\n",
            "Iteration: 0, loss: 2.4138513963222503, gradient norm: tensor(2.9466)\n",
            "Iteration: 1000, loss: 1.6859508979320526, gradient norm: tensor(2.4635)\n",
            "Iteration: 2000, loss: 1.1242235572934152, gradient norm: tensor(2.0058)\n",
            "Iteration: 3000, loss: 0.7075497685670853, gradient norm: tensor(1.5725)\n",
            "Iteration: 4000, loss: 0.40768171960115435, gradient norm: tensor(1.1637)\n",
            "Iteration: 5000, loss: 0.20519939582049845, gradient norm: tensor(0.7879)\n",
            "Iteration: 6000, loss: 0.08448041922226548, gradient norm: tensor(0.4622)\n",
            "Iteration: 7000, loss: 0.025622670946642757, gradient norm: tensor(0.2125)\n",
            "Iteration: 8000, loss: 0.005180615116260014, gradient norm: tensor(0.0636)\n",
            "Iteration: 9000, loss: 0.0010436879806220531, gradient norm: tensor(0.0101)\n",
            "Iteration: 10000, loss: 0.0004979259049287066, gradient norm: tensor(0.0034)\n",
            "Iteration: 11000, loss: 0.0003324548406235408, gradient norm: tensor(0.0023)\n",
            "Iteration: 12000, loss: 0.0002458479464548873, gradient norm: tensor(0.0015)\n",
            "Iteration: 13000, loss: 0.0002042340192710981, gradient norm: tensor(0.0011)\n",
            "Iteration: 14000, loss: 0.00018054430342453998, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 0.00015857046870223712, gradient norm: tensor(0.0008)\n",
            "Iteration: 16000, loss: 0.00013599978739512153, gradient norm: tensor(0.0007)\n",
            "Iteration: 17000, loss: 0.00011413294051453704, gradient norm: tensor(0.0006)\n",
            "Iteration: 18000, loss: 9.270135455881245e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 19000, loss: 7.144851940029184e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 5.051872595504392e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 3.172641466699133e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 2.0270886989237624e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.4418621889490169e-05, gradient norm: tensor(6.7852e-05)\n",
            "Iteration: 24000, loss: 1.0896080948441522e-05, gradient norm: tensor(4.5593e-05)\n",
            "Iteration: 25000, loss: 8.414709847329504e-06, gradient norm: tensor(2.9555e-05)\n",
            "Iteration: 26000, loss: 6.5365025279788825e-06, gradient norm: tensor(2.0102e-05)\n",
            "Iteration: 27000, loss: 5.0561921129883555e-06, gradient norm: tensor(2.7886e-05)\n",
            "Iteration: 28000, loss: 3.9411832556197626e-06, gradient norm: tensor(9.9387e-06)\n",
            "Iteration: 29000, loss: 3.188341013810714e-06, gradient norm: tensor(4.7368e-05)\n",
            "Iteration: 30000, loss: 2.7440198823569518e-06, gradient norm: tensor(1.1525e-05)\n",
            "Iteration: 31000, loss: 2.517565869766258e-06, gradient norm: tensor(1.6731e-05)\n",
            "Iteration: 32000, loss: 2.40937526837115e-06, gradient norm: tensor(2.9090e-06)\n",
            "Iteration: 33000, loss: 2.357120510396271e-06, gradient norm: tensor(1.2472e-05)\n",
            "Iteration: 34000, loss: 2.3302746253648367e-06, gradient norm: tensor(5.4577e-05)\n",
            "Iteration: 35000, loss: 2.3149100231876217e-06, gradient norm: tensor(9.6244e-06)\n",
            "Iteration: 36000, loss: 2.3048019420457423e-06, gradient norm: tensor(2.8785e-05)\n",
            "Iteration: 37000, loss: 2.2971835867338085e-06, gradient norm: tensor(3.1171e-06)\n",
            "Iteration: 38000, loss: 2.2908476828433777e-06, gradient norm: tensor(1.8383e-05)\n",
            "Iteration: 39000, loss: 2.2851976632409786e-06, gradient norm: tensor(8.0160e-05)\n",
            "Iteration: 40000, loss: 2.279892539490902e-06, gradient norm: tensor(8.7563e-06)\n",
            "Iteration: 41000, loss: 2.2748429466901145e-06, gradient norm: tensor(7.9778e-05)\n",
            "Iteration: 42000, loss: 2.2699550427205396e-06, gradient norm: tensor(1.4759e-05)\n",
            "Iteration: 43000, loss: 2.2651965980458042e-06, gradient norm: tensor(2.5765e-05)\n",
            "Iteration: 44000, loss: 2.260568465317192e-06, gradient norm: tensor(1.5984e-05)\n",
            "Iteration: 45000, loss: 2.2560463276022346e-06, gradient norm: tensor(3.7671e-06)\n",
            "Iteration: 46000, loss: 2.251639175710807e-06, gradient norm: tensor(5.9009e-06)\n",
            "Iteration: 47000, loss: 2.2473425005955506e-06, gradient norm: tensor(8.3201e-05)\n",
            "Iteration: 48000, loss: 2.243152540131632e-06, gradient norm: tensor(6.2728e-06)\n",
            "Iteration: 49000, loss: 2.2390160399936577e-06, gradient norm: tensor(6.3942e-05)\n",
            "Iteration: 50000, loss: 2.235013990457446e-06, gradient norm: tensor(1.5502e-05)\n",
            "Iteration: 51000, loss: 2.23108538057204e-06, gradient norm: tensor(3.3547e-06)\n",
            "Iteration: 52000, loss: 2.227264074008417e-06, gradient norm: tensor(1.0784e-06)\n",
            "Iteration: 0, loss: 6.200893243312835, gradient norm: tensor(5.2937)\n",
            "Iteration: 1000, loss: 5.057734318733216, gradient norm: tensor(4.7443)\n",
            "Iteration: 2000, loss: 4.068966694355011, gradient norm: tensor(4.2155)\n",
            "Iteration: 3000, loss: 3.2149284801483153, gradient norm: tensor(3.6941)\n",
            "Iteration: 4000, loss: 2.4832947335243225, gradient norm: tensor(3.2069)\n",
            "Iteration: 5000, loss: 1.857702231168747, gradient norm: tensor(2.7439)\n",
            "Iteration: 6000, loss: 1.33200144135952, gradient norm: tensor(2.2911)\n",
            "Iteration: 7000, loss: 0.9031953582763672, gradient norm: tensor(1.8512)\n",
            "Iteration: 8000, loss: 0.5676041251420975, gradient norm: tensor(1.4302)\n",
            "Iteration: 9000, loss: 0.32023365500569345, gradient norm: tensor(1.0351)\n",
            "Iteration: 10000, loss: 0.15439958460628986, gradient norm: tensor(0.6755)\n",
            "Iteration: 11000, loss: 0.05863143270649016, gradient norm: tensor(0.3723)\n",
            "Iteration: 12000, loss: 0.01542515179514885, gradient norm: tensor(0.1526)\n",
            "Iteration: 13000, loss: 0.0027752208126476034, gradient norm: tensor(0.0372)\n",
            "Iteration: 14000, loss: 0.0008324178368784487, gradient norm: tensor(0.0056)\n",
            "Iteration: 15000, loss: 0.0005651009901775978, gradient norm: tensor(0.0037)\n",
            "Iteration: 16000, loss: 0.0004330384597124066, gradient norm: tensor(0.0030)\n",
            "Iteration: 17000, loss: 0.00034697901824256406, gradient norm: tensor(0.0023)\n",
            "Iteration: 18000, loss: 0.00029179668208234946, gradient norm: tensor(0.0017)\n",
            "Iteration: 19000, loss: 0.00025598908445681446, gradient norm: tensor(0.0013)\n",
            "Iteration: 20000, loss: 0.00023046909032564144, gradient norm: tensor(0.0010)\n",
            "Iteration: 21000, loss: 0.00020528009414556435, gradient norm: tensor(0.0008)\n",
            "Iteration: 22000, loss: 0.00017464729757921306, gradient norm: tensor(0.0007)\n",
            "Iteration: 23000, loss: 0.00013967672397120624, gradient norm: tensor(0.0006)\n",
            "Iteration: 24000, loss: 0.00010440131590439705, gradient norm: tensor(0.0004)\n",
            "Iteration: 25000, loss: 7.308536367781926e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 26000, loss: 4.865043047539075e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 3.197390672175971e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 28000, loss: 2.1876786559005267e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 29000, loss: 1.6206174340368307e-05, gradient norm: tensor(5.0776e-05)\n",
            "Iteration: 30000, loss: 1.2831339645344997e-05, gradient norm: tensor(3.4919e-05)\n",
            "Iteration: 31000, loss: 1.041201884709153e-05, gradient norm: tensor(2.6546e-05)\n",
            "Iteration: 32000, loss: 8.45218528274927e-06, gradient norm: tensor(2.0280e-05)\n",
            "Iteration: 33000, loss: 6.83795854820346e-06, gradient norm: tensor(1.6780e-05)\n",
            "Iteration: 34000, loss: 5.203419334520731e-06, gradient norm: tensor(1.4941e-05)\n",
            "Iteration: 35000, loss: 3.7876292124110478e-06, gradient norm: tensor(1.4674e-05)\n",
            "Iteration: 36000, loss: 2.799612176204391e-06, gradient norm: tensor(1.1849e-05)\n",
            "Iteration: 37000, loss: 2.068185678012924e-06, gradient norm: tensor(8.3981e-06)\n",
            "Iteration: 38000, loss: 1.5253170987534758e-06, gradient norm: tensor(1.4329e-05)\n",
            "Iteration: 39000, loss: 1.1253877312356053e-06, gradient norm: tensor(3.3369e-05)\n",
            "Iteration: 40000, loss: 8.356804585787358e-07, gradient norm: tensor(1.1384e-05)\n",
            "Iteration: 41000, loss: 6.276445507182871e-07, gradient norm: tensor(9.6207e-06)\n",
            "Iteration: 42000, loss: 4.788262217800821e-07, gradient norm: tensor(1.2598e-05)\n",
            "Iteration: 43000, loss: 3.720193891751933e-07, gradient norm: tensor(6.5973e-06)\n",
            "Iteration: 44000, loss: 2.9412014828267273e-07, gradient norm: tensor(2.5155e-05)\n",
            "Iteration: 45000, loss: 2.3647836235340946e-07, gradient norm: tensor(9.5810e-06)\n",
            "Iteration: 46000, loss: 1.9349353601683106e-07, gradient norm: tensor(6.3420e-06)\n",
            "Iteration: 47000, loss: 1.6111277149377657e-07, gradient norm: tensor(3.7201e-05)\n",
            "Iteration: 48000, loss: 1.3622581600714055e-07, gradient norm: tensor(1.2757e-05)\n",
            "Iteration: 49000, loss: 1.1693548631086514e-07, gradient norm: tensor(2.8860e-05)\n",
            "Iteration: 50000, loss: 1.0173141655656081e-07, gradient norm: tensor(1.0246e-05)\n",
            "Iteration: 51000, loss: 8.961578690502848e-08, gradient norm: tensor(6.9660e-06)\n",
            "Iteration: 52000, loss: 7.989698547561374e-08, gradient norm: tensor(7.2055e-05)\n",
            "Iteration: 53000, loss: 7.198141427267046e-08, gradient norm: tensor(2.6932e-06)\n",
            "Iteration: 54000, loss: 6.546974481835832e-08, gradient norm: tensor(4.9687e-05)\n",
            "Iteration: 55000, loss: 6.005593480651328e-08, gradient norm: tensor(8.1472e-06)\n",
            "Iteration: 56000, loss: 5.5515387131777064e-08, gradient norm: tensor(1.4088e-06)\n",
            "Iteration: 0, loss: 2.292871783018112, gradient norm: tensor(2.7320)\n",
            "Iteration: 1000, loss: 1.6757060767412186, gradient norm: tensor(2.3113)\n",
            "Iteration: 2000, loss: 1.130466859459877, gradient norm: tensor(1.9020)\n",
            "Iteration: 3000, loss: 0.6594226232767105, gradient norm: tensor(1.4681)\n",
            "Iteration: 4000, loss: 0.3073810565918684, gradient norm: tensor(0.9706)\n",
            "Iteration: 5000, loss: 0.11367010495066643, gradient norm: tensor(0.5273)\n",
            "Iteration: 6000, loss: 0.037022979982197286, gradient norm: tensor(0.2415)\n",
            "Iteration: 7000, loss: 0.012561115994118154, gradient norm: tensor(0.0801)\n",
            "Iteration: 8000, loss: 0.006747252829838544, gradient norm: tensor(0.0186)\n",
            "Iteration: 9000, loss: 0.005453728029504419, gradient norm: tensor(0.0103)\n",
            "Iteration: 10000, loss: 0.004761456942651421, gradient norm: tensor(0.0083)\n",
            "Iteration: 11000, loss: 0.004204426795244217, gradient norm: tensor(0.0069)\n",
            "Iteration: 12000, loss: 0.003671853096690029, gradient norm: tensor(0.0062)\n",
            "Iteration: 13000, loss: 0.0030119532607495786, gradient norm: tensor(0.0061)\n",
            "Iteration: 14000, loss: 0.0021363746026763692, gradient norm: tensor(0.0059)\n",
            "Iteration: 15000, loss: 0.001214734037697781, gradient norm: tensor(0.0045)\n",
            "Iteration: 16000, loss: 0.000537711831391789, gradient norm: tensor(0.0027)\n",
            "Iteration: 17000, loss: 0.00020793900683202082, gradient norm: tensor(0.0015)\n",
            "Iteration: 18000, loss: 7.513091095097479e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 3.0170594225637616e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 1.884059109215741e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 1.3018167682275816e-05, gradient norm: tensor(8.8644e-05)\n",
            "Iteration: 22000, loss: 7.894928831319703e-06, gradient norm: tensor(6.0324e-05)\n",
            "Iteration: 23000, loss: 4.30857027959064e-06, gradient norm: tensor(3.7126e-05)\n",
            "Iteration: 24000, loss: 2.290257797881168e-06, gradient norm: tensor(2.1940e-05)\n",
            "Iteration: 25000, loss: 1.2170779722850966e-06, gradient norm: tensor(1.2648e-05)\n",
            "Iteration: 26000, loss: 6.495204399641353e-07, gradient norm: tensor(9.3904e-06)\n",
            "Iteration: 27000, loss: 3.502754067312708e-07, gradient norm: tensor(6.0170e-06)\n",
            "Iteration: 28000, loss: 1.8983762767277313e-07, gradient norm: tensor(2.6819e-06)\n",
            "Iteration: 29000, loss: 1.0394763352650216e-07, gradient norm: tensor(3.1886e-05)\n",
            "Iteration: 30000, loss: 5.935599774531397e-08, gradient norm: tensor(2.3069e-06)\n",
            "Iteration: 31000, loss: 3.6624362687476266e-08, gradient norm: tensor(7.1030e-06)\n",
            "Iteration: 32000, loss: 2.4780175049698984e-08, gradient norm: tensor(3.8676e-06)\n",
            "Iteration: 33000, loss: 1.8251779623668085e-08, gradient norm: tensor(2.2775e-06)\n",
            "Iteration: 34000, loss: 1.4503197046877858e-08, gradient norm: tensor(3.2311e-05)\n",
            "Iteration: 35000, loss: 1.2311216119442748e-08, gradient norm: tensor(7.9460e-06)\n",
            "Iteration: 36000, loss: 1.1004237118861227e-08, gradient norm: tensor(6.1097e-06)\n",
            "Iteration: 37000, loss: 1.01775225740397e-08, gradient norm: tensor(1.7689e-05)\n",
            "Iteration: 38000, loss: 9.651970695578882e-09, gradient norm: tensor(2.6596e-06)\n",
            "Iteration: 39000, loss: 9.28141241907099e-09, gradient norm: tensor(1.4996e-05)\n",
            "Iteration: 40000, loss: 9.009860870712317e-09, gradient norm: tensor(1.8529e-05)\n",
            "Iteration: 41000, loss: 8.79392379271593e-09, gradient norm: tensor(3.8862e-06)\n",
            "Iteration: 42000, loss: 8.61130022222767e-09, gradient norm: tensor(5.0746e-06)\n",
            "Iteration: 43000, loss: 8.455841152610333e-09, gradient norm: tensor(1.2614e-05)\n",
            "Iteration: 44000, loss: 8.308788784994192e-09, gradient norm: tensor(2.9879e-05)\n",
            "Iteration: 45000, loss: 8.165894763045856e-09, gradient norm: tensor(8.0582e-06)\n",
            "Iteration: 46000, loss: 8.040176607160277e-09, gradient norm: tensor(2.6756e-05)\n",
            "Iteration: 47000, loss: 7.9049497525574e-09, gradient norm: tensor(7.1774e-06)\n",
            "Iteration: 48000, loss: 7.78084932573364e-09, gradient norm: tensor(3.7217e-06)\n",
            "Iteration: 49000, loss: 7.668374671254696e-09, gradient norm: tensor(1.0645e-05)\n",
            "Iteration: 50000, loss: 7.53788203322614e-09, gradient norm: tensor(8.9734e-06)\n",
            "Iteration: 51000, loss: 7.426121516385109e-09, gradient norm: tensor(2.8411e-05)\n",
            "Iteration: 52000, loss: 7.310217581490974e-09, gradient norm: tensor(1.6433e-05)\n",
            "Iteration: 53000, loss: 7.1943954043085514e-09, gradient norm: tensor(1.9093e-05)\n",
            "Iteration: 54000, loss: 7.0804172449534515e-09, gradient norm: tensor(8.3483e-06)\n",
            "Iteration: 55000, loss: 6.977488749448924e-09, gradient norm: tensor(1.2918e-05)\n",
            "Iteration: 56000, loss: 6.867369094276654e-09, gradient norm: tensor(3.9308e-05)\n",
            "Iteration: 57000, loss: 6.764159063976649e-09, gradient norm: tensor(1.1351e-06)\n",
            "Iteration: 0, loss: 6.714404825687408, gradient norm: tensor(5.6407)\n",
            "Iteration: 1000, loss: 5.397136310100556, gradient norm: tensor(4.9493)\n",
            "Iteration: 2000, loss: 4.3513321986198426, gradient norm: tensor(4.3702)\n",
            "Iteration: 3000, loss: 3.4679077541828156, gradient norm: tensor(3.8586)\n",
            "Iteration: 4000, loss: 2.696104539871216, gradient norm: tensor(3.3793)\n",
            "Iteration: 5000, loss: 2.0286159065961837, gradient norm: tensor(2.9107)\n",
            "Iteration: 6000, loss: 1.4666855391263962, gradient norm: tensor(2.4461)\n",
            "Iteration: 7000, loss: 1.0089175174832343, gradient norm: tensor(1.9912)\n",
            "Iteration: 8000, loss: 0.6504358563423157, gradient norm: tensor(1.5539)\n",
            "Iteration: 9000, loss: 0.3846333098113537, gradient norm: tensor(1.1428)\n",
            "Iteration: 10000, loss: 0.20275861713290214, gradient norm: tensor(0.7699)\n",
            "Iteration: 11000, loss: 0.09235514360666275, gradient norm: tensor(0.4527)\n",
            "Iteration: 12000, loss: 0.0367077594678849, gradient norm: tensor(0.2114)\n",
            "Iteration: 13000, loss: 0.01605537448823452, gradient norm: tensor(0.0666)\n",
            "Iteration: 14000, loss: 0.010637897072359919, gradient norm: tensor(0.0175)\n",
            "Iteration: 15000, loss: 0.008453553213272244, gradient norm: tensor(0.0141)\n",
            "Iteration: 16000, loss: 0.006710530889220536, gradient norm: tensor(0.0135)\n",
            "Iteration: 17000, loss: 0.005287604117300362, gradient norm: tensor(0.0121)\n",
            "Iteration: 18000, loss: 0.004100555983139202, gradient norm: tensor(0.0098)\n",
            "Iteration: 19000, loss: 0.003168144192546606, gradient norm: tensor(0.0070)\n",
            "Iteration: 20000, loss: 0.002555857469793409, gradient norm: tensor(0.0047)\n",
            "Iteration: 21000, loss: 0.0021899037205148487, gradient norm: tensor(0.0041)\n",
            "Iteration: 22000, loss: 0.001787330242455937, gradient norm: tensor(0.0061)\n",
            "Iteration: 23000, loss: 0.0008374685055459849, gradient norm: tensor(0.0036)\n",
            "Iteration: 24000, loss: 0.00026319762028288095, gradient norm: tensor(0.0017)\n",
            "Iteration: 25000, loss: 0.00010886446247604908, gradient norm: tensor(0.0009)\n",
            "Iteration: 26000, loss: 4.783331811631797e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 27000, loss: 2.756832212980953e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 1.9211057562642962e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 29000, loss: 1.3111400200614298e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 8.358626421340887e-06, gradient norm: tensor(7.7245e-05)\n",
            "Iteration: 31000, loss: 4.954493300829199e-06, gradient norm: tensor(6.5976e-05)\n",
            "Iteration: 32000, loss: 2.771512378785701e-06, gradient norm: tensor(2.7910e-05)\n",
            "Iteration: 33000, loss: 1.6531293520074542e-06, gradient norm: tensor(1.9614e-05)\n",
            "Iteration: 34000, loss: 1.1527372167847717e-06, gradient norm: tensor(8.7005e-06)\n",
            "Iteration: 35000, loss: 8.840245807846258e-07, gradient norm: tensor(5.6620e-06)\n",
            "Iteration: 36000, loss: 6.998726932465616e-07, gradient norm: tensor(1.1963e-05)\n",
            "Iteration: 37000, loss: 5.529181402152971e-07, gradient norm: tensor(5.8578e-06)\n",
            "Iteration: 38000, loss: 4.259598051987723e-07, gradient norm: tensor(5.8623e-05)\n",
            "Iteration: 39000, loss: 3.2580777980228956e-07, gradient norm: tensor(3.3226e-05)\n",
            "Iteration: 40000, loss: 2.657555310179305e-07, gradient norm: tensor(4.0387e-05)\n",
            "Iteration: 41000, loss: 2.3643161776476519e-07, gradient norm: tensor(3.3683e-05)\n",
            "Iteration: 42000, loss: 2.20644844432627e-07, gradient norm: tensor(2.4210e-05)\n",
            "Iteration: 43000, loss: 2.095940775888039e-07, gradient norm: tensor(5.9933e-06)\n",
            "Iteration: 44000, loss: 2.0031744956838793e-07, gradient norm: tensor(6.9356e-06)\n",
            "Iteration: 45000, loss: 1.9200066340374632e-07, gradient norm: tensor(2.3325e-05)\n",
            "Iteration: 46000, loss: 1.8438841622980816e-07, gradient norm: tensor(5.3726e-06)\n",
            "Iteration: 47000, loss: 1.7737027820885488e-07, gradient norm: tensor(1.3628e-05)\n",
            "Iteration: 48000, loss: 1.7078857894148314e-07, gradient norm: tensor(2.6105e-05)\n",
            "Iteration: 49000, loss: 1.646371514425482e-07, gradient norm: tensor(1.3167e-05)\n",
            "Iteration: 50000, loss: 1.5887882442200407e-07, gradient norm: tensor(3.5776e-06)\n",
            "Iteration: 51000, loss: 1.5347998119352723e-07, gradient norm: tensor(1.7786e-05)\n",
            "Iteration: 52000, loss: 1.4836646525395735e-07, gradient norm: tensor(5.1519e-06)\n",
            "Iteration: 53000, loss: 1.4357321092006715e-07, gradient norm: tensor(3.4090e-05)\n",
            "Iteration: 54000, loss: 1.390251336346182e-07, gradient norm: tensor(1.0960e-05)\n",
            "Iteration: 55000, loss: 1.3471209506121795e-07, gradient norm: tensor(1.6953e-05)\n",
            "Iteration: 56000, loss: 1.3064244751603838e-07, gradient norm: tensor(1.9583e-05)\n",
            "Iteration: 57000, loss: 1.2674996948192073e-07, gradient norm: tensor(1.9154e-06)\n",
            "Iteration: 0, loss: 1.9923838497400284, gradient norm: tensor(2.7934)\n",
            "Iteration: 1000, loss: 1.3583934646844864, gradient norm: tensor(2.3024)\n",
            "Iteration: 2000, loss: 0.8873204210996628, gradient norm: tensor(1.8408)\n",
            "Iteration: 3000, loss: 0.5420188287198544, gradient norm: tensor(1.4065)\n",
            "Iteration: 4000, loss: 0.29837837055325506, gradient norm: tensor(1.0040)\n",
            "Iteration: 5000, loss: 0.14008871483057736, gradient norm: tensor(0.6451)\n",
            "Iteration: 6000, loss: 0.05130396810919046, gradient norm: tensor(0.3485)\n",
            "Iteration: 7000, loss: 0.012726510806474835, gradient norm: tensor(0.1390)\n",
            "Iteration: 8000, loss: 0.0019790936458157376, gradient norm: tensor(0.0322)\n",
            "Iteration: 9000, loss: 0.0005219817912729923, gradient norm: tensor(0.0033)\n",
            "Iteration: 10000, loss: 0.00041974243090953675, gradient norm: tensor(0.0016)\n",
            "Iteration: 11000, loss: 0.0003961084519396536, gradient norm: tensor(0.0015)\n",
            "Iteration: 12000, loss: 0.00038037023827200753, gradient norm: tensor(0.0014)\n",
            "Iteration: 13000, loss: 0.0003647058645437937, gradient norm: tensor(0.0013)\n",
            "Iteration: 14000, loss: 0.00034655453654704616, gradient norm: tensor(0.0012)\n",
            "Iteration: 15000, loss: 0.0003232299619703554, gradient norm: tensor(0.0011)\n",
            "Iteration: 16000, loss: 0.0002930026670510415, gradient norm: tensor(0.0010)\n",
            "Iteration: 17000, loss: 0.0002543009130895371, gradient norm: tensor(0.0008)\n",
            "Iteration: 18000, loss: 0.00020396277471445502, gradient norm: tensor(0.0008)\n",
            "Iteration: 19000, loss: 0.00013790677572978892, gradient norm: tensor(0.0007)\n",
            "Iteration: 20000, loss: 6.1003964905467e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 1.607427244744031e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 9.094590897802845e-06, gradient norm: tensor(5.4603e-05)\n",
            "Iteration: 23000, loss: 7.107284391167923e-06, gradient norm: tensor(3.4073e-05)\n",
            "Iteration: 24000, loss: 5.622854879675287e-06, gradient norm: tensor(2.2583e-05)\n",
            "Iteration: 25000, loss: 4.6786250736658985e-06, gradient norm: tensor(3.9754e-05)\n",
            "Iteration: 26000, loss: 3.976673194074465e-06, gradient norm: tensor(1.6897e-05)\n",
            "Iteration: 27000, loss: 2.8355800043300404e-06, gradient norm: tensor(1.2917e-05)\n",
            "Iteration: 28000, loss: 1.6527458218433822e-06, gradient norm: tensor(1.0811e-05)\n",
            "Iteration: 29000, loss: 1.146025249227023e-06, gradient norm: tensor(2.4958e-05)\n",
            "Iteration: 30000, loss: 8.695126670090758e-07, gradient norm: tensor(5.1280e-06)\n",
            "Iteration: 31000, loss: 6.917614617236723e-07, gradient norm: tensor(7.1233e-06)\n",
            "Iteration: 32000, loss: 5.806270969515026e-07, gradient norm: tensor(9.6490e-06)\n",
            "Iteration: 33000, loss: 5.036951412762391e-07, gradient norm: tensor(3.2663e-06)\n",
            "Iteration: 34000, loss: 4.4634575320401384e-07, gradient norm: tensor(8.6295e-06)\n",
            "Iteration: 35000, loss: 4.012249749507646e-07, gradient norm: tensor(1.4198e-05)\n",
            "Iteration: 36000, loss: 3.645200249593472e-07, gradient norm: tensor(2.1486e-05)\n",
            "Iteration: 37000, loss: 3.3390185339499115e-07, gradient norm: tensor(2.3395e-05)\n",
            "Iteration: 38000, loss: 3.078165780436848e-07, gradient norm: tensor(9.9733e-06)\n",
            "Iteration: 39000, loss: 2.8523954779302584e-07, gradient norm: tensor(2.8917e-05)\n",
            "Iteration: 40000, loss: 2.656519457957529e-07, gradient norm: tensor(1.0037e-05)\n",
            "Iteration: 41000, loss: 2.480889698972533e-07, gradient norm: tensor(4.3467e-05)\n",
            "Iteration: 42000, loss: 2.322820764675271e-07, gradient norm: tensor(4.5667e-06)\n",
            "Iteration: 43000, loss: 2.1800086301482226e-07, gradient norm: tensor(1.4290e-05)\n",
            "Iteration: 44000, loss: 2.0493469426696719e-07, gradient norm: tensor(2.1399e-05)\n",
            "Iteration: 45000, loss: 1.928405694684443e-07, gradient norm: tensor(6.8147e-06)\n",
            "Iteration: 46000, loss: 1.815320592442049e-07, gradient norm: tensor(1.9458e-05)\n",
            "Iteration: 47000, loss: 1.708810691383178e-07, gradient norm: tensor(5.9559e-06)\n",
            "Iteration: 48000, loss: 1.6077883827847472e-07, gradient norm: tensor(1.6919e-05)\n",
            "Iteration: 49000, loss: 1.512016389568771e-07, gradient norm: tensor(3.3225e-05)\n",
            "Iteration: 50000, loss: 1.421655680786671e-07, gradient norm: tensor(5.7753e-06)\n",
            "Iteration: 51000, loss: 1.3376602440473562e-07, gradient norm: tensor(6.9335e-05)\n",
            "Iteration: 52000, loss: 1.260486840806152e-07, gradient norm: tensor(6.3190e-06)\n",
            "Iteration: 53000, loss: 1.1905972418446709e-07, gradient norm: tensor(2.3346e-05)\n",
            "Iteration: 54000, loss: 1.1277920044960865e-07, gradient norm: tensor(1.4245e-05)\n",
            "Iteration: 55000, loss: 1.0718528765352176e-07, gradient norm: tensor(6.1545e-06)\n",
            "Iteration: 56000, loss: 1.0218202778844443e-07, gradient norm: tensor(2.6931e-05)\n",
            "Iteration: 57000, loss: 9.769650591096024e-08, gradient norm: tensor(2.2107e-05)\n",
            "Iteration: 58000, loss: 9.362616157915226e-08, gradient norm: tensor(1.5182e-05)\n",
            "Iteration: 59000, loss: 8.992028378429496e-08, gradient norm: tensor(9.3017e-06)\n",
            "Iteration: 60000, loss: 8.64576882904089e-08, gradient norm: tensor(1.2011e-05)\n",
            "Iteration: 61000, loss: 8.323355261552479e-08, gradient norm: tensor(3.5119e-06)\n",
            "Iteration: 62000, loss: 8.020228919036754e-08, gradient norm: tensor(1.8326e-05)\n",
            "Iteration: 63000, loss: 7.732727875975343e-08, gradient norm: tensor(2.3194e-06)\n",
            "Iteration: 64000, loss: 7.463164111243258e-08, gradient norm: tensor(6.0593e-06)\n",
            "Iteration: 65000, loss: 7.207390705588068e-08, gradient norm: tensor(2.0295e-06)\n",
            "Iteration: 66000, loss: 6.962290693479644e-08, gradient norm: tensor(3.8893e-05)\n",
            "Iteration: 67000, loss: 6.729411187222923e-08, gradient norm: tensor(7.0627e-06)\n",
            "Iteration: 68000, loss: 6.504924171224502e-08, gradient norm: tensor(9.4697e-06)\n",
            "Iteration: 69000, loss: 6.293808495883013e-08, gradient norm: tensor(5.0419e-05)\n",
            "Iteration: 70000, loss: 6.089311661128249e-08, gradient norm: tensor(2.3404e-05)\n",
            "Iteration: 71000, loss: 5.8952704392822855e-08, gradient norm: tensor(7.4212e-06)\n",
            "Iteration: 72000, loss: 5.70804297161942e-08, gradient norm: tensor(9.5709e-06)\n",
            "Iteration: 73000, loss: 5.529416865357462e-08, gradient norm: tensor(1.5016e-05)\n",
            "Iteration: 74000, loss: 5.35586218752826e-08, gradient norm: tensor(1.7666e-05)\n",
            "Iteration: 75000, loss: 5.192444583457245e-08, gradient norm: tensor(3.6701e-05)\n",
            "Iteration: 76000, loss: 5.033458715075767e-08, gradient norm: tensor(2.1051e-06)\n",
            "Iteration: 77000, loss: 4.8803232566285715e-08, gradient norm: tensor(2.5660e-05)\n",
            "Iteration: 78000, loss: 4.7334188590042455e-08, gradient norm: tensor(1.6234e-06)\n",
            "Iteration: 0, loss: 10.680996730804443, gradient norm: tensor(6.9673)\n",
            "Iteration: 1000, loss: 9.141773859977722, gradient norm: tensor(6.4440)\n",
            "Iteration: 2000, loss: 7.7503809099197385, gradient norm: tensor(5.9370)\n",
            "Iteration: 3000, loss: 6.494979235172272, gradient norm: tensor(5.4269)\n",
            "Iteration: 4000, loss: 5.371342082977295, gradient norm: tensor(4.9173)\n",
            "Iteration: 5000, loss: 4.370279811382294, gradient norm: tensor(4.4122)\n",
            "Iteration: 6000, loss: 3.484564112186432, gradient norm: tensor(3.9137)\n",
            "Iteration: 7000, loss: 2.709865607500076, gradient norm: tensor(3.4228)\n",
            "Iteration: 8000, loss: 2.043043228030205, gradient norm: tensor(2.9415)\n",
            "Iteration: 9000, loss: 1.481131677031517, gradient norm: tensor(2.4718)\n",
            "Iteration: 10000, loss: 1.0210045598745345, gradient norm: tensor(2.0162)\n",
            "Iteration: 11000, loss: 0.6584287009835244, gradient norm: tensor(1.5815)\n",
            "Iteration: 12000, loss: 0.3859781255424023, gradient norm: tensor(1.1736)\n",
            "Iteration: 13000, loss: 0.196196985244751, gradient norm: tensor(0.7939)\n",
            "Iteration: 14000, loss: 0.08111662324890494, gradient norm: tensor(0.4678)\n",
            "Iteration: 15000, loss: 0.02412762353848666, gradient norm: tensor(0.2179)\n",
            "Iteration: 16000, loss: 0.004225058975280262, gradient norm: tensor(0.0665)\n",
            "Iteration: 17000, loss: 0.0003949730807580636, gradient norm: tensor(0.0096)\n",
            "Iteration: 18000, loss: 7.067773925155052e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 19000, loss: 3.33470954283257e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 2.143760325452604e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.6515188209268673e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 1.327113314800954e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.1477181610644038e-05, gradient norm: tensor(8.6373e-05)\n",
            "Iteration: 24000, loss: 1.0669088565009588e-05, gradient norm: tensor(7.4777e-05)\n",
            "Iteration: 25000, loss: 1.03129481740325e-05, gradient norm: tensor(6.9127e-05)\n",
            "Iteration: 26000, loss: 1.0023363573964162e-05, gradient norm: tensor(6.5233e-05)\n",
            "Iteration: 27000, loss: 9.643978354688442e-06, gradient norm: tensor(5.9572e-05)\n",
            "Iteration: 28000, loss: 9.13237834265601e-06, gradient norm: tensor(5.1746e-05)\n",
            "Iteration: 29000, loss: 8.49368227954983e-06, gradient norm: tensor(5.2119e-05)\n",
            "Iteration: 30000, loss: 7.76095951005118e-06, gradient norm: tensor(4.4536e-05)\n",
            "Iteration: 31000, loss: 6.954384759865206e-06, gradient norm: tensor(2.9822e-05)\n",
            "Iteration: 32000, loss: 6.0080157600168605e-06, gradient norm: tensor(2.7266e-05)\n",
            "Iteration: 33000, loss: 4.74543947575512e-06, gradient norm: tensor(2.4068e-05)\n",
            "Iteration: 34000, loss: 3.24684640668238e-06, gradient norm: tensor(1.6431e-05)\n",
            "Iteration: 35000, loss: 1.981264627602286e-06, gradient norm: tensor(1.2287e-05)\n",
            "Iteration: 36000, loss: 7.699553075326549e-07, gradient norm: tensor(6.0621e-06)\n",
            "Iteration: 37000, loss: 3.47330890434705e-07, gradient norm: tensor(2.5641e-05)\n",
            "Iteration: 38000, loss: 2.227009654518497e-07, gradient norm: tensor(1.8722e-05)\n",
            "Iteration: 39000, loss: 1.7421977544529455e-07, gradient norm: tensor(1.3012e-06)\n",
            "Iteration: 0, loss: 3.6984766685962676, gradient norm: tensor(3.8461)\n",
            "Iteration: 1000, loss: 2.85048787522316, gradient norm: tensor(3.2923)\n",
            "Iteration: 2000, loss: 2.160715508699417, gradient norm: tensor(2.7910)\n",
            "Iteration: 3000, loss: 1.5866440390348435, gradient norm: tensor(2.3512)\n",
            "Iteration: 4000, loss: 1.1031306258440017, gradient norm: tensor(1.9484)\n",
            "Iteration: 5000, loss: 0.7089478155970573, gradient norm: tensor(1.5551)\n",
            "Iteration: 6000, loss: 0.41364939537644385, gradient norm: tensor(1.1614)\n",
            "Iteration: 7000, loss: 0.21341899736225606, gradient norm: tensor(0.7881)\n",
            "Iteration: 8000, loss: 0.0933563936650753, gradient norm: tensor(0.4634)\n",
            "Iteration: 9000, loss: 0.033878484506160024, gradient norm: tensor(0.2155)\n",
            "Iteration: 10000, loss: 0.011661936996039002, gradient norm: tensor(0.0670)\n",
            "Iteration: 11000, loss: 0.005880416916683316, gradient norm: tensor(0.0147)\n",
            "Iteration: 12000, loss: 0.004076883338857442, gradient norm: tensor(0.0095)\n",
            "Iteration: 13000, loss: 0.002881058959290385, gradient norm: tensor(0.0069)\n",
            "Iteration: 14000, loss: 0.002178696289425716, gradient norm: tensor(0.0046)\n",
            "Iteration: 15000, loss: 0.001848919652053155, gradient norm: tensor(0.0045)\n",
            "Iteration: 16000, loss: 0.0015602259259903804, gradient norm: tensor(0.0055)\n",
            "Iteration: 17000, loss: 0.0010467847103718668, gradient norm: tensor(0.0050)\n",
            "Iteration: 18000, loss: 0.0005198607292841189, gradient norm: tensor(0.0033)\n",
            "Iteration: 19000, loss: 0.00022400318415020592, gradient norm: tensor(0.0019)\n",
            "Iteration: 20000, loss: 7.933885142483632e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 21000, loss: 3.711993921388057e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 2.8429970716388197e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 2.2243750308916787e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 1.6292413800329088e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.1217268729524222e-05, gradient norm: tensor(7.9548e-05)\n",
            "Iteration: 26000, loss: 7.360542415881355e-06, gradient norm: tensor(5.3050e-05)\n",
            "Iteration: 27000, loss: 4.691470062653025e-06, gradient norm: tensor(5.6763e-05)\n",
            "Iteration: 28000, loss: 2.967112909118441e-06, gradient norm: tensor(3.1687e-05)\n",
            "Iteration: 29000, loss: 1.8854741344966897e-06, gradient norm: tensor(1.3915e-05)\n",
            "Iteration: 30000, loss: 1.2003596233967074e-06, gradient norm: tensor(1.1204e-05)\n",
            "Iteration: 31000, loss: 7.561083318705642e-07, gradient norm: tensor(5.6986e-06)\n",
            "Iteration: 32000, loss: 4.684582167726603e-07, gradient norm: tensor(5.4001e-06)\n",
            "Iteration: 33000, loss: 2.9279693600869906e-07, gradient norm: tensor(4.0002e-05)\n",
            "Iteration: 34000, loss: 2.0126015897403705e-07, gradient norm: tensor(1.4048e-05)\n",
            "Iteration: 35000, loss: 1.6244809202703437e-07, gradient norm: tensor(3.6627e-06)\n",
            "Iteration: 36000, loss: 1.4616393575295206e-07, gradient norm: tensor(9.5909e-06)\n",
            "Iteration: 37000, loss: 1.372641991963519e-07, gradient norm: tensor(8.1949e-07)\n",
            "Iteration: 0, loss: 2.884450383901596, gradient norm: tensor(3.3501)\n",
            "Iteration: 1000, loss: 2.038286343336105, gradient norm: tensor(2.8199)\n",
            "Iteration: 2000, loss: 1.3968501722812652, gradient norm: tensor(2.3354)\n",
            "Iteration: 3000, loss: 0.922680326640606, gradient norm: tensor(1.8762)\n",
            "Iteration: 4000, loss: 0.5745200380086899, gradient norm: tensor(1.4425)\n",
            "Iteration: 5000, loss: 0.3247753902375698, gradient norm: tensor(1.0391)\n",
            "Iteration: 6000, loss: 0.1597414721250534, gradient norm: tensor(0.6769)\n",
            "Iteration: 7000, loss: 0.0648237788528204, gradient norm: tensor(0.3741)\n",
            "Iteration: 8000, loss: 0.02136098088044673, gradient norm: tensor(0.1563)\n",
            "Iteration: 9000, loss: 0.007088552294299007, gradient norm: tensor(0.0431)\n",
            "Iteration: 10000, loss: 0.0034873987834434955, gradient norm: tensor(0.0147)\n",
            "Iteration: 11000, loss: 0.0021737517818110064, gradient norm: tensor(0.0097)\n",
            "Iteration: 12000, loss: 0.0014267749884165823, gradient norm: tensor(0.0066)\n",
            "Iteration: 13000, loss: 0.0009516140821506269, gradient norm: tensor(0.0041)\n",
            "Iteration: 14000, loss: 0.0006979078519507311, gradient norm: tensor(0.0021)\n",
            "Iteration: 15000, loss: 0.0006153392851119861, gradient norm: tensor(0.0013)\n",
            "Iteration: 16000, loss: 0.0005848965993500314, gradient norm: tensor(0.0011)\n",
            "Iteration: 17000, loss: 0.0005523451087065041, gradient norm: tensor(0.0010)\n",
            "Iteration: 18000, loss: 0.0005117390661616809, gradient norm: tensor(0.0009)\n",
            "Iteration: 19000, loss: 0.00046374489876325244, gradient norm: tensor(0.0008)\n",
            "Iteration: 20000, loss: 0.0004091171761683654, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 0.00034409823856549335, gradient norm: tensor(0.0007)\n",
            "Iteration: 22000, loss: 0.00025764365150826053, gradient norm: tensor(0.0007)\n",
            "Iteration: 23000, loss: 0.00013784866004425567, gradient norm: tensor(0.0006)\n",
            "Iteration: 24000, loss: 5.067043940289295e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 1.9440110030700453e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 4.574575787273716e-06, gradient norm: tensor(1.5811e-05)\n",
            "Iteration: 27000, loss: 3.2332118914837336e-06, gradient norm: tensor(1.1762e-05)\n",
            "Iteration: 28000, loss: 2.7224753346217767e-06, gradient norm: tensor(3.4062e-05)\n",
            "Iteration: 29000, loss: 2.3937294686220413e-06, gradient norm: tensor(6.0239e-05)\n",
            "Iteration: 30000, loss: 2.1912039374001325e-06, gradient norm: tensor(6.5051e-06)\n",
            "Iteration: 31000, loss: 1.9837640965079116e-06, gradient norm: tensor(2.2333e-05)\n",
            "Iteration: 32000, loss: 1.713956157004759e-06, gradient norm: tensor(5.3441e-06)\n",
            "Iteration: 33000, loss: 1.3956277227862302e-06, gradient norm: tensor(7.9769e-06)\n",
            "Iteration: 34000, loss: 1.0811351103257039e-06, gradient norm: tensor(7.3572e-06)\n",
            "Iteration: 35000, loss: 8.088645395218918e-07, gradient norm: tensor(1.7173e-05)\n",
            "Iteration: 36000, loss: 5.789856995193076e-07, gradient norm: tensor(2.7044e-05)\n",
            "Iteration: 37000, loss: 3.9669709761369634e-07, gradient norm: tensor(2.4402e-05)\n",
            "Iteration: 38000, loss: 2.692142618059279e-07, gradient norm: tensor(4.3281e-06)\n",
            "Iteration: 39000, loss: 1.9366589806679713e-07, gradient norm: tensor(7.3579e-06)\n",
            "Iteration: 40000, loss: 1.549040552504266e-07, gradient norm: tensor(6.4892e-05)\n",
            "Iteration: 41000, loss: 1.3629077685095582e-07, gradient norm: tensor(6.0839e-05)\n",
            "Iteration: 42000, loss: 1.2696785826449286e-07, gradient norm: tensor(6.4055e-06)\n",
            "Iteration: 43000, loss: 1.2167756582925904e-07, gradient norm: tensor(6.8653e-06)\n",
            "Iteration: 44000, loss: 1.1820313788035719e-07, gradient norm: tensor(5.4389e-05)\n",
            "Iteration: 45000, loss: 1.155904380070183e-07, gradient norm: tensor(2.1290e-06)\n",
            "Iteration: 46000, loss: 1.1342040213691007e-07, gradient norm: tensor(3.1478e-05)\n",
            "Iteration: 47000, loss: 1.1156140858759045e-07, gradient norm: tensor(2.6182e-05)\n",
            "Iteration: 48000, loss: 1.0988575688486435e-07, gradient norm: tensor(1.7444e-05)\n",
            "Iteration: 49000, loss: 1.0836762054822202e-07, gradient norm: tensor(2.5986e-05)\n",
            "Iteration: 50000, loss: 1.069962593476248e-07, gradient norm: tensor(2.7690e-05)\n",
            "Iteration: 51000, loss: 1.0570463333436919e-07, gradient norm: tensor(5.1830e-06)\n",
            "Iteration: 52000, loss: 1.0453863983173051e-07, gradient norm: tensor(2.3936e-05)\n",
            "Iteration: 53000, loss: 1.034380805009505e-07, gradient norm: tensor(1.0279e-05)\n",
            "Iteration: 54000, loss: 1.0243309083790563e-07, gradient norm: tensor(3.9812e-05)\n",
            "Iteration: 55000, loss: 1.0145344334944184e-07, gradient norm: tensor(3.7104e-06)\n",
            "Iteration: 56000, loss: 1.0057067697033517e-07, gradient norm: tensor(4.0284e-06)\n",
            "Iteration: 57000, loss: 9.971908621508873e-08, gradient norm: tensor(8.6388e-07)\n",
            "Iteration: 0, loss: 8.174834695339204, gradient norm: tensor(6.2099)\n",
            "Iteration: 1000, loss: 6.81917210149765, gradient norm: tensor(5.5987)\n",
            "Iteration: 2000, loss: 5.642485061168671, gradient norm: tensor(5.0070)\n",
            "Iteration: 3000, loss: 4.617386342048645, gradient norm: tensor(4.4419)\n",
            "Iteration: 4000, loss: 3.7163327243328093, gradient norm: tensor(3.9662)\n",
            "Iteration: 5000, loss: 2.908923258304596, gradient norm: tensor(3.5202)\n",
            "Iteration: 6000, loss: 2.2052803263664247, gradient norm: tensor(3.0495)\n",
            "Iteration: 7000, loss: 1.6139389563798905, gradient norm: tensor(2.5806)\n",
            "Iteration: 8000, loss: 1.1262936441898346, gradient norm: tensor(2.1281)\n",
            "Iteration: 9000, loss: 0.7357881197929382, gradient norm: tensor(1.6889)\n",
            "Iteration: 10000, loss: 0.4396590510606766, gradient norm: tensor(1.2687)\n",
            "Iteration: 11000, loss: 0.23160043375194073, gradient norm: tensor(0.8798)\n",
            "Iteration: 12000, loss: 0.10099073738232255, gradient norm: tensor(0.5387)\n",
            "Iteration: 13000, loss: 0.03269060968235135, gradient norm: tensor(0.2681)\n",
            "Iteration: 14000, loss: 0.006482700901920907, gradient norm: tensor(0.0925)\n",
            "Iteration: 15000, loss: 0.0006090117666317382, gradient norm: tensor(0.0166)\n",
            "Iteration: 16000, loss: 7.548959102132358e-05, gradient norm: tensor(0.0011)\n",
            "Iteration: 17000, loss: 6.297658124822192e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 18000, loss: 6.12189430212311e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 19000, loss: 5.847112150513567e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 5.49388330327929e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 21000, loss: 5.1688762032426896e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 4.843843643175205e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 23000, loss: 4.437231885094661e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 24000, loss: 3.90638505195966e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 25000, loss: 3.2541041835429495e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 2.5245719949452905e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 1.811688022735325e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 28000, loss: 1.2115833391362685e-05, gradient norm: tensor(9.4256e-05)\n",
            "Iteration: 29000, loss: 7.652088414943136e-06, gradient norm: tensor(6.3488e-05)\n",
            "Iteration: 30000, loss: 4.631251582168261e-06, gradient norm: tensor(3.9215e-05)\n",
            "Iteration: 31000, loss: 2.717758487960964e-06, gradient norm: tensor(2.5642e-05)\n",
            "Iteration: 32000, loss: 1.5545852874083722e-06, gradient norm: tensor(1.5498e-05)\n",
            "Iteration: 33000, loss: 7.958107545960047e-07, gradient norm: tensor(8.6086e-06)\n",
            "Iteration: 34000, loss: 3.7996604740442307e-07, gradient norm: tensor(9.4154e-06)\n",
            "Iteration: 35000, loss: 1.9264096755478021e-07, gradient norm: tensor(5.3512e-06)\n",
            "Iteration: 36000, loss: 1.0226114043376811e-07, gradient norm: tensor(5.6911e-06)\n",
            "Iteration: 37000, loss: 6.109410476895505e-08, gradient norm: tensor(7.6813e-06)\n",
            "Iteration: 38000, loss: 4.305419619399231e-08, gradient norm: tensor(8.6678e-06)\n",
            "Iteration: 39000, loss: 3.50490016884919e-08, gradient norm: tensor(1.0619e-05)\n",
            "Iteration: 40000, loss: 3.115593451497034e-08, gradient norm: tensor(3.8780e-05)\n",
            "Iteration: 41000, loss: 2.9010788757943828e-08, gradient norm: tensor(6.5724e-06)\n",
            "Iteration: 42000, loss: 2.7655183540531426e-08, gradient norm: tensor(6.2594e-06)\n",
            "Iteration: 43000, loss: 2.6676486207577454e-08, gradient norm: tensor(4.2029e-05)\n",
            "Iteration: 44000, loss: 2.5882749062233758e-08, gradient norm: tensor(1.7706e-05)\n",
            "Iteration: 45000, loss: 2.516886936732021e-08, gradient norm: tensor(3.7141e-05)\n",
            "Iteration: 46000, loss: 2.4528208689034158e-08, gradient norm: tensor(1.6275e-06)\n",
            "Iteration: 0, loss: 5.749371350288391, gradient norm: tensor(5.1820)\n",
            "Iteration: 1000, loss: 4.486887751102447, gradient norm: tensor(4.4663)\n",
            "Iteration: 2000, loss: 3.469338283777237, gradient norm: tensor(3.8367)\n",
            "Iteration: 3000, loss: 2.6500878200531006, gradient norm: tensor(3.2775)\n",
            "Iteration: 4000, loss: 1.982608239531517, gradient norm: tensor(2.7668)\n",
            "Iteration: 5000, loss: 1.4362210958003998, gradient norm: tensor(2.2913)\n",
            "Iteration: 6000, loss: 0.9910128159523011, gradient norm: tensor(1.8568)\n",
            "Iteration: 7000, loss: 0.6310938713848591, gradient norm: tensor(1.4486)\n",
            "Iteration: 8000, loss: 0.36016169542074206, gradient norm: tensor(1.0510)\n",
            "Iteration: 9000, loss: 0.18163757110387088, gradient norm: tensor(0.6886)\n",
            "Iteration: 10000, loss: 0.07935983431339264, gradient norm: tensor(0.3853)\n",
            "Iteration: 11000, loss: 0.030988966142758728, gradient norm: tensor(0.1662)\n",
            "Iteration: 12000, loss: 0.013298102201893926, gradient norm: tensor(0.0493)\n",
            "Iteration: 13000, loss: 0.0075209546419791875, gradient norm: tensor(0.0167)\n",
            "Iteration: 14000, loss: 0.0048735868353396656, gradient norm: tensor(0.0119)\n",
            "Iteration: 15000, loss: 0.003200837608426809, gradient norm: tensor(0.0094)\n",
            "Iteration: 16000, loss: 0.0020216040757950397, gradient norm: tensor(0.0074)\n",
            "Iteration: 17000, loss: 0.0011852962741977534, gradient norm: tensor(0.0055)\n",
            "Iteration: 18000, loss: 0.0006308835688396357, gradient norm: tensor(0.0036)\n",
            "Iteration: 19000, loss: 0.00031607804239320106, gradient norm: tensor(0.0020)\n",
            "Iteration: 20000, loss: 0.00017747032642364502, gradient norm: tensor(0.0009)\n",
            "Iteration: 21000, loss: 0.00013040741800068645, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 0.0001129331628108048, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 0.00010139076715859119, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 9.021677308919607e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 7.873028344329214e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 26000, loss: 6.771719639073126e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 27000, loss: 5.7684361356223236e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 28000, loss: 4.8578375335637246e-05, gradient norm: tensor(9.2990e-05)\n",
            "Iteration: 29000, loss: 4.034740103816148e-05, gradient norm: tensor(8.0059e-05)\n",
            "Iteration: 30000, loss: 3.3096204402681904e-05, gradient norm: tensor(6.7276e-05)\n",
            "Iteration: 31000, loss: 2.6891721307038096e-05, gradient norm: tensor(6.1090e-05)\n",
            "Iteration: 32000, loss: 2.169565083022462e-05, gradient norm: tensor(4.7455e-05)\n",
            "Iteration: 33000, loss: 1.7406239705451298e-05, gradient norm: tensor(4.1402e-05)\n",
            "Iteration: 34000, loss: 1.3904498708143364e-05, gradient norm: tensor(3.2412e-05)\n",
            "Iteration: 35000, loss: 1.107815865361772e-05, gradient norm: tensor(2.8506e-05)\n",
            "Iteration: 36000, loss: 8.830561022477923e-06, gradient norm: tensor(2.1368e-05)\n",
            "Iteration: 37000, loss: 7.084736468186748e-06, gradient norm: tensor(3.1128e-05)\n",
            "Iteration: 38000, loss: 5.7670352925924815e-06, gradient norm: tensor(1.2147e-05)\n",
            "Iteration: 39000, loss: 4.806678282875509e-06, gradient norm: tensor(1.1427e-05)\n",
            "Iteration: 40000, loss: 4.139084806865867e-06, gradient norm: tensor(7.8577e-06)\n",
            "Iteration: 41000, loss: 3.704824789565464e-06, gradient norm: tensor(2.9705e-05)\n",
            "Iteration: 42000, loss: 3.4442858386682928e-06, gradient norm: tensor(6.3570e-05)\n",
            "Iteration: 43000, loss: 3.293822747991726e-06, gradient norm: tensor(8.8105e-06)\n",
            "Iteration: 44000, loss: 3.1620416225450754e-06, gradient norm: tensor(9.0380e-06)\n",
            "Iteration: 45000, loss: 2.959077869036264e-06, gradient norm: tensor(2.8873e-05)\n",
            "Iteration: 46000, loss: 2.7966297191142075e-06, gradient norm: tensor(7.3081e-06)\n",
            "Iteration: 47000, loss: 2.6992709106252732e-06, gradient norm: tensor(4.8984e-05)\n",
            "Iteration: 48000, loss: 2.6339980393004226e-06, gradient norm: tensor(1.6908e-05)\n",
            "Iteration: 49000, loss: 2.5857356788492324e-06, gradient norm: tensor(6.9274e-06)\n",
            "Iteration: 50000, loss: 2.5477448248238943e-06, gradient norm: tensor(4.4602e-05)\n",
            "Iteration: 51000, loss: 2.5160279708416057e-06, gradient norm: tensor(4.8243e-05)\n",
            "Iteration: 52000, loss: 2.4881518445454278e-06, gradient norm: tensor(2.4978e-05)\n",
            "Iteration: 53000, loss: 2.4621281133931918e-06, gradient norm: tensor(3.2894e-06)\n",
            "Iteration: 54000, loss: 2.4359995811664705e-06, gradient norm: tensor(7.2832e-06)\n",
            "Iteration: 55000, loss: 2.4070505894542295e-06, gradient norm: tensor(2.7481e-06)\n",
            "Iteration: 56000, loss: 2.3709011734354134e-06, gradient norm: tensor(6.7592e-06)\n",
            "Iteration: 57000, loss: 2.3214447312511767e-06, gradient norm: tensor(8.5880e-06)\n",
            "Iteration: 58000, loss: 2.2622950291406596e-06, gradient norm: tensor(8.4211e-06)\n",
            "Iteration: 59000, loss: 2.213971886476429e-06, gradient norm: tensor(2.3494e-05)\n",
            "Iteration: 60000, loss: 2.1809928934999336e-06, gradient norm: tensor(1.5142e-05)\n",
            "Iteration: 61000, loss: 2.1549667033013973e-06, gradient norm: tensor(5.0796e-06)\n",
            "Iteration: 62000, loss: 2.1321047652236304e-06, gradient norm: tensor(4.1591e-05)\n",
            "Iteration: 63000, loss: 2.111616955289719e-06, gradient norm: tensor(1.8101e-05)\n",
            "Iteration: 64000, loss: 2.0930282116751188e-06, gradient norm: tensor(2.6961e-06)\n",
            "Iteration: 65000, loss: 2.0761757796208256e-06, gradient norm: tensor(7.3808e-06)\n",
            "Iteration: 66000, loss: 2.0606809810033157e-06, gradient norm: tensor(7.5031e-06)\n",
            "Iteration: 67000, loss: 2.046470493041852e-06, gradient norm: tensor(1.5075e-05)\n",
            "Iteration: 68000, loss: 2.03339345648601e-06, gradient norm: tensor(7.3905e-06)\n",
            "Iteration: 69000, loss: 2.021261080926706e-06, gradient norm: tensor(4.1988e-06)\n",
            "Iteration: 70000, loss: 2.009959825727492e-06, gradient norm: tensor(2.6022e-05)\n",
            "Iteration: 71000, loss: 1.9994097556264022e-06, gradient norm: tensor(4.1575e-06)\n",
            "Iteration: 72000, loss: 1.989530170476428e-06, gradient norm: tensor(4.7144e-05)\n",
            "Iteration: 73000, loss: 1.9802799163244343e-06, gradient norm: tensor(3.3393e-06)\n",
            "Iteration: 74000, loss: 1.971561352547724e-06, gradient norm: tensor(3.8399e-06)\n",
            "Iteration: 75000, loss: 1.9633077943126407e-06, gradient norm: tensor(3.9655e-06)\n",
            "Iteration: 76000, loss: 1.9554921441340413e-06, gradient norm: tensor(6.0833e-05)\n",
            "Iteration: 77000, loss: 1.9480522728372308e-06, gradient norm: tensor(3.7077e-05)\n",
            "Iteration: 78000, loss: 1.9409839294439733e-06, gradient norm: tensor(7.9869e-06)\n",
            "Iteration: 79000, loss: 1.9342910350133023e-06, gradient norm: tensor(1.4848e-05)\n",
            "Iteration: 80000, loss: 1.927886872408635e-06, gradient norm: tensor(1.6177e-06)\n",
            "Iteration: 0, loss: 4.326817346572876, gradient norm: tensor(4.3827)\n",
            "Iteration: 1000, loss: 3.387508739709854, gradient norm: tensor(3.8320)\n",
            "Iteration: 2000, loss: 2.600763560295105, gradient norm: tensor(3.3393)\n",
            "Iteration: 3000, loss: 1.9345363671779632, gradient norm: tensor(2.8573)\n",
            "Iteration: 4000, loss: 1.3815494698286057, gradient norm: tensor(2.3866)\n",
            "Iteration: 5000, loss: 0.9352110889554024, gradient norm: tensor(1.9310)\n",
            "Iteration: 6000, loss: 0.5890169385969639, gradient norm: tensor(1.4956)\n",
            "Iteration: 7000, loss: 0.33533082887530324, gradient norm: tensor(1.0879)\n",
            "Iteration: 8000, loss: 0.16465368232131003, gradient norm: tensor(0.7194)\n",
            "Iteration: 9000, loss: 0.06443814913928508, gradient norm: tensor(0.4077)\n",
            "Iteration: 10000, loss: 0.017640564248431474, gradient norm: tensor(0.1769)\n",
            "Iteration: 11000, loss: 0.0029487818921916185, gradient norm: tensor(0.0478)\n",
            "Iteration: 12000, loss: 0.0005789835411706008, gradient norm: tensor(0.0072)\n",
            "Iteration: 13000, loss: 0.0004013796603830997, gradient norm: tensor(0.0041)\n",
            "Iteration: 14000, loss: 0.0003396968793240376, gradient norm: tensor(0.0033)\n",
            "Iteration: 15000, loss: 0.0002651889535482042, gradient norm: tensor(0.0026)\n",
            "Iteration: 16000, loss: 0.00021510437194956466, gradient norm: tensor(0.0020)\n",
            "Iteration: 17000, loss: 0.00017903151652717497, gradient norm: tensor(0.0016)\n",
            "Iteration: 18000, loss: 0.00014396377217781265, gradient norm: tensor(0.0012)\n",
            "Iteration: 19000, loss: 0.0001104776265565306, gradient norm: tensor(0.0009)\n",
            "Iteration: 20000, loss: 8.129935645411024e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 21000, loss: 5.883712688228116e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 22000, loss: 4.276472020501387e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 3.0702066791491234e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 2.154910871468019e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.4871560893880087e-05, gradient norm: tensor(9.7076e-05)\n",
            "Iteration: 26000, loss: 1.0084463518978737e-05, gradient norm: tensor(6.7375e-05)\n",
            "Iteration: 27000, loss: 6.65065703469736e-06, gradient norm: tensor(4.3446e-05)\n",
            "Iteration: 28000, loss: 4.052705544381752e-06, gradient norm: tensor(3.0206e-05)\n",
            "Iteration: 29000, loss: 2.1553650640271373e-06, gradient norm: tensor(2.2551e-05)\n",
            "Iteration: 30000, loss: 1.1176059868489574e-06, gradient norm: tensor(1.5248e-05)\n",
            "Iteration: 31000, loss: 6.499665627757168e-07, gradient norm: tensor(8.8597e-06)\n",
            "Iteration: 32000, loss: 4.696198544991148e-07, gradient norm: tensor(1.2935e-05)\n",
            "Iteration: 33000, loss: 3.995418737758882e-07, gradient norm: tensor(4.2665e-05)\n",
            "Iteration: 34000, loss: 3.6030343795800945e-07, gradient norm: tensor(3.9535e-06)\n",
            "Iteration: 35000, loss: 3.322063662949404e-07, gradient norm: tensor(1.3473e-05)\n",
            "Iteration: 36000, loss: 3.1078458840738674e-07, gradient norm: tensor(7.7158e-06)\n",
            "Iteration: 37000, loss: 2.937045782402947e-07, gradient norm: tensor(4.2448e-06)\n",
            "Iteration: 38000, loss: 2.7949550840844494e-07, gradient norm: tensor(6.5666e-06)\n",
            "Iteration: 39000, loss: 2.672138101047494e-07, gradient norm: tensor(5.3740e-05)\n",
            "Iteration: 40000, loss: 2.5639378867481353e-07, gradient norm: tensor(1.8229e-06)\n",
            "Iteration: 0, loss: 8.19858731508255, gradient norm: tensor(6.1547)\n",
            "Iteration: 1000, loss: 6.867591919898987, gradient norm: tensor(5.5832)\n",
            "Iteration: 2000, loss: 5.699077883243561, gradient norm: tensor(5.0371)\n",
            "Iteration: 3000, loss: 4.665093501091003, gradient norm: tensor(4.5192)\n",
            "Iteration: 4000, loss: 3.74746200633049, gradient norm: tensor(4.0312)\n",
            "Iteration: 5000, loss: 2.9371559903621676, gradient norm: tensor(3.5563)\n",
            "Iteration: 6000, loss: 2.233657323002815, gradient norm: tensor(3.0842)\n",
            "Iteration: 7000, loss: 1.6354824225902558, gradient norm: tensor(2.6165)\n",
            "Iteration: 8000, loss: 1.1404406605362891, gradient norm: tensor(2.1556)\n",
            "Iteration: 9000, loss: 0.7464674268960952, gradient norm: tensor(1.7078)\n",
            "Iteration: 10000, loss: 0.4489692400097847, gradient norm: tensor(1.2843)\n",
            "Iteration: 11000, loss: 0.23930428628623485, gradient norm: tensor(0.8957)\n",
            "Iteration: 12000, loss: 0.10617534339427948, gradient norm: tensor(0.5545)\n",
            "Iteration: 13000, loss: 0.035210749564692376, gradient norm: tensor(0.2806)\n",
            "Iteration: 14000, loss: 0.007242528767557815, gradient norm: tensor(0.0994)\n",
            "Iteration: 15000, loss: 0.00070077513258002, gradient norm: tensor(0.0187)\n",
            "Iteration: 16000, loss: 4.304049922211561e-05, gradient norm: tensor(0.0012)\n",
            "Iteration: 17000, loss: 1.3688230047591787e-05, gradient norm: tensor(9.8464e-05)\n",
            "Iteration: 18000, loss: 7.511956417147303e-06, gradient norm: tensor(3.7687e-05)\n",
            "Iteration: 19000, loss: 6.107157108999672e-06, gradient norm: tensor(3.7069e-05)\n",
            "Iteration: 20000, loss: 5.857131727680099e-06, gradient norm: tensor(3.9864e-05)\n",
            "Iteration: 21000, loss: 5.823266142215289e-06, gradient norm: tensor(4.0234e-05)\n",
            "Iteration: 22000, loss: 5.794979257643718e-06, gradient norm: tensor(4.0089e-05)\n",
            "Iteration: 23000, loss: 5.749237389409246e-06, gradient norm: tensor(3.9012e-05)\n",
            "Iteration: 24000, loss: 5.6750055014163085e-06, gradient norm: tensor(3.7175e-05)\n",
            "Iteration: 25000, loss: 5.555916374760272e-06, gradient norm: tensor(3.7744e-05)\n",
            "Iteration: 26000, loss: 5.368169481243967e-06, gradient norm: tensor(3.6473e-05)\n",
            "Iteration: 27000, loss: 5.08074169374595e-06, gradient norm: tensor(3.3721e-05)\n",
            "Iteration: 28000, loss: 4.660301887724927e-06, gradient norm: tensor(3.2320e-05)\n",
            "Iteration: 29000, loss: 4.082847215840957e-06, gradient norm: tensor(3.8941e-05)\n",
            "Iteration: 30000, loss: 3.3520781041715964e-06, gradient norm: tensor(2.2007e-05)\n",
            "Iteration: 31000, loss: 2.5471188184837956e-06, gradient norm: tensor(1.7152e-05)\n",
            "Iteration: 32000, loss: 1.8509741917114298e-06, gradient norm: tensor(1.3474e-05)\n",
            "Iteration: 33000, loss: 1.3832138348561785e-06, gradient norm: tensor(8.1015e-06)\n",
            "Iteration: 34000, loss: 1.0742471646949524e-06, gradient norm: tensor(5.0844e-06)\n",
            "Iteration: 35000, loss: 8.213785650923455e-07, gradient norm: tensor(1.8847e-05)\n",
            "Iteration: 36000, loss: 6.065217348805163e-07, gradient norm: tensor(4.2462e-06)\n",
            "Iteration: 37000, loss: 4.5233521308318815e-07, gradient norm: tensor(1.3911e-05)\n",
            "Iteration: 38000, loss: 3.443842071533254e-07, gradient norm: tensor(1.9409e-05)\n",
            "Iteration: 39000, loss: 2.663560586171343e-07, gradient norm: tensor(2.6971e-06)\n",
            "Iteration: 40000, loss: 2.08774341402318e-07, gradient norm: tensor(1.7801e-05)\n",
            "Iteration: 41000, loss: 1.656642302378941e-07, gradient norm: tensor(3.3255e-05)\n",
            "Iteration: 42000, loss: 1.3301593492087704e-07, gradient norm: tensor(5.0022e-06)\n",
            "Iteration: 43000, loss: 1.0800035820324183e-07, gradient norm: tensor(4.8599e-05)\n",
            "Iteration: 44000, loss: 8.884894323557546e-08, gradient norm: tensor(4.5888e-06)\n",
            "Iteration: 45000, loss: 7.383034240859843e-08, gradient norm: tensor(4.0137e-06)\n",
            "Iteration: 46000, loss: 6.200482008367203e-08, gradient norm: tensor(1.9122e-05)\n",
            "Iteration: 47000, loss: 5.2637140633748915e-08, gradient norm: tensor(1.4013e-05)\n",
            "Iteration: 48000, loss: 4.516729189418811e-08, gradient norm: tensor(9.1954e-06)\n",
            "Iteration: 49000, loss: 3.9166421125003124e-08, gradient norm: tensor(1.0742e-05)\n",
            "Iteration: 50000, loss: 3.432586944285276e-08, gradient norm: tensor(1.6532e-06)\n",
            "Iteration: 0, loss: 3.467765401124954, gradient norm: tensor(3.6894)\n",
            "Iteration: 1000, loss: 2.6638339478969573, gradient norm: tensor(3.1854)\n",
            "Iteration: 2000, loss: 2.003765747666359, gradient norm: tensor(2.7235)\n",
            "Iteration: 3000, loss: 1.4511939948797226, gradient norm: tensor(2.3256)\n",
            "Iteration: 4000, loss: 0.9705867051482201, gradient norm: tensor(1.9196)\n",
            "Iteration: 5000, loss: 0.5846590430736541, gradient norm: tensor(1.4754)\n",
            "Iteration: 6000, loss: 0.31820025700330734, gradient norm: tensor(1.0512)\n",
            "Iteration: 7000, loss: 0.15176673870533705, gradient norm: tensor(0.6828)\n",
            "Iteration: 8000, loss: 0.057789543641731146, gradient norm: tensor(0.3776)\n",
            "Iteration: 9000, loss: 0.015450940836686642, gradient norm: tensor(0.1577)\n",
            "Iteration: 10000, loss: 0.0026742899737437257, gradient norm: tensor(0.0400)\n",
            "Iteration: 11000, loss: 0.0005577097151544876, gradient norm: tensor(0.0050)\n",
            "Iteration: 12000, loss: 0.00024337817951163742, gradient norm: tensor(0.0016)\n",
            "Iteration: 13000, loss: 0.00011924047821230488, gradient norm: tensor(0.0006)\n",
            "Iteration: 14000, loss: 8.403225903748535e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 15000, loss: 7.81015766406199e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 7.289733966172206e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 6.179481394428876e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 4.437525951652788e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 3.072799175970431e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 2.3696858217590487e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 21000, loss: 2.0149431340541924e-05, gradient norm: tensor(9.2132e-05)\n",
            "Iteration: 22000, loss: 1.7920736609085e-05, gradient norm: tensor(8.0866e-05)\n",
            "Iteration: 23000, loss: 1.5934046571601356e-05, gradient norm: tensor(6.7672e-05)\n",
            "Iteration: 24000, loss: 1.3815580380651227e-05, gradient norm: tensor(5.5249e-05)\n",
            "Iteration: 25000, loss: 1.1652893034806766e-05, gradient norm: tensor(4.1290e-05)\n",
            "Iteration: 26000, loss: 9.672679281720775e-06, gradient norm: tensor(3.0253e-05)\n",
            "Iteration: 27000, loss: 7.990609676198801e-06, gradient norm: tensor(2.3418e-05)\n",
            "Iteration: 28000, loss: 6.546777187850239e-06, gradient norm: tensor(2.7320e-05)\n",
            "Iteration: 29000, loss: 5.2405443520910925e-06, gradient norm: tensor(2.1772e-05)\n",
            "Iteration: 30000, loss: 4.066323953338724e-06, gradient norm: tensor(4.3164e-05)\n",
            "Iteration: 31000, loss: 3.067954445441501e-06, gradient norm: tensor(1.1213e-05)\n",
            "Iteration: 32000, loss: 2.274211539543103e-06, gradient norm: tensor(1.3047e-05)\n",
            "Iteration: 33000, loss: 1.6784517636097008e-06, gradient norm: tensor(5.4279e-05)\n",
            "Iteration: 34000, loss: 1.239880990851816e-06, gradient norm: tensor(8.7607e-06)\n",
            "Iteration: 35000, loss: 9.198914949024584e-07, gradient norm: tensor(2.2841e-05)\n",
            "Iteration: 36000, loss: 6.877517695329516e-07, gradient norm: tensor(4.8741e-05)\n",
            "Iteration: 37000, loss: 5.200067222972393e-07, gradient norm: tensor(5.6168e-05)\n",
            "Iteration: 38000, loss: 3.9862143930236015e-07, gradient norm: tensor(6.4033e-06)\n",
            "Iteration: 39000, loss: 3.0991432237215124e-07, gradient norm: tensor(3.8650e-06)\n",
            "Iteration: 40000, loss: 2.443184957030553e-07, gradient norm: tensor(5.6270e-06)\n",
            "Iteration: 41000, loss: 1.9509601176537217e-07, gradient norm: tensor(1.2184e-05)\n",
            "Iteration: 42000, loss: 1.578750342616786e-07, gradient norm: tensor(3.6657e-06)\n",
            "Iteration: 43000, loss: 1.2934683224585796e-07, gradient norm: tensor(5.1606e-05)\n",
            "Iteration: 44000, loss: 1.072705864260115e-07, gradient norm: tensor(2.1987e-06)\n",
            "Iteration: 45000, loss: 9.001140838904575e-08, gradient norm: tensor(2.2297e-06)\n",
            "Iteration: 46000, loss: 7.636672414434997e-08, gradient norm: tensor(2.9755e-05)\n",
            "Iteration: 47000, loss: 6.545207685348941e-08, gradient norm: tensor(6.5682e-06)\n",
            "Iteration: 48000, loss: 5.669615457648547e-08, gradient norm: tensor(2.9611e-05)\n",
            "Iteration: 49000, loss: 4.961683296045294e-08, gradient norm: tensor(3.9404e-06)\n",
            "Iteration: 50000, loss: 4.3862495370916576e-08, gradient norm: tensor(6.7075e-06)\n",
            "Iteration: 51000, loss: 3.9122297156524155e-08, gradient norm: tensor(1.5509e-06)\n",
            "Iteration: 0, loss: 1.1132363328337669, gradient norm: tensor(1.7156)\n",
            "Iteration: 1000, loss: 0.6919620627760887, gradient norm: tensor(1.2833)\n",
            "Iteration: 2000, loss: 0.40040712973475456, gradient norm: tensor(0.8949)\n",
            "Iteration: 3000, loss: 0.21415481555461885, gradient norm: tensor(0.5588)\n",
            "Iteration: 4000, loss: 0.10886842296272516, gradient norm: tensor(0.2953)\n",
            "Iteration: 5000, loss: 0.05895433277636766, gradient norm: tensor(0.1351)\n",
            "Iteration: 6000, loss: 0.03694135278090835, gradient norm: tensor(0.0733)\n",
            "Iteration: 7000, loss: 0.024451846105977892, gradient norm: tensor(0.0534)\n",
            "Iteration: 8000, loss: 0.015808209668844938, gradient norm: tensor(0.0427)\n",
            "Iteration: 9000, loss: 0.009366453897673637, gradient norm: tensor(0.0329)\n",
            "Iteration: 10000, loss: 0.004555885614128783, gradient norm: tensor(0.0208)\n",
            "Iteration: 11000, loss: 0.0017856483440846205, gradient norm: tensor(0.0104)\n",
            "Iteration: 12000, loss: 0.0006873143460834399, gradient norm: tensor(0.0048)\n",
            "Iteration: 13000, loss: 0.0002934243318013614, gradient norm: tensor(0.0025)\n",
            "Iteration: 14000, loss: 0.00012600467314769048, gradient norm: tensor(0.0012)\n",
            "Iteration: 15000, loss: 5.950083913921844e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 16000, loss: 3.5435190166026584e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 17000, loss: 2.3682495249886414e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 1.5706659304669302e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 1.1012566178578708e-05, gradient norm: tensor(7.8103e-05)\n",
            "Iteration: 20000, loss: 8.339244299349956e-06, gradient norm: tensor(5.6057e-05)\n",
            "Iteration: 21000, loss: 6.1017806187919635e-06, gradient norm: tensor(4.3149e-05)\n",
            "Iteration: 22000, loss: 3.9653272694977205e-06, gradient norm: tensor(3.0944e-05)\n",
            "Iteration: 23000, loss: 2.1694658097430873e-06, gradient norm: tensor(1.5973e-05)\n",
            "Iteration: 24000, loss: 1.234048564015211e-06, gradient norm: tensor(6.1262e-06)\n",
            "Iteration: 25000, loss: 9.494255925233119e-07, gradient norm: tensor(9.3005e-06)\n",
            "Iteration: 26000, loss: 8.76504794405264e-07, gradient norm: tensor(8.9865e-06)\n",
            "Iteration: 27000, loss: 8.363243523490382e-07, gradient norm: tensor(1.8725e-06)\n",
            "Iteration: 0, loss: 4.549020649433136, gradient norm: tensor(4.2991)\n",
            "Iteration: 1000, loss: 3.578560599565506, gradient norm: tensor(3.7994)\n",
            "Iteration: 2000, loss: 2.7580689177513125, gradient norm: tensor(3.3188)\n",
            "Iteration: 3000, loss: 2.0719574065208435, gradient norm: tensor(2.8535)\n",
            "Iteration: 4000, loss: 1.501819826602936, gradient norm: tensor(2.4142)\n",
            "Iteration: 5000, loss: 1.0259197036623955, gradient norm: tensor(1.9835)\n",
            "Iteration: 6000, loss: 0.6437964578568935, gradient norm: tensor(1.5382)\n",
            "Iteration: 7000, loss: 0.3676902493238449, gradient norm: tensor(1.1143)\n",
            "Iteration: 8000, loss: 0.18607413526624442, gradient norm: tensor(0.7424)\n",
            "Iteration: 9000, loss: 0.07826230561360717, gradient norm: tensor(0.4277)\n",
            "Iteration: 10000, loss: 0.024847945970483124, gradient norm: tensor(0.1910)\n",
            "Iteration: 11000, loss: 0.005227342150639742, gradient norm: tensor(0.0532)\n",
            "Iteration: 12000, loss: 0.001300942229689099, gradient norm: tensor(0.0076)\n",
            "Iteration: 13000, loss: 0.0008286729771643877, gradient norm: tensor(0.0031)\n",
            "Iteration: 14000, loss: 0.0006698108808486723, gradient norm: tensor(0.0024)\n",
            "Iteration: 15000, loss: 0.0005441382229328155, gradient norm: tensor(0.0021)\n",
            "Iteration: 16000, loss: 0.0004019947812776081, gradient norm: tensor(0.0019)\n",
            "Iteration: 17000, loss: 0.00024289631731517147, gradient norm: tensor(0.0015)\n",
            "Iteration: 18000, loss: 0.00012585961043077987, gradient norm: tensor(0.0011)\n",
            "Iteration: 19000, loss: 6.437918285882915e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 3.875240457637119e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 2.9701446957915322e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 2.439935963593598e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 1.943572107666114e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.4887029869896651e-05, gradient norm: tensor(9.7594e-05)\n",
            "Iteration: 25000, loss: 1.0859248531232878e-05, gradient norm: tensor(7.2614e-05)\n",
            "Iteration: 26000, loss: 7.367155830252159e-06, gradient norm: tensor(5.1024e-05)\n",
            "Iteration: 27000, loss: 4.544020781850122e-06, gradient norm: tensor(3.5797e-05)\n",
            "Iteration: 28000, loss: 2.5718972584627408e-06, gradient norm: tensor(2.2154e-05)\n",
            "Iteration: 29000, loss: 1.3666910115261998e-06, gradient norm: tensor(1.3646e-05)\n",
            "Iteration: 30000, loss: 6.893995286816335e-07, gradient norm: tensor(1.8111e-05)\n",
            "Iteration: 31000, loss: 3.3241021780838764e-07, gradient norm: tensor(1.1027e-05)\n",
            "Iteration: 32000, loss: 1.5562569710425579e-07, gradient norm: tensor(4.9317e-05)\n",
            "Iteration: 33000, loss: 7.224376757619666e-08, gradient norm: tensor(4.5119e-06)\n",
            "Iteration: 34000, loss: 3.4465129246541436e-08, gradient norm: tensor(4.4206e-06)\n",
            "Iteration: 35000, loss: 1.7552829833178407e-08, gradient norm: tensor(7.5809e-06)\n",
            "Iteration: 36000, loss: 9.555295823471255e-09, gradient norm: tensor(5.7225e-06)\n",
            "Iteration: 37000, loss: 5.536982797149648e-09, gradient norm: tensor(2.3259e-05)\n",
            "Iteration: 38000, loss: 3.428029156316015e-09, gradient norm: tensor(3.3934e-06)\n",
            "Iteration: 39000, loss: 2.246044180309603e-09, gradient norm: tensor(2.3501e-06)\n",
            "Iteration: 40000, loss: 1.553329868508868e-09, gradient norm: tensor(6.4892e-06)\n",
            "Iteration: 41000, loss: 1.1208693259101566e-09, gradient norm: tensor(3.5210e-05)\n",
            "Iteration: 42000, loss: 8.514180839802599e-10, gradient norm: tensor(8.9320e-06)\n",
            "Iteration: 43000, loss: 6.689749407762236e-10, gradient norm: tensor(4.5869e-05)\n",
            "Iteration: 44000, loss: 5.541145709497819e-10, gradient norm: tensor(7.9486e-06)\n",
            "Iteration: 45000, loss: 4.4871147916736296e-10, gradient norm: tensor(3.0625e-06)\n",
            "Iteration: 46000, loss: 3.855502439253922e-10, gradient norm: tensor(2.9159e-06)\n",
            "Iteration: 47000, loss: 3.4353336771097977e-10, gradient norm: tensor(9.9513e-06)\n",
            "Iteration: 48000, loss: 3.0068607087541113e-10, gradient norm: tensor(7.6394e-06)\n",
            "Iteration: 49000, loss: 2.745736046028124e-10, gradient norm: tensor(3.4282e-05)\n",
            "Iteration: 50000, loss: 2.391804291013155e-10, gradient norm: tensor(1.2372e-05)\n",
            "Iteration: 51000, loss: 2.3990664030359987e-10, gradient norm: tensor(9.7469e-06)\n",
            "Iteration: 52000, loss: 2.0777488950879385e-10, gradient norm: tensor(5.0367e-06)\n",
            "Iteration: 53000, loss: 1.991705332257676e-10, gradient norm: tensor(5.9537e-06)\n",
            "Iteration: 54000, loss: 1.8753956801398353e-10, gradient norm: tensor(4.3098e-06)\n",
            "Iteration: 55000, loss: 1.770095527237059e-10, gradient norm: tensor(5.2833e-06)\n",
            "Iteration: 56000, loss: 1.7494515933830224e-10, gradient norm: tensor(9.0456e-06)\n",
            "Iteration: 57000, loss: 1.5696851651120224e-10, gradient norm: tensor(1.1744e-05)\n",
            "Iteration: 58000, loss: 1.5580634836476203e-10, gradient norm: tensor(2.9474e-06)\n",
            "Iteration: 59000, loss: 1.565268057425462e-10, gradient norm: tensor(8.4653e-07)\n",
            "Iteration: 0, loss: 4.2208398594856265, gradient norm: tensor(4.2463)\n",
            "Iteration: 1000, loss: 3.2517209508419036, gradient norm: tensor(3.7336)\n",
            "Iteration: 2000, loss: 2.460117489337921, gradient norm: tensor(3.2345)\n",
            "Iteration: 3000, loss: 1.812676638841629, gradient norm: tensor(2.7539)\n",
            "Iteration: 4000, loss: 1.2809713093042374, gradient norm: tensor(2.2871)\n",
            "Iteration: 5000, loss: 0.8551144838929177, gradient norm: tensor(1.8354)\n",
            "Iteration: 6000, loss: 0.5280653037428856, gradient norm: tensor(1.4044)\n",
            "Iteration: 7000, loss: 0.2927180681079626, gradient norm: tensor(1.0031)\n",
            "Iteration: 8000, loss: 0.13866162710636853, gradient norm: tensor(0.6454)\n",
            "Iteration: 9000, loss: 0.05172011769004166, gradient norm: tensor(0.3496)\n",
            "Iteration: 10000, loss: 0.013606820154935122, gradient norm: tensor(0.1402)\n",
            "Iteration: 11000, loss: 0.0026264288884121924, gradient norm: tensor(0.0330)\n",
            "Iteration: 12000, loss: 0.0007928114113165066, gradient norm: tensor(0.0038)\n",
            "Iteration: 13000, loss: 0.0004888921464735177, gradient norm: tensor(0.0016)\n",
            "Iteration: 14000, loss: 0.0003822743324271869, gradient norm: tensor(0.0012)\n",
            "Iteration: 15000, loss: 0.00033141101637738755, gradient norm: tensor(0.0011)\n",
            "Iteration: 16000, loss: 0.00029765321844024586, gradient norm: tensor(0.0011)\n",
            "Iteration: 17000, loss: 0.0002670270339003764, gradient norm: tensor(0.0011)\n",
            "Iteration: 18000, loss: 0.00023328206400037742, gradient norm: tensor(0.0011)\n",
            "Iteration: 19000, loss: 0.00019218295710743405, gradient norm: tensor(0.0011)\n",
            "Iteration: 20000, loss: 0.00014066650692984693, gradient norm: tensor(0.0010)\n",
            "Iteration: 21000, loss: 8.056484467306291e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 22000, loss: 2.941246982118173e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 7.651697236269683e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 24000, loss: 1.8406388097673698e-06, gradient norm: tensor(3.0244e-05)\n",
            "Iteration: 25000, loss: 4.365740192895373e-07, gradient norm: tensor(5.5984e-06)\n",
            "Iteration: 26000, loss: 3.381876292110064e-07, gradient norm: tensor(6.3193e-06)\n",
            "Iteration: 27000, loss: 3.073560046118473e-07, gradient norm: tensor(4.3423e-06)\n",
            "Iteration: 28000, loss: 2.784746540669403e-07, gradient norm: tensor(3.3854e-06)\n",
            "Iteration: 29000, loss: 2.529390831398359e-07, gradient norm: tensor(7.4745e-06)\n",
            "Iteration: 30000, loss: 2.314073465186084e-07, gradient norm: tensor(1.2699e-05)\n",
            "Iteration: 31000, loss: 2.1257993863343927e-07, gradient norm: tensor(5.7934e-06)\n",
            "Iteration: 32000, loss: 1.9646765885283913e-07, gradient norm: tensor(2.3728e-05)\n",
            "Iteration: 33000, loss: 1.838355349690346e-07, gradient norm: tensor(1.8582e-05)\n",
            "Iteration: 34000, loss: 1.7381036273889094e-07, gradient norm: tensor(1.2836e-06)\n",
            "Iteration: 0, loss: 5.336788078308105, gradient norm: tensor(4.8363)\n",
            "Iteration: 1000, loss: 4.2832864766120915, gradient norm: tensor(4.3294)\n",
            "Iteration: 2000, loss: 3.3770818972587584, gradient norm: tensor(3.8320)\n",
            "Iteration: 3000, loss: 2.6002032697200774, gradient norm: tensor(3.3436)\n",
            "Iteration: 4000, loss: 1.9412616515159606, gradient norm: tensor(2.8635)\n",
            "Iteration: 5000, loss: 1.3923208335638046, gradient norm: tensor(2.3946)\n",
            "Iteration: 6000, loss: 0.9470581746697426, gradient norm: tensor(1.9405)\n",
            "Iteration: 7000, loss: 0.599584045857191, gradient norm: tensor(1.5061)\n",
            "Iteration: 8000, loss: 0.343139961540699, gradient norm: tensor(1.0988)\n",
            "Iteration: 9000, loss: 0.16943642503768205, gradient norm: tensor(0.7297)\n",
            "Iteration: 10000, loss: 0.06687654471769928, gradient norm: tensor(0.4164)\n",
            "Iteration: 11000, loss: 0.018512186461593957, gradient norm: tensor(0.1829)\n",
            "Iteration: 12000, loss: 0.003037745694047771, gradient norm: tensor(0.0504)\n",
            "Iteration: 13000, loss: 0.00039382269357156476, gradient norm: tensor(0.0065)\n",
            "Iteration: 14000, loss: 0.00011543451625766465, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 4.52875493447209e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 16000, loss: 2.3197976694063982e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 17000, loss: 1.5477539588573565e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 18000, loss: 1.1734235100448132e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 9.739128959154186e-06, gradient norm: tensor(9.2674e-05)\n",
            "Iteration: 20000, loss: 8.902445919375168e-06, gradient norm: tensor(8.2997e-05)\n",
            "Iteration: 21000, loss: 8.526995279680706e-06, gradient norm: tensor(7.3669e-05)\n",
            "Iteration: 22000, loss: 8.161400195604075e-06, gradient norm: tensor(6.2130e-05)\n",
            "Iteration: 23000, loss: 7.700384672716609e-06, gradient norm: tensor(5.1614e-05)\n",
            "Iteration: 24000, loss: 7.128943548195821e-06, gradient norm: tensor(4.4441e-05)\n",
            "Iteration: 25000, loss: 6.448562073273933e-06, gradient norm: tensor(3.9339e-05)\n",
            "Iteration: 26000, loss: 5.6677512388887405e-06, gradient norm: tensor(3.2294e-05)\n",
            "Iteration: 27000, loss: 4.7971119015528525e-06, gradient norm: tensor(3.1351e-05)\n",
            "Iteration: 28000, loss: 3.935662864932965e-06, gradient norm: tensor(3.7617e-05)\n",
            "Iteration: 29000, loss: 3.207140494396299e-06, gradient norm: tensor(1.5245e-05)\n",
            "Iteration: 30000, loss: 2.678794339090018e-06, gradient norm: tensor(1.1553e-05)\n",
            "Iteration: 31000, loss: 2.3431426991464834e-06, gradient norm: tensor(3.2875e-05)\n",
            "Iteration: 32000, loss: 2.149142592543285e-06, gradient norm: tensor(6.4132e-06)\n",
            "Iteration: 33000, loss: 2.042575333007335e-06, gradient norm: tensor(8.8649e-06)\n",
            "Iteration: 34000, loss: 1.9849441907808795e-06, gradient norm: tensor(2.0751e-05)\n",
            "Iteration: 35000, loss: 1.950040347992399e-06, gradient norm: tensor(7.5793e-06)\n",
            "Iteration: 36000, loss: 1.9246588715304823e-06, gradient norm: tensor(6.4906e-06)\n",
            "Iteration: 37000, loss: 1.9032442879733936e-06, gradient norm: tensor(2.5070e-05)\n",
            "Iteration: 38000, loss: 1.8843329422679745e-06, gradient norm: tensor(1.4474e-05)\n",
            "Iteration: 39000, loss: 1.8685031774339222e-06, gradient norm: tensor(3.1955e-06)\n",
            "Iteration: 40000, loss: 1.8559132504378795e-06, gradient norm: tensor(2.5385e-05)\n",
            "Iteration: 41000, loss: 1.8459933580743382e-06, gradient norm: tensor(6.6930e-06)\n",
            "Iteration: 42000, loss: 1.8377578867330157e-06, gradient norm: tensor(1.9467e-05)\n",
            "Iteration: 43000, loss: 1.8305556868654094e-06, gradient norm: tensor(2.8727e-06)\n",
            "Iteration: 44000, loss: 1.8240602813648366e-06, gradient norm: tensor(3.2072e-05)\n",
            "Iteration: 45000, loss: 1.8181002683377302e-06, gradient norm: tensor(4.7298e-05)\n",
            "Iteration: 46000, loss: 1.8126035051864164e-06, gradient norm: tensor(3.4524e-05)\n",
            "Iteration: 47000, loss: 1.8075171186637817e-06, gradient norm: tensor(3.2082e-05)\n",
            "Iteration: 48000, loss: 1.802795257390244e-06, gradient norm: tensor(5.1768e-06)\n",
            "Iteration: 49000, loss: 1.7984146172693728e-06, gradient norm: tensor(3.1201e-05)\n",
            "Iteration: 50000, loss: 1.7943340349120263e-06, gradient norm: tensor(2.7693e-05)\n",
            "Iteration: 51000, loss: 1.7905090267049673e-06, gradient norm: tensor(3.9034e-06)\n",
            "Iteration: 52000, loss: 1.7869462888029374e-06, gradient norm: tensor(2.7639e-05)\n",
            "Iteration: 53000, loss: 1.7836006228435509e-06, gradient norm: tensor(1.6052e-06)\n",
            "Iteration: 0, loss: 4.746793606758118, gradient norm: tensor(4.5956)\n",
            "Iteration: 1000, loss: 3.76167276263237, gradient norm: tensor(4.0693)\n",
            "Iteration: 2000, loss: 2.922856992006302, gradient norm: tensor(3.5630)\n",
            "Iteration: 3000, loss: 2.2114527547359466, gradient norm: tensor(3.0710)\n",
            "Iteration: 4000, loss: 1.6157234344482423, gradient norm: tensor(2.5916)\n",
            "Iteration: 5000, loss: 1.1278243637084961, gradient norm: tensor(2.1252)\n",
            "Iteration: 6000, loss: 0.7418035284876824, gradient norm: tensor(1.6756)\n",
            "Iteration: 7000, loss: 0.45122662687301635, gradient norm: tensor(1.2533)\n",
            "Iteration: 8000, loss: 0.2468228499889374, gradient norm: tensor(0.8726)\n",
            "Iteration: 9000, loss: 0.11557854812592268, gradient norm: tensor(0.5432)\n",
            "Iteration: 10000, loss: 0.043647000594064594, gradient norm: tensor(0.2746)\n",
            "Iteration: 11000, loss: 0.014336573812179267, gradient norm: tensor(0.0974)\n",
            "Iteration: 12000, loss: 0.005906600308138877, gradient norm: tensor(0.0218)\n",
            "Iteration: 13000, loss: 0.003462607861030847, gradient norm: tensor(0.0096)\n",
            "Iteration: 14000, loss: 0.002134717360022478, gradient norm: tensor(0.0077)\n",
            "Iteration: 15000, loss: 0.0012260132039664314, gradient norm: tensor(0.0059)\n",
            "Iteration: 16000, loss: 0.0006271381233236752, gradient norm: tensor(0.0041)\n",
            "Iteration: 17000, loss: 0.00027337674821319526, gradient norm: tensor(0.0024)\n",
            "Iteration: 18000, loss: 0.00010261452028134954, gradient norm: tensor(0.0011)\n",
            "Iteration: 19000, loss: 4.228746400985983e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 2.510067086222989e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.7790067762689432e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.34054589925654e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 23000, loss: 1.0628024630022992e-05, gradient norm: tensor(8.2180e-05)\n",
            "Iteration: 24000, loss: 8.611069112703263e-06, gradient norm: tensor(6.6342e-05)\n",
            "Iteration: 25000, loss: 6.729576313773577e-06, gradient norm: tensor(5.6415e-05)\n",
            "Iteration: 26000, loss: 4.815257894279057e-06, gradient norm: tensor(3.9162e-05)\n",
            "Iteration: 27000, loss: 3.2531090607790247e-06, gradient norm: tensor(2.2661e-05)\n",
            "Iteration: 28000, loss: 2.3507064313434968e-06, gradient norm: tensor(1.2581e-05)\n",
            "Iteration: 29000, loss: 1.922996080111261e-06, gradient norm: tensor(7.0990e-06)\n",
            "Iteration: 30000, loss: 1.6904287626857694e-06, gradient norm: tensor(1.0176e-05)\n",
            "Iteration: 31000, loss: 1.4815775532497355e-06, gradient norm: tensor(7.8001e-06)\n",
            "Iteration: 32000, loss: 1.2179680231838574e-06, gradient norm: tensor(1.6854e-05)\n",
            "Iteration: 33000, loss: 9.002707291756451e-07, gradient norm: tensor(6.5702e-06)\n",
            "Iteration: 34000, loss: 6.409579272030896e-07, gradient norm: tensor(9.8148e-06)\n",
            "Iteration: 35000, loss: 4.987959516995489e-07, gradient norm: tensor(1.9358e-05)\n",
            "Iteration: 36000, loss: 4.2712689329960084e-07, gradient norm: tensor(2.3165e-05)\n",
            "Iteration: 37000, loss: 3.8333178127913927e-07, gradient norm: tensor(1.7701e-05)\n",
            "Iteration: 38000, loss: 3.508969732308742e-07, gradient norm: tensor(4.8753e-06)\n",
            "Iteration: 39000, loss: 3.2461951261097967e-07, gradient norm: tensor(6.9183e-06)\n",
            "Iteration: 40000, loss: 3.0250396821429606e-07, gradient norm: tensor(3.5544e-06)\n",
            "Iteration: 41000, loss: 2.8354754806514395e-07, gradient norm: tensor(1.5983e-05)\n",
            "Iteration: 42000, loss: 2.670128481838674e-07, gradient norm: tensor(2.2025e-05)\n",
            "Iteration: 43000, loss: 2.523734412704925e-07, gradient norm: tensor(7.1886e-06)\n",
            "Iteration: 44000, loss: 2.393588843148109e-07, gradient norm: tensor(8.4490e-06)\n",
            "Iteration: 45000, loss: 2.2772118600755674e-07, gradient norm: tensor(3.0366e-06)\n",
            "Iteration: 46000, loss: 2.171598183764445e-07, gradient norm: tensor(2.5269e-05)\n",
            "Iteration: 47000, loss: 2.0752556969227952e-07, gradient norm: tensor(1.1349e-05)\n",
            "Iteration: 48000, loss: 1.9868688677604496e-07, gradient norm: tensor(2.4488e-05)\n",
            "Iteration: 49000, loss: 1.9051912229883784e-07, gradient norm: tensor(8.7524e-06)\n",
            "Iteration: 50000, loss: 1.8298539822581007e-07, gradient norm: tensor(3.8649e-06)\n",
            "Iteration: 51000, loss: 1.7598628376447322e-07, gradient norm: tensor(6.1991e-06)\n",
            "Iteration: 52000, loss: 1.69467956524727e-07, gradient norm: tensor(8.0064e-06)\n",
            "Iteration: 53000, loss: 1.6339759544337085e-07, gradient norm: tensor(1.6199e-05)\n",
            "Iteration: 54000, loss: 1.5768094665702393e-07, gradient norm: tensor(2.5387e-06)\n",
            "Iteration: 55000, loss: 1.5227177998156094e-07, gradient norm: tensor(1.6405e-05)\n",
            "Iteration: 56000, loss: 1.4719948450192534e-07, gradient norm: tensor(1.2969e-05)\n",
            "Iteration: 57000, loss: 1.4242347965875978e-07, gradient norm: tensor(5.5785e-06)\n",
            "Iteration: 58000, loss: 1.3789374457928716e-07, gradient norm: tensor(2.6224e-06)\n",
            "Iteration: 59000, loss: 1.3359946430568926e-07, gradient norm: tensor(3.7472e-05)\n",
            "Iteration: 60000, loss: 1.2953133925464045e-07, gradient norm: tensor(1.8198e-05)\n",
            "Iteration: 61000, loss: 1.2566307641748154e-07, gradient norm: tensor(2.5388e-05)\n",
            "Iteration: 62000, loss: 1.2197337447616975e-07, gradient norm: tensor(1.4064e-06)\n",
            "Iteration: 0, loss: 3.3176834573745726, gradient norm: tensor(3.7553)\n",
            "Iteration: 1000, loss: 2.5040599613189696, gradient norm: tensor(3.2513)\n",
            "Iteration: 2000, loss: 1.8290531245470047, gradient norm: tensor(2.7640)\n",
            "Iteration: 3000, loss: 1.2778075585365296, gradient norm: tensor(2.2850)\n",
            "Iteration: 4000, loss: 0.845216159582138, gradient norm: tensor(1.8233)\n",
            "Iteration: 5000, loss: 0.519955954670906, gradient norm: tensor(1.3907)\n",
            "Iteration: 6000, loss: 0.28726201683282854, gradient norm: tensor(0.9916)\n",
            "Iteration: 7000, loss: 0.1350053733512759, gradient norm: tensor(0.6359)\n",
            "Iteration: 8000, loss: 0.04951624454371631, gradient norm: tensor(0.3420)\n",
            "Iteration: 9000, loss: 0.012545787032227963, gradient norm: tensor(0.1355)\n",
            "Iteration: 10000, loss: 0.002335999664384872, gradient norm: tensor(0.0325)\n",
            "Iteration: 11000, loss: 0.0009183061559451744, gradient norm: tensor(0.0093)\n",
            "Iteration: 12000, loss: 0.0007240762942819856, gradient norm: tensor(0.0074)\n",
            "Iteration: 13000, loss: 0.0005709430034039542, gradient norm: tensor(0.0059)\n",
            "Iteration: 14000, loss: 0.000419860649475595, gradient norm: tensor(0.0044)\n",
            "Iteration: 15000, loss: 0.00028541292739100755, gradient norm: tensor(0.0031)\n",
            "Iteration: 16000, loss: 0.00017829662567237393, gradient norm: tensor(0.0021)\n",
            "Iteration: 17000, loss: 0.00010235323428787523, gradient norm: tensor(0.0013)\n",
            "Iteration: 18000, loss: 5.546475834489684e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 19000, loss: 3.0329919078212696e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 1.7966864361369518e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.158781971753342e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 7.734556957984751e-06, gradient norm: tensor(8.9645e-05)\n",
            "Iteration: 23000, loss: 5.092332138247002e-06, gradient norm: tensor(5.7497e-05)\n",
            "Iteration: 24000, loss: 3.3085124655372056e-06, gradient norm: tensor(3.7602e-05)\n",
            "Iteration: 25000, loss: 2.185071257031268e-06, gradient norm: tensor(2.0768e-05)\n",
            "Iteration: 26000, loss: 1.500614832593783e-06, gradient norm: tensor(1.3976e-05)\n",
            "Iteration: 27000, loss: 1.0880948118483502e-06, gradient norm: tensor(5.1732e-05)\n",
            "Iteration: 28000, loss: 8.344154865653763e-07, gradient norm: tensor(9.2648e-06)\n",
            "Iteration: 29000, loss: 6.656663242097238e-07, gradient norm: tensor(5.6020e-06)\n",
            "Iteration: 30000, loss: 5.429806140000437e-07, gradient norm: tensor(8.4922e-06)\n",
            "Iteration: 31000, loss: 4.5316079743429327e-07, gradient norm: tensor(1.6359e-05)\n",
            "Iteration: 32000, loss: 3.884234700706202e-07, gradient norm: tensor(5.9503e-05)\n",
            "Iteration: 33000, loss: 3.388754277864336e-07, gradient norm: tensor(3.5641e-06)\n",
            "Iteration: 34000, loss: 2.991778411285395e-07, gradient norm: tensor(7.3229e-06)\n",
            "Iteration: 35000, loss: 2.6593799046281676e-07, gradient norm: tensor(7.9283e-05)\n",
            "Iteration: 36000, loss: 2.3742577002394682e-07, gradient norm: tensor(1.2009e-05)\n",
            "Iteration: 37000, loss: 2.12535270335934e-07, gradient norm: tensor(1.4975e-05)\n",
            "Iteration: 38000, loss: 1.910315846487265e-07, gradient norm: tensor(6.9729e-05)\n",
            "Iteration: 39000, loss: 1.7325431657866375e-07, gradient norm: tensor(3.7740e-06)\n",
            "Iteration: 40000, loss: 1.5919106432704665e-07, gradient norm: tensor(2.9665e-05)\n",
            "Iteration: 41000, loss: 1.487051606687828e-07, gradient norm: tensor(1.1800e-06)\n",
            "Iteration: 0, loss: 8.000662912368774, gradient norm: tensor(5.9981)\n",
            "Iteration: 1000, loss: 6.576520097732544, gradient norm: tensor(5.4034)\n",
            "Iteration: 2000, loss: 5.328786515235901, gradient norm: tensor(4.8605)\n",
            "Iteration: 3000, loss: 4.231579374551773, gradient norm: tensor(4.3339)\n",
            "Iteration: 4000, loss: 3.296715167760849, gradient norm: tensor(3.8067)\n",
            "Iteration: 5000, loss: 2.524000015258789, gradient norm: tensor(3.2929)\n",
            "Iteration: 6000, loss: 1.8838717751502991, gradient norm: tensor(2.8062)\n",
            "Iteration: 7000, loss: 1.3506277770996094, gradient norm: tensor(2.3427)\n",
            "Iteration: 8000, loss: 0.9162497044205665, gradient norm: tensor(1.8935)\n",
            "Iteration: 9000, loss: 0.5778708302378655, gradient norm: tensor(1.4627)\n",
            "Iteration: 10000, loss: 0.32988326309621335, gradient norm: tensor(1.0601)\n",
            "Iteration: 11000, loss: 0.16260048134624958, gradient norm: tensor(0.6969)\n",
            "Iteration: 12000, loss: 0.06398748918622732, gradient norm: tensor(0.3896)\n",
            "Iteration: 13000, loss: 0.018352139978669583, gradient norm: tensor(0.1642)\n",
            "Iteration: 14000, loss: 0.004334354866761714, gradient norm: tensor(0.0420)\n",
            "Iteration: 15000, loss: 0.001987623261171393, gradient norm: tensor(0.0064)\n",
            "Iteration: 16000, loss: 0.001626769488910213, gradient norm: tensor(0.0047)\n",
            "Iteration: 17000, loss: 0.0014134449677076191, gradient norm: tensor(0.0049)\n",
            "Iteration: 18000, loss: 0.001217460878426209, gradient norm: tensor(0.0051)\n",
            "Iteration: 19000, loss: 0.000998639733181335, gradient norm: tensor(0.0049)\n",
            "Iteration: 20000, loss: 0.000742036348383408, gradient norm: tensor(0.0042)\n",
            "Iteration: 21000, loss: 0.00048743962176376953, gradient norm: tensor(0.0029)\n",
            "Iteration: 22000, loss: 0.00031014580474584365, gradient norm: tensor(0.0016)\n",
            "Iteration: 23000, loss: 0.0002298672840697691, gradient norm: tensor(0.0008)\n",
            "Iteration: 24000, loss: 0.00019031427189474925, gradient norm: tensor(0.0006)\n",
            "Iteration: 25000, loss: 0.00015326885772810783, gradient norm: tensor(0.0004)\n",
            "Iteration: 26000, loss: 0.00011740503032342531, gradient norm: tensor(0.0003)\n",
            "Iteration: 27000, loss: 8.649231173330918e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 28000, loss: 6.2456434530759e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 29000, loss: 4.5255046941747425e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 30000, loss: 3.355259686213685e-05, gradient norm: tensor(9.3462e-05)\n",
            "Iteration: 31000, loss: 2.561376470475807e-05, gradient norm: tensor(6.5533e-05)\n",
            "Iteration: 32000, loss: 1.999978143066983e-05, gradient norm: tensor(4.6595e-05)\n",
            "Iteration: 33000, loss: 1.580550879043585e-05, gradient norm: tensor(3.6847e-05)\n",
            "Iteration: 34000, loss: 1.2547640627417422e-05, gradient norm: tensor(2.8574e-05)\n",
            "Iteration: 35000, loss: 9.974937239348947e-06, gradient norm: tensor(4.1801e-05)\n",
            "Iteration: 36000, loss: 7.951607834456808e-06, gradient norm: tensor(3.2900e-05)\n",
            "Iteration: 37000, loss: 6.394905421984731e-06, gradient norm: tensor(1.7580e-05)\n",
            "Iteration: 38000, loss: 5.2392688867257674e-06, gradient norm: tensor(1.9485e-05)\n",
            "Iteration: 39000, loss: 4.421400316459767e-06, gradient norm: tensor(1.9491e-05)\n",
            "Iteration: 40000, loss: 3.8783834220339484e-06, gradient norm: tensor(7.0918e-06)\n",
            "Iteration: 41000, loss: 3.5444800550976653e-06, gradient norm: tensor(6.5160e-06)\n",
            "Iteration: 42000, loss: 3.3539422688591004e-06, gradient norm: tensor(9.5219e-06)\n",
            "Iteration: 43000, loss: 3.2512642683286684e-06, gradient norm: tensor(3.0565e-06)\n",
            "Iteration: 44000, loss: 3.1952641857060372e-06, gradient norm: tensor(2.8503e-05)\n",
            "Iteration: 45000, loss: 3.1422413851487363e-06, gradient norm: tensor(7.3683e-06)\n",
            "Iteration: 46000, loss: 2.9801512196172554e-06, gradient norm: tensor(7.1429e-05)\n",
            "Iteration: 47000, loss: 2.77436629244221e-06, gradient norm: tensor(2.4959e-05)\n",
            "Iteration: 48000, loss: 2.6591086805183293e-06, gradient norm: tensor(2.3785e-05)\n",
            "Iteration: 49000, loss: 2.5916632130247308e-06, gradient norm: tensor(6.7415e-06)\n",
            "Iteration: 50000, loss: 2.544848533716504e-06, gradient norm: tensor(1.8267e-06)\n",
            "Iteration: 0, loss: 2.6822510778903963, gradient norm: tensor(3.3291)\n",
            "Iteration: 1000, loss: 1.9449152327775956, gradient norm: tensor(2.8333)\n",
            "Iteration: 2000, loss: 1.369145723104477, gradient norm: tensor(2.3484)\n",
            "Iteration: 3000, loss: 0.9183438565731049, gradient norm: tensor(1.8879)\n",
            "Iteration: 4000, loss: 0.5738419806361198, gradient norm: tensor(1.4527)\n",
            "Iteration: 5000, loss: 0.3243464857637882, gradient norm: tensor(1.0488)\n",
            "Iteration: 6000, loss: 0.1578141390606761, gradient norm: tensor(0.6862)\n",
            "Iteration: 7000, loss: 0.06086760200187564, gradient norm: tensor(0.3812)\n",
            "Iteration: 8000, loss: 0.016561880844645202, gradient norm: tensor(0.1595)\n",
            "Iteration: 9000, loss: 0.0030897357622161506, gradient norm: tensor(0.0406)\n",
            "Iteration: 10000, loss: 0.0008345535297412425, gradient norm: tensor(0.0073)\n",
            "Iteration: 11000, loss: 0.000488296933734091, gradient norm: tensor(0.0047)\n",
            "Iteration: 12000, loss: 0.00032237750265630893, gradient norm: tensor(0.0036)\n",
            "Iteration: 13000, loss: 0.00022506013623205945, gradient norm: tensor(0.0025)\n",
            "Iteration: 14000, loss: 0.00017386957463168072, gradient norm: tensor(0.0016)\n",
            "Iteration: 15000, loss: 0.00014854994566121605, gradient norm: tensor(0.0010)\n",
            "Iteration: 16000, loss: 0.0001304981401117402, gradient norm: tensor(0.0008)\n",
            "Iteration: 17000, loss: 0.00010987921932974132, gradient norm: tensor(0.0007)\n",
            "Iteration: 18000, loss: 8.605725477536908e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 19000, loss: 6.063028675271198e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 20000, loss: 3.538164076599059e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 21000, loss: 1.534385999184451e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 8.146874009526073e-06, gradient norm: tensor(7.9687e-05)\n",
            "Iteration: 23000, loss: 5.349943749024533e-06, gradient norm: tensor(6.1544e-05)\n",
            "Iteration: 24000, loss: 3.5357291951640944e-06, gradient norm: tensor(3.1255e-05)\n",
            "Iteration: 25000, loss: 2.3847582694997983e-06, gradient norm: tensor(1.6933e-05)\n",
            "Iteration: 26000, loss: 1.8549012355606464e-06, gradient norm: tensor(7.4593e-05)\n",
            "Iteration: 27000, loss: 1.6577016468772854e-06, gradient norm: tensor(6.2159e-06)\n",
            "Iteration: 28000, loss: 1.5723165973895449e-06, gradient norm: tensor(3.0700e-06)\n",
            "Iteration: 29000, loss: 1.4994180708072236e-06, gradient norm: tensor(2.9632e-05)\n",
            "Iteration: 30000, loss: 1.4136229511905185e-06, gradient norm: tensor(3.9674e-06)\n",
            "Iteration: 31000, loss: 1.3233440350859381e-06, gradient norm: tensor(2.7947e-05)\n",
            "Iteration: 32000, loss: 1.2360626112695173e-06, gradient norm: tensor(6.4716e-06)\n",
            "Iteration: 33000, loss: 1.1553350085478087e-06, gradient norm: tensor(5.3178e-06)\n",
            "Iteration: 34000, loss: 1.0801051885209744e-06, gradient norm: tensor(3.2387e-06)\n",
            "Iteration: 35000, loss: 1.0099150125597589e-06, gradient norm: tensor(1.8081e-05)\n",
            "Iteration: 36000, loss: 9.444341322932814e-07, gradient norm: tensor(6.6420e-06)\n",
            "Iteration: 37000, loss: 8.826368149925657e-07, gradient norm: tensor(7.5274e-06)\n",
            "Iteration: 38000, loss: 8.24250192351883e-07, gradient norm: tensor(2.0166e-05)\n",
            "Iteration: 39000, loss: 7.692372593055552e-07, gradient norm: tensor(5.3459e-06)\n",
            "Iteration: 40000, loss: 7.167011768842713e-07, gradient norm: tensor(7.7453e-06)\n",
            "Iteration: 41000, loss: 6.66303099706056e-07, gradient norm: tensor(1.3877e-05)\n",
            "Iteration: 42000, loss: 6.181711834756243e-07, gradient norm: tensor(5.8826e-06)\n",
            "Iteration: 43000, loss: 5.72187641751043e-07, gradient norm: tensor(1.0972e-05)\n",
            "Iteration: 44000, loss: 5.279633970189934e-07, gradient norm: tensor(2.8335e-05)\n",
            "Iteration: 45000, loss: 4.854137037852979e-07, gradient norm: tensor(1.5349e-05)\n",
            "Iteration: 46000, loss: 4.444577755862156e-07, gradient norm: tensor(6.5912e-05)\n",
            "Iteration: 47000, loss: 4.0524935928942794e-07, gradient norm: tensor(8.8679e-06)\n",
            "Iteration: 48000, loss: 3.676777724876956e-07, gradient norm: tensor(1.9116e-05)\n",
            "Iteration: 49000, loss: 3.318152094209381e-07, gradient norm: tensor(5.8019e-06)\n",
            "Iteration: 50000, loss: 2.9745649189294456e-07, gradient norm: tensor(1.8061e-05)\n",
            "Iteration: 51000, loss: 2.649049762055711e-07, gradient norm: tensor(1.7932e-06)\n",
            "Iteration: 0, loss: 6.6314309883117675, gradient norm: tensor(5.2413)\n",
            "Iteration: 1000, loss: 5.411707247257232, gradient norm: tensor(4.6642)\n",
            "Iteration: 2000, loss: 4.380271735906601, gradient norm: tensor(4.1177)\n",
            "Iteration: 3000, loss: 3.4945041687488554, gradient norm: tensor(3.6134)\n",
            "Iteration: 4000, loss: 2.7202491037845613, gradient norm: tensor(3.1815)\n",
            "Iteration: 5000, loss: 2.0349848351478577, gradient norm: tensor(2.7878)\n",
            "Iteration: 6000, loss: 1.4505079489946366, gradient norm: tensor(2.3702)\n",
            "Iteration: 7000, loss: 0.9799816336035728, gradient norm: tensor(1.9391)\n",
            "Iteration: 8000, loss: 0.6182225972115993, gradient norm: tensor(1.5116)\n",
            "Iteration: 9000, loss: 0.3543217874467373, gradient norm: tensor(1.1050)\n",
            "Iteration: 10000, loss: 0.17569524359703065, gradient norm: tensor(0.7352)\n",
            "Iteration: 11000, loss: 0.06966449877247215, gradient norm: tensor(0.4195)\n",
            "Iteration: 12000, loss: 0.019700216062366962, gradient norm: tensor(0.1842)\n",
            "Iteration: 13000, loss: 0.003542620655731298, gradient norm: tensor(0.0515)\n",
            "Iteration: 14000, loss: 0.000549407389073167, gradient norm: tensor(0.0079)\n",
            "Iteration: 15000, loss: 0.00018056146505114157, gradient norm: tensor(0.0022)\n",
            "Iteration: 16000, loss: 0.00010373499671550235, gradient norm: tensor(0.0010)\n",
            "Iteration: 17000, loss: 8.689976625464624e-05, gradient norm: tensor(0.0008)\n",
            "Iteration: 18000, loss: 7.88850673052366e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 6.333577804616652e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 20000, loss: 4.218513031810289e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 21000, loss: 2.9280198223204934e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 22000, loss: 2.3795268452886377e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 2.108543803478824e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 1.8603984759465675e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.5057612546115706e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 1.069173723590211e-05, gradient norm: tensor(7.4667e-05)\n",
            "Iteration: 27000, loss: 7.366434897903673e-06, gradient norm: tensor(5.8314e-05)\n",
            "Iteration: 28000, loss: 5.564714057072706e-06, gradient norm: tensor(3.0122e-05)\n",
            "Iteration: 29000, loss: 4.508696235461684e-06, gradient norm: tensor(1.9790e-05)\n",
            "Iteration: 30000, loss: 3.7988457188475877e-06, gradient norm: tensor(1.7688e-05)\n",
            "Iteration: 31000, loss: 3.368396342921187e-06, gradient norm: tensor(1.2897e-05)\n",
            "Iteration: 32000, loss: 3.179104420269141e-06, gradient norm: tensor(9.2946e-06)\n",
            "Iteration: 33000, loss: 3.122213085589465e-06, gradient norm: tensor(1.2609e-05)\n",
            "Iteration: 34000, loss: 3.0752314326036866e-06, gradient norm: tensor(2.9048e-06)\n",
            "Iteration: 35000, loss: 3.0372039470876187e-06, gradient norm: tensor(1.0522e-06)\n",
            "Iteration: 0, loss: 2.6764831812381744, gradient norm: tensor(3.2898)\n",
            "Iteration: 1000, loss: 1.9572855931520463, gradient norm: tensor(2.7873)\n",
            "Iteration: 2000, loss: 1.3807542275190354, gradient norm: tensor(2.3183)\n",
            "Iteration: 3000, loss: 0.9215004252195358, gradient norm: tensor(1.8728)\n",
            "Iteration: 4000, loss: 0.5698509863913059, gradient norm: tensor(1.4440)\n",
            "Iteration: 5000, loss: 0.31723627158999446, gradient norm: tensor(1.0398)\n",
            "Iteration: 6000, loss: 0.1511785929352045, gradient norm: tensor(0.6753)\n",
            "Iteration: 7000, loss: 0.056682375125586985, gradient norm: tensor(0.3715)\n",
            "Iteration: 8000, loss: 0.014463027921970933, gradient norm: tensor(0.1535)\n",
            "Iteration: 9000, loss: 0.0020829574494273403, gradient norm: tensor(0.0378)\n",
            "Iteration: 10000, loss: 0.0002561165076185716, gradient norm: tensor(0.0040)\n",
            "Iteration: 11000, loss: 0.00012812425805896056, gradient norm: tensor(0.0012)\n",
            "Iteration: 12000, loss: 0.00010826952005299972, gradient norm: tensor(0.0011)\n",
            "Iteration: 13000, loss: 9.898443281417713e-05, gradient norm: tensor(0.0010)\n",
            "Iteration: 14000, loss: 9.137195875518955e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 15000, loss: 8.368948378483765e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 16000, loss: 7.451754590874771e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 17000, loss: 6.0662968604447086e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 18000, loss: 4.455004303963506e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 19000, loss: 3.1249218502125585e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 2.164482278021751e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.5650799894501687e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 1.2329221723121009e-05, gradient norm: tensor(6.2701e-05)\n",
            "Iteration: 23000, loss: 1.0383104702668788e-05, gradient norm: tensor(4.6846e-05)\n",
            "Iteration: 24000, loss: 8.676358024786168e-06, gradient norm: tensor(3.7789e-05)\n",
            "Iteration: 25000, loss: 6.831119050730194e-06, gradient norm: tensor(2.9948e-05)\n",
            "Iteration: 26000, loss: 5.1043873218077355e-06, gradient norm: tensor(2.2125e-05)\n",
            "Iteration: 27000, loss: 3.602406073923703e-06, gradient norm: tensor(1.6884e-05)\n",
            "Iteration: 28000, loss: 2.2563641643955633e-06, gradient norm: tensor(1.1261e-05)\n",
            "Iteration: 29000, loss: 1.3889501972244034e-06, gradient norm: tensor(6.1925e-06)\n",
            "Iteration: 30000, loss: 1.0022301570984383e-06, gradient norm: tensor(7.3132e-06)\n",
            "Iteration: 31000, loss: 7.705834978537495e-07, gradient norm: tensor(7.2948e-06)\n",
            "Iteration: 32000, loss: 6.076893162685337e-07, gradient norm: tensor(6.1334e-06)\n",
            "Iteration: 33000, loss: 5.12909094737779e-07, gradient norm: tensor(1.5636e-05)\n",
            "Iteration: 34000, loss: 4.440321297352057e-07, gradient norm: tensor(2.8239e-06)\n",
            "Iteration: 35000, loss: 3.9153035052663655e-07, gradient norm: tensor(2.6810e-06)\n",
            "Iteration: 36000, loss: 3.5022132547624095e-07, gradient norm: tensor(5.2935e-06)\n",
            "Iteration: 37000, loss: 3.167089820408364e-07, gradient norm: tensor(8.5328e-06)\n",
            "Iteration: 38000, loss: 2.8932595401442995e-07, gradient norm: tensor(3.2352e-06)\n",
            "Iteration: 39000, loss: 2.667838795957778e-07, gradient norm: tensor(4.3725e-05)\n",
            "Iteration: 40000, loss: 2.4771096605036293e-07, gradient norm: tensor(1.0382e-05)\n",
            "Iteration: 41000, loss: 2.3150435778518385e-07, gradient norm: tensor(7.8774e-06)\n",
            "Iteration: 42000, loss: 2.1769857583819886e-07, gradient norm: tensor(3.2834e-06)\n",
            "Iteration: 43000, loss: 2.057852961456774e-07, gradient norm: tensor(2.4845e-05)\n",
            "Iteration: 44000, loss: 1.954282701461807e-07, gradient norm: tensor(1.6958e-05)\n",
            "Iteration: 45000, loss: 1.8637504575735874e-07, gradient norm: tensor(2.1235e-05)\n",
            "Iteration: 46000, loss: 1.784953710028958e-07, gradient norm: tensor(2.0305e-05)\n",
            "Iteration: 47000, loss: 1.7156012243901842e-07, gradient norm: tensor(4.9461e-05)\n",
            "Iteration: 48000, loss: 1.6548121698178876e-07, gradient norm: tensor(8.0402e-06)\n",
            "Iteration: 49000, loss: 1.60072487119578e-07, gradient norm: tensor(4.5552e-06)\n",
            "Iteration: 50000, loss: 1.5531507622768004e-07, gradient norm: tensor(6.4487e-06)\n",
            "Iteration: 51000, loss: 1.5106756318061799e-07, gradient norm: tensor(2.9673e-05)\n",
            "Iteration: 52000, loss: 1.4727245756773756e-07, gradient norm: tensor(8.3689e-06)\n",
            "Iteration: 53000, loss: 1.4389174332052335e-07, gradient norm: tensor(6.9481e-06)\n",
            "Iteration: 54000, loss: 1.4084327290220245e-07, gradient norm: tensor(5.0093e-06)\n",
            "Iteration: 55000, loss: 1.3811780296180132e-07, gradient norm: tensor(8.2278e-06)\n",
            "Iteration: 56000, loss: 1.3567274211823134e-07, gradient norm: tensor(1.0171e-05)\n",
            "Iteration: 57000, loss: 1.33456562608103e-07, gradient norm: tensor(1.7231e-05)\n",
            "Iteration: 58000, loss: 1.3145918346424424e-07, gradient norm: tensor(1.1981e-05)\n",
            "Iteration: 59000, loss: 1.2965316449253805e-07, gradient norm: tensor(6.1014e-06)\n",
            "Iteration: 60000, loss: 1.2802717895965542e-07, gradient norm: tensor(6.5135e-05)\n",
            "Iteration: 61000, loss: 1.265573895778971e-07, gradient norm: tensor(1.1589e-05)\n",
            "Iteration: 62000, loss: 1.252140194196727e-07, gradient norm: tensor(6.6716e-06)\n",
            "Iteration: 63000, loss: 1.2399216070946295e-07, gradient norm: tensor(2.8978e-06)\n",
            "Iteration: 64000, loss: 1.2290175514806378e-07, gradient norm: tensor(3.8130e-05)\n",
            "Iteration: 65000, loss: 1.218684580521767e-07, gradient norm: tensor(1.1523e-05)\n",
            "Iteration: 66000, loss: 1.2095347176455107e-07, gradient norm: tensor(1.5618e-06)\n",
            "Iteration: 0, loss: 5.499455357074738, gradient norm: tensor(4.8406)\n",
            "Iteration: 1000, loss: 4.358453298330307, gradient norm: tensor(4.3195)\n",
            "Iteration: 2000, loss: 3.403098443031311, gradient norm: tensor(3.8310)\n",
            "Iteration: 3000, loss: 2.6049425168037414, gradient norm: tensor(3.3425)\n",
            "Iteration: 4000, loss: 1.9403893741369247, gradient norm: tensor(2.8612)\n",
            "Iteration: 5000, loss: 1.391474218249321, gradient norm: tensor(2.3918)\n",
            "Iteration: 6000, loss: 0.9470536739230156, gradient norm: tensor(1.9381)\n",
            "Iteration: 7000, loss: 0.5997311878800392, gradient norm: tensor(1.5044)\n",
            "Iteration: 8000, loss: 0.34310603301227094, gradient norm: tensor(1.0971)\n",
            "Iteration: 9000, loss: 0.1694274225309491, gradient norm: tensor(0.7274)\n",
            "Iteration: 10000, loss: 0.06713714676722884, gradient norm: tensor(0.4134)\n",
            "Iteration: 11000, loss: 0.019304810096509754, gradient norm: tensor(0.1804)\n",
            "Iteration: 12000, loss: 0.004241161587880925, gradient norm: tensor(0.0493)\n",
            "Iteration: 13000, loss: 0.0018047873253235593, gradient norm: tensor(0.0065)\n",
            "Iteration: 14000, loss: 0.0016478032157756389, gradient norm: tensor(0.0028)\n",
            "Iteration: 15000, loss: 0.0016082024403149262, gradient norm: tensor(0.0032)\n",
            "Iteration: 16000, loss: 0.0015419184658676385, gradient norm: tensor(0.0037)\n",
            "Iteration: 17000, loss: 0.0014212017867248506, gradient norm: tensor(0.0044)\n",
            "Iteration: 18000, loss: 0.0011960666346130893, gradient norm: tensor(0.0049)\n",
            "Iteration: 19000, loss: 0.0008357448829920031, gradient norm: tensor(0.0044)\n",
            "Iteration: 20000, loss: 0.00046487241581780837, gradient norm: tensor(0.0029)\n",
            "Iteration: 21000, loss: 0.00024674967321334406, gradient norm: tensor(0.0014)\n",
            "Iteration: 22000, loss: 0.00016838682630623226, gradient norm: tensor(0.0007)\n",
            "Iteration: 23000, loss: 0.00013512546189303976, gradient norm: tensor(0.0005)\n",
            "Iteration: 24000, loss: 0.00010472414761170512, gradient norm: tensor(0.0004)\n",
            "Iteration: 25000, loss: 7.609638307621936e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 26000, loss: 5.2311925388494275e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 27000, loss: 3.4560000840428985e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 2.2268710903517786e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 29000, loss: 1.4136852129922773e-05, gradient norm: tensor(7.7327e-05)\n",
            "Iteration: 30000, loss: 8.943612174334703e-06, gradient norm: tensor(4.9734e-05)\n",
            "Iteration: 31000, loss: 5.7741001364775e-06, gradient norm: tensor(4.7823e-05)\n",
            "Iteration: 32000, loss: 3.97526129063408e-06, gradient norm: tensor(2.2723e-05)\n",
            "Iteration: 33000, loss: 3.064790819053087e-06, gradient norm: tensor(7.2970e-05)\n",
            "Iteration: 34000, loss: 2.6747503486603817e-06, gradient norm: tensor(4.5210e-06)\n",
            "Iteration: 35000, loss: 2.537962437827446e-06, gradient norm: tensor(7.5276e-06)\n",
            "Iteration: 36000, loss: 2.492588550921937e-06, gradient norm: tensor(1.2958e-05)\n",
            "Iteration: 37000, loss: 2.473988016618023e-06, gradient norm: tensor(0.0002)\n",
            "Iteration: 38000, loss: 2.463990203978028e-06, gradient norm: tensor(6.7938e-05)\n",
            "Iteration: 39000, loss: 2.457678355540338e-06, gradient norm: tensor(3.4710e-05)\n",
            "Iteration: 40000, loss: 2.453490128118574e-06, gradient norm: tensor(5.2181e-06)\n",
            "Iteration: 41000, loss: 2.4505551325546547e-06, gradient norm: tensor(7.5661e-05)\n",
            "Iteration: 42000, loss: 2.4484636053330177e-06, gradient norm: tensor(1.7849e-06)\n",
            "Iteration: 0, loss: 3.6348481755256654, gradient norm: tensor(3.2538)\n",
            "Iteration: 1000, loss: 2.8125925776958467, gradient norm: tensor(2.6487)\n",
            "Iteration: 2000, loss: 2.178075256586075, gradient norm: tensor(2.1672)\n",
            "Iteration: 3000, loss: 1.6599411627054215, gradient norm: tensor(1.8433)\n",
            "Iteration: 4000, loss: 1.1902853562235831, gradient norm: tensor(1.6556)\n",
            "Iteration: 5000, loss: 0.7540440273880958, gradient norm: tensor(1.4456)\n",
            "Iteration: 6000, loss: 0.39357449957728385, gradient norm: tensor(1.0513)\n",
            "Iteration: 7000, loss: 0.16055739413201808, gradient norm: tensor(0.6108)\n",
            "Iteration: 8000, loss: 0.051126102957874536, gradient norm: tensor(0.2718)\n",
            "Iteration: 9000, loss: 0.01669432391785085, gradient norm: tensor(0.0897)\n",
            "Iteration: 10000, loss: 0.009057848947122693, gradient norm: tensor(0.0348)\n",
            "Iteration: 11000, loss: 0.006684520055539906, gradient norm: tensor(0.0223)\n",
            "Iteration: 12000, loss: 0.005162683380767703, gradient norm: tensor(0.0142)\n",
            "Iteration: 13000, loss: 0.004044342644978315, gradient norm: tensor(0.0097)\n",
            "Iteration: 14000, loss: 0.003103369805496186, gradient norm: tensor(0.0076)\n",
            "Iteration: 15000, loss: 0.0022164014411391693, gradient norm: tensor(0.0065)\n",
            "Iteration: 16000, loss: 0.0013585381036973559, gradient norm: tensor(0.0053)\n",
            "Iteration: 17000, loss: 0.0006304256946605165, gradient norm: tensor(0.0033)\n",
            "Iteration: 18000, loss: 0.00021026635253656423, gradient norm: tensor(0.0013)\n",
            "Iteration: 19000, loss: 8.469231409981149e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 20000, loss: 6.672713501757244e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 6.0169395030243325e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 22000, loss: 5.0658151722018374e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 23000, loss: 3.665503946467652e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 2.0542051150187036e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 8.681613798671605e-06, gradient norm: tensor(5.7994e-05)\n",
            "Iteration: 26000, loss: 3.4974223237895786e-06, gradient norm: tensor(2.3277e-05)\n",
            "Iteration: 27000, loss: 2.0513994218163135e-06, gradient norm: tensor(9.3085e-06)\n",
            "Iteration: 28000, loss: 1.6691673523610008e-06, gradient norm: tensor(8.3989e-06)\n",
            "Iteration: 29000, loss: 1.4263905945881562e-06, gradient norm: tensor(6.3820e-06)\n",
            "Iteration: 30000, loss: 1.186906096563689e-06, gradient norm: tensor(4.6325e-05)\n",
            "Iteration: 31000, loss: 9.87247514842693e-07, gradient norm: tensor(2.6162e-05)\n",
            "Iteration: 32000, loss: 8.299930340172068e-07, gradient norm: tensor(6.3447e-05)\n",
            "Iteration: 33000, loss: 7.019723421421986e-07, gradient norm: tensor(2.0553e-05)\n",
            "Iteration: 34000, loss: 5.918987417317112e-07, gradient norm: tensor(3.8470e-05)\n",
            "Iteration: 35000, loss: 4.923107123602222e-07, gradient norm: tensor(2.5257e-06)\n",
            "Iteration: 36000, loss: 4.0109426768708546e-07, gradient norm: tensor(8.2763e-06)\n",
            "Iteration: 37000, loss: 3.1934633423702507e-07, gradient norm: tensor(1.4597e-05)\n",
            "Iteration: 38000, loss: 2.49051036391279e-07, gradient norm: tensor(3.5138e-05)\n",
            "Iteration: 39000, loss: 1.9196850912805985e-07, gradient norm: tensor(4.3549e-05)\n",
            "Iteration: 40000, loss: 1.4890802242462087e-07, gradient norm: tensor(4.9505e-05)\n",
            "Iteration: 41000, loss: 1.185610895646505e-07, gradient norm: tensor(8.6134e-06)\n",
            "Iteration: 42000, loss: 9.808442920444804e-08, gradient norm: tensor(1.3715e-05)\n",
            "Iteration: 43000, loss: 8.47317147645299e-08, gradient norm: tensor(2.3785e-05)\n",
            "Iteration: 44000, loss: 7.607149687771652e-08, gradient norm: tensor(5.1892e-06)\n",
            "Iteration: 45000, loss: 7.034150164031416e-08, gradient norm: tensor(8.7655e-06)\n",
            "Iteration: 46000, loss: 6.643759006408345e-08, gradient norm: tensor(3.4179e-05)\n",
            "Iteration: 47000, loss: 6.362639835799655e-08, gradient norm: tensor(1.5753e-05)\n",
            "Iteration: 48000, loss: 6.148232115066321e-08, gradient norm: tensor(1.1438e-06)\n",
            "Iteration: 0, loss: 2.1350111587047578, gradient norm: tensor(2.8699)\n",
            "Iteration: 1000, loss: 1.5125541135072709, gradient norm: tensor(2.4009)\n",
            "Iteration: 2000, loss: 1.014612252831459, gradient norm: tensor(1.9299)\n",
            "Iteration: 3000, loss: 0.6380585276782512, gradient norm: tensor(1.4884)\n",
            "Iteration: 4000, loss: 0.3666039418578148, gradient norm: tensor(1.0804)\n",
            "Iteration: 5000, loss: 0.18439880016446114, gradient norm: tensor(0.7141)\n",
            "Iteration: 6000, loss: 0.07676608997955918, gradient norm: tensor(0.4054)\n",
            "Iteration: 7000, loss: 0.0254236220670864, gradient norm: tensor(0.1776)\n",
            "Iteration: 8000, loss: 0.00787664078688249, gradient norm: tensor(0.0510)\n",
            "Iteration: 9000, loss: 0.0035402559598442167, gradient norm: tensor(0.0114)\n",
            "Iteration: 10000, loss: 0.0022720668299589307, gradient norm: tensor(0.0061)\n",
            "Iteration: 11000, loss: 0.001606814463972114, gradient norm: tensor(0.0046)\n",
            "Iteration: 12000, loss: 0.001087303422857076, gradient norm: tensor(0.0036)\n",
            "Iteration: 13000, loss: 0.0006936878719134256, gradient norm: tensor(0.0027)\n",
            "Iteration: 14000, loss: 0.0004167070853291079, gradient norm: tensor(0.0020)\n",
            "Iteration: 15000, loss: 0.00022475381458934862, gradient norm: tensor(0.0014)\n",
            "Iteration: 16000, loss: 0.00010391249910753687, gradient norm: tensor(0.0008)\n",
            "Iteration: 17000, loss: 4.6061056265898514e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 18000, loss: 2.8135120070146514e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 19000, loss: 2.2935354922083205e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 20000, loss: 1.960945642531442e-05, gradient norm: tensor(8.3396e-05)\n",
            "Iteration: 21000, loss: 1.6991354395941017e-05, gradient norm: tensor(6.8049e-05)\n",
            "Iteration: 22000, loss: 1.4735860949258495e-05, gradient norm: tensor(5.6162e-05)\n",
            "Iteration: 23000, loss: 1.2461048654586194e-05, gradient norm: tensor(4.6754e-05)\n",
            "Iteration: 24000, loss: 9.992345185310115e-06, gradient norm: tensor(3.8188e-05)\n",
            "Iteration: 25000, loss: 7.4510918002488325e-06, gradient norm: tensor(2.9549e-05)\n",
            "Iteration: 26000, loss: 5.165404268154816e-06, gradient norm: tensor(2.2763e-05)\n",
            "Iteration: 27000, loss: 3.340467368843747e-06, gradient norm: tensor(1.5677e-05)\n",
            "Iteration: 28000, loss: 1.987634858551246e-06, gradient norm: tensor(2.6424e-05)\n",
            "Iteration: 29000, loss: 1.062200567560012e-06, gradient norm: tensor(4.0361e-05)\n",
            "Iteration: 30000, loss: 5.190923773170653e-07, gradient norm: tensor(2.1844e-05)\n",
            "Iteration: 31000, loss: 2.5923810039785167e-07, gradient norm: tensor(2.4036e-06)\n",
            "Iteration: 32000, loss: 1.4997187230392228e-07, gradient norm: tensor(7.5033e-06)\n",
            "Iteration: 33000, loss: 1.0473454822346184e-07, gradient norm: tensor(2.4820e-05)\n",
            "Iteration: 34000, loss: 8.310658669330451e-08, gradient norm: tensor(4.4954e-05)\n",
            "Iteration: 35000, loss: 6.973956160294392e-08, gradient norm: tensor(3.5328e-06)\n",
            "Iteration: 36000, loss: 6.011463230848335e-08, gradient norm: tensor(5.7737e-05)\n",
            "Iteration: 37000, loss: 5.268250716738976e-08, gradient norm: tensor(5.3365e-06)\n",
            "Iteration: 38000, loss: 4.680966399561726e-08, gradient norm: tensor(3.3842e-05)\n",
            "Iteration: 39000, loss: 4.210519669811674e-08, gradient norm: tensor(4.7837e-05)\n",
            "Iteration: 40000, loss: 3.8225327926966204e-08, gradient norm: tensor(3.5234e-06)\n",
            "Iteration: 41000, loss: 3.501678668271779e-08, gradient norm: tensor(1.5834e-06)\n",
            "Iteration: 0, loss: 8.045737263202668, gradient norm: tensor(6.5036)\n",
            "Iteration: 1000, loss: 6.531724418640136, gradient norm: tensor(5.8000)\n",
            "Iteration: 2000, loss: 5.2343916959762575, gradient norm: tensor(5.0954)\n",
            "Iteration: 3000, loss: 4.150540356397629, gradient norm: tensor(4.4271)\n",
            "Iteration: 4000, loss: 3.259536817073822, gradient norm: tensor(3.8205)\n",
            "Iteration: 5000, loss: 2.520165822982788, gradient norm: tensor(3.2686)\n",
            "Iteration: 6000, loss: 1.897045291543007, gradient norm: tensor(2.7788)\n",
            "Iteration: 7000, loss: 1.3688641649484634, gradient norm: tensor(2.3324)\n",
            "Iteration: 8000, loss: 0.9343373532295227, gradient norm: tensor(1.8965)\n",
            "Iteration: 9000, loss: 0.5936919277906418, gradient norm: tensor(1.4700)\n",
            "Iteration: 10000, loss: 0.342460489526391, gradient norm: tensor(1.0697)\n",
            "Iteration: 11000, loss: 0.1720093926116824, gradient norm: tensor(0.7099)\n",
            "Iteration: 12000, loss: 0.0697490659058094, gradient norm: tensor(0.4044)\n",
            "Iteration: 13000, loss: 0.020613112295977773, gradient norm: tensor(0.1758)\n",
            "Iteration: 14000, loss: 0.004635127485729754, gradient norm: tensor(0.0478)\n",
            "Iteration: 15000, loss: 0.0015921791821019724, gradient norm: tensor(0.0094)\n",
            "Iteration: 16000, loss: 0.0009732305204379373, gradient norm: tensor(0.0058)\n",
            "Iteration: 17000, loss: 0.0006646199965034611, gradient norm: tensor(0.0043)\n",
            "Iteration: 18000, loss: 0.00046322679071454333, gradient norm: tensor(0.0031)\n",
            "Iteration: 19000, loss: 0.00028415622100874313, gradient norm: tensor(0.0019)\n",
            "Iteration: 20000, loss: 0.00014706090924300952, gradient norm: tensor(0.0010)\n",
            "Iteration: 21000, loss: 7.774365799195948e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 22000, loss: 4.77784594768309e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 23000, loss: 3.228270987710857e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 24000, loss: 2.360538531320344e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 25000, loss: 1.8846030068743857e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 26000, loss: 1.608969132121274e-05, gradient norm: tensor(7.6681e-05)\n",
            "Iteration: 27000, loss: 1.42729479512127e-05, gradient norm: tensor(5.7525e-05)\n",
            "Iteration: 28000, loss: 1.2742753217025893e-05, gradient norm: tensor(4.5202e-05)\n",
            "Iteration: 29000, loss: 1.0553943040576996e-05, gradient norm: tensor(5.2461e-05)\n",
            "Iteration: 30000, loss: 6.575860363227548e-06, gradient norm: tensor(4.3694e-05)\n",
            "Iteration: 31000, loss: 2.3021844536970094e-06, gradient norm: tensor(1.4525e-05)\n",
            "Iteration: 32000, loss: 9.491065177940073e-07, gradient norm: tensor(5.6714e-06)\n",
            "Iteration: 33000, loss: 6.610239847191224e-07, gradient norm: tensor(3.0535e-06)\n",
            "Iteration: 34000, loss: 5.762152677561971e-07, gradient norm: tensor(1.1204e-05)\n",
            "Iteration: 35000, loss: 5.086560177005595e-07, gradient norm: tensor(2.7569e-06)\n",
            "Iteration: 36000, loss: 4.58170698863114e-07, gradient norm: tensor(1.9232e-06)\n",
            "Iteration: 0, loss: 1.9127892862558364, gradient norm: tensor(2.8454)\n",
            "Iteration: 1000, loss: 1.2733824915885925, gradient norm: tensor(2.2863)\n",
            "Iteration: 2000, loss: 0.8139468571543693, gradient norm: tensor(1.7753)\n",
            "Iteration: 3000, loss: 0.49194619819521906, gradient norm: tensor(1.3356)\n",
            "Iteration: 4000, loss: 0.2670666650980711, gradient norm: tensor(0.9428)\n",
            "Iteration: 5000, loss: 0.12219560261070728, gradient norm: tensor(0.5946)\n",
            "Iteration: 6000, loss: 0.0430310479439795, gradient norm: tensor(0.3101)\n",
            "Iteration: 7000, loss: 0.010353091428987682, gradient norm: tensor(0.1161)\n",
            "Iteration: 8000, loss: 0.0019789532070280985, gradient norm: tensor(0.0252)\n",
            "Iteration: 9000, loss: 0.0008726455121650361, gradient norm: tensor(0.0078)\n",
            "Iteration: 10000, loss: 0.000650831758568529, gradient norm: tensor(0.0065)\n",
            "Iteration: 11000, loss: 0.0004882683152973186, gradient norm: tensor(0.0051)\n",
            "Iteration: 12000, loss: 0.0003680233748455066, gradient norm: tensor(0.0038)\n",
            "Iteration: 13000, loss: 0.0002784220074099721, gradient norm: tensor(0.0027)\n",
            "Iteration: 14000, loss: 0.0002011513706529513, gradient norm: tensor(0.0020)\n",
            "Iteration: 15000, loss: 0.00013556702894129558, gradient norm: tensor(0.0013)\n",
            "Iteration: 16000, loss: 8.554143121000379e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 17000, loss: 5.012247486229171e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 18000, loss: 2.6574058349069673e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 19000, loss: 1.3156616835658497e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 20000, loss: 7.274996083651786e-06, gradient norm: tensor(8.9310e-05)\n",
            "Iteration: 21000, loss: 4.698486433426297e-06, gradient norm: tensor(5.0636e-05)\n",
            "Iteration: 22000, loss: 3.3800128305756516e-06, gradient norm: tensor(2.5012e-05)\n",
            "Iteration: 23000, loss: 2.8303778390181835e-06, gradient norm: tensor(1.5565e-05)\n",
            "Iteration: 24000, loss: 2.6005546112628508e-06, gradient norm: tensor(8.6762e-06)\n",
            "Iteration: 25000, loss: 2.4258989944883068e-06, gradient norm: tensor(6.6161e-06)\n",
            "Iteration: 26000, loss: 2.2587875275803527e-06, gradient norm: tensor(2.8470e-05)\n",
            "Iteration: 27000, loss: 2.1362101610975514e-06, gradient norm: tensor(1.5866e-05)\n",
            "Iteration: 28000, loss: 2.0806552067824668e-06, gradient norm: tensor(1.4575e-06)\n",
            "Iteration: 0, loss: 9.319236721992493, gradient norm: tensor(6.7367)\n",
            "Iteration: 1000, loss: 7.729376696109772, gradient norm: tensor(6.0570)\n",
            "Iteration: 2000, loss: 6.377634174346924, gradient norm: tensor(5.4373)\n",
            "Iteration: 3000, loss: 5.226167023181915, gradient norm: tensor(4.8743)\n",
            "Iteration: 4000, loss: 4.231094397544861, gradient norm: tensor(4.3474)\n",
            "Iteration: 5000, loss: 3.3631406021118164, gradient norm: tensor(3.8393)\n",
            "Iteration: 6000, loss: 2.608252258062363, gradient norm: tensor(3.3464)\n",
            "Iteration: 7000, loss: 1.9594711478948592, gradient norm: tensor(2.8690)\n",
            "Iteration: 8000, loss: 1.4128875609636307, gradient norm: tensor(2.4063)\n",
            "Iteration: 9000, loss: 0.9661339172720909, gradient norm: tensor(1.9560)\n",
            "Iteration: 10000, loss: 0.6159497028589249, gradient norm: tensor(1.5224)\n",
            "Iteration: 11000, loss: 0.35663913238048556, gradient norm: tensor(1.1145)\n",
            "Iteration: 12000, loss: 0.17938111095875503, gradient norm: tensor(0.7443)\n",
            "Iteration: 13000, loss: 0.07235296076908708, gradient norm: tensor(0.4287)\n",
            "Iteration: 14000, loss: 0.020303151874803006, gradient norm: tensor(0.1904)\n",
            "Iteration: 15000, loss: 0.0032149764619534833, gradient norm: tensor(0.0533)\n",
            "Iteration: 16000, loss: 0.0002638058678639936, gradient norm: tensor(0.0068)\n",
            "Iteration: 17000, loss: 3.713765960310411e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 18000, loss: 1.4478980383501038e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 19000, loss: 6.674048908280383e-06, gradient norm: tensor(9.2787e-05)\n",
            "Iteration: 20000, loss: 3.6749377875366916e-06, gradient norm: tensor(3.3650e-05)\n",
            "Iteration: 21000, loss: 2.8670632768807993e-06, gradient norm: tensor(1.2091e-05)\n",
            "Iteration: 22000, loss: 2.701460202843009e-06, gradient norm: tensor(5.9450e-06)\n",
            "Iteration: 23000, loss: 2.681115588757166e-06, gradient norm: tensor(5.4694e-06)\n",
            "Iteration: 24000, loss: 2.678159657989454e-06, gradient norm: tensor(5.8455e-06)\n",
            "Iteration: 25000, loss: 2.6753744823508896e-06, gradient norm: tensor(1.1016e-05)\n",
            "Iteration: 26000, loss: 2.6720545679381756e-06, gradient norm: tensor(9.6000e-06)\n",
            "Iteration: 27000, loss: 2.6673126212699572e-06, gradient norm: tensor(5.2853e-06)\n",
            "Iteration: 28000, loss: 2.659846906681196e-06, gradient norm: tensor(8.9598e-06)\n",
            "Iteration: 29000, loss: 2.647992686888756e-06, gradient norm: tensor(4.9371e-06)\n",
            "Iteration: 30000, loss: 2.6296696316876477e-06, gradient norm: tensor(5.0282e-06)\n",
            "Iteration: 31000, loss: 2.6024875312486985e-06, gradient norm: tensor(1.5315e-05)\n",
            "Iteration: 32000, loss: 2.564439847219546e-06, gradient norm: tensor(3.9872e-06)\n",
            "Iteration: 33000, loss: 2.515226192826958e-06, gradient norm: tensor(4.4433e-05)\n",
            "Iteration: 34000, loss: 2.4571204487529032e-06, gradient norm: tensor(4.2996e-06)\n",
            "Iteration: 35000, loss: 2.3930892659791425e-06, gradient norm: tensor(1.3124e-05)\n",
            "Iteration: 36000, loss: 2.3230700480780795e-06, gradient norm: tensor(2.4213e-06)\n",
            "Iteration: 37000, loss: 2.2473791730135415e-06, gradient norm: tensor(5.7075e-06)\n",
            "Iteration: 38000, loss: 2.1685456526938653e-06, gradient norm: tensor(1.5650e-05)\n",
            "Iteration: 39000, loss: 2.11032871061434e-06, gradient norm: tensor(3.0409e-05)\n",
            "Iteration: 40000, loss: 2.0758750497407162e-06, gradient norm: tensor(3.2131e-06)\n",
            "Iteration: 41000, loss: 2.0500243920196226e-06, gradient norm: tensor(5.6372e-06)\n",
            "Iteration: 42000, loss: 2.0283958078834986e-06, gradient norm: tensor(1.4992e-05)\n",
            "Iteration: 43000, loss: 2.0096889345495582e-06, gradient norm: tensor(1.2205e-06)\n",
            "Iteration: 0, loss: 8.858537605285644, gradient norm: tensor(6.3693)\n",
            "Iteration: 1000, loss: 7.478152540206909, gradient norm: tensor(5.8406)\n",
            "Iteration: 2000, loss: 6.2457313690185545, gradient norm: tensor(5.3239)\n",
            "Iteration: 3000, loss: 5.1441367168426515, gradient norm: tensor(4.8128)\n",
            "Iteration: 4000, loss: 4.16452244591713, gradient norm: tensor(4.3042)\n",
            "Iteration: 5000, loss: 3.301930594205856, gradient norm: tensor(3.8066)\n",
            "Iteration: 6000, loss: 2.5506813764572143, gradient norm: tensor(3.3192)\n",
            "Iteration: 7000, loss: 1.9066180288791656, gradient norm: tensor(2.8408)\n",
            "Iteration: 8000, loss: 1.366750340104103, gradient norm: tensor(2.3739)\n",
            "Iteration: 9000, loss: 0.9275817382931709, gradient norm: tensor(1.9221)\n",
            "Iteration: 10000, loss: 0.5849354072511196, gradient norm: tensor(1.4889)\n",
            "Iteration: 11000, loss: 0.3331816468089819, gradient norm: tensor(1.0830)\n",
            "Iteration: 12000, loss: 0.1635223704725504, gradient norm: tensor(0.7161)\n",
            "Iteration: 13000, loss: 0.06384556284174323, gradient norm: tensor(0.4056)\n",
            "Iteration: 14000, loss: 0.01731695185927674, gradient norm: tensor(0.1757)\n",
            "Iteration: 15000, loss: 0.0027349351726006716, gradient norm: tensor(0.0472)\n",
            "Iteration: 16000, loss: 0.00040812626882689074, gradient norm: tensor(0.0058)\n",
            "Iteration: 17000, loss: 0.00025820277514867487, gradient norm: tensor(0.0018)\n",
            "Iteration: 18000, loss: 0.0002329684553842526, gradient norm: tensor(0.0017)\n",
            "Iteration: 19000, loss: 0.0002143779074685881, gradient norm: tensor(0.0016)\n",
            "Iteration: 20000, loss: 0.00019758995219308417, gradient norm: tensor(0.0015)\n",
            "Iteration: 21000, loss: 0.0001787008480314398, gradient norm: tensor(0.0013)\n",
            "Iteration: 22000, loss: 0.0001514458000747254, gradient norm: tensor(0.0010)\n",
            "Iteration: 23000, loss: 0.00012296583623538026, gradient norm: tensor(0.0008)\n",
            "Iteration: 24000, loss: 9.717698067834136e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 25000, loss: 7.06024931751017e-05, gradient norm: tensor(0.0005)\n",
            "Iteration: 26000, loss: 4.404381494168774e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 27000, loss: 2.1239766147118645e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 28000, loss: 6.492724520740012e-06, gradient norm: tensor(0.0001)\n",
            "Iteration: 29000, loss: 1.1832950419261578e-06, gradient norm: tensor(2.0360e-05)\n",
            "Iteration: 30000, loss: 3.751878648472484e-07, gradient norm: tensor(2.3147e-05)\n",
            "Iteration: 31000, loss: 2.4053979765881197e-07, gradient norm: tensor(3.8406e-06)\n",
            "Iteration: 32000, loss: 1.8237343468285872e-07, gradient norm: tensor(4.4997e-06)\n",
            "Iteration: 33000, loss: 1.3750063030926186e-07, gradient norm: tensor(4.1411e-06)\n",
            "Iteration: 34000, loss: 1.0300008104024983e-07, gradient norm: tensor(8.8253e-06)\n",
            "Iteration: 35000, loss: 8.011940435181941e-08, gradient norm: tensor(2.1285e-06)\n",
            "Iteration: 36000, loss: 6.680053225949222e-08, gradient norm: tensor(4.3256e-05)\n",
            "Iteration: 37000, loss: 5.991382836612046e-08, gradient norm: tensor(9.4358e-06)\n",
            "Iteration: 38000, loss: 5.6404927288866704e-08, gradient norm: tensor(1.2670e-05)\n",
            "Iteration: 39000, loss: 5.433239713426019e-08, gradient norm: tensor(2.1700e-06)\n",
            "Iteration: 40000, loss: 5.286984138663797e-08, gradient norm: tensor(1.6505e-06)\n",
            "Iteration: 0, loss: 3.120193224668503, gradient norm: tensor(3.7405)\n",
            "Iteration: 1000, loss: 2.219173204660416, gradient norm: tensor(3.1291)\n",
            "Iteration: 2000, loss: 1.5310514155626298, gradient norm: tensor(2.5483)\n",
            "Iteration: 3000, loss: 1.0252074124217034, gradient norm: tensor(2.0248)\n",
            "Iteration: 4000, loss: 0.6588730836510658, gradient norm: tensor(1.5671)\n",
            "Iteration: 5000, loss: 0.3918138117790222, gradient norm: tensor(1.1585)\n",
            "Iteration: 6000, loss: 0.20595532366633415, gradient norm: tensor(0.7878)\n",
            "Iteration: 7000, loss: 0.09084526923671364, gradient norm: tensor(0.4663)\n",
            "Iteration: 8000, loss: 0.0316354623362422, gradient norm: tensor(0.2197)\n",
            "Iteration: 9000, loss: 0.008817267478443682, gradient norm: tensor(0.0706)\n",
            "Iteration: 10000, loss: 0.002750380400219001, gradient norm: tensor(0.0164)\n",
            "Iteration: 11000, loss: 0.001203015606792178, gradient norm: tensor(0.0073)\n",
            "Iteration: 12000, loss: 0.0006196981321263592, gradient norm: tensor(0.0038)\n",
            "Iteration: 13000, loss: 0.00041326184387435207, gradient norm: tensor(0.0023)\n",
            "Iteration: 14000, loss: 0.00034666775638470427, gradient norm: tensor(0.0021)\n",
            "Iteration: 15000, loss: 0.00030108501025824806, gradient norm: tensor(0.0019)\n",
            "Iteration: 16000, loss: 0.0002478529552172404, gradient norm: tensor(0.0017)\n",
            "Iteration: 17000, loss: 0.0001846818022313528, gradient norm: tensor(0.0014)\n",
            "Iteration: 18000, loss: 0.00011422519822372124, gradient norm: tensor(0.0010)\n",
            "Iteration: 19000, loss: 5.485527081691544e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 20000, loss: 2.1574280427557823e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 1.0805461346535594e-05, gradient norm: tensor(0.0001)\n",
            "Iteration: 22000, loss: 7.989404817180912e-06, gradient norm: tensor(7.6426e-05)\n",
            "Iteration: 23000, loss: 5.913219974445383e-06, gradient norm: tensor(5.3243e-05)\n",
            "Iteration: 24000, loss: 4.266221605348619e-06, gradient norm: tensor(3.3280e-05)\n",
            "Iteration: 25000, loss: 3.0679014673751227e-06, gradient norm: tensor(1.5373e-05)\n",
            "Iteration: 26000, loss: 2.439500049831622e-06, gradient norm: tensor(2.0817e-05)\n",
            "Iteration: 27000, loss: 2.218286637798883e-06, gradient norm: tensor(4.3661e-06)\n",
            "Iteration: 28000, loss: 2.153813747554523e-06, gradient norm: tensor(2.7794e-06)\n",
            "Iteration: 29000, loss: 2.1284016734171017e-06, gradient norm: tensor(4.6594e-06)\n",
            "Iteration: 30000, loss: 2.116486242357496e-06, gradient norm: tensor(7.7565e-06)\n",
            "Iteration: 31000, loss: 2.109751752641387e-06, gradient norm: tensor(6.2913e-06)\n",
            "Iteration: 32000, loss: 2.1048256667199893e-06, gradient norm: tensor(4.7938e-05)\n",
            "Iteration: 33000, loss: 2.100700422488444e-06, gradient norm: tensor(1.2647e-05)\n",
            "Iteration: 34000, loss: 2.0969910819985673e-06, gradient norm: tensor(8.9927e-06)\n",
            "Iteration: 35000, loss: 2.09360026565264e-06, gradient norm: tensor(5.5333e-06)\n",
            "Iteration: 36000, loss: 2.090461956868239e-06, gradient norm: tensor(1.7971e-06)\n",
            "Iteration: 0, loss: 3.369193813562393, gradient norm: tensor(3.7614)\n",
            "Iteration: 1000, loss: 2.5470728800296785, gradient norm: tensor(3.2624)\n",
            "Iteration: 2000, loss: 1.8637599030733107, gradient norm: tensor(2.7798)\n",
            "Iteration: 3000, loss: 1.30620366024971, gradient norm: tensor(2.3091)\n",
            "Iteration: 4000, loss: 0.8683865341544151, gradient norm: tensor(1.8493)\n",
            "Iteration: 5000, loss: 0.5369572497606278, gradient norm: tensor(1.4170)\n",
            "Iteration: 6000, loss: 0.297933880969882, gradient norm: tensor(1.0152)\n",
            "Iteration: 7000, loss: 0.14081653985381126, gradient norm: tensor(0.6554)\n",
            "Iteration: 8000, loss: 0.051832041408866644, gradient norm: tensor(0.3570)\n",
            "Iteration: 9000, loss: 0.01263617995660752, gradient norm: tensor(0.1445)\n",
            "Iteration: 10000, loss: 0.00155112175756949, gradient norm: tensor(0.0343)\n",
            "Iteration: 11000, loss: 7.090293253259005e-05, gradient norm: tensor(0.0032)\n",
            "Iteration: 12000, loss: 8.914099634239391e-06, gradient norm: tensor(8.1254e-05)\n",
            "Iteration: 13000, loss: 7.664628933071072e-06, gradient norm: tensor(3.9853e-05)\n",
            "Iteration: 14000, loss: 7.176561210599175e-06, gradient norm: tensor(3.7690e-05)\n",
            "Iteration: 15000, loss: 6.578239205282444e-06, gradient norm: tensor(3.4912e-05)\n",
            "Iteration: 16000, loss: 5.8834231590481065e-06, gradient norm: tensor(3.2085e-05)\n",
            "Iteration: 17000, loss: 5.0824234672290914e-06, gradient norm: tensor(2.8759e-05)\n",
            "Iteration: 18000, loss: 4.16554633557098e-06, gradient norm: tensor(2.3574e-05)\n",
            "Iteration: 19000, loss: 3.520482580825046e-06, gradient norm: tensor(1.8141e-05)\n",
            "Iteration: 20000, loss: 3.3561273978648385e-06, gradient norm: tensor(2.0011e-05)\n",
            "Iteration: 21000, loss: 3.320583533422905e-06, gradient norm: tensor(1.6196e-05)\n",
            "Iteration: 22000, loss: 3.2747166576427844e-06, gradient norm: tensor(1.4852e-05)\n",
            "Iteration: 23000, loss: 3.203531867484344e-06, gradient norm: tensor(1.5185e-05)\n",
            "Iteration: 24000, loss: 3.0972937252045085e-06, gradient norm: tensor(1.8724e-05)\n",
            "Iteration: 25000, loss: 2.9478243918674706e-06, gradient norm: tensor(1.2523e-05)\n",
            "Iteration: 26000, loss: 2.75281699828156e-06, gradient norm: tensor(1.0394e-05)\n",
            "Iteration: 27000, loss: 2.515039910349515e-06, gradient norm: tensor(9.9674e-06)\n",
            "Iteration: 28000, loss: 2.2320967618725263e-06, gradient norm: tensor(1.3408e-05)\n",
            "Iteration: 29000, loss: 1.8981533042961019e-06, gradient norm: tensor(6.7600e-06)\n",
            "Iteration: 30000, loss: 1.5309269746239806e-06, gradient norm: tensor(6.1941e-06)\n",
            "Iteration: 31000, loss: 1.1833041133968436e-06, gradient norm: tensor(9.0737e-06)\n",
            "Iteration: 32000, loss: 9.185829063653728e-07, gradient norm: tensor(1.8357e-05)\n",
            "Iteration: 33000, loss: 7.350705581643524e-07, gradient norm: tensor(4.1592e-06)\n",
            "Iteration: 34000, loss: 6.052606555613238e-07, gradient norm: tensor(8.1663e-06)\n",
            "Iteration: 35000, loss: 5.116203538477748e-07, gradient norm: tensor(2.9917e-05)\n",
            "Iteration: 36000, loss: 4.4251510709614195e-07, gradient norm: tensor(8.8526e-06)\n",
            "Iteration: 37000, loss: 3.9043227491220025e-07, gradient norm: tensor(3.2509e-06)\n",
            "Iteration: 38000, loss: 3.503467220866696e-07, gradient norm: tensor(3.3530e-05)\n",
            "Iteration: 39000, loss: 3.1855525782020775e-07, gradient norm: tensor(2.2139e-05)\n",
            "Iteration: 40000, loss: 2.9290677014159884e-07, gradient norm: tensor(1.9228e-05)\n",
            "Iteration: 41000, loss: 2.717391319890794e-07, gradient norm: tensor(4.8825e-05)\n",
            "Iteration: 42000, loss: 2.538043715389904e-07, gradient norm: tensor(6.2177e-06)\n",
            "Iteration: 43000, loss: 2.384365653256282e-07, gradient norm: tensor(2.3386e-06)\n",
            "Iteration: 44000, loss: 2.250229375846402e-07, gradient norm: tensor(7.2898e-06)\n",
            "Iteration: 45000, loss: 2.1314555718277007e-07, gradient norm: tensor(2.6567e-05)\n",
            "Iteration: 46000, loss: 2.0249487407397738e-07, gradient norm: tensor(9.7346e-06)\n",
            "Iteration: 47000, loss: 1.928935785286967e-07, gradient norm: tensor(2.1699e-05)\n",
            "Iteration: 48000, loss: 1.840972445990019e-07, gradient norm: tensor(3.2714e-06)\n",
            "Iteration: 49000, loss: 1.7600809589168876e-07, gradient norm: tensor(2.9006e-05)\n",
            "Iteration: 50000, loss: 1.684972294100362e-07, gradient norm: tensor(8.0657e-06)\n",
            "Iteration: 51000, loss: 1.6150563310191046e-07, gradient norm: tensor(1.0087e-05)\n",
            "Iteration: 52000, loss: 1.549875772326459e-07, gradient norm: tensor(1.8113e-05)\n",
            "Iteration: 53000, loss: 1.488434721750309e-07, gradient norm: tensor(2.0684e-05)\n",
            "Iteration: 54000, loss: 1.4306253541462866e-07, gradient norm: tensor(4.6240e-06)\n",
            "Iteration: 55000, loss: 1.3757987679241522e-07, gradient norm: tensor(9.0106e-06)\n",
            "Iteration: 56000, loss: 1.324004174989568e-07, gradient norm: tensor(3.2341e-05)\n",
            "Iteration: 57000, loss: 1.274785882543483e-07, gradient norm: tensor(1.2601e-06)\n",
            "Iteration: 0, loss: 3.832843712568283, gradient norm: tensor(4.0542)\n",
            "Iteration: 1000, loss: 2.8748270144462587, gradient norm: tensor(3.5216)\n",
            "Iteration: 2000, loss: 2.081590621948242, gradient norm: tensor(2.9689)\n",
            "Iteration: 3000, loss: 1.4595946291685105, gradient norm: tensor(2.4450)\n",
            "Iteration: 4000, loss: 0.9841936523318291, gradient norm: tensor(1.9681)\n",
            "Iteration: 5000, loss: 0.6258910118043423, gradient norm: tensor(1.5304)\n",
            "Iteration: 6000, loss: 0.36291966411471366, gradient norm: tensor(1.1247)\n",
            "Iteration: 7000, loss: 0.18287973608076571, gradient norm: tensor(0.7539)\n",
            "Iteration: 8000, loss: 0.07485840928554535, gradient norm: tensor(0.4359)\n",
            "Iteration: 9000, loss: 0.022817299215123057, gradient norm: tensor(0.1958)\n",
            "Iteration: 10000, loss: 0.005433041352080181, gradient norm: tensor(0.0567)\n",
            "Iteration: 11000, loss: 0.0019268211886519567, gradient norm: tensor(0.0101)\n",
            "Iteration: 12000, loss: 0.0011265025337925181, gradient norm: tensor(0.0049)\n",
            "Iteration: 13000, loss: 0.0006843413012684323, gradient norm: tensor(0.0033)\n",
            "Iteration: 14000, loss: 0.0004118039808236062, gradient norm: tensor(0.0023)\n",
            "Iteration: 15000, loss: 0.0002386165904608788, gradient norm: tensor(0.0016)\n",
            "Iteration: 16000, loss: 0.0001327955303058843, gradient norm: tensor(0.0012)\n",
            "Iteration: 17000, loss: 7.335105177480727e-05, gradient norm: tensor(0.0009)\n",
            "Iteration: 18000, loss: 4.0104050754962374e-05, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 2.0284653233829886e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 20000, loss: 1.0045134572465031e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 21000, loss: 6.861271789603052e-06, gradient norm: tensor(6.6104e-05)\n",
            "Iteration: 22000, loss: 6.049025388165319e-06, gradient norm: tensor(5.4437e-05)\n",
            "Iteration: 23000, loss: 5.313352394296089e-06, gradient norm: tensor(4.9082e-05)\n",
            "Iteration: 24000, loss: 4.379884644095e-06, gradient norm: tensor(3.9865e-05)\n",
            "Iteration: 25000, loss: 3.301636422293086e-06, gradient norm: tensor(2.8912e-05)\n",
            "Iteration: 26000, loss: 2.369477202591952e-06, gradient norm: tensor(2.0773e-05)\n",
            "Iteration: 27000, loss: 1.647956214583246e-06, gradient norm: tensor(1.3757e-05)\n",
            "Iteration: 28000, loss: 1.0999691864981287e-06, gradient norm: tensor(1.6108e-05)\n",
            "Iteration: 29000, loss: 7.115845875205195e-07, gradient norm: tensor(1.1335e-05)\n",
            "Iteration: 30000, loss: 4.601323591941764e-07, gradient norm: tensor(3.7140e-06)\n",
            "Iteration: 31000, loss: 3.0965206130417756e-07, gradient norm: tensor(1.1827e-05)\n",
            "Iteration: 32000, loss: 2.235919807276332e-07, gradient norm: tensor(2.1519e-05)\n",
            "Iteration: 33000, loss: 1.7547886577062856e-07, gradient norm: tensor(4.3532e-06)\n",
            "Iteration: 34000, loss: 1.4769858159979776e-07, gradient norm: tensor(1.4510e-05)\n",
            "Iteration: 35000, loss: 1.3004127275451082e-07, gradient norm: tensor(2.4153e-05)\n",
            "Iteration: 36000, loss: 1.1768473598294804e-07, gradient norm: tensor(6.1552e-05)\n",
            "Iteration: 37000, loss: 1.0842609532346614e-07, gradient norm: tensor(2.8068e-05)\n",
            "Iteration: 38000, loss: 1.0124126030319758e-07, gradient norm: tensor(4.2194e-05)\n",
            "Iteration: 39000, loss: 9.551945266395024e-08, gradient norm: tensor(2.5426e-05)\n",
            "Iteration: 40000, loss: 9.085369720907011e-08, gradient norm: tensor(8.3433e-06)\n",
            "Iteration: 41000, loss: 8.702157591500282e-08, gradient norm: tensor(6.1707e-06)\n",
            "Iteration: 42000, loss: 8.377886916832722e-08, gradient norm: tensor(5.1403e-06)\n",
            "Iteration: 43000, loss: 8.100650538267473e-08, gradient norm: tensor(4.8064e-06)\n",
            "Iteration: 44000, loss: 7.856029762365324e-08, gradient norm: tensor(4.7252e-05)\n",
            "Iteration: 45000, loss: 7.635839025965652e-08, gradient norm: tensor(1.4040e-05)\n",
            "Iteration: 46000, loss: 7.433902011655391e-08, gradient norm: tensor(1.0090e-05)\n",
            "Iteration: 47000, loss: 7.24425391922523e-08, gradient norm: tensor(9.3658e-06)\n",
            "Iteration: 48000, loss: 7.063189607237064e-08, gradient norm: tensor(3.1138e-06)\n",
            "Iteration: 49000, loss: 6.891652562046601e-08, gradient norm: tensor(1.0641e-05)\n",
            "Iteration: 50000, loss: 6.726873380813459e-08, gradient norm: tensor(3.8248e-06)\n",
            "Iteration: 51000, loss: 6.567148224689845e-08, gradient norm: tensor(1.4738e-06)\n",
            "Iteration: 0, loss: 4.745101443290711, gradient norm: tensor(4.5781)\n",
            "Iteration: 1000, loss: 3.7378594207763673, gradient norm: tensor(4.0435)\n",
            "Iteration: 2000, loss: 2.893886786222458, gradient norm: tensor(3.5367)\n",
            "Iteration: 3000, loss: 2.182630523085594, gradient norm: tensor(3.0479)\n",
            "Iteration: 4000, loss: 1.5890597285032273, gradient norm: tensor(2.5715)\n",
            "Iteration: 5000, loss: 1.1044449226856232, gradient norm: tensor(2.1096)\n",
            "Iteration: 6000, loss: 0.7208346903920174, gradient norm: tensor(1.6668)\n",
            "Iteration: 7000, loss: 0.43079398927092555, gradient norm: tensor(1.2485)\n",
            "Iteration: 8000, loss: 0.22703429360687732, gradient norm: tensor(0.8633)\n",
            "Iteration: 9000, loss: 0.09935309487953782, gradient norm: tensor(0.5264)\n",
            "Iteration: 10000, loss: 0.03267402450740337, gradient norm: tensor(0.2597)\n",
            "Iteration: 11000, loss: 0.007042565208626911, gradient norm: tensor(0.0882)\n",
            "Iteration: 12000, loss: 0.0010680162306525745, gradient norm: tensor(0.0156)\n",
            "Iteration: 13000, loss: 0.00031726567263831387, gradient norm: tensor(0.0017)\n",
            "Iteration: 14000, loss: 0.00023631041700718924, gradient norm: tensor(0.0010)\n",
            "Iteration: 15000, loss: 0.00022526132168422918, gradient norm: tensor(0.0009)\n",
            "Iteration: 16000, loss: 0.00021549606417829636, gradient norm: tensor(0.0009)\n",
            "Iteration: 17000, loss: 0.00019703708983433897, gradient norm: tensor(0.0008)\n",
            "Iteration: 18000, loss: 0.0001736102522118017, gradient norm: tensor(0.0007)\n",
            "Iteration: 19000, loss: 0.00014882108115125446, gradient norm: tensor(0.0007)\n",
            "Iteration: 20000, loss: 0.00012170551009330665, gradient norm: tensor(0.0007)\n",
            "Iteration: 21000, loss: 9.161961006611818e-05, gradient norm: tensor(0.0006)\n",
            "Iteration: 22000, loss: 6.189279917089151e-05, gradient norm: tensor(0.0004)\n",
            "Iteration: 23000, loss: 3.8562768029805736e-05, gradient norm: tensor(0.0003)\n",
            "Iteration: 24000, loss: 2.235316561200307e-05, gradient norm: tensor(0.0002)\n",
            "Iteration: 25000, loss: 1.1248605013861379e-05, gradient norm: tensor(7.9291e-05)\n",
            "Iteration: 26000, loss: 5.583626019188159e-06, gradient norm: tensor(5.1209e-05)\n",
            "Iteration: 27000, loss: 1.6370646538916845e-06, gradient norm: tensor(1.0545e-05)\n",
            "Iteration: 28000, loss: 9.100824744336933e-07, gradient norm: tensor(8.6321e-06)\n",
            "Iteration: 29000, loss: 7.728379158038479e-07, gradient norm: tensor(5.7685e-06)\n",
            "Iteration: 30000, loss: 6.556345011858866e-07, gradient norm: tensor(7.6753e-06)\n",
            "Iteration: 31000, loss: 5.204611384499458e-07, gradient norm: tensor(2.9073e-05)\n",
            "Iteration: 32000, loss: 3.961391547875337e-07, gradient norm: tensor(5.5320e-06)\n",
            "Iteration: 33000, loss: 3.191081759723602e-07, gradient norm: tensor(1.1095e-05)\n",
            "Iteration: 34000, loss: 2.836638993244378e-07, gradient norm: tensor(3.0536e-05)\n",
            "Iteration: 35000, loss: 2.634967117671749e-07, gradient norm: tensor(1.2040e-05)\n",
            "Iteration: 36000, loss: 2.4799582260470744e-07, gradient norm: tensor(2.7069e-05)\n",
            "Iteration: 37000, loss: 2.3458070023707478e-07, gradient norm: tensor(1.5179e-05)\n",
            "Iteration: 38000, loss: 2.225664697874663e-07, gradient norm: tensor(8.1962e-06)\n",
            "Iteration: 39000, loss: 2.1177152004270283e-07, gradient norm: tensor(2.8622e-06)\n",
            "Iteration: 40000, loss: 2.0196963977525685e-07, gradient norm: tensor(8.8580e-06)\n",
            "Iteration: 41000, loss: 1.9299827219754206e-07, gradient norm: tensor(2.6978e-05)\n",
            "Iteration: 42000, loss: 1.8478213812045395e-07, gradient norm: tensor(1.2991e-05)\n",
            "Iteration: 43000, loss: 1.7719322475784338e-07, gradient norm: tensor(1.5955e-05)\n",
            "Iteration: 44000, loss: 1.7013268376331326e-07, gradient norm: tensor(2.9393e-05)\n",
            "Iteration: 45000, loss: 1.6355515268173805e-07, gradient norm: tensor(1.1654e-05)\n",
            "Iteration: 46000, loss: 1.5743926684308463e-07, gradient norm: tensor(1.0276e-05)\n",
            "Iteration: 47000, loss: 1.517156664618824e-07, gradient norm: tensor(5.4181e-05)\n",
            "Iteration: 48000, loss: 1.463659086482494e-07, gradient norm: tensor(3.7345e-05)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-50bf5a355a09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_experiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mstudent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStudentNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_student\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mlast_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3a7d7ed3ca4e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x, y_labels, N, Ninner, Nstart, maxtime, nlopt_threshold, collect_history)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mloss_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mloss_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    234\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "file_experiment_header = ['loss', 'gradient norm', 'smallest eigenvalue', 'student size']\n",
        "\n",
        "for i in range(0, H_student):\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_traj_x')\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_traj_y')\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_a')\n",
        "\n",
        "file_experiment_header.append('teacher_neurons_x')\n",
        "file_experiment_header.append('teacher_neurons_y')\n",
        "\n",
        "\n",
        "file_experiment_data = open('experiments_data.csv', 'w')\n",
        "writer = csv.writer(file_experiment_data)\n",
        "writer.writerow(file_experiment_header)\n",
        "\n",
        "for num_experiment in range(0, num_experiments):\n",
        "  student_model = StudentNetwork(D_in, H_student, D_out)\n",
        "  loss_vals, trace = train(student_model, data, y_labels)\n",
        "  last_loss_val = loss_vals[-1]\n",
        "\n",
        "  loss_grad = torch.autograd.grad(nn.MSELoss()(student_model(data), y_labels), student_model.parameters(), create_graph=True)\n",
        "  grad_norm, hessian = eval_hessian(loss_grad, student_model)\n",
        "  smallest_eigenvalue = np.min(np.linalg.eigvals(hessian))\n",
        "\n",
        "  row = [last_loss_val, grad_norm, smallest_eigenvalue, H_student]\n",
        "\n",
        "  for i in range(0, H_student):\n",
        "    neuron_w_x = []\n",
        "    neuron_w_y = []\n",
        "    neuron_a = []\n",
        "    for (inp_weights, out_weights) in trace:\n",
        "      neuron_w_x.append(inp_weights[i][0])\n",
        "      neuron_w_y.append(inp_weights[i][1])\n",
        "      neuron_a.append(out_weights[0][i])\n",
        "    row.append(neuron_w_x)\n",
        "    row.append(neuron_w_y)\n",
        "    row.append(neuron_a[-1])\n",
        "\n",
        "  teacher_neurons_x = [0.6, -0.5, -0.2, 0.1]\n",
        "  teacher_neurons_y = [0.5, 0.5, -0.6, -0.6]\n",
        "  row.append(teacher_neurons_x)\n",
        "  row.append(teacher_neurons_y)\n",
        "\n",
        "  writer.writerow(row)\n",
        "\n",
        "file_experiment_data.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F7kBJPoLS_h"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYMYC8b5VqT1"
      },
      "outputs": [],
      "source": [
        "# teacher_neurons_x = [0.6, -0.5, -0.2, 0.1]\n",
        "# teacher_neurons_y = [0.5, 0.5, -0.6, -0.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mflgj_AbLToB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# neuron_0_w_x = []\n",
        "# neuron_0_w_y = []\n",
        "# neuron_0_a = []\n",
        "\n",
        "# neuron_1_w_x = []\n",
        "# neuron_1_w_y = []\n",
        "# neuron_1_a = []\n",
        "\n",
        "# neuron_2_w_x = []\n",
        "# neuron_2_w_y = []\n",
        "# neuron_2_a = []\n",
        "\n",
        "# neuron_3_w_x = []\n",
        "# neuron_3_w_y = []\n",
        "# neuron_3_a = []\n",
        "\n",
        "# neuron_4_w_x = []\n",
        "# neuron_4_w_y = []\n",
        "# neuron_4_a = []\n",
        "\n",
        "# for (inp_weights, out_weights) in trace:\n",
        "#   neuron_0_w_x.append(inp_weights[0][0])\n",
        "#   neuron_0_w_y.append(inp_weights[0][1])\n",
        "#   neuron_0_a.append(out_weights[0][0])\n",
        "\n",
        "#   neuron_1_w_x.append(inp_weights[1][0])\n",
        "#   neuron_1_w_y.append(inp_weights[1][1])\n",
        "#   neuron_1_a.append(out_weights[0][1])\n",
        "\n",
        "#   neuron_2_w_x.append(inp_weights[2][0])\n",
        "#   neuron_2_w_y.append(inp_weights[2][1])\n",
        "#   neuron_2_a.append(out_weights[0][2])\n",
        "\n",
        "#   neuron_3_w_x.append(inp_weights[3][0])\n",
        "#   neuron_3_w_y.append(inp_weights[3][1])\n",
        "#   neuron_3_a.append(out_weights[0][3])\n",
        "\n",
        "#   neuron_4_w_x.append(inp_weights[4][0])\n",
        "#   neuron_4_w_y.append(inp_weights[4][1])\n",
        "#   neuron_4_a.append(out_weights[0][4])\n",
        "\n",
        "# plt.plot(neuron_0_w_x, neuron_0_w_y)\n",
        "# plt.plot(neuron_1_w_x, neuron_1_w_y)\n",
        "# plt.plot(neuron_2_w_x, neuron_2_w_y)\n",
        "# plt.plot(neuron_3_w_x, neuron_3_w_y)\n",
        "# plt.plot(neuron_4_w_x, neuron_4_w_y)\n",
        "\n",
        "# plt.scatter(teacher_neurons_x, teacher_neurons_y, marker=\"*\")\n",
        "\n",
        "# outgoing_weights = [neuron_0_a[-1], neuron_1_a[-1], neuron_2_a[-1], neuron_3_a[-1], neuron_4_a[-1]]\n",
        "# plt.scatter([neuron_0_w_x[-1], neuron_1_w_x[-1], neuron_2_w_x[-1], neuron_3_w_x[-1], neuron_4_w_x[-1]],\n",
        "#             [neuron_0_w_y[-1], neuron_1_w_y[-1], neuron_2_w_y[-1], neuron_3_w_y[-1], neuron_4_w_y[-1]],\n",
        "#             c = outgoing_weights,\n",
        "#             cmap=matplotlib.cm.jet)\n",
        "# plt.colorbar()\n",
        "\n",
        "# # Teacher's neurons\n",
        "# #[0.6, -0.5, -0.2, 0.1],\n",
        "# #[0.5, 0.5, -0.6, -0.6],"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFGntTav-tw"
      },
      "source": [
        "# Checking for local minima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyQnFDNxL1gj"
      },
      "outputs": [],
      "source": [
        "# class DummyNetwork(nn.Module):\n",
        "#   def __init__(self, D_in, H, D_out, w_in, w_out):\n",
        "#     \"\"\"\n",
        "#     In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "#     member variables.\n",
        "\n",
        "#     D_in: input dimension\n",
        "#     H: dimension of hidden layer\n",
        "#     D_out: output dimension of the first layer\n",
        "#     \"\"\"\n",
        "#     super(DummyNetwork, self).__init__()\n",
        "#     self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "#     self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "#     self.linear1.weight = torch.nn.Parameter(w_in)\n",
        "#     self.linear2.weight = torch.nn.Parameter(w_out)\n",
        "#   def forward(self, x):\n",
        "#     \"\"\"\n",
        "#     In the forward function we accept a Variable of input data and we must\n",
        "#     return a Variable of output data. We can use Modules defined in the\n",
        "#     constructor as well as arbitrary operators on Variables.\n",
        "#     \"\"\"\n",
        "#     h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "#     y_pred = self.linear2(h_sigmoid)\n",
        "#     return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCKnpOwFOJtL"
      },
      "outputs": [],
      "source": [
        "# print (trace[-1])\n",
        "# def loss_reducer(w_in, w_out):\n",
        "#   dummy_model = DummyNetwork(D_in, H_teacher, D_out, w_in, w_out)\n",
        "#   return nn.MSELoss()(dummy_model(data), y_labels)\n",
        "  \n",
        "# print(loss_reducer(torch.Tensor(trace[-1][0]), torch.Tensor(trace[-1][1])))\n",
        "# H = torch.autograd.functional.hessian(loss_reducer, (torch.Tensor(trace[-1][0]), torch.Tensor(trace[-1][1])))\n",
        "# eval Hessian matrix\n",
        "# def eval_hessian(loss_grad, model):\n",
        "#     cnt = 0\n",
        "#     for g in loss_grad:\n",
        "#         g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "#         cnt = 1\n",
        "#     print(torch.norm(g_vector))\n",
        "#     l = g_vector.size(0)\n",
        "#     hessian = torch.zeros(l, l)\n",
        "#     for idx in range(l):\n",
        "#         grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "#         cnt = 0\n",
        "#         for g in grad2rd: \n",
        "#             g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "#             cnt = 1\n",
        "#         hessian[idx] = g2\n",
        "#     return hessian.cpu().data.numpy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeuUCGjU594D3VbunZCz4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}