{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theostoican/MastersThesis/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4cKo7_tWleLC",
        "outputId": "a5fd9f8b-3eb3-471f-e03b-9979fe817549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlopt in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from nlopt) (1.19.5)\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "!pip install nlopt\n",
        "import nlopt\n",
        "from numpy import *\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Various modelling parameters"
      ],
      "metadata": {
        "id": "w8SjHkIMoI_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#N is batch size; D_in is input dimension;\n",
        "#H is the dimension of the hidden layer; D_out is output dimension.\n",
        "N, D_in, H_teacher, H_student, D_out = 1, 2, 4, 5, 1\n",
        "num_experiments = 1"
      ],
      "metadata": {
        "id": "F8ktizbMoKWO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ptEq_k5KeK6"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7WSK5OS-_Jb",
        "outputId": "6a49a89f-8dfc-47a3-e7df-e15cac59ba46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1681\n"
          ]
        }
      ],
      "source": [
        "def construct_dataset():\n",
        "  data = []\n",
        "  for y in np.arange(-5, 5.1, .25):\n",
        "    for x in np.arange(-5, 5.1, .25):\n",
        "      data.append([x, y])\n",
        "  return data\n",
        "\n",
        "data = torch.DoubleTensor(construct_dataset()) \n",
        "print(len(construct_dataset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZEwEx3Kg9_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QLlxqBcnzo2e"
      },
      "outputs": [],
      "source": [
        "class TeacherNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(TeacherNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "    self.linear1.weight = torch.nn.Parameter(torch.transpose(torch.DoubleTensor([[0.6, -0.5, -0.2, 0.1], [0.5, 0.5, -0.6, -0.6]]), 0, 1))\n",
        "    self.linear2.weight = torch.nn.Parameter(torch.transpose(torch.DoubleTensor([[1], [-1], [1], [-1]]), 0, 1))\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "COkLdjEOmBJV"
      },
      "outputs": [],
      "source": [
        "class StudentNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(StudentNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False).double()\n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False).double()\n",
        "    nn.init.xavier_uniform_(self.linear1.weight)\n",
        "    nn.init.xavier_uniform_(self.linear2.weight)\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out, w_in, w_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(DummyNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False).double()\n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False).double()\n",
        "    self.linear1.weight = torch.nn.Parameter(w_in)\n",
        "    self.linear2.weight = torch.nn.Parameter(w_out)\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "Ln-SxNRAIvbu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation of the labels based on the teacher model."
      ],
      "metadata": {
        "id": "Xh4A8WS3HzeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ILSXsIRu3U18"
      },
      "outputs": [],
      "source": [
        "teacher_model = TeacherNetwork(D_in, H_teacher, D_out)\n",
        "y_labels = teacher_model(data).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZrYxLLwKk6Q"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second order optimization helper method."
      ],
      "metadata": {
        "id": "E4ptvgubJBbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_obj(weights, grad):\n",
        "  w_in = weights[0 : H_student * 2]\n",
        "  w_out = weights[H_student * 2 :]\n",
        "\n",
        "  w_in_torch_format = []\n",
        "  for i in range(H_student):\n",
        "    w_in_torch_format.append([w_in[2 * i], w_in[2 * i + 1]])\n",
        "  w_in_torch_format = torch.DoubleTensor(w_in_torch_format)\n",
        "\n",
        "  w_out_torch_format = torch.DoubleTensor([w_out])\n",
        "\n",
        "  dummy_model = DummyNetwork(D_in, H_student, D_out, w_in_torch_format, w_out_torch_format)\n",
        "\n",
        "  loss_val = nn.MSELoss()(dummy_model(data), y_labels)\n",
        "  \n",
        "  if grad.size > 0:\n",
        "    loss_grad = torch.autograd.grad(loss_val, dummy_model.parameters(), create_graph=True)\n",
        "    gradients = loss_grad[0].reshape(H_student * 2).detach().numpy()\n",
        "    gradients = np.append(gradients, loss_grad[1][0].detach().numpy())\n",
        "    grad[:] = gradients\n",
        "\n",
        "  return loss_val.item()"
      ],
      "metadata": {
        "id": "Mv7x3L3pJDYJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kS2Ri_ya9Npv"
      },
      "outputs": [],
      "source": [
        "def train(model, x, y_labels, N = 10, Ninner = 10 ** 5, Nstart = 10,\n",
        "          maxtime = 3600, nlopt_threshold = 1e-7,\n",
        "          collect_history = True):\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  for param_group in optimizer.param_groups:\n",
        "        lr = param_group['lr']\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_vals = []\n",
        "  trace = []\n",
        "  if collect_history:\n",
        "    trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                  deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "  for i in range(1, N + 1):\n",
        "    loss_tmp = []\n",
        "    for j in range(1, Ninner + 1):\n",
        "      y = model(x)\n",
        "      loss = loss_fn(y, y_labels)\n",
        "      loss_grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
        "      loss_tmp.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(retain_graph=True)\n",
        "      optimizer.step()\n",
        "      if i == 1 and (j % Nstart == 0) and j < Ninner:\n",
        "        loss_vals.append(np.mean(loss_tmp[j - Nstart : j]))\n",
        "        if collect_history:\n",
        "          trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                        deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    loss_vals.append(np.mean(loss_tmp))\n",
        "    if collect_history:\n",
        "      trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                    deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    cnt = 0\n",
        "    for g in loss_grad:\n",
        "        g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "        cnt = 1\n",
        "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i, np.mean(loss_tmp), torch.norm(g_vector)))\n",
        "    \n",
        "    # Adjust the learning rate.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr / (1 + i)\n",
        "    \n",
        "    # stopping criterion\n",
        "    if np.mean(loss_tmp) < nlopt_threshold or i == N:\n",
        "        w_in = None\n",
        "        for w in trace[-1][0]:\n",
        "          if w_in is None:\n",
        "            w_in = w\n",
        "          else:\n",
        "            w_in = np.append(w_in, w) \n",
        "        w_out = trace[-1][1][0]\n",
        "        weights = np.append(w_in, w_out)\n",
        "\n",
        "        opt = nlopt.opt(nlopt.LD_SLSQP, len(weights))\n",
        "        opt.set_lower_bounds([w - 10 for w in weights])\n",
        "        opt.set_upper_bounds([w + 10 for w in weights])\n",
        "        opt.set_min_objective(loss_obj)\n",
        "        opt.set_maxtime(maxtime)\n",
        "        final_weights = opt.optimize(weights)\n",
        "        return loss_vals, trace, final_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hessian evaluation"
      ],
      "metadata": {
        "id": "HlZpml_4MOQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_hessian(loss_grad, model):\n",
        "  cnt = 0\n",
        "  for g in loss_grad:\n",
        "      g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "      cnt = 1\n",
        "  grad_norm = torch.norm(g_vector)\n",
        "  l = g_vector.size(0)\n",
        "  hessian = torch.zeros((l, l), dtype = torch.float64)\n",
        "  for idx in range(l):\n",
        "      grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "      cnt = 0\n",
        "      for g in grad2rd: \n",
        "          g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "          cnt = 1\n",
        "      hessian[idx] = g2\n",
        "  # Symmetrize the Hessian.\n",
        "  hessian = (hessian + hessian.T) / 2\n",
        "  return grad_norm.detach().numpy(), hessian.detach().numpy()"
      ],
      "metadata": {
        "id": "mXEHxyklMQgi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-4dScUFCP-2",
        "outputId": "98736c59-d458-4f46-c207-4719a377b226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100, loss: 0.5757672203405921, gradient norm: tensor(1.3830, dtype=torch.float64)\n",
            "Iteration: 200, loss: 0.33436703097172, gradient norm: tensor(1.1365, dtype=torch.float64)\n",
            "Iteration: 300, loss: 0.23932154961015215, gradient norm: tensor(0.9846, dtype=torch.float64)\n",
            "Iteration: 400, loss: 0.18522761692665035, gradient norm: tensor(0.8759, dtype=torch.float64)\n",
            "Iteration: 500, loss: 0.1496795581572889, gradient norm: tensor(0.7920, dtype=torch.float64)\n",
            "Iteration: 600, loss: 0.12437412398564046, gradient norm: tensor(0.7242, dtype=torch.float64)\n",
            "Iteration: 700, loss: 0.10540044240765488, gradient norm: tensor(0.6675, dtype=torch.float64)\n",
            "Iteration: 800, loss: 0.09064195419591424, gradient norm: tensor(0.6191, dtype=torch.float64)\n",
            "Iteration: 900, loss: 0.07884246339295255, gradient norm: tensor(0.5769, dtype=torch.float64)\n",
            "Iteration: 1000, loss: 0.06920522402823324, gradient norm: tensor(0.5397, dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "file_experiment_header = ['loss', 'gradient norm', 'smallest eigenvalue', 'student size']\n",
        "\n",
        "for i in range(0, H_student):\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_traj_x')\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_traj_y')\n",
        "  file_experiment_header.append('neuron_' + str(i) + '_a')\n",
        "\n",
        "file_experiment_header.append('teacher_neurons_x')\n",
        "file_experiment_header.append('teacher_neurons_y')\n",
        "\n",
        "\n",
        "file_experiment_data = open('experiments_data.csv', 'w')\n",
        "writer = csv.writer(file_experiment_data)\n",
        "writer.writerow(file_experiment_header)\n",
        "\n",
        "for num_experiment in range(0, num_experiments):\n",
        "  student_model = StudentNetwork(D_in, H_student, D_out)\n",
        "  loss_vals, trace, final_weights = train(student_model, data, y_labels)\n",
        "\n",
        "  w_in = torch.DoubleTensor(final_weights[0 : 2 * H_student].reshape(H_student, 2))\n",
        "  w_out = torch.DoubleTensor([final_weights[2 * H_student :]])\n",
        "  dummy_model = DummyNetwork(D_in, H_student, D_out, w_in, w_out)\n",
        "  last_loss_val = nn.MSELoss()(dummy_model(data), y_labels).item()\n",
        "  loss_grad = torch.autograd.grad(nn.MSELoss()(dummy_model(data), y_labels), dummy_model.parameters(), create_graph=True)\n",
        "  grad_norm, hessian = eval_hessian(loss_grad, dummy_model)\n",
        "  smallest_eigenvalue = np.min(np.linalg.eigvals(hessian))\n",
        "\n",
        "  row = [last_loss_val, grad_norm, smallest_eigenvalue, H_student]\n",
        "\n",
        "  for i in range(0, H_student):\n",
        "    neuron_w_x = []\n",
        "    neuron_w_y = []\n",
        "    neuron_a = []\n",
        "    for (inp_weights, out_weights) in trace:\n",
        "      neuron_w_x.append(inp_weights[i][0])\n",
        "      neuron_w_y.append(inp_weights[i][1])\n",
        "      neuron_a.append(out_weights[0][i])\n",
        "    neuron_w_x.append(w_in[i][0])\n",
        "    neuron_w_y.append(w_in[i][1])\n",
        "    neuron_a.append(w_out[0][i])\n",
        "    row.append(neuron_w_x)\n",
        "    row.append(neuron_w_y)\n",
        "    row.append(neuron_a[-1])\n",
        "\n",
        "  teacher_neurons_x = [0.6, -0.5, -0.2, 0.1]\n",
        "  teacher_neurons_y = [0.5, 0.5, -0.6, -0.6]\n",
        "  row.append(teacher_neurons_x)\n",
        "  row.append(teacher_neurons_y)\n",
        "\n",
        "  writer.writerow(row)\n",
        "\n",
        "file_experiment_data.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F7kBJPoLS_h"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JYMYC8b5VqT1"
      },
      "outputs": [],
      "source": [
        "# teacher_neurons_x = [0.6, -0.5, -0.2, 0.1]\n",
        "# teacher_neurons_y = [0.5, 0.5, -0.6, -0.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mflgj_AbLToB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# neuron_0_w_x = []\n",
        "# neuron_0_w_y = []\n",
        "# neuron_0_a = []\n",
        "\n",
        "# neuron_1_w_x = []\n",
        "# neuron_1_w_y = []\n",
        "# neuron_1_a = []\n",
        "\n",
        "# neuron_2_w_x = []\n",
        "# neuron_2_w_y = []\n",
        "# neuron_2_a = []\n",
        "\n",
        "# neuron_3_w_x = []\n",
        "# neuron_3_w_y = []\n",
        "# neuron_3_a = []\n",
        "\n",
        "# neuron_4_w_x = []\n",
        "# neuron_4_w_y = []\n",
        "# neuron_4_a = []\n",
        "\n",
        "# for (inp_weights, out_weights) in trace:\n",
        "#   neuron_0_w_x.append(inp_weights[0][0])\n",
        "#   neuron_0_w_y.append(inp_weights[0][1])\n",
        "#   neuron_0_a.append(out_weights[0][0])\n",
        "\n",
        "#   neuron_1_w_x.append(inp_weights[1][0])\n",
        "#   neuron_1_w_y.append(inp_weights[1][1])\n",
        "#   neuron_1_a.append(out_weights[0][1])\n",
        "\n",
        "#   neuron_2_w_x.append(inp_weights[2][0])\n",
        "#   neuron_2_w_y.append(inp_weights[2][1])\n",
        "#   neuron_2_a.append(out_weights[0][2])\n",
        "\n",
        "#   neuron_3_w_x.append(inp_weights[3][0])\n",
        "#   neuron_3_w_y.append(inp_weights[3][1])\n",
        "#   neuron_3_a.append(out_weights[0][3])\n",
        "\n",
        "#   neuron_4_w_x.append(inp_weights[4][0])\n",
        "#   neuron_4_w_y.append(inp_weights[4][1])\n",
        "#   neuron_4_a.append(out_weights[0][4])\n",
        "\n",
        "# plt.plot(neuron_0_w_x, neuron_0_w_y)\n",
        "# plt.plot(neuron_1_w_x, neuron_1_w_y)\n",
        "# plt.plot(neuron_2_w_x, neuron_2_w_y)\n",
        "# plt.plot(neuron_3_w_x, neuron_3_w_y)\n",
        "# plt.plot(neuron_4_w_x, neuron_4_w_y)\n",
        "\n",
        "# plt.scatter(teacher_neurons_x, teacher_neurons_y, marker=\"*\")\n",
        "\n",
        "# outgoing_weights = [neuron_0_a[-1], neuron_1_a[-1], neuron_2_a[-1], neuron_3_a[-1], neuron_4_a[-1]]\n",
        "# plt.scatter([neuron_0_w_x[-1], neuron_1_w_x[-1], neuron_2_w_x[-1], neuron_3_w_x[-1], neuron_4_w_x[-1]],\n",
        "#             [neuron_0_w_y[-1], neuron_1_w_y[-1], neuron_2_w_y[-1], neuron_3_w_y[-1], neuron_4_w_y[-1]],\n",
        "#             c = outgoing_weights,\n",
        "#             cmap=matplotlib.cm.jet)\n",
        "# plt.colorbar()\n",
        "\n",
        "# # Teacher's neurons\n",
        "# #[0.6, -0.5, -0.2, 0.1],\n",
        "# #[0.5, 0.5, -0.6, -0.6],"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFGntTav-tw"
      },
      "source": [
        "# Checking for local minima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VyQnFDNxL1gj"
      },
      "outputs": [],
      "source": [
        "# class DummyNetwork(nn.Module):\n",
        "#   def __init__(self, D_in, H, D_out, w_in, w_out):\n",
        "#     \"\"\"\n",
        "#     In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "#     member variables.\n",
        "\n",
        "#     D_in: input dimension\n",
        "#     H: dimension of hidden layer\n",
        "#     D_out: output dimension of the first layer\n",
        "#     \"\"\"\n",
        "#     super(DummyNetwork, self).__init__()\n",
        "#     self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "#     self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "#     self.linear1.weight = torch.nn.Parameter(w_in)\n",
        "#     self.linear2.weight = torch.nn.Parameter(w_out)\n",
        "#   def forward(self, x):\n",
        "#     \"\"\"\n",
        "#     In the forward function we accept a Variable of input data and we must\n",
        "#     return a Variable of output data. We can use Modules defined in the\n",
        "#     constructor as well as arbitrary operators on Variables.\n",
        "#     \"\"\"\n",
        "#     h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "#     y_pred = self.linear2(h_sigmoid)\n",
        "#     return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XCKnpOwFOJtL"
      },
      "outputs": [],
      "source": [
        "# print (trace[-1])\n",
        "# def loss_reducer(w_in, w_out):\n",
        "#   dummy_model = DummyNetwork(D_in, H_teacher, D_out, w_in, w_out)\n",
        "#   return nn.MSELoss()(dummy_model(data), y_labels)\n",
        "  \n",
        "# print(loss_reducer(torch.Tensor(trace[-1][0]), torch.Tensor(trace[-1][1])))\n",
        "# H = torch.autograd.functional.hessian(loss_reducer, (torch.Tensor(trace[-1][0]), torch.Tensor(trace[-1][1])))\n",
        "# eval Hessian matrix\n",
        "# def eval_hessian(loss_grad, model):\n",
        "#     cnt = 0\n",
        "#     for g in loss_grad:\n",
        "#         g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "#         cnt = 1\n",
        "#     print(torch.norm(g_vector))\n",
        "#     l = g_vector.size(0)\n",
        "#     hessian = torch.zeros(l, l)\n",
        "#     for idx in range(l):\n",
        "#         grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "#         cnt = 0\n",
        "#         for g in grad2rd: \n",
        "#             g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "#             cnt = 1\n",
        "#         hessian[idx] = g2\n",
        "#     return hessian.cpu().data.numpy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODfrJbmQliBGQCnk/zgRlQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}