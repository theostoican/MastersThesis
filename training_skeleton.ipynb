{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theostoican/MastersThesis/blob/main/training_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cKo7_tWleLC",
        "outputId": "69d24935-f730-4d95-dcaa-145addcc6b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nlopt\n",
            "  Downloading nlopt-2.7.0-cp37-cp37m-manylinux2014_x86_64.whl (420 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 92 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 266 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 276 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 286 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 296 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 307 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 317 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 327 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 337 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 348 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 358 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 368 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 378 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 389 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 399 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 409 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 420 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from nlopt) (1.19.5)\n",
            "Installing collected packages: nlopt\n",
            "Successfully installed nlopt-2.7.0\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "import csv\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "!pip install nlopt\n",
        "import nlopt\n",
        "from numpy import *\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8SjHkIMoI_B"
      },
      "source": [
        "# Various modelling parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F8ktizbMoKWO"
      },
      "outputs": [],
      "source": [
        "# N is batch size; D_in is input dimension;\n",
        "# H is the dimension of the hidden layer; D_out is output dimension.\n",
        "N, D_in, H_teacher, H_student, D_out = 1, 2, 4, 5, 1\n",
        "num_experiments = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ptEq_k5KeK6"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7WSK5OS-_Jb",
        "outputId": "3d8248c0-bfd8-43f3-f478-98e22236a0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1681\n"
          ]
        }
      ],
      "source": [
        "def construct_dataset():\n",
        "  data = []\n",
        "  for y in np.arange(-5, 5.1, .25):\n",
        "    for x in np.arange(-5, 5.1, .25):\n",
        "      data.append([x, y])\n",
        "  return data\n",
        "\n",
        "data = torch.DoubleTensor(construct_dataset()) \n",
        "print(len(construct_dataset()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ZEwEx3Kg9_"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkTpfNcm_Pdb"
      },
      "source": [
        "## Teacher's model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLlxqBcnzo2e"
      },
      "outputs": [],
      "source": [
        "class TeacherNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(TeacherNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False) \n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False)\n",
        "    self.linear1.weight = torch.nn.Parameter(torch.transpose(torch.DoubleTensor([[0.6, -0.5, -0.2, 0.1], [0.5, 0.5, -0.6, -0.6]]), 0, 1))\n",
        "    self.linear2.weight = torch.nn.Parameter(torch.transpose(torch.DoubleTensor([[1], [-1], [1], [-1]]), 0, 1))\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0F8cNUY_Pdd"
      },
      "source": [
        "## Student's model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "COkLdjEOmBJV"
      },
      "outputs": [],
      "source": [
        "class StudentNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(StudentNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False).double()\n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False).double()\n",
        "    nn.init.xavier_uniform_(self.linear1.weight)\n",
        "    nn.init.xavier_uniform_(self.linear2.weight)\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy network used for evaluationg the loss from given weights"
      ],
      "metadata": {
        "id": "XsSCoU1Zh2KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyNetwork(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out, w_in, w_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "\n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension of the first layer\n",
        "    \"\"\"\n",
        "    super(DummyNetwork, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H, bias=False).double()\n",
        "    self.linear2 = nn.Linear(H, D_out, bias=False).double()\n",
        "    self.linear1.weight = torch.nn.Parameter(w_in)\n",
        "    self.linear2.weight = torch.nn.Parameter(w_out)\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must\n",
        "    return a Variable of output data. We can use Modules defined in the\n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_sigmoid = torch.sigmoid(self.linear1(x))\n",
        "    y_pred = self.linear2(h_sigmoid)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "QTu0KSlGh5p9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh4A8WS3HzeU"
      },
      "source": [
        "# Generation of the labels based on the teacher model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ILSXsIRu3U18"
      },
      "outputs": [],
      "source": [
        "teacher_model = TeacherNetwork(D_in, H_teacher, D_out)\n",
        "y_labels = teacher_model(data).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvlf6LkG_Pdf"
      },
      "source": [
        "# Perturbation training helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cJvcWAXy_Pdg"
      },
      "outputs": [],
      "source": [
        "# def train_by_perturbation(eps, num_iter):\n",
        "#     for i in range(0, num_iter):\n",
        "#         mu, sigma = 0, 0.1\n",
        "#         perturbation = eps * np.random.normal(mu, sigma, H_student * D_in).astype(np.float64).reshape((H_student, D_in))\n",
        "        \n",
        "#         # Perturb the weights of the NN.\n",
        "#         params = OrderedDict([(\"w_in\", torch.Tensor(trace[-1][0] + perturbation)),\n",
        "#                               (\"w_out\", torch.Tensor(trace[-1][1]))])\n",
        "        \n",
        "#         # Set the bounds for second-order optimization.\n",
        "#         lb_dict = OrderedDict([(\"w_in\", torch.Tensor(trace[-1][0] - 0.3 + perturbation)), \n",
        "#                                (\"w_out\", torch.Tensor(trace[-1][1] - 1))])\n",
        "#         ub_dict = OrderedDict([(\"w_in\", torch.Tensor(trace[-1][0] + 0.3 + perturbation)),\n",
        "#                                (\"w_out\", torch.Tensor(trace[-1][1] + 1))])\n",
        "        \n",
        "#         # Perform second-order optimization.\n",
        "#         params = minimize(d_loss, params, method=\"SLSQP\", lb_dict = lb_dict, ub_dict = ub_dict,\n",
        "#                   options={\"disp\": True , \"maxiter\": 10 ** 4}, tol=1e-40)\n",
        "        \n",
        "#         # Check the new smallest eigenvalue\n",
        "#         dummy_model = DummyNetwork(D_in, H_student, D_out, params[\"w_in\"], params[\"w_out\"])\n",
        "#         print(nn.MSELoss()(dummy_model(data), y_labels))\n",
        "#         loss_grad = torch.autograd.grad(nn.MSELoss()(dummy_model(data), y_labels), dummy_model.parameters(), create_graph=True)\n",
        "#         grad_norm, hessian = eval_hessian(loss_grad, dummy_model)\n",
        "#         # print(hessian)\n",
        "#         smallest_eigenvalue = np.min(np.linalg.eigvals(hessian))\n",
        "#         print('new smallest eigenvelue:', smallest_eigenvalue)\n",
        "#         if smallest_eigenvalue >= 0:\n",
        "#             print ('number of iterations: ', i + 1)\n",
        "#             break\n",
        "#     print(\"old params: \", trace[-1][0])\n",
        "#     print(\"new params: \", params[\"w_in\"])\n",
        "#     return params, nn.MSELoss()(dummy_model(data), y_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqXdBvb_Pdh"
      },
      "source": [
        "# Plotting helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "anLCR0DR_Pdh"
      },
      "outputs": [],
      "source": [
        "def plot_trajectories():\n",
        "    teacher_neurons_x = [0.6, -0.5, -0.2, 0.1]\n",
        "    teacher_neurons_y = [0.5, 0.5, -0.6, -0.6]\n",
        "\n",
        "    neuron_0_w_x = []\n",
        "    neuron_0_w_y = []\n",
        "    neuron_0_a = []\n",
        "\n",
        "    neuron_1_w_x = []\n",
        "    neuron_1_w_y = []\n",
        "    neuron_1_a = []\n",
        "\n",
        "    neuron_2_w_x = []\n",
        "    neuron_2_w_y = []\n",
        "    neuron_2_a = []\n",
        "\n",
        "    neuron_3_w_x = []\n",
        "    neuron_3_w_y = []\n",
        "    neuron_3_a = []\n",
        "\n",
        "    neuron_4_w_x = []\n",
        "    neuron_4_w_y = []\n",
        "    neuron_4_a = []\n",
        "\n",
        "    for (inp_weights, out_weights) in trace:\n",
        "      neuron_0_w_x.append(inp_weights[0][0])\n",
        "      neuron_0_w_y.append(inp_weights[0][1])\n",
        "      neuron_0_a.append(out_weights[0][0])\n",
        "\n",
        "      neuron_1_w_x.append(inp_weights[1][0])\n",
        "      neuron_1_w_y.append(inp_weights[1][1])\n",
        "      neuron_1_a.append(out_weights[0][1])\n",
        "\n",
        "      neuron_2_w_x.append(inp_weights[2][0])\n",
        "      neuron_2_w_y.append(inp_weights[2][1])\n",
        "      neuron_2_a.append(out_weights[0][2])\n",
        "\n",
        "      neuron_3_w_x.append(inp_weights[3][0])\n",
        "      neuron_3_w_y.append(inp_weights[3][1])\n",
        "      neuron_3_a.append(out_weights[0][3])\n",
        "\n",
        "      neuron_4_w_x.append(inp_weights[4][0])\n",
        "      neuron_4_w_y.append(inp_weights[4][1])\n",
        "      neuron_4_a.append(out_weights[0][4])\n",
        "\n",
        "    plt.plot(neuron_0_w_x, neuron_0_w_y)\n",
        "    plt.plot(neuron_1_w_x, neuron_1_w_y)\n",
        "    plt.plot(neuron_2_w_x, neuron_2_w_y)\n",
        "    plt.plot(neuron_3_w_x, neuron_3_w_y)\n",
        "    plt.plot(neuron_4_w_x, neuron_4_w_y)\n",
        "\n",
        "    plt.scatter(teacher_neurons_x, teacher_neurons_y, marker=\"*\")\n",
        "\n",
        "    outgoing_weights = [neuron_0_a[-1], neuron_1_a[-1], neuron_2_a[-1], neuron_3_a[-1], neuron_4_a[-1]]\n",
        "    plt.scatter([neuron_0_w_x[-1], neuron_1_w_x[-1], neuron_2_w_x[-1], neuron_3_w_x[-1], neuron_4_w_x[-1]],\n",
        "                [neuron_0_w_y[-1], neuron_1_w_y[-1], neuron_2_w_y[-1], neuron_3_w_y[-1], neuron_4_w_y[-1]],\n",
        "                c = outgoing_weights,\n",
        "                cmap=matplotlib.cm.jet)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('w1')\n",
        "    plt.ylabel('w2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlZpml_4MOQD"
      },
      "source": [
        "# Hessian evaluation helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mXEHxyklMQgi"
      },
      "outputs": [],
      "source": [
        "def eval_hessian(loss_grad, model):\n",
        "  cnt = 0\n",
        "  for g in loss_grad:\n",
        "      g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "      cnt = 1\n",
        "  grad_norm = torch.norm(g_vector)\n",
        "  l = g_vector.size(0)\n",
        "  hessian = torch.zeros((l, l), dtype = torch.float64)\n",
        "  for idx in range(l):\n",
        "      grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
        "      cnt = 0\n",
        "      for g in grad2rd: \n",
        "          g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
        "          cnt = 1\n",
        "      hessian[idx] = g2\n",
        "  # Symmetrize the Hessian.\n",
        "  hessian = (hessian + hessian.T) / 2\n",
        "  return grad_norm.detach().numpy(), hessian.detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmW9yGtT_Pdi"
      },
      "source": [
        "# Second order optimization helper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_-2cmpi_Pdj"
      },
      "source": [
        "## Helper used for computing the loss and its derivative from weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ATdrt6v-_Pdj"
      },
      "outputs": [],
      "source": [
        "def loss_obj(weights, grad):\n",
        "  w_in = weights[0 : H_student * 2]\n",
        "  w_out = weights[H_student * 2 :]\n",
        "\n",
        "  w_in_torch_format = []\n",
        "  for i in range(H_student):\n",
        "    w_in_torch_format.append([w_in[2 * i], w_in[2 * i + 1]])\n",
        "  w_in_torch_format = torch.DoubleTensor(w_in_torch_format)\n",
        "\n",
        "  w_out_torch_format = torch.DoubleTensor([w_out])\n",
        "\n",
        "  dummy_model = DummyNetwork(D_in, H_student, D_out, w_in_torch_format, w_out_torch_format)\n",
        "\n",
        "  loss_val = nn.MSELoss()(dummy_model(data), y_labels)\n",
        "  \n",
        "  if grad.size > 0:\n",
        "    loss_grad = torch.autograd.grad(loss_val, dummy_model.parameters(), create_graph=True)\n",
        "    gradients = loss_grad[0].reshape(H_student * 2).detach().numpy()\n",
        "    gradients = np.append(gradients, loss_grad[1][0].detach().numpy())\n",
        "    grad[:] = gradients\n",
        "\n",
        "  return loss_val.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTT1cD4m_Pdk"
      },
      "source": [
        "# Helper for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gUWSPsPi_Pdk"
      },
      "outputs": [],
      "source": [
        "def train(model, x, y_labels, N = 10, Ninner = 10 ** 5, Nstart = 10,\n",
        "          maxtime = 10 ** 3, nlopt_threshold = 1e-7,\n",
        "          collect_history = True):\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "  for param_group in optimizer.param_groups:\n",
        "        lr = param_group['lr']\n",
        "\n",
        "  loss_fn = nn.MSELoss()\n",
        "  loss_vals = []\n",
        "  trace = []\n",
        "  if collect_history:\n",
        "    trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                  deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "  for i in range(1, N + 1):\n",
        "    loss_tmp = []\n",
        "    for j in range(1, Ninner + 1):\n",
        "      y = model(x)\n",
        "      loss = loss_fn(y, y_labels)\n",
        "      loss_grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
        "      loss_tmp.append(loss.item())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(retain_graph=True)\n",
        "      optimizer.step()\n",
        "      if i == 1 and (j % Nstart == 0) and j < Ninner:\n",
        "        loss_vals.append(np.mean(loss_tmp[j - Nstart : j]))\n",
        "        if collect_history:\n",
        "          trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                        deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    loss_vals.append(np.mean(loss_tmp))\n",
        "    if collect_history:\n",
        "      trace.append((deepcopy(model.linear1.weight.data.detach().numpy()),\n",
        "                    deepcopy(model.linear2.weight.data.detach().numpy())))\n",
        "    cnt = 0\n",
        "    for g in loss_grad:\n",
        "        g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
        "        cnt = 1\n",
        "    print(\"Iteration: %d, loss: %s, gradient norm: %s\" % (Ninner * i, np.mean(loss_tmp), torch.norm(g_vector)))\n",
        "    \n",
        "    # Adjust the learning rate.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr / (1 + i)\n",
        "    \n",
        "    # stopping criterion\n",
        "    if np.mean(loss_tmp) < nlopt_threshold or i == N:\n",
        "        w_in = None\n",
        "        for w in trace[-1][0]:\n",
        "          if w_in is None:\n",
        "            w_in = w\n",
        "          else:\n",
        "            w_in = np.append(w_in, w) \n",
        "        w_out = trace[-1][1][0]\n",
        "        weights = np.append(w_in, w_out)\n",
        "\n",
        "        opt = nlopt.opt(nlopt.LD_SLSQP, len(weights))\n",
        "        opt.set_lower_bounds([w - 10 for w in weights])\n",
        "        opt.set_upper_bounds([w + 10 for w in weights])\n",
        "        opt.set_min_objective(loss_obj)\n",
        "        opt.set_maxtime(maxtime)\n",
        "        final_weights = opt.optimize(weights)\n",
        "        return loss_vals, trace, final_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8HjfUxcupO"
      },
      "source": [
        "# Actual training and smallest eigenvalue computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-4dScUFCP-2",
        "outputId": "e5de41d2-eee1-4993-ab04-c9d7a13b7087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100000, loss: 0.004528424295875313, gradient norm: tensor(3.7299e-06, dtype=torch.float64)\n",
            "Iteration: 200000, loss: 8.368234307548941e-05, gradient norm: tensor(1.1594e-06, dtype=torch.float64)\n",
            "Iteration: 300000, loss: 8.36811803894474e-05, gradient norm: tensor(9.5640e-05, dtype=torch.float64)\n",
            "Iteration: 400000, loss: 8.368077263460807e-05, gradient norm: tensor(3.0340e-05, dtype=torch.float64)\n",
            "Iteration: 500000, loss: 8.368058854769845e-05, gradient norm: tensor(3.5781e-06, dtype=torch.float64)\n",
            "Iteration: 600000, loss: 8.368048687629633e-05, gradient norm: tensor(1.6670e-05, dtype=torch.float64)\n",
            "Iteration: 700000, loss: 8.368042562799143e-05, gradient norm: tensor(2.1875e-05, dtype=torch.float64)\n",
            "Iteration: 800000, loss: 8.368038605958554e-05, gradient norm: tensor(3.1631e-05, dtype=torch.float64)\n",
            "Iteration: 900000, loss: 8.36803584954059e-05, gradient norm: tensor(5.4482e-07, dtype=torch.float64)\n",
            "Iteration: 1000000, loss: 8.368033956131107e-05, gradient norm: tensor(1.2341e-06, dtype=torch.float64)\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368028648843703e-05\n",
            "1.310957166746325e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368036859601142e-05\n",
            "2.589203618370286e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025741380509e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025741380509e-05\n",
            "1.2291455777362373e-06\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025646647504e-05\n",
            "4.206566617146624e-07\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025632103265e-05\n",
            "7.657720602110828e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025631540783e-05\n",
            "6.675084632522578e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025629744972e-05\n",
            "9.900712508692065e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025627706763e-05\n",
            "1.2354136741252584e-07\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025626045467e-05\n",
            "8.69060237742219e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625534157e-05\n",
            "3.4689461061600245e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562541257e-05\n",
            "2.3181141061061794e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562532041e-05\n",
            "2.716102592879513e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562518023e-05\n",
            "3.092511543424384e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625078793e-05\n",
            "2.1721119865749214e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625041655e-05\n",
            "9.356244999675181e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625031457e-05\n",
            "7.513539979187556e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625019814e-05\n",
            "1.0328314739554693e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624990339e-05\n",
            "1.7709717557380416e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624927892e-05\n",
            "2.7524440147870092e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562481918e-05\n",
            "3.399686134728787e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624705988e-05\n",
            "2.6936425943371262e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624652866e-05\n",
            "1.0458155451852205e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624644047e-05\n",
            "2.0270845819061204e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624643356e-05\n",
            "1.5620592521000737e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624642986e-05\n",
            "1.6627768364381614e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624641605e-05\n",
            "2.163683378580477e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624638674e-05\n",
            "3.0534616279207636e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624631884e-05\n",
            "4.333220629446117e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624620542e-05\n",
            "5.0493113914624665e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624608635e-05\n",
            "3.92648220081256e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624603056e-05\n",
            "1.8231779990353152e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624602016e-05\n",
            "1.1571795737812331e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624601804e-05\n",
            "1.129691327542707e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624601455e-05\n",
            "1.152583557619871e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624600607e-05\n",
            "1.271892604785364e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562459878e-05\n",
            "1.502154003319322e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624595722e-05\n",
            "1.615033325283811e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624592701e-05\n",
            "1.1957603214099558e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591392e-05\n",
            "5.099265974375597e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591191e-05\n",
            "2.867430743830288e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591174e-05\n",
            "2.7580975358082437e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591163e-05\n",
            "2.7364513501379993e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591111e-05\n",
            "2.7608105383889706e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624591012e-05\n",
            "2.9440436406735246e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624590764e-05\n",
            "3.425230837287492e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624590306e-05\n",
            "3.9473261927076075e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589718e-05\n",
            "3.675630602562848e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562458937e-05\n",
            "2.5688915266430405e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562458929e-05\n",
            "2.0542859478160282e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589279e-05\n",
            "1.9703500484147335e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589273e-05\n",
            "1.9367865009661075e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589249e-05\n",
            "1.8894057434873322e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589203e-05\n",
            "1.889155179442836e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624589073e-05\n",
            "2.0528402831783625e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624588753e-05\n",
            "2.532377587890854e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562458812e-05\n",
            "3.2155719730814357e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624587178e-05\n",
            "3.330275572205912e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586429e-05\n",
            "2.2094990443848166e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586186e-05\n",
            "1.1203787882808957e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586165e-05\n",
            "9.445670512995426e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586159e-05\n",
            "9.301184668881514e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586163e-05\n",
            "9.214647989812006e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586165e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586169e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586163e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586159e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586162e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586163e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586162e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586163e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586159e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586158e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586158e-05\n",
            "9.30118465733439e-11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3.5188440400889257e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.814084971559174e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535282e-05\n",
            "1.992485060375484e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.814085112603428e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "2.702152780646858e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.814085112603428e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534834e-05\n",
            "1.5820040995551267e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.814085495056136e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624554331e-05\n",
            "8.162441118931776e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534265e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.814084130547426e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "4.478439340931745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.814084347178697e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534146e-05\n",
            "4.0844158209152576e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.814084416183365e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534149e-05\n",
            "4.4819883863181485e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.814084416183365e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453593e-05\n",
            "2.475587511098118e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.814084497714046e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624583001e-05\n",
            "1.270941062330776e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534533e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "7.493045116805593e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.012809258744383e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "7.493045251616274e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.5598954455820298e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "7.493045251616274e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534444e-05\n",
            "1.0343619361760434e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "7.493045251616274e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624543162e-05\n",
            "5.440084902390937e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534169e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "7.493045032288359e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.522417540006105e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "7.493045082403969e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534174e-05\n",
            "4.5955409120590805e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "7.49304521666458e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536301e-05\n",
            "2.682573657239143e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.986394354524702e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.9986886392453975e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394182478433e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.4125132697514676e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "6.986394197947052e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534145e-05\n",
            "3.630548142953045e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "6.986394197947052e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535393e-05\n",
            "2.0564664399103935e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "6.986394197947052e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624567512e-05\n",
            "1.0529195417213227e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534413e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394018787641e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.271086151294293e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394018787641e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534338e-05\n",
            "8.634680718675613e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394145160451e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.124552132440471e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394145160451e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534182e-05\n",
            "5.094183573516095e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394145160451e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536448e-05\n",
            "2.790144599255069e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.986394145160451e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624594219e-05\n",
            "1.4197782493697012e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534672e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.986394109214302e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025626048104e-05\n",
            "7.134301585533074e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562454909e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534231e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "6.98639317332362e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.2805473176225384e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "6.688254915062364e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.9873806425382975e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.672880948256421e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.983804247470955e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.67288100809798e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.2566480325704236e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672881081276686e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534329e-05\n",
            "8.444952068992334e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672881081276686e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540052e-05\n",
            "4.453441696971644e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534146e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672879924759062e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.5912662153695587e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672879924759062e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.5715786657489854e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.67288054976051e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534117e-05\n",
            "2.415699488403265e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.67288054976051e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534722e-05\n",
            "1.431419166409363e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.67288054976051e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624550345e-05\n",
            "7.389298478019327e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534248e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672880244948655e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624943592e-05\n",
            "3.718201355071662e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538122e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534138e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.672878954907001e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.3054438073574057e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672873815650098e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.2054069318004684e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672873815650098e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "1.0338313145667245e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672873815650098e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534261e-05\n",
            "7.259722826286387e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672873887134682e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538561e-05\n",
            "3.86755918850753e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453414e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.672873960661893e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534154e-05\n",
            "4.025946183137138e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "6.672876519470426e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "1.488576942151794e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.672861842829365e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.9432039768613744e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.672861858253836e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "1.2521581471181697e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.672861858253836e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534321e-05\n",
            "8.399258352637383e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.672861858253836e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624539934e-05\n",
            "4.430782977334479e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453415e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.672861929781743e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624683248e-05\n",
            "2.2490354125284466e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535551e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672861985360177e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "4.2852891298530195e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "6.672861985360177e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "9.724370509739693e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767736257591e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "3.9497318350054475e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767736257591e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "1.1018177596120423e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767736257591e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453427e-05\n",
            "7.489315122217387e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767741521215e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "4.850325806199425e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767741521215e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "5.035118421209135e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767741521215e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534154e-05\n",
            "4.167611121826879e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767741521215e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535677e-05\n",
            "2.29901966850016e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534112e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "6.38767741521215e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624574485e-05\n",
            "1.171516183666499e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534485e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "6.38767723887094e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625548383e-05\n",
            "5.880258914163856e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562454414e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534184e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "6.387669778987182e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.062884592318172e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "3.9052793619505533e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.7691571390477455e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2654871856306436e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "5.911912270169034e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534138e-05\n",
            "3.47342095783236e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940799234065183e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535057e-05\n",
            "1.825266835549146e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.940798917923818e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562455835e-05\n",
            "9.262581621218892e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534345e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801547266414e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.784268980771841e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801547266414e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.293955014275545e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801547266414e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "5.1275869899356505e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801107504841e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.665359451969387e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801107504841e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.0618373807158206e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.940801107504841e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "9.631851552020477e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.423793109885568e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.924847999454827e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "7.914441294302293e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534127e-05\n",
            "3.192789278635223e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534788e-05\n",
            "1.543299573931904e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.33198081163124e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624551524e-05\n",
            "7.669254249775396e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534265e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.33198097183521e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624989505e-05\n",
            "3.912313268533976e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538625e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453414e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331981520367286e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.3072337026711786e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331981520367286e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.095851126819653e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331981520367286e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534165e-05\n",
            "4.847097270597762e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331981520367286e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535885e-05\n",
            "2.377766320484636e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.3319818945378434e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "7.34980835096117e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.3319818945378434e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453413e-05\n",
            "2.9931684369878034e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.3319821031075535e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534848e-05\n",
            "1.4565552057792358e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.3319821031075535e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624553167e-05\n",
            "7.248614998150238e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534283e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331982277537506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "1.6586968080172784e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.331982277537506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534309e-05\n",
            "7.831153606547779e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.094215578811713e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "7.338830890811272e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534124e-05\n",
            "2.7546582951858467e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534784e-05\n",
            "1.3141736556404079e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534116e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624551428e-05\n",
            "6.515392007522862e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534266e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916395068816635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624968668e-05\n",
            "3.252330373375961e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534138e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.59164983587574e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.346429768915176e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.12734477458763e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "8.389551717815867e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534145e-05\n",
            "3.4936477909387525e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535435e-05\n",
            "1.704613641644047e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624567931e-05\n",
            "8.487542651104778e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534422e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.591652197795748e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625382279e-05\n",
            "4.2403966051741924e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624542525e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453418e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.122319894641625e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "8.811794983803618e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534162e-05\n",
            "3.799568867563203e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453576e-05\n",
            "1.8666186515625064e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624576113e-05\n",
            "9.306248165935978e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534508e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.591704704626084e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562558736e-05\n",
            "4.650616663636869e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624544566e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534199e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916974355766506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.9516178598882467e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916974355766506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "7.345219161151538e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916974355766506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534138e-05\n",
            "3.008468337915402e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916974355766506e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535126e-05\n",
            "1.4712937121125144e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5916900337415875e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.863816970005074e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916938907909234e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "6.949482769088753e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916938907909234e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534131e-05\n",
            "2.8998689236142415e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916938907909234e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535086e-05\n",
            "1.4293088621356966e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5916938907909234e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624559329e-05\n",
            "7.134434198005731e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534338e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.591685038213022e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.701937677998344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.5924675718635175e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.096777901645612e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.6139326946860176e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "4.063259980785493e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.6139326946860176e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "8.713152003835281e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.6139326946860176e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534165e-05\n",
            "3.8309543844010105e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.6139326946860176e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453585e-05\n",
            "1.891892985509127e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614140663932866e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "4.031958591318759e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614140663932866e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "8.847339521542492e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614140663932866e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534169e-05\n",
            "3.962489342313152e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614140663932866e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535984e-05\n",
            "1.964014537856089e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534112e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614140663932866e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562458177e-05\n",
            "9.809163233078014e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534557e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.614143987868662e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625729193e-05\n",
            "4.903953852362121e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624545977e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534206e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.614140853611175e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.535745579025326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "3.971178709471478e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.535745579025326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.020445631371415e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.535745579025326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534123e-05\n",
            "2.2105347864957613e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.910864949873704e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "3.976181495985223e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534132e-05\n",
            "2.3153080858833955e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453492e-05\n",
            "1.2680425090045352e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534112e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624555053e-05\n",
            "6.4602160446070924e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534307e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.534160036254457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625059664e-05\n",
            "3.2422577865355996e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624539319e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453415e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.534167412291587e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025637694649e-05\n",
            "1.6231882035108348e-07\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624665544e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535394e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.534158007909332e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.8506772257845854e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.458106184260182e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.820803691652811e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.697060834212047e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.664368319105168e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534139e-05\n",
            "2.9191114338920256e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535359e-05\n",
            "1.573278254637661e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534117e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624565978e-05\n",
            "7.987189602396369e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534412e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.458094437852495e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562533285e-05\n",
            "4.005770036277213e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624542043e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534172e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.458100052390784e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.9966212639529924e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.458100052390784e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.1011362258304326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.4578607511865405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.709816865974976e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.613006782220663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.347657126628796e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534142e-05\n",
            "2.9036193353426264e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535245e-05\n",
            "1.5760073197615851e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624563068e-05\n",
            "8.01035511022666e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534382e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "4.4578571112322654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625260157e-05\n",
            "4.018289097206583e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624541314e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534165e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.457854141614069e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.319043727195288e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.6559947924636663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.319043727195288e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "5.7526947245959876e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.319043727195288e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "1.9482344636917533e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.319043727195288e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453448e-05\n",
            "9.114660890810558e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.411685850195628e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.6700249771965694e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.691785625671773e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "5.310530188802917e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534112e-05\n",
            "1.616419274982791e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453435e-05\n",
            "7.370279943435053e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562454053e-05\n",
            "3.6252176965828856e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534163e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139563549090676e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624695308e-05\n",
            "1.8068738579004965e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535698e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.4139612304877985e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.523019440110803e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.138298520877332e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "8.620677680801868e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534147e-05\n",
            "3.4751787708496783e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535465e-05\n",
            "1.6751122232464328e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624568446e-05\n",
            "8.31841981724747e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534437e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.4139647208529855e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625394008e-05\n",
            "4.1536482844517687e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562454267e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534184e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.413957707458351e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.829671223966457e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110401181988344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.754855559014837e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110401181988344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "5.133679320240471e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110401181988344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.4549799232072958e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110401181988344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534298e-05\n",
            "6.51153237838012e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110401181988344e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624539141e-05\n",
            "3.1925171235913407e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534147e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.7809882970995086e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "5.3629528226884274e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534112e-05\n",
            "1.6102530217122329e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534356e-05\n",
            "7.325998680065741e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540494e-05\n",
            "3.602540519440544e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534161e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.511045774838745e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624694495e-05\n",
            "1.7955003685747136e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535692e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534116e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110421504248635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "4.5585044630809916e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110421504248635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "1.1156244160103393e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.5110421504248635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534206e-05\n",
            "4.816414016156745e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5110463399310265e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.714912476519409e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5110463399310265e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.889527884949782e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5110463399310265e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "1.3251059381579519e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5110463399310265e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453426e-05\n",
            "5.884486139084439e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.5110463399310265e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538183e-05\n",
            "2.8829892354650904e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534136e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.33373853343035e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.867050221858602e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.33373853343035e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "7.011529425398216e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.33373853343035e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534223e-05\n",
            "4.83391259012821e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.33373853343035e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624537485e-05\n",
            "2.5823619022259608e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534123e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.33373853343035e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562461994e-05\n",
            "1.3081336710177595e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534927e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "5.333732026968169e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.394368951158661e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.333724735554839e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.4304965067608816e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.333724735554839e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "1.3386377634263933e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.333724735554839e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534441e-05\n",
            "8.146092730951724e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.3337191556818e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.592916279646597e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337179290499e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "4.3622366593285713e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337179290499e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.7529767292465596e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337179290499e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534145e-05\n",
            "2.783814538204192e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.333723082625546e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624539438e-05\n",
            "3.233844066037886e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453415e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.333723082625546e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624669115e-05\n",
            "1.6340396472879678e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535417e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.333719687035969e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534245e-05\n",
            "5.255021079682841e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.333719757940445e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538092e-05\n",
            "2.797139036478533e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534134e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.333719757940445e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624635321e-05\n",
            "1.415575177278794e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535078e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337160755277e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.724704793534698e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337160755277e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "7.952213839579244e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337160755277e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453426e-05\n",
            "5.37960085337573e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337160755277e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538312e-05\n",
            "2.8573861290449543e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534134e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.3337160755277e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624640712e-05\n",
            "1.4458086655630931e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535135e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "5.3337123154612626e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.4499798086572117e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.3337132450826067e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "3.301955397570877e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.3337132450826067e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.2793765171322588e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "5.3337132450826067e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534433e-05\n",
            "7.923761935771495e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.333705515096446e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.862723092169994e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.333705515096446e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "6.83255167718349e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.333705515096446e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534223e-05\n",
            "4.760759672632722e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "5.333705515096446e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453746e-05\n",
            "2.5473516989136385e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534127e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "5.3337014084831665e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.0357183430408952e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "5.3337014084831665e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534334e-05\n",
            "6.624043263692919e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "5.3337014084831665e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540311e-05\n",
            "3.4778703424668817e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534157e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "5.3337014084831665e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624691161e-05\n",
            "1.7558218827208066e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535637e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.676355793642048e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "3.206154674573803e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "1.748966286546958e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453467e-05\n",
            "1.0306675009384432e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624548746e-05\n",
            "5.32143860527191e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534233e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "5.3336979111821934e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624903006e-05\n",
            "2.677709983149177e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453773e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534132e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "5.333692485240439e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025633797332e-05\n",
            "1.3425270079196853e-07\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624626437e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534993e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "5.3336873505164045e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "3.7395998681941156e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "5.148844050639571e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.71252916487654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.704944243593069e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.71252916487654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "3.6750223919278445e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.71252916487654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.536167429546313e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710629725736966e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.4307702034323414e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710629684398801e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.3292047818020674e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710629684398801e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.330127666779862e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710629684398801e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534149e-05\n",
            "2.2335103601982947e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "3.710629080507334e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562462556e-05\n",
            "9.684914449924973e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534992e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "3.710628511407484e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534126e-05\n",
            "1.7482831691480485e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "3.710628511407484e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534959e-05\n",
            "9.352851151402994e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "3.710628511407484e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624556064e-05\n",
            "4.749136260281729e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534306e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "3.710628486543409e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625090197e-05\n",
            "2.3913604882901584e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624539605e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534147e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710631626752411e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.442421166884085e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710631626752411e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "1.4276225350082765e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.710631501831683e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.2244923998392474e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.710510814979192e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.2582181794425673e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.710510814979192e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "5.321260418479027e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.710510814979192e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453417e-05\n",
            "2.905509703209067e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.710510814979192e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536248e-05\n",
            "1.5201380006800391e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.710510814979192e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624588417e-05\n",
            "7.674908741464127e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534624e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510530000112e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.904487606602472e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510530000112e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534165e-05\n",
            "2.634787661208974e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510530000112e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535904e-05\n",
            "1.3835201719262283e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510231165079e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.1035850900224938e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510231165079e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534441e-05\n",
            "6.053151193299817e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510231165079e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624542993e-05\n",
            "3.098707044920399e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534177e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710510231165079e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624757777e-05\n",
            "1.556889493264811e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245363e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534117e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.71050987582636e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.1530124323720808e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105097350478663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "4.0351165987644186e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105097350478663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534134e-05\n",
            "2.0499430489193542e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105097350478663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535175e-05\n",
            "1.0905617113312642e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105097350478663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624561344e-05\n",
            "5.528025561062952e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534352e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105097172457996e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625220678e-05\n",
            "2.7772897242802435e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540903e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534161e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.710509876412865e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.173371737499512e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.710509876412865e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "9.015997306512123e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710509720449481e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534342e-05\n",
            "5.15530815329804e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710509720449481e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540463e-05\n",
            "2.650052562221155e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534154e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.710509720449481e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624694421e-05\n",
            "1.3326430706470957e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535669e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105093415437646e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "5.22792201917847e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105093415437646e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453417e-05\n",
            "2.865042638060685e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105093415437646e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536164e-05\n",
            "1.5010239165080804e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534119e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.7105093415437646e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624586352e-05\n",
            "7.58048257801243e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534608e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.710509014089334e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025625861234e-05\n",
            "3.8232812782581584e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624547286e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453422e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.71051077348581e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "3.188642724099118e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.618203840588558e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.2525417795981816e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.618203404097156e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.589218558546788e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.618203404097156e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534147e-05\n",
            "2.34718226835833e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182029914080524e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.1955327422956604e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182029914080524e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "8.0913597549609e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182029914080524e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534291e-05\n",
            "4.4095820421245805e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182029914080524e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453911e-05\n",
            "2.267294177887758e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534147e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182029914080524e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624660439e-05\n",
            "1.1403448394617905e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535343e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.618202678982732e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534685e-05\n",
            "7.712321249568003e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.6182027241129836e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624549273e-05\n",
            "3.929590076605746e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534243e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.6181920672404736e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2284146966090345e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.2686920005695104e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2284146966090345e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.633946813332461e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2284146966090345e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "7.823259035512117e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2284146966090345e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534203e-05\n",
            "3.443800718564053e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.228415121564294e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624537542e-05\n",
            "1.9100392996245253e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534134e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417020923158e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.2858699257065325e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417020923158e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.8319367163081546e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417526706758e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "9.600971217193696e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417526706758e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534262e-05\n",
            "4.389712375354132e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417878198932e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.581969067100668e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417878198932e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "7.229985843144552e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417878198932e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534182e-05\n",
            "3.106361769978874e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228417878198932e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536126e-05\n",
            "1.530193995702349e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534124e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228418127213753e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624590557e-05\n",
            "8.033500749782436e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453466e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841845110013e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.825603606518003e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841845110013e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "9.455564484857483e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841845110013e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453425e-05\n",
            "4.3051152434911894e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841845110013e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624537935e-05\n",
            "2.1313535029005865e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534139e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841845110013e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624630405e-05\n",
            "1.0640144095422423e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535054e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841884987806e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.8972867364069666e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841884987806e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "1.7261157840328444e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841884987806e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534669e-05\n",
            "8.331315733556879e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.22841884987806e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624548535e-05\n",
            "4.14716333568136e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453424e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534107e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228418898743755e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "6.292290069368858e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228418898743755e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534145e-05\n",
            "2.5757468125856294e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228418898743755e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535435e-05\n",
            "1.2631261211976995e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.228418898743755e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624567739e-05\n",
            "6.298540798054029e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534428e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284189934769544e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.993437441959849e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284189934769544e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534124e-05\n",
            "1.784046032239625e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284189934769544e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453469e-05\n",
            "8.621042397895516e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284191033448644e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.490585883239434e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284191033448644e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "6.21433687924849e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284191033448644e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534142e-05\n",
            "2.522096227951371e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284191033448644e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535303e-05\n",
            "1.2351612648753468e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "3.2284191033448644e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624564601e-05\n",
            "6.157723461930278e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534399e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.2284201090720535e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.3767585653952635e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "3.34264024786379e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.4267696936767595e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "1.3944622130889158e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534425e-05\n",
            "6.623638336209778e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624542527e-05\n",
            "3.2911156394569275e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453418e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "3.2326750642987405e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624745348e-05\n",
            "1.6437855121095302e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536195e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.232678781781721e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.3597299039635823e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.232683704573639e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.363812519732663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.377518724669967e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.363812519732663e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.4383618256117125e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810429067122e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.6556118689652616e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810429067122e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "5.894805041560824e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810429067122e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534119e-05\n",
            "2.140771536663879e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810429067122e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534588e-05\n",
            "1.0242745765427212e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.4816211632998774e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "4.292299192259903e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "1.1125035348952071e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453421e-05\n",
            "4.996268213403424e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624537003e-05\n",
            "2.459748312147039e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534121e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363810450730354e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624607357e-05\n",
            "1.2264461500965051e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534811e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "3.363814796499348e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025643884338e-05\n",
            "1.990164374094429e-07\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624727333e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624536001e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534117e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "3.363809837795549e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.1162924104498675e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0634096089992435e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "3.996659887572954e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0634096089992435e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "4.04490862380729e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0634096089992435e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "8.902784121066202e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0634096089992435e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534336e-05\n",
            "4.379173386634422e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0634096089992435e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624540335e-05\n",
            "2.2183116504178367e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534154e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.063409119397868e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624699142e-05\n",
            "1.1406705684119536e-08\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535729e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534115e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "4.0634108762714834e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.487317163563531e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0633647292259375e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "3.7920480780459276e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0633647292259375e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.491704606506108e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0633647292259375e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453414e-05\n",
            "1.9820489562433043e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0633647292259375e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535236e-05\n",
            "1.041569571011202e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534109e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.0633641734894864e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624538995e-05\n",
            "2.1927775664302815e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534145e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "4.063366111045996e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.7938140574859654e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0327937738885256e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "3.6755234826639125e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0327937738885256e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "4.5443562805686526e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534104e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "4.0327937738885256e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534142e-05\n",
            "2.218427121291608e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "3.575339890848565e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "3.6686045268098694e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534086e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.5753402285181326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "4.3956064207786896e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.5753402285181326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534108e-05\n",
            "1.48059314489991e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.5753402285181326e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534532e-05\n",
            "7.340624760773531e-10\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534092e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.575340379832694e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624545621e-05\n",
            "3.733027268521737e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245342e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534093e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534094e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534103e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534089e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.5753404256923985e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624535352e-05\n",
            "1.2475527077978582e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.36802562453411e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534096e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534101e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534088e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534085e-05\n",
            "3.5753404256923985e-11\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624566123e-05\n",
            "6.2507403308742905e-09\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534399e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.3680256245341e-05\n",
            "tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>) 8.368025624534097e-05\n"
          ]
        }
      ],
      "source": [
        "student_model = StudentNetwork(D_in, H_student, D_out)\n",
        "loss_vals, trace, final_weights = train(student_model, data, y_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPiYAjodaXek",
        "outputId": "9fc2b357-dfc2-4e5d-efc1-14729b8ae5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at the end:  tensor(8.3680e-05, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
            "gradient norm at the end:  3.605989900204259e-11\n",
            "smallest eigenvalue at the end:  1.695944732106038e-06\n"
          ]
        }
      ],
      "source": [
        "w_in = torch.DoubleTensor(final_weights[0 : 2 * H_student].reshape(H_student, 2))\n",
        "w_out = torch.DoubleTensor([final_weights[2 * H_student :]])\n",
        "dummy_model = DummyNetwork(D_in, H_student, D_out, w_in, w_out)\n",
        "loss_grad = torch.autograd.grad(nn.MSELoss()(dummy_model(data), y_labels), dummy_model.parameters(), create_graph=True)\n",
        "grad_norm, hessian = eval_hessian(loss_grad, dummy_model)\n",
        "smallest_eigenvalue = np.min(np.linalg.eigvals(hessian))\n",
        "\n",
        "print('loss at the end: ', nn.MSELoss()(dummy_model(data), y_labels))\n",
        "print('gradient norm at the end: ', grad_norm)\n",
        "print('smallest eigenvalue at the end: ', smallest_eigenvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F7kBJPoLS_h"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QC5qnBy_Pdl"
      },
      "source": [
        "Teacher's neurons:\n",
        "\n",
        "w_x: [0.6, -0.5, -0.2, 0.1]\n",
        "\n",
        "w_y: [0.5, 0.5, -0.6, -0.6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mflgj_AbLToB",
        "outputId": "a5bb9db4-0174-4451-9fa3-333814234392"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.scatter([w_in[0][0], w_in[1][0], w_in[2][0], w_in[3][0], w_in[4][0]],\n",
        "            [w_in[0][1], w_in[1][1], w_in[2][1], w_in[3][1], w_in[4][1]], marker=\"P\")\n",
        "plot_trajectories()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "training_skeleton.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}